# Comparing `tmp/modal-0.62.87-py3-none-any.whl.zip` & `tmp/modal-0.62.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,205 +1,199 @@
-Zip file size: 453278 bytes, number of entries: 203
--rw-r--r--  2.0 unx     2100 b- defN 24-Apr-18 02:16 modal/__init__.py
--rw-r--r--  2.0 unx     1141 b- defN 24-Apr-18 02:16 modal/__main__.py
--rw-r--r--  2.0 unx    15476 b- defN 24-Apr-18 02:16 modal/_asgi.py
--rw-r--r--  2.0 unx    28448 b- defN 24-Apr-18 02:16 modal/_container_entrypoint.py
--rw-r--r--  2.0 unx     4349 b- defN 24-Apr-18 02:16 modal/_container_exec.py
--rw-r--r--  2.0 unx    28709 b- defN 24-Apr-18 02:16 modal/_container_io_manager.py
--rw-r--r--  2.0 unx    12668 b- defN 24-Apr-18 02:16 modal/_container_io_manager.pyi
--rw-r--r--  2.0 unx      450 b- defN 24-Apr-18 02:16 modal/_ipython.py
--rw-r--r--  2.0 unx      929 b- defN 24-Apr-18 02:16 modal/_location.py
--rw-r--r--  2.0 unx    20565 b- defN 24-Apr-18 02:16 modal/_output.py
--rw-r--r--  2.0 unx     1906 b- defN 24-Apr-18 02:16 modal/_proxy_tunnel.py
--rw-r--r--  2.0 unx     1336 b- defN 24-Apr-18 02:16 modal/_pty.py
--rw-r--r--  2.0 unx     7077 b- defN 24-Apr-18 02:16 modal/_resolver.py
--rw-r--r--  2.0 unx     1142 b- defN 24-Apr-18 02:16 modal/_resources.py
--rw-r--r--  2.0 unx     1658 b- defN 24-Apr-18 02:16 modal/_sandbox_shell.py
--rw-r--r--  2.0 unx    12133 b- defN 24-Apr-18 02:16 modal/_serialization.py
--rw-r--r--  2.0 unx    10029 b- defN 24-Apr-18 02:16 modal/_traceback.py
--rw-r--r--  2.0 unx     5073 b- defN 24-Apr-18 02:16 modal/_tunnel.py
--rw-r--r--  2.0 unx     1337 b- defN 24-Apr-18 02:16 modal/_tunnel.pyi
--rw-r--r--  2.0 unx     3609 b- defN 24-Apr-18 02:16 modal/_watcher.py
--rw-r--r--  2.0 unx    35268 b- defN 24-Apr-18 02:16 modal/app.py
--rw-r--r--  2.0 unx    18642 b- defN 24-Apr-18 02:16 modal/app.pyi
--rw-r--r--  2.0 unx      748 b- defN 24-Apr-18 02:16 modal/app_utils.py
--rw-r--r--  2.0 unx      613 b- defN 24-Apr-18 02:16 modal/app_utils.pyi
--rw-r--r--  2.0 unx     2524 b- defN 24-Apr-18 02:16 modal/call_graph.py
--rw-r--r--  2.0 unx    10828 b- defN 24-Apr-18 02:16 modal/client.py
--rw-r--r--  2.0 unx     4006 b- defN 24-Apr-18 02:16 modal/client.pyi
--rw-r--r--  2.0 unx     5550 b- defN 24-Apr-18 02:16 modal/cloud_bucket_mount.py
--rw-r--r--  2.0 unx     1217 b- defN 24-Apr-18 02:16 modal/cloud_bucket_mount.pyi
--rw-r--r--  2.0 unx    12738 b- defN 24-Apr-18 02:16 modal/cls.py
--rw-r--r--  2.0 unx     6525 b- defN 24-Apr-18 02:16 modal/cls.pyi
--rw-r--r--  2.0 unx     9980 b- defN 24-Apr-18 02:16 modal/config.py
--rw-r--r--  2.0 unx    10777 b- defN 24-Apr-18 02:16 modal/dict.py
--rw-r--r--  2.0 unx     5809 b- defN 24-Apr-18 02:16 modal/dict.pyi
--rw-r--r--  2.0 unx     2452 b- defN 24-Apr-18 02:16 modal/environments.py
--rw-r--r--  2.0 unx     1439 b- defN 24-Apr-18 02:16 modal/environments.pyi
--rw-r--r--  2.0 unx     5868 b- defN 24-Apr-18 02:16 modal/exception.py
--rw-r--r--  2.0 unx      316 b- defN 24-Apr-18 02:16 modal/experimental.py
--rw-r--r--  2.0 unx    67936 b- defN 24-Apr-18 02:16 modal/functions.py
--rw-r--r--  2.0 unx    23960 b- defN 24-Apr-18 02:16 modal/functions.pyi
--rw-r--r--  2.0 unx     8038 b- defN 24-Apr-18 02:16 modal/gpu.py
--rw-r--r--  2.0 unx    68298 b- defN 24-Apr-18 02:16 modal/image.py
--rw-r--r--  2.0 unx    18339 b- defN 24-Apr-18 02:16 modal/image.pyi
--rw-r--r--  2.0 unx    23177 b- defN 24-Apr-18 02:16 modal/mount.py
--rw-r--r--  2.0 unx     9588 b- defN 24-Apr-18 02:16 modal/mount.pyi
--rw-r--r--  2.0 unx    14380 b- defN 24-Apr-18 02:16 modal/network_file_system.py
--rw-r--r--  2.0 unx     6280 b- defN 24-Apr-18 02:16 modal/network_file_system.pyi
--rw-r--r--  2.0 unx     8329 b- defN 24-Apr-18 02:16 modal/object.py
--rw-r--r--  2.0 unx     7573 b- defN 24-Apr-18 02:16 modal/object.pyi
--rw-r--r--  2.0 unx    19988 b- defN 24-Apr-18 02:16 modal/partial_function.py
--rw-r--r--  2.0 unx     6172 b- defN 24-Apr-18 02:16 modal/partial_function.pyi
--rw-r--r--  2.0 unx     1307 b- defN 24-Apr-18 02:16 modal/proxy.py
--rw-r--r--  2.0 unx      428 b- defN 24-Apr-18 02:16 modal/proxy.pyi
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-18 02:16 modal/py.typed
--rw-r--r--  2.0 unx    17136 b- defN 24-Apr-18 02:16 modal/queue.py
--rw-r--r--  2.0 unx     7397 b- defN 24-Apr-18 02:16 modal/queue.pyi
--rw-r--r--  2.0 unx     3730 b- defN 24-Apr-18 02:16 modal/retries.py
--rw-r--r--  2.0 unx    19091 b- defN 24-Apr-18 02:16 modal/runner.py
--rw-r--r--  2.0 unx     5753 b- defN 24-Apr-18 02:16 modal/runner.pyi
--rw-r--r--  2.0 unx      461 b- defN 24-Apr-18 02:16 modal/running_app.py
--rw-r--r--  2.0 unx    15040 b- defN 24-Apr-18 02:16 modal/sandbox.py
--rw-r--r--  2.0 unx     6514 b- defN 24-Apr-18 02:16 modal/sandbox.pyi
--rw-r--r--  2.0 unx     2621 b- defN 24-Apr-18 02:16 modal/schedule.py
--rw-r--r--  2.0 unx      662 b- defN 24-Apr-18 02:16 modal/scheduler_placement.py
--rw-r--r--  2.0 unx     8916 b- defN 24-Apr-18 02:16 modal/secret.py
--rw-r--r--  2.0 unx     2225 b- defN 24-Apr-18 02:16 modal/secret.pyi
--rw-r--r--  2.0 unx     4800 b- defN 24-Apr-18 02:16 modal/serving.py
--rw-r--r--  2.0 unx     2977 b- defN 24-Apr-18 02:16 modal/serving.pyi
--rw-r--r--  2.0 unx      888 b- defN 24-Apr-18 02:16 modal/shared_volume.py
--rw-r--r--  2.0 unx      405 b- defN 24-Apr-18 02:16 modal/shared_volume.pyi
--rw-r--r--  2.0 unx     6742 b- defN 24-Apr-18 02:16 modal/token_flow.py
--rw-r--r--  2.0 unx     1890 b- defN 24-Apr-18 02:16 modal/token_flow.pyi
--rw-r--r--  2.0 unx    28278 b- defN 24-Apr-18 02:16 modal/volume.py
--rw-r--r--  2.0 unx     9748 b- defN 24-Apr-18 02:16 modal/volume.pyi
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/_utils/__init__.py
--rw-r--r--  2.0 unx      465 b- defN 24-Apr-18 02:16 modal/_utils/app_utils.py
--rw-r--r--  2.0 unx    16049 b- defN 24-Apr-18 02:16 modal/_utils/async_utils.py
--rw-r--r--  2.0 unx    15054 b- defN 24-Apr-18 02:16 modal/_utils/blob_utils.py
--rw-r--r--  2.0 unx    15339 b- defN 24-Apr-18 02:16 modal/_utils/function_utils.py
--rw-r--r--  2.0 unx     7927 b- defN 24-Apr-18 02:16 modal/_utils/grpc_testing.py
--rw-r--r--  2.0 unx     9518 b- defN 24-Apr-18 02:16 modal/_utils/grpc_utils.py
--rw-r--r--  2.0 unx     1790 b- defN 24-Apr-18 02:16 modal/_utils/hash_utils.py
--rw-r--r--  2.0 unx     1426 b- defN 24-Apr-18 02:16 modal/_utils/http_utils.py
--rw-r--r--  2.0 unx     1311 b- defN 24-Apr-18 02:16 modal/_utils/logger.py
--rw-r--r--  2.0 unx     2341 b- defN 24-Apr-18 02:16 modal/_utils/mount_utils.py
--rw-r--r--  2.0 unx     1640 b- defN 24-Apr-18 02:16 modal/_utils/package_utils.py
--rw-r--r--  2.0 unx     3857 b- defN 24-Apr-18 02:16 modal/_utils/rand_pb_testing.py
--rw-r--r--  2.0 unx     3633 b- defN 24-Apr-18 02:16 modal/_utils/shell_utils.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/_vendor/__init__.py
--rw-r--r--  2.0 unx    22144 b- defN 24-Apr-18 02:16 modal/_vendor/a2wsgi_wsgi.py
--rw-r--r--  2.0 unx    55225 b- defN 24-Apr-18 02:16 modal/_vendor/cloudpickle.py
--rw-r--r--  2.0 unx     9722 b- defN 24-Apr-18 02:16 modal/_vendor/tblib.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/cli/__init__.py
--rw-r--r--  2.0 unx     2234 b- defN 24-Apr-18 02:16 modal/cli/_download.py
--rw-r--r--  2.0 unx     3041 b- defN 24-Apr-18 02:16 modal/cli/app.py
--rw-r--r--  2.0 unx     1294 b- defN 24-Apr-18 02:16 modal/cli/config.py
--rw-r--r--  2.0 unx     1748 b- defN 24-Apr-18 02:16 modal/cli/container.py
--rw-r--r--  2.0 unx     3538 b- defN 24-Apr-18 02:16 modal/cli/entry_point.py
--rw-r--r--  2.0 unx     3414 b- defN 24-Apr-18 02:16 modal/cli/environment.py
--rw-r--r--  2.0 unx     9529 b- defN 24-Apr-18 02:16 modal/cli/import_refs.py
--rw-r--r--  2.0 unx     2136 b- defN 24-Apr-18 02:16 modal/cli/launch.py
--rw-r--r--  2.0 unx     8256 b- defN 24-Apr-18 02:16 modal/cli/network_file_system.py
--rw-r--r--  2.0 unx     3109 b- defN 24-Apr-18 02:16 modal/cli/profile.py
--rw-r--r--  2.0 unx    13769 b- defN 24-Apr-18 02:16 modal/cli/run.py
--rw-r--r--  2.0 unx     4181 b- defN 24-Apr-18 02:16 modal/cli/secret.py
--rw-r--r--  2.0 unx     1875 b- defN 24-Apr-18 02:16 modal/cli/token.py
--rw-r--r--  2.0 unx     1330 b- defN 24-Apr-18 02:16 modal/cli/utils.py
--rw-r--r--  2.0 unx    10775 b- defN 24-Apr-18 02:16 modal/cli/volume.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/cli/programs/__init__.py
--rw-r--r--  2.0 unx     2143 b- defN 24-Apr-18 02:16 modal/cli/programs/run_jupyter.py
--rw-r--r--  2.0 unx     1898 b- defN 24-Apr-18 02:16 modal/cli/programs/vscode.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/extensions/__init__.py
--rw-r--r--  2.0 unx      988 b- defN 24-Apr-18 02:16 modal/extensions/ipython.py
--rw-r--r--  2.0 unx      400 b- defN 24-Apr-18 02:16 modal/requirements/2023.12.312.txt
--rw-r--r--  2.0 unx      502 b- defN 24-Apr-18 02:16 modal/requirements/2023.12.txt
--rw-r--r--  2.0 unx      520 b- defN 24-Apr-18 02:16 modal/requirements/2024.04.txt
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_docs/__init__.py
--rw-r--r--  2.0 unx     3801 b- defN 24-Apr-18 02:16 modal_docs/gen_cli_docs.py
--rw-r--r--  2.0 unx     6555 b- defN 24-Apr-18 02:16 modal_docs/gen_reference_docs.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_docs/mdmd/__init__.py
--rw-r--r--  2.0 unx     6268 b- defN 24-Apr-18 02:16 modal_docs/mdmd/mdmd.py
--rw-r--r--  2.0 unx     3243 b- defN 24-Apr-18 02:16 modal_docs/mdmd/signatures.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_global_objects/__init__.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_global_objects/images/__init__.py
--rw-r--r--  2.0 unx      324 b- defN 24-Apr-18 02:16 modal_global_objects/images/conda.py
--rw-r--r--  2.0 unx      330 b- defN 24-Apr-18 02:16 modal_global_objects/images/debian_slim.py
--rw-r--r--  2.0 unx      329 b- defN 24-Apr-18 02:16 modal_global_objects/images/micromamba.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_global_objects/mounts/__init__.py
--rw-r--r--  2.0 unx      767 b- defN 24-Apr-18 02:16 modal_global_objects/mounts/modal_client_package.py
--rw-r--r--  2.0 unx     1870 b- defN 24-Apr-18 02:16 modal_global_objects/mounts/python_standalone.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_proto/__init__.py
--rw-r--r--  2.0 unx    56704 b- defN 24-Apr-18 02:16 modal_proto/api.proto
--rw-r--r--  2.0 unx    85985 b- defN 24-Apr-18 02:16 modal_proto/api_grpc.py
--rw-r--r--  2.0 unx   219214 b- defN 24-Apr-18 02:16 modal_proto/api_pb2.py
--rw-r--r--  2.0 unx   185802 b- defN 24-Apr-18 02:16 modal_proto/api_pb2_grpc.py
--rw-r--r--  2.0 unx      488 b- defN 24-Apr-18 02:16 modal_proto/options.proto
--rw-r--r--  2.0 unx      125 b- defN 24-Apr-18 02:16 modal_proto/options_grpc.py
--rw-r--r--  2.0 unx     1827 b- defN 24-Apr-18 02:16 modal_proto/options_pb2.py
--rw-r--r--  2.0 unx      159 b- defN 24-Apr-18 02:16 modal_proto/options_pb2_grpc.py
--rw-r--r--  2.0 unx      470 b- defN 24-Apr-18 02:16 modal_version/__init__.py
--rw-r--r--  2.0 unx      105 b- defN 24-Apr-18 02:16 modal_version/__main__.py
--rw-r--r--  2.0 unx      149 b- defN 24-Apr-18 02:16 modal_version/_version_generated.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 test/__init__.py
--rw-r--r--  2.0 unx      199 b- defN 24-Apr-18 02:16 test/aio_test.py
--rw-r--r--  2.0 unx     7247 b- defN 24-Apr-18 02:16 test/async_utils_test.py
--rw-r--r--  2.0 unx     2539 b- defN 24-Apr-18 02:16 test/blob_test.py
--rw-r--r--  2.0 unx     4665 b- defN 24-Apr-18 02:16 test/cli_imports_test.py
--rw-r--r--  2.0 unx    27017 b- defN 24-Apr-18 02:16 test/cli_test.py
--rw-r--r--  2.0 unx     6426 b- defN 24-Apr-18 02:16 test/client_test.py
--rw-r--r--  2.0 unx      530 b- defN 24-Apr-18 02:16 test/cloud_bucket_mount_test.py
--rw-r--r--  2.0 unx    16131 b- defN 24-Apr-18 02:16 test/cls_test.py
--rw-r--r--  2.0 unx     5121 b- defN 24-Apr-18 02:16 test/config_test.py
--rw-r--r--  2.0 unx    61518 b- defN 24-Apr-18 02:16 test/conftest.py
--rw-r--r--  2.0 unx     1506 b- defN 24-Apr-18 02:16 test/container_app_test.py
--rw-r--r--  2.0 unx    50596 b- defN 24-Apr-18 02:16 test/container_test.py
--rw-r--r--  2.0 unx      398 b- defN 24-Apr-18 02:16 test/cpu_test.py
--rw-r--r--  2.0 unx     1850 b- defN 24-Apr-18 02:16 test/decorator_test.py
--rw-r--r--  2.0 unx     1055 b- defN 24-Apr-18 02:16 test/deprecation_test.py
--rw-r--r--  2.0 unx     1286 b- defN 24-Apr-18 02:16 test/dict_test.py
--rw-r--r--  2.0 unx     2152 b- defN 24-Apr-18 02:16 test/e2e_test.py
--rw-r--r--  2.0 unx      165 b- defN 24-Apr-18 02:16 test/error_test.py
--rw-r--r--  2.0 unx      966 b- defN 24-Apr-18 02:16 test/function_serialization_test.py
--rw-r--r--  2.0 unx    23757 b- defN 24-Apr-18 02:16 test/function_test.py
--rw-r--r--  2.0 unx     1892 b- defN 24-Apr-18 02:16 test/function_utils_test.py
--rw-r--r--  2.0 unx     4571 b- defN 24-Apr-18 02:16 test/gpu_test.py
--rw-r--r--  2.0 unx     5048 b- defN 24-Apr-18 02:16 test/grpc_utils_test.py
--rw-r--r--  2.0 unx     1709 b- defN 24-Apr-18 02:16 test/helpers.py
--rw-r--r--  2.0 unx    32798 b- defN 24-Apr-18 02:16 test/image_test.py
--rw-r--r--  2.0 unx     2722 b- defN 24-Apr-18 02:16 test/live_reload_test.py
--rw-r--r--  2.0 unx     2160 b- defN 24-Apr-18 02:16 test/lookup_test.py
--rw-r--r--  2.0 unx     5118 b- defN 24-Apr-18 02:16 test/mdmd_test.py
--rw-r--r--  2.0 unx     5475 b- defN 24-Apr-18 02:16 test/mount_test.py
--rw-r--r--  2.0 unx    12461 b- defN 24-Apr-18 02:16 test/mounted_files_test.py
--rw-r--r--  2.0 unx     6265 b- defN 24-Apr-18 02:16 test/network_file_system_test.py
--rw-r--r--  2.0 unx     2157 b- defN 24-Apr-18 02:16 test/notebook_test.py
--rw-r--r--  2.0 unx     1291 b- defN 24-Apr-18 02:16 test/object_test.py
--rw-r--r--  2.0 unx      795 b- defN 24-Apr-18 02:16 test/package_utils_test.py
--rw-r--r--  2.0 unx     3667 b- defN 24-Apr-18 02:16 test/queue_test.py
--rw-r--r--  2.0 unx     1782 b- defN 24-Apr-18 02:16 test/resolver_test.py
--rw-r--r--  2.0 unx     1806 b- defN 24-Apr-18 02:16 test/retries_test.py
--rw-r--r--  2.0 unx     2909 b- defN 24-Apr-18 02:16 test/runner_test.py
--rw-r--r--  2.0 unx     5254 b- defN 24-Apr-18 02:16 test/sandbox_test.py
--rw-r--r--  2.0 unx      354 b- defN 24-Apr-18 02:16 test/schedule_test.py
--rw-r--r--  2.0 unx      737 b- defN 24-Apr-18 02:16 test/scheduler_placement_test.py
--rw-r--r--  2.0 unx     2506 b- defN 24-Apr-18 02:16 test/secret_test.py
--rw-r--r--  2.0 unx     1867 b- defN 24-Apr-18 02:16 test/serialization_test.py
--rw-r--r--  2.0 unx      555 b- defN 24-Apr-18 02:16 test/stub_composition_test.py
--rw-r--r--  2.0 unx    10547 b- defN 24-Apr-18 02:16 test/stub_test.py
--rw-r--r--  2.0 unx     7320 b- defN 24-Apr-18 02:16 test/test_asgi_wrapper.py
--rw-r--r--  2.0 unx      614 b- defN 24-Apr-18 02:16 test/token_flow_test.py
--rw-r--r--  2.0 unx     4747 b- defN 24-Apr-18 02:16 test/traceback_test.py
--rw-r--r--  2.0 unx      768 b- defN 24-Apr-18 02:16 test/tunnel_test.py
--rw-r--r--  2.0 unx     2679 b- defN 24-Apr-18 02:16 test/utils_test.py
--rw-r--r--  2.0 unx      403 b- defN 24-Apr-18 02:16 test/version_test.py
--rw-r--r--  2.0 unx    14309 b- defN 24-Apr-18 02:16 test/volume_test.py
--rw-r--r--  2.0 unx     1193 b- defN 24-Apr-18 02:16 test/watcher_test.py
--rw-r--r--  2.0 unx     4172 b- defN 24-Apr-18 02:16 test/webhook_test.py
--rw-r--r--  2.0 unx    10173 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/LICENSE
--rw-r--r--  2.0 unx     2302 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/WHEEL
--rw-r--r--  2.0 unx       46 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       69 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    16058 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/RECORD
-203 files, 1939989 bytes uncompressed, 428698 bytes compressed:  77.9%
+Zip file size: 436952 bytes, number of entries: 197
+-rw-r--r--  2.0 unx     2102 b- defN 24-Mar-28 16:25 modal/__init__.py
+-rw-r--r--  2.0 unx     1141 b- defN 24-Mar-28 16:25 modal/__main__.py
+-rw-r--r--  2.0 unx    15476 b- defN 24-Mar-28 16:25 modal/_asgi.py
+-rw-r--r--  2.0 unx    52214 b- defN 24-Mar-28 16:25 modal/_container_entrypoint.py
+-rw-r--r--  2.0 unx    11148 b- defN 24-Mar-28 16:25 modal/_container_entrypoint.pyi
+-rw-r--r--  2.0 unx     4349 b- defN 24-Mar-28 16:25 modal/_container_exec.py
+-rw-r--r--  2.0 unx      450 b- defN 24-Mar-28 16:25 modal/_ipython.py
+-rw-r--r--  2.0 unx      929 b- defN 24-Mar-28 16:25 modal/_location.py
+-rw-r--r--  2.0 unx    20565 b- defN 24-Mar-28 16:25 modal/_output.py
+-rw-r--r--  2.0 unx     1906 b- defN 24-Mar-28 16:25 modal/_proxy_tunnel.py
+-rw-r--r--  2.0 unx     1336 b- defN 24-Mar-28 16:25 modal/_pty.py
+-rw-r--r--  2.0 unx     7208 b- defN 24-Mar-28 16:25 modal/_resolver.py
+-rw-r--r--  2.0 unx     1658 b- defN 24-Mar-28 16:25 modal/_sandbox_shell.py
+-rw-r--r--  2.0 unx    12329 b- defN 24-Mar-28 16:25 modal/_serialization.py
+-rw-r--r--  2.0 unx    10029 b- defN 24-Mar-28 16:25 modal/_traceback.py
+-rw-r--r--  2.0 unx     5073 b- defN 24-Mar-28 16:25 modal/_tunnel.py
+-rw-r--r--  2.0 unx     1337 b- defN 24-Mar-28 16:25 modal/_tunnel.pyi
+-rw-r--r--  2.0 unx     3609 b- defN 24-Mar-28 16:25 modal/_watcher.py
+-rw-r--r--  2.0 unx    15555 b- defN 24-Mar-28 16:25 modal/app.py
+-rw-r--r--  2.0 unx     8360 b- defN 24-Mar-28 16:25 modal/app.pyi
+-rw-r--r--  2.0 unx     2524 b- defN 24-Mar-28 16:25 modal/call_graph.py
+-rw-r--r--  2.0 unx     9751 b- defN 24-Mar-28 16:25 modal/client.py
+-rw-r--r--  2.0 unx     2942 b- defN 24-Mar-28 16:25 modal/cloud_bucket_mount.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-Mar-28 16:25 modal/cloud_bucket_mount.pyi
+-rw-r--r--  2.0 unx    12799 b- defN 24-Mar-28 16:25 modal/cls.py
+-rw-r--r--  2.0 unx     6482 b- defN 24-Mar-28 16:25 modal/cls.pyi
+-rw-r--r--  2.0 unx     9934 b- defN 24-Mar-28 16:25 modal/config.py
+-rw-r--r--  2.0 unx    10194 b- defN 24-Mar-28 16:25 modal/dict.py
+-rw-r--r--  2.0 unx     5809 b- defN 24-Mar-28 16:25 modal/dict.pyi
+-rw-r--r--  2.0 unx     2452 b- defN 24-Mar-28 16:25 modal/environments.py
+-rw-r--r--  2.0 unx     1439 b- defN 24-Mar-28 16:25 modal/environments.pyi
+-rw-r--r--  2.0 unx     5865 b- defN 24-Mar-28 16:25 modal/exception.py
+-rw-r--r--  2.0 unx      293 b- defN 24-Mar-28 16:25 modal/experimental.py
+-rw-r--r--  2.0 unx    63450 b- defN 24-Mar-28 16:25 modal/functions.py
+-rw-r--r--  2.0 unx    19548 b- defN 24-Mar-28 16:25 modal/functions.pyi
+-rw-r--r--  2.0 unx     8038 b- defN 24-Mar-28 16:25 modal/gpu.py
+-rw-r--r--  2.0 unx    62658 b- defN 24-Mar-28 16:25 modal/image.py
+-rw-r--r--  2.0 unx    17457 b- defN 24-Mar-28 16:25 modal/image.pyi
+-rw-r--r--  2.0 unx    23182 b- defN 24-Mar-28 16:25 modal/mount.py
+-rw-r--r--  2.0 unx     9588 b- defN 24-Mar-28 16:25 modal/mount.pyi
+-rw-r--r--  2.0 unx    14356 b- defN 24-Mar-28 16:25 modal/network_file_system.py
+-rw-r--r--  2.0 unx     6404 b- defN 24-Mar-28 16:25 modal/network_file_system.pyi
+-rw-r--r--  2.0 unx     8258 b- defN 24-Mar-28 16:25 modal/object.py
+-rw-r--r--  2.0 unx     7573 b- defN 24-Mar-28 16:25 modal/object.pyi
+-rw-r--r--  2.0 unx    19988 b- defN 24-Mar-28 16:25 modal/partial_function.py
+-rw-r--r--  2.0 unx     6270 b- defN 24-Mar-28 16:25 modal/partial_function.pyi
+-rw-r--r--  2.0 unx     1307 b- defN 24-Mar-28 16:25 modal/proxy.py
+-rw-r--r--  2.0 unx      428 b- defN 24-Mar-28 16:25 modal/proxy.pyi
+-rw-r--r--  2.0 unx        0 b- defN 24-Mar-28 16:25 modal/py.typed
+-rw-r--r--  2.0 unx    12296 b- defN 24-Mar-28 16:25 modal/queue.py
+-rw-r--r--  2.0 unx     5673 b- defN 24-Mar-28 16:25 modal/queue.pyi
+-rw-r--r--  2.0 unx      400 b- defN 24-Mar-28 16:25 modal/requirements.312.txt
+-rw-r--r--  2.0 unx      502 b- defN 24-Mar-28 16:25 modal/requirements.txt
+-rw-r--r--  2.0 unx     3730 b- defN 24-Mar-28 16:25 modal/retries.py
+-rw-r--r--  2.0 unx    12295 b- defN 24-Mar-28 16:25 modal/runner.py
+-rw-r--r--  2.0 unx     3114 b- defN 24-Mar-28 16:25 modal/runner.pyi
+-rw-r--r--  2.0 unx    15301 b- defN 24-Mar-28 16:25 modal/sandbox.py
+-rw-r--r--  2.0 unx     6466 b- defN 24-Mar-28 16:25 modal/sandbox.pyi
+-rw-r--r--  2.0 unx     2621 b- defN 24-Mar-28 16:25 modal/schedule.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Mar-28 16:25 modal/scheduler_placement.py
+-rw-r--r--  2.0 unx     8898 b- defN 24-Mar-28 16:25 modal/secret.py
+-rw-r--r--  2.0 unx     2225 b- defN 24-Mar-28 16:25 modal/secret.pyi
+-rw-r--r--  2.0 unx     4681 b- defN 24-Mar-28 16:25 modal/serving.py
+-rw-r--r--  2.0 unx     1957 b- defN 24-Mar-28 16:25 modal/serving.pyi
+-rw-r--r--  2.0 unx      888 b- defN 24-Mar-28 16:25 modal/shared_volume.py
+-rw-r--r--  2.0 unx      405 b- defN 24-Mar-28 16:25 modal/shared_volume.pyi
+-rw-r--r--  2.0 unx    33852 b- defN 24-Mar-28 16:25 modal/stub.py
+-rw-r--r--  2.0 unx    17567 b- defN 24-Mar-28 16:25 modal/stub.pyi
+-rw-r--r--  2.0 unx     6771 b- defN 24-Mar-28 16:25 modal/token_flow.py
+-rw-r--r--  2.0 unx     1890 b- defN 24-Mar-28 16:25 modal/token_flow.pyi
+-rw-r--r--  2.0 unx    24410 b- defN 24-Mar-28 16:25 modal/volume.py
+-rw-r--r--  2.0 unx     9111 b- defN 24-Mar-28 16:25 modal/volume.pyi
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/_utils/__init__.py
+-rw-r--r--  2.0 unx      465 b- defN 24-Mar-28 16:25 modal/_utils/app_utils.py
+-rw-r--r--  2.0 unx    12930 b- defN 24-Mar-28 16:25 modal/_utils/async_utils.py
+-rw-r--r--  2.0 unx    15054 b- defN 24-Mar-28 16:25 modal/_utils/blob_utils.py
+-rw-r--r--  2.0 unx    13520 b- defN 24-Mar-28 16:25 modal/_utils/function_utils.py
+-rw-r--r--  2.0 unx     7780 b- defN 24-Mar-28 16:25 modal/_utils/grpc_testing.py
+-rw-r--r--  2.0 unx     9322 b- defN 24-Mar-28 16:25 modal/_utils/grpc_utils.py
+-rw-r--r--  2.0 unx     1597 b- defN 24-Mar-28 16:25 modal/_utils/hash_utils.py
+-rw-r--r--  2.0 unx     1426 b- defN 24-Mar-28 16:25 modal/_utils/http_utils.py
+-rw-r--r--  2.0 unx     1311 b- defN 24-Mar-28 16:25 modal/_utils/logger.py
+-rw-r--r--  2.0 unx     2327 b- defN 24-Mar-28 16:25 modal/_utils/mount_utils.py
+-rw-r--r--  2.0 unx     1640 b- defN 24-Mar-28 16:25 modal/_utils/package_utils.py
+-rw-r--r--  2.0 unx     3871 b- defN 24-Mar-28 16:25 modal/_utils/rand_pb_testing.py
+-rw-r--r--  2.0 unx     3633 b- defN 24-Mar-28 16:25 modal/_utils/shell_utils.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/_vendor/__init__.py
+-rw-r--r--  2.0 unx    22144 b- defN 24-Mar-28 16:25 modal/_vendor/a2wsgi_wsgi.py
+-rw-r--r--  2.0 unx    55225 b- defN 24-Mar-28 16:25 modal/_vendor/cloudpickle.py
+-rw-r--r--  2.0 unx     9722 b- defN 24-Mar-28 16:25 modal/_vendor/tblib.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/cli/__init__.py
+-rw-r--r--  2.0 unx     2601 b- defN 24-Mar-28 16:25 modal/cli/_download.py
+-rw-r--r--  2.0 unx     3068 b- defN 24-Mar-28 16:25 modal/cli/app.py
+-rw-r--r--  2.0 unx     1294 b- defN 24-Mar-28 16:25 modal/cli/config.py
+-rw-r--r--  2.0 unx     1748 b- defN 24-Mar-28 16:25 modal/cli/container.py
+-rw-r--r--  2.0 unx     3538 b- defN 24-Mar-28 16:25 modal/cli/entry_point.py
+-rw-r--r--  2.0 unx     3414 b- defN 24-Mar-28 16:25 modal/cli/environment.py
+-rw-r--r--  2.0 unx     9083 b- defN 24-Mar-28 16:25 modal/cli/import_refs.py
+-rw-r--r--  2.0 unx     1702 b- defN 24-Mar-28 16:25 modal/cli/launch.py
+-rw-r--r--  2.0 unx     8449 b- defN 24-Mar-28 16:25 modal/cli/network_file_system.py
+-rw-r--r--  2.0 unx     3109 b- defN 24-Mar-28 16:25 modal/cli/profile.py
+-rw-r--r--  2.0 unx    13798 b- defN 24-Mar-28 16:25 modal/cli/run.py
+-rw-r--r--  2.0 unx     4181 b- defN 24-Mar-28 16:25 modal/cli/secret.py
+-rw-r--r--  2.0 unx     1875 b- defN 24-Mar-28 16:25 modal/cli/token.py
+-rw-r--r--  2.0 unx     1330 b- defN 24-Mar-28 16:25 modal/cli/utils.py
+-rw-r--r--  2.0 unx    10871 b- defN 24-Mar-28 16:25 modal/cli/volume.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/cli/programs/__init__.py
+-rw-r--r--  2.0 unx     2010 b- defN 24-Mar-28 16:25 modal/cli/programs/run_jupyter.py
+-rw-r--r--  2.0 unx     1765 b- defN 24-Mar-28 16:25 modal/cli/programs/vscode.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/extensions/__init__.py
+-rw-r--r--  2.0 unx      988 b- defN 24-Mar-28 16:25 modal/extensions/ipython.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_docs/__init__.py
+-rw-r--r--  2.0 unx     3801 b- defN 24-Mar-28 16:25 modal_docs/gen_cli_docs.py
+-rw-r--r--  2.0 unx     6555 b- defN 24-Mar-28 16:25 modal_docs/gen_reference_docs.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_docs/mdmd/__init__.py
+-rw-r--r--  2.0 unx     6268 b- defN 24-Mar-28 16:25 modal_docs/mdmd/mdmd.py
+-rw-r--r--  2.0 unx     3095 b- defN 24-Mar-28 16:25 modal_docs/mdmd/signatures.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_global_objects/__init__.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_global_objects/images/__init__.py
+-rw-r--r--  2.0 unx      324 b- defN 24-Mar-28 16:25 modal_global_objects/images/conda.py
+-rw-r--r--  2.0 unx      330 b- defN 24-Mar-28 16:25 modal_global_objects/images/debian_slim.py
+-rw-r--r--  2.0 unx      329 b- defN 24-Mar-28 16:25 modal_global_objects/images/micromamba.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_global_objects/mounts/__init__.py
+-rw-r--r--  2.0 unx      767 b- defN 24-Mar-28 16:25 modal_global_objects/mounts/modal_client_package.py
+-rw-r--r--  2.0 unx     1870 b- defN 24-Mar-28 16:25 modal_global_objects/mounts/python_standalone.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_proto/__init__.py
+-rw-r--r--  2.0 unx    56021 b- defN 24-Mar-28 16:25 modal_proto/api.proto
+-rw-r--r--  2.0 unx    85328 b- defN 24-Mar-28 16:25 modal_proto/api_grpc.py
+-rw-r--r--  2.0 unx   217994 b- defN 24-Mar-28 16:25 modal_proto/api_pb2.py
+-rw-r--r--  2.0 unx   184288 b- defN 24-Mar-28 16:25 modal_proto/api_pb2_grpc.py
+-rw-r--r--  2.0 unx      488 b- defN 24-Mar-28 16:25 modal_proto/options.proto
+-rw-r--r--  2.0 unx      125 b- defN 24-Mar-28 16:25 modal_proto/options_grpc.py
+-rw-r--r--  2.0 unx     1827 b- defN 24-Mar-28 16:25 modal_proto/options_pb2.py
+-rw-r--r--  2.0 unx      159 b- defN 24-Mar-28 16:25 modal_proto/options_pb2_grpc.py
+-rw-r--r--  2.0 unx      470 b- defN 24-Mar-28 16:25 modal_version/__init__.py
+-rw-r--r--  2.0 unx      105 b- defN 24-Mar-28 16:25 modal_version/__main__.py
+-rw-r--r--  2.0 unx      148 b- defN 24-Mar-28 16:25 modal_version/_version_generated.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 test/__init__.py
+-rw-r--r--  2.0 unx      203 b- defN 24-Mar-28 16:25 test/aio_test.py
+-rw-r--r--  2.0 unx     6792 b- defN 24-Mar-28 16:25 test/async_utils_test.py
+-rw-r--r--  2.0 unx     2539 b- defN 24-Mar-28 16:25 test/blob_test.py
+-rw-r--r--  2.0 unx     4680 b- defN 24-Mar-28 16:25 test/cli_imports_test.py
+-rw-r--r--  2.0 unx    26607 b- defN 24-Mar-28 16:25 test/cli_test.py
+-rw-r--r--  2.0 unx     6426 b- defN 24-Mar-28 16:25 test/client_test.py
+-rw-r--r--  2.0 unx    15906 b- defN 24-Mar-28 16:25 test/cls_test.py
+-rw-r--r--  2.0 unx     4768 b- defN 24-Mar-28 16:25 test/config_test.py
+-rw-r--r--  2.0 unx    59766 b- defN 24-Mar-28 16:25 test/conftest.py
+-rw-r--r--  2.0 unx      895 b- defN 24-Mar-28 16:25 test/container_app_test.py
+-rw-r--r--  2.0 unx    50061 b- defN 24-Mar-28 16:25 test/container_test.py
+-rw-r--r--  2.0 unx      405 b- defN 24-Mar-28 16:25 test/cpu_test.py
+-rw-r--r--  2.0 unx     1871 b- defN 24-Mar-28 16:25 test/decorator_test.py
+-rw-r--r--  2.0 unx     1055 b- defN 24-Mar-28 16:25 test/deprecation_test.py
+-rw-r--r--  2.0 unx      851 b- defN 24-Mar-28 16:25 test/dict_test.py
+-rw-r--r--  2.0 unx     2152 b- defN 24-Mar-28 16:25 test/e2e_test.py
+-rw-r--r--  2.0 unx      165 b- defN 24-Mar-28 16:25 test/error_test.py
+-rw-r--r--  2.0 unx      971 b- defN 24-Mar-28 16:25 test/function_serialization_test.py
+-rw-r--r--  2.0 unx    19697 b- defN 24-Mar-28 16:25 test/function_test.py
+-rw-r--r--  2.0 unx     1892 b- defN 24-Mar-28 16:25 test/function_utils_test.py
+-rw-r--r--  2.0 unx     4611 b- defN 24-Mar-28 16:25 test/gpu_test.py
+-rw-r--r--  2.0 unx     5048 b- defN 24-Mar-28 16:25 test/grpc_utils_test.py
+-rw-r--r--  2.0 unx     1443 b- defN 24-Mar-28 16:25 test/helpers.py
+-rw-r--r--  2.0 unx    25515 b- defN 24-Mar-28 16:25 test/image_test.py
+-rw-r--r--  2.0 unx     2742 b- defN 24-Mar-28 16:25 test/live_reload_test.py
+-rw-r--r--  2.0 unx     2172 b- defN 24-Mar-28 16:25 test/lookup_test.py
+-rw-r--r--  2.0 unx     5118 b- defN 24-Mar-28 16:25 test/mdmd_test.py
+-rw-r--r--  2.0 unx     5489 b- defN 24-Mar-28 16:25 test/mount_test.py
+-rw-r--r--  2.0 unx    12471 b- defN 24-Mar-28 16:25 test/mounted_files_test.py
+-rw-r--r--  2.0 unx     6073 b- defN 24-Mar-28 16:25 test/network_file_system_test.py
+-rw-r--r--  2.0 unx     2157 b- defN 24-Mar-28 16:25 test/notebook_test.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-Mar-28 16:25 test/object_test.py
+-rw-r--r--  2.0 unx      795 b- defN 24-Mar-28 16:25 test/package_utils_test.py
+-rw-r--r--  2.0 unx     3270 b- defN 24-Mar-28 16:25 test/queue_test.py
+-rw-r--r--  2.0 unx     1751 b- defN 24-Mar-28 16:25 test/resolver_test.py
+-rw-r--r--  2.0 unx     1817 b- defN 24-Mar-28 16:25 test/retries_test.py
+-rw-r--r--  2.0 unx     2582 b- defN 24-Mar-28 16:25 test/runner_test.py
+-rw-r--r--  2.0 unx     5282 b- defN 24-Mar-28 16:25 test/sandbox_test.py
+-rw-r--r--  2.0 unx      359 b- defN 24-Mar-28 16:25 test/schedule_test.py
+-rw-r--r--  2.0 unx      742 b- defN 24-Mar-28 16:25 test/scheduler_placement_test.py
+-rw-r--r--  2.0 unx     2527 b- defN 24-Mar-28 16:25 test/secret_test.py
+-rw-r--r--  2.0 unx     1867 b- defN 24-Mar-28 16:25 test/serialization_test.py
+-rw-r--r--  2.0 unx      558 b- defN 24-Mar-28 16:25 test/stub_composition_test.py
+-rw-r--r--  2.0 unx    11284 b- defN 24-Mar-28 16:25 test/stub_test.py
+-rw-r--r--  2.0 unx     7320 b- defN 24-Mar-28 16:25 test/test_asgi_wrapper.py
+-rw-r--r--  2.0 unx      614 b- defN 24-Mar-28 16:25 test/token_flow_test.py
+-rw-r--r--  2.0 unx     4747 b- defN 24-Mar-28 16:25 test/traceback_test.py
+-rw-r--r--  2.0 unx      768 b- defN 24-Mar-28 16:25 test/tunnel_test.py
+-rw-r--r--  2.0 unx     2679 b- defN 24-Mar-28 16:25 test/utils_test.py
+-rw-r--r--  2.0 unx      403 b- defN 24-Mar-28 16:25 test/version_test.py
+-rw-r--r--  2.0 unx    12461 b- defN 24-Mar-28 16:25 test/volume_test.py
+-rw-r--r--  2.0 unx     1193 b- defN 24-Mar-28 16:25 test/watcher_test.py
+-rw-r--r--  2.0 unx     4205 b- defN 24-Mar-28 16:25 test/webhook_test.py
+-rw-r--r--  2.0 unx    10173 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2301 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx       46 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       69 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    15540 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/RECORD
+197 files, 1877599 bytes uncompressed, 413184 bytes compressed:  78.0%
```

## zipnote {}

```diff
@@ -6,21 +6,18 @@
 
 Filename: modal/_asgi.py
 Comment: 
 
 Filename: modal/_container_entrypoint.py
 Comment: 
 
-Filename: modal/_container_exec.py
-Comment: 
-
-Filename: modal/_container_io_manager.py
+Filename: modal/_container_entrypoint.pyi
 Comment: 
 
-Filename: modal/_container_io_manager.pyi
+Filename: modal/_container_exec.py
 Comment: 
 
 Filename: modal/_ipython.py
 Comment: 
 
 Filename: modal/_location.py
 Comment: 
@@ -33,17 +30,14 @@
 
 Filename: modal/_pty.py
 Comment: 
 
 Filename: modal/_resolver.py
 Comment: 
 
-Filename: modal/_resources.py
-Comment: 
-
 Filename: modal/_sandbox_shell.py
 Comment: 
 
 Filename: modal/_serialization.py
 Comment: 
 
 Filename: modal/_traceback.py
@@ -60,29 +54,20 @@
 
 Filename: modal/app.py
 Comment: 
 
 Filename: modal/app.pyi
 Comment: 
 
-Filename: modal/app_utils.py
-Comment: 
-
-Filename: modal/app_utils.pyi
-Comment: 
-
 Filename: modal/call_graph.py
 Comment: 
 
 Filename: modal/client.py
 Comment: 
 
-Filename: modal/client.pyi
-Comment: 
-
 Filename: modal/cloud_bucket_mount.py
 Comment: 
 
 Filename: modal/cloud_bucket_mount.pyi
 Comment: 
 
 Filename: modal/cls.py
@@ -162,26 +147,29 @@
 
 Filename: modal/queue.py
 Comment: 
 
 Filename: modal/queue.pyi
 Comment: 
 
+Filename: modal/requirements.312.txt
+Comment: 
+
+Filename: modal/requirements.txt
+Comment: 
+
 Filename: modal/retries.py
 Comment: 
 
 Filename: modal/runner.py
 Comment: 
 
 Filename: modal/runner.pyi
 Comment: 
 
-Filename: modal/running_app.py
-Comment: 
-
 Filename: modal/sandbox.py
 Comment: 
 
 Filename: modal/sandbox.pyi
 Comment: 
 
 Filename: modal/schedule.py
@@ -204,14 +192,20 @@
 
 Filename: modal/shared_volume.py
 Comment: 
 
 Filename: modal/shared_volume.pyi
 Comment: 
 
+Filename: modal/stub.py
+Comment: 
+
+Filename: modal/stub.pyi
+Comment: 
+
 Filename: modal/token_flow.py
 Comment: 
 
 Filename: modal/token_flow.pyi
 Comment: 
 
 Filename: modal/volume.py
@@ -333,23 +327,14 @@
 
 Filename: modal/extensions/__init__.py
 Comment: 
 
 Filename: modal/extensions/ipython.py
 Comment: 
 
-Filename: modal/requirements/2023.12.312.txt
-Comment: 
-
-Filename: modal/requirements/2023.12.txt
-Comment: 
-
-Filename: modal/requirements/2024.04.txt
-Comment: 
-
 Filename: modal_docs/__init__.py
 Comment: 
 
 Filename: modal_docs/gen_cli_docs.py
 Comment: 
 
 Filename: modal_docs/gen_reference_docs.py
@@ -441,17 +426,14 @@
 
 Filename: test/cli_test.py
 Comment: 
 
 Filename: test/client_test.py
 Comment: 
 
-Filename: test/cloud_bucket_mount_test.py
-Comment: 
-
 Filename: test/cls_test.py
 Comment: 
 
 Filename: test/config_test.py
 Comment: 
 
 Filename: test/conftest.py
@@ -585,26 +567,26 @@
 
 Filename: test/watcher_test.py
 Comment: 
 
 Filename: test/webhook_test.py
 Comment: 
 
-Filename: modal-0.62.87.dist-info/LICENSE
+Filename: modal-0.62.9.dist-info/LICENSE
 Comment: 
 
-Filename: modal-0.62.87.dist-info/METADATA
+Filename: modal-0.62.9.dist-info/METADATA
 Comment: 
 
-Filename: modal-0.62.87.dist-info/WHEEL
+Filename: modal-0.62.9.dist-info/WHEEL
 Comment: 
 
-Filename: modal-0.62.87.dist-info/entry_points.txt
+Filename: modal-0.62.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: modal-0.62.87.dist-info/top_level.txt
+Filename: modal-0.62.9.dist-info/top_level.txt
 Comment: 
 
-Filename: modal-0.62.87.dist-info/RECORD
+Filename: modal-0.62.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## modal/__init__.py

```diff
@@ -5,18 +5,16 @@
     raise RuntimeError("This version of Modal requires at least Python 3.8")
 if sys.version_info[:2] >= (3, 13):
     raise RuntimeError("This version of Modal does not support Python 3.13+")
 
 from modal_version import __version__
 
 try:
-
-    from ._container_io_manager import interact, is_local
     from ._tunnel import Tunnel, forward
-    from .app import App, Stub
+    from .app import container_app, interact, is_local
     from .client import Client
     from .cloud_bucket_mount import CloudBucketMount
     from .cls import Cls
     from .dict import Dict
     from .exception import Error
     from .functions import Function, current_function_call_id, current_input_id
     from .image import Image
@@ -27,27 +25,27 @@
     from .queue import Queue
     from .retries import Retries
     from .sandbox import Sandbox
     from .schedule import Cron, Period
     from .scheduler_placement import SchedulerPlacement
     from .secret import Secret
     from .shared_volume import SharedVolume
+    from .stub import Stub
     from .volume import Volume
 except Exception:
     print()
     print("#" * 80)
     print("#" + "Something with the Modal installation seems broken.".center(78) + "#")
     print("#" + "Please email support@modal.com and we will try to help!".center(78) + "#")
     print("#" * 80)
     print()
     raise
 
 __all__ = [
     "__version__",
-    "App",
     "Client",
     "Cls",
     "Cron",
     "Dict",
     "Error",
     "Function",
     "Image",
@@ -63,14 +61,15 @@
     "Secret",
     "SharedVolume",
     "Stub",
     "Tunnel",
     "Volume",
     "asgi_app",
     "build",
+    "container_app",
     "current_function_call_id",
     "current_input_id",
     "enter",
     "exit",
     "forward",
     "is_local",
     "method",
```

## modal/_container_entrypoint.py

```diff
@@ -1,61 +1,67 @@
 # Copyright Modal Labs 2022
+from __future__ import annotations
+
 import asyncio
 import base64
+import contextlib
 import importlib
 import inspect
+import json
+import math
+import os
 import signal
 import sys
 import threading
 import time
+import traceback
+from collections.abc import Iterable
 from dataclasses import dataclass
-from typing import TYPE_CHECKING, Any, Callable, List, Optional, Sequence, Type
+from pathlib import Path
+from typing import TYPE_CHECKING, Any, AsyncGenerator, AsyncIterator, Callable, List, Optional, Set, Tuple, Type
 
-from google.protobuf.message import Message
-from synchronicity import Interface
+from grpclib import Status
 
 from modal_proto import api_pb2
 
 from ._asgi import (
     asgi_app_wrapper,
     get_ip_address,
     wait_for_web_server,
     web_server_proxy,
     webhook_asgi_app,
     wsgi_app_wrapper,
 )
-from ._container_io_manager import ContainerIOManager, UserException, _ContainerIOManager, interact
 from ._proxy_tunnel import proxy_tunnel
-from ._serialization import deserialize
-from ._utils.async_utils import TaskContext, synchronizer
+from ._serialization import deserialize, deserialize_data_format, serialize, serialize_data_format
+from ._traceback import extract_traceback
+from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
+from ._utils.blob_utils import MAX_OBJECT_SIZE_BYTES, blob_download, blob_upload
 from ._utils.function_utils import LocalFunctionError, is_async as get_is_async, is_global_function, method_has_params
-from .app import App, _App
-from .client import Client, _Client
+from ._utils.grpc_utils import retry_transient_errors
+from .app import ContainerApp, _container_app, _ContainerApp, interact
+from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, Client, _Client
 from .cls import Cls
-from .config import logger
-from .exception import ExecutionError, InputCancellation, InvalidError
-from .functions import Function, _Function, _set_current_context_ids
+from .config import config, logger
+from .exception import InputCancellation, InvalidError
+from .functions import Function, _Function, _set_current_context_ids, _stream_function_call_data
 from .partial_function import _find_callables_for_obj, _PartialFunctionFlags
-from .running_app import RunningApp
+from .stub import _Stub
 
 if TYPE_CHECKING:
     from types import ModuleType
 
+MAX_OUTPUT_BATCH_SIZE: int = 49
 
-@dataclass
-class ImportedFunction:
-    obj: Any
-    fun: Callable
-    app: Optional[_App]
-    is_async: bool
-    is_generator: bool
-    data_format: int  # api_pb2.DataFormat
-    input_concurrency: int
-    is_auto_snapshot: bool
-    function: _Function
+RTT_S: float = 0.5  # conservative estimate of RTT in seconds.
+
+
+class UserException(Exception):
+    # Used to shut down the task gracefully
+    pass
 
 
 class UserCodeEventLoop:
     """Run an async event loop as a context manager and handle signals.
 
     This will run all *user supplied* async code, i.e. async functions, as well as async enter/exit managers
 
@@ -112,56 +118,581 @@
                 raise KeyboardInterrupt()
         finally:
             self.loop.remove_signal_handler(signal.SIGUSR1)
             if not ignore_sigint:
                 self.loop.remove_signal_handler(signal.SIGINT)
 
 
+class _FunctionIOManager:
+    """Synchronizes all RPC calls and network operations for a running container.
+
+    TODO: maybe we shouldn't synchronize the whole class.
+    Then we could potentially move a bunch of the global functions onto it.
+    """
+
+    _GENERATOR_STOP_SENTINEL = object()
+
+    def __init__(self, container_args: api_pb2.ContainerArguments, client: _Client):
+        self.cancelled_input_ids: Set[str] = set()
+        self.task_id = container_args.task_id
+        self.function_id = container_args.function_id
+        self.app_id = container_args.app_id
+        self.function_def = container_args.function_def
+        self.checkpoint_id = container_args.checkpoint_id
+
+        self.calls_completed = 0
+        self.total_user_time: float = 0.0
+        self.current_input_id: Optional[str] = None
+        self.current_input_started_at: Optional[float] = None
+
+        self._stub_name = self.function_def.stub_name
+        self._input_concurrency: Optional[int] = None
+
+        self._semaphore: Optional[asyncio.Semaphore] = None
+        self._environment_name = container_args.environment_name
+        self._waiting_for_checkpoint = False
+        self._heartbeat_loop = None
+
+        self._client = client
+        assert isinstance(self._client, _Client)
+
+    async def initialize_app(self) -> _ContainerApp:
+        await _container_app.init(self._client, self.app_id, self._stub_name, self._environment_name, self.function_def)
+        return _container_app
+
+    async def _run_heartbeat_loop(self):
+        while 1:
+            t0 = time.monotonic()
+            try:
+                if await self._heartbeat_handle_cancellations():
+                    # got a cancellation event, fine to start another heartbeat immediately
+                    # since the cancellation queue should be empty on the worker server
+                    # however, we wait at least 1s to prevent short-circuiting the heartbeat loop
+                    # in case there is ever a bug. This means it will take at least 1s between
+                    # two subsequent cancellations on the same task at the moment
+                    await asyncio.sleep(1.0)
+                    continue
+            except Exception as exc:
+                # don't stop heartbeat loop if there are transient exceptions!
+                time_elapsed = time.monotonic() - t0
+                error = exc
+                logger.warning(f"Heartbeat attempt failed ({time_elapsed=}, {error=})")
+
+            heartbeat_duration = time.monotonic() - t0
+            time_until_next_hearbeat = max(0.0, HEARTBEAT_INTERVAL - heartbeat_duration)
+            await asyncio.sleep(time_until_next_hearbeat)
+
+    async def _heartbeat_handle_cancellations(self) -> bool:
+        # Return True if a cancellation event was received, in that case we shouldn't wait too long for another heartbeat
+
+        # Don't send heartbeats for tasks waiting to be checkpointed.
+        # Calling gRPC methods open new connections which block the
+        # checkpointing process.
+        if self._waiting_for_checkpoint:
+            return False
+
+        request = api_pb2.ContainerHeartbeatRequest(supports_graceful_input_cancellation=True)
+        if self.current_input_id is not None:
+            request.current_input_id = self.current_input_id
+        if self.current_input_started_at is not None:
+            request.current_input_started_at = self.current_input_started_at
+
+        # TODO(erikbern): capture exceptions?
+        response = await retry_transient_errors(
+            self._client.stub.ContainerHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT
+        )
+
+        if response.HasField("cancel_input_event"):
+            # Pause processing of the current input by signaling self a SIGUSR1.
+            input_ids_to_cancel = response.cancel_input_event.input_ids
+            if input_ids_to_cancel:
+                if self._input_concurrency > 1:
+                    logger.info(
+                        "Shutting down task to stop some subset of inputs (concurrent functions don't support fine-grained cancellation)"
+                    )
+                    # This is equivalent to a task cancellation or preemption from worker code,
+                    # except we do not send a SIGKILL to forcefully exit after 30 seconds.
+                    #
+                    # SIGINT always interrupts the main thread, but not any auxiliary threads. On a
+                    # sync function without concurrent inputs, this raises a KeyboardInterrupt. When
+                    # there are concurrent inputs, we cannot interrupt the thread pool, but the
+                    # interpreter stops waiting for daemon threads and exits. On async functions,
+                    # this signal lands outside the event loop, stopping `run_until_complete()`.
+                    os.kill(os.getpid(), signal.SIGINT)
+
+                elif self.current_input_id in input_ids_to_cancel:
+                    # This goes to a registered signal handler for sync Modal functions, or to the
+                    # `SignalHandlingEventLoop` for async functions.
+                    #
+                    # We only send this signal on functions that do not have concurrent inputs enabled.
+                    # This allows us to do fine-grained input cancellation. On sync functions, the
+                    # SIGUSR1 signal should interrupt the main thread where user code is running,
+                    # raising an InputCancellation() exception. On async functions, the signal should
+                    # reach a handler in SignalHandlingEventLoop, which cancels the task.
+                    os.kill(os.getpid(), signal.SIGUSR1)
+            return True
+        return False
+
+    @contextlib.asynccontextmanager
+    async def heartbeats(self):
+        async with TaskContext() as tc:
+            self._heartbeat_loop = t = tc.create_task(self._run_heartbeat_loop())
+            t.set_name("heartbeat loop")
+            try:
+                yield
+            finally:
+                t.cancel()
+
+    def stop_heartbeat(self):
+        if self._heartbeat_loop:
+            self._heartbeat_loop.cancel()
+
+    async def get_serialized_function(self) -> Tuple[Optional[Any], Callable]:
+        # Fetch the serialized function definition
+        request = api_pb2.FunctionGetSerializedRequest(function_id=self.function_id)
+        response = await self._client.stub.FunctionGetSerialized(request)
+        fun = self.deserialize(response.function_serialized)
+
+        if response.class_serialized:
+            cls = self.deserialize(response.class_serialized)
+        else:
+            cls = None
+
+        return cls, fun
+
+    def serialize(self, obj: Any) -> bytes:
+        return serialize(obj)
+
+    def deserialize(self, data: bytes) -> Any:
+        return deserialize(data, self._client)
+
+    @synchronizer.no_io_translation
+    def serialize_data_format(self, obj: Any, data_format: int) -> bytes:
+        return serialize_data_format(obj, data_format)
+
+    def deserialize_data_format(self, data: bytes, data_format: int) -> Any:
+        return deserialize_data_format(data, data_format, self._client)
+
+    async def get_data_in(self, function_call_id: str) -> AsyncIterator[Any]:
+        """Read from the `data_in` stream of a function call."""
+        async for data in _stream_function_call_data(self._client, function_call_id, "data_in"):
+            yield data
+
+    async def put_data_out(
+        self,
+        function_call_id: str,
+        start_index: int,
+        data_format: int,
+        messages_bytes: List[Any],
+    ) -> None:
+        """Put data onto the `data_out` stream of a function call.
+
+        This is used for generator outputs, which includes web endpoint responses. Note that this
+        was introduced as a performance optimization in client version 0.57, so older clients will
+        still use the previous Postgres-backed system based on `FunctionPutOutputs()`.
+        """
+        data_chunks: List[api_pb2.DataChunk] = []
+        for i, message_bytes in enumerate(messages_bytes):
+            chunk = api_pb2.DataChunk(data_format=data_format, index=start_index + i)  # type: ignore
+            if len(message_bytes) > MAX_OBJECT_SIZE_BYTES:
+                chunk.data_blob_id = await blob_upload(message_bytes, self._client.stub)
+            else:
+                chunk.data = message_bytes
+            data_chunks.append(chunk)
+
+        req = api_pb2.FunctionCallPutDataRequest(function_call_id=function_call_id, data_chunks=data_chunks)
+        await retry_transient_errors(self._client.stub.FunctionCallPutDataOut, req)
+
+    async def generator_output_task(self, function_call_id: str, data_format: int, message_rx: asyncio.Queue) -> None:
+        """Task that feeds generator outputs into a function call's `data_out` stream."""
+        index = 1
+        received_sentinel = False
+        while not received_sentinel:
+            message = await message_rx.get()
+            if message is self._GENERATOR_STOP_SENTINEL:
+                break
+            # ASGI 'http.response.start' and 'http.response.body' msgs are observed to be separated by 1ms.
+            # If we don't sleep here for 1ms we end up with an extra call to .put_data_out().
+            if index == 1:
+                await asyncio.sleep(0.001)
+            messages_bytes = [serialize_data_format(message, data_format)]
+            total_size = len(messages_bytes[0]) + 512
+            while total_size < 16 * 1024 * 1024:  # 16 MiB, maximum size in a single message
+                try:
+                    message = message_rx.get_nowait()
+                except asyncio.QueueEmpty:
+                    break
+                if message is self._GENERATOR_STOP_SENTINEL:
+                    received_sentinel = True
+                    break
+                else:
+                    messages_bytes.append(serialize_data_format(message, data_format))
+                    total_size += len(messages_bytes[-1]) + 512  # 512 bytes for estimated framing overhead
+            await self.put_data_out(function_call_id, index, data_format, messages_bytes)
+            index += len(messages_bytes)
+
+    async def _queue_create(self, size: int) -> asyncio.Queue:
+        """Create a queue, on the synchronicity event loop (needed on Python 3.8 and 3.9)."""
+        return asyncio.Queue(size)
+
+    async def _queue_put(self, queue: asyncio.Queue, value: Any) -> None:
+        """Put a value onto a queue, using the synchronicity event loop."""
+        await queue.put(value)
+
+    async def populate_input_blobs(self, item: api_pb2.FunctionInput):
+        args = await blob_download(item.args_blob_id, self._client.stub)
+
+        # Mutating
+        item.ClearField("args_blob_id")
+        item.args = args
+        return item
+
+    def get_average_call_time(self) -> float:
+        if self.calls_completed == 0:
+            return 0
+
+        return self.total_user_time / self.calls_completed
+
+    def get_max_inputs_to_fetch(self):
+        if self.calls_completed == 0:
+            return 1
+
+        return math.ceil(RTT_S / max(self.get_average_call_time(), 1e-6))
+
+    @synchronizer.no_io_translation
+    async def _generate_inputs(self) -> AsyncIterator[Tuple[str, str, api_pb2.FunctionInput]]:
+        request = api_pb2.FunctionGetInputsRequest(function_id=self.function_id)
+        eof_received = False
+        iteration = 0
+        while not eof_received and _container_app.fetching_inputs:
+            request.average_call_time = self.get_average_call_time()
+            request.max_values = self.get_max_inputs_to_fetch()  # Deprecated; remove.
+            request.input_concurrency = self._input_concurrency
+
+            await self._semaphore.acquire()
+            yielded = False
+            try:
+                # If number of active inputs is at max queue size, this will block.
+                iteration += 1
+                response: api_pb2.FunctionGetInputsResponse = await retry_transient_errors(
+                    self._client.stub.FunctionGetInputs, request
+                )
+
+                if response.rate_limit_sleep_duration:
+                    logger.info(
+                        "Task exceeded rate limit, sleeping for %.2fs before trying again."
+                        % response.rate_limit_sleep_duration
+                    )
+                    await asyncio.sleep(response.rate_limit_sleep_duration)
+                elif response.inputs:
+                    # for input cancellations and concurrency logic we currently assume
+                    # that there is no input buffering in the container
+                    assert len(response.inputs) == 1
+
+                    for item in response.inputs:
+                        if item.kill_switch:
+                            logger.debug(f"Task {self.task_id} input kill signal input.")
+                            eof_received = True
+                            break
+                        if item.input_id in self.cancelled_input_ids:
+                            continue
+
+                        # If we got a pointer to a blob, download it from S3.
+                        if item.input.WhichOneof("args_oneof") == "args_blob_id":
+                            input_pb = await self.populate_input_blobs(item.input)
+                        else:
+                            input_pb = item.input
+
+                        # If yielded, allow semaphore to be released via complete_call
+                        yield (item.input_id, item.function_call_id, input_pb)
+                        yielded = True
+
+                        # We only support max_inputs = 1 at the moment
+                        if item.input.final_input or self.function_def.max_inputs == 1:
+                            eof_received = True
+                            break
+            finally:
+                if not yielded:
+                    self._semaphore.release()
+
+    @synchronizer.no_io_translation
+    async def run_inputs_outputs(self, input_concurrency: int = 1) -> AsyncIterator[Tuple[str, str, Any, Any]]:
+        # Ensure we do not fetch new inputs when container is too busy.
+        # Before trying to fetch an input, acquire the semaphore:
+        # - if no input is fetched, release the semaphore.
+        # - or, when the output for the fetched input is sent, release the semaphore.
+        self._input_concurrency = input_concurrency
+        self._semaphore = asyncio.Semaphore(input_concurrency)
+
+        try:
+            async for input_id, function_call_id, input_pb in self._generate_inputs():
+                args, kwargs = self.deserialize(input_pb.args) if input_pb.args else ((), {})
+                self.current_input_id, self.current_input_started_at = (input_id, time.time())
+                yield input_id, function_call_id, args, kwargs
+                self.current_input_id, self.current_input_started_at = (None, None)
+        finally:
+            # collect all active input slots, meaning all inputs have wrapped up.
+            for _ in range(input_concurrency):
+                await self._semaphore.acquire()
+
+    async def _push_output(self, input_id, started_at: float, data_format=api_pb2.DATA_FORMAT_UNSPECIFIED, **kwargs):
+        # upload data to S3 if too big.
+        if "data" in kwargs and kwargs["data"] and len(kwargs["data"]) > MAX_OBJECT_SIZE_BYTES:
+            data_blob_id = await blob_upload(kwargs["data"], self._client.stub)
+            # mutating kwargs.
+            del kwargs["data"]
+            kwargs["data_blob_id"] = data_blob_id
+
+        output = api_pb2.FunctionPutOutputsItem(
+            input_id=input_id,
+            input_started_at=started_at,
+            output_created_at=time.time(),
+            result=api_pb2.GenericResult(**kwargs),
+            data_format=data_format,
+        )
+
+        await retry_transient_errors(
+            self._client.stub.FunctionPutOutputs,
+            api_pb2.FunctionPutOutputsRequest(outputs=[output]),
+            additional_status_codes=[Status.RESOURCE_EXHAUSTED],
+            max_retries=None,  # Retry indefinitely, trying every 1s.
+        )
+
+    def serialize_exception(self, exc: BaseException) -> Optional[bytes]:
+        try:
+            return self.serialize(exc)
+        except Exception as serialization_exc:
+            logger.info(f"Failed to serialize exception {exc}: {serialization_exc}")
+            # We can't always serialize exceptions.
+            return None
+
+    def serialize_traceback(self, exc: BaseException) -> Tuple[Optional[bytes], Optional[bytes]]:
+        serialized_tb, tb_line_cache = None, None
+
+        try:
+            tb_dict, line_cache = extract_traceback(exc, self.task_id)
+            serialized_tb = self.serialize(tb_dict)
+            tb_line_cache = self.serialize(line_cache)
+        except Exception:
+            logger.info("Failed to serialize exception traceback.")
+
+        return serialized_tb, tb_line_cache
+
+    @contextlib.asynccontextmanager
+    async def handle_user_exception(self) -> AsyncGenerator[None, None]:
+        """Sets the task as failed in a way where it's not retried.
+
+        Used for handling exceptions from container lifecycle methods at the moment, which should
+        trigger a task failure state.
+        """
+        try:
+            yield
+        except KeyboardInterrupt:
+            # Send no task result in case we get sigint:ed by the runner
+            # The status of the input should have been handled externally already in that case
+            raise
+        except BaseException as exc:
+            # Since this is on a different thread, sys.exc_info() can't find the exception in the stack.
+            traceback.print_exception(type(exc), exc, exc.__traceback__)
+
+            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
+
+            result = api_pb2.GenericResult(
+                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
+                data=self.serialize_exception(exc),
+                exception=repr(exc),
+                traceback="".join(traceback.format_exception(type(exc), exc, exc.__traceback__)),
+                serialized_tb=serialized_tb,
+                tb_line_cache=tb_line_cache,
+            )
+
+            req = api_pb2.TaskResultRequest(result=result)
+            await retry_transient_errors(self._client.stub.TaskResult, req)
+
+            # Shut down the task gracefully
+            raise UserException()
+
+    @contextlib.asynccontextmanager
+    async def handle_input_exception(self, input_id, started_at: float) -> AsyncGenerator[None, None]:
+        """Handle an exception while processing a function input."""
+        try:
+            yield
+        except KeyboardInterrupt:
+            raise
+        except (InputCancellation, asyncio.CancelledError):
+            # just skip creating any output for this input and keep going with the next instead
+            # it should have been marked as cancelled already in the backend at this point so it
+            # won't be retried
+            logger.warning(f"The current input ({input_id=}) was cancelled by a user request")
+            await self.complete_call(started_at)
+            return
+        except BaseException as exc:
+            # print exception so it's logged
+            traceback.print_exc()
+            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
+
+            # Note: we're not serializing the traceback since it contains
+            # local references that means we can't unpickle it. We *are*
+            # serializing the exception, which may have some issues (there
+            # was an earlier note about it that it might not be possible
+            # to unpickle it in some cases). Let's watch out for issues.
+            await self._push_output(
+                input_id,
+                started_at=started_at,
+                data_format=api_pb2.DATA_FORMAT_PICKLE,
+                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
+                data=self.serialize_exception(exc),
+                exception=repr(exc),
+                traceback=traceback.format_exc(),
+                serialized_tb=serialized_tb,
+                tb_line_cache=tb_line_cache,
+            )
+            await self.complete_call(started_at)
+
+    async def complete_call(self, started_at):
+        self.total_user_time += time.time() - started_at
+        self.calls_completed += 1
+        self._semaphore.release()
+
+    @synchronizer.no_io_translation
+    async def push_output(self, input_id, started_at: float, data: Any, data_format: int) -> None:
+        await self._push_output(
+            input_id,
+            started_at=started_at,
+            data_format=data_format,
+            status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
+            data=self.serialize_data_format(data, data_format),
+        )
+        await self.complete_call(started_at)
+
+    async def restore(self) -> None:
+        # Busy-wait for restore. `/__modal/restore-state.json` is created
+        # by the worker process with updates to the container config.
+        restored_path = Path(config.get("restore_state_path"))
+        start = time.perf_counter()
+        while not restored_path.exists():
+            logger.debug(f"Waiting for restore (elapsed={time.perf_counter() - start:.3f}s)")
+            await asyncio.sleep(0.01)
+            continue
+
+        logger.debug("Container: restored")
+
+        # Look for state file and create new client with updated credentials.
+        # State data is serialized with key-value pairs, example: {"task_id": "tk-000"}
+        with restored_path.open("r") as file:
+            restored_state = json.load(file)
+
+        # Local FunctionIOManager state.
+        for key in ["task_id", "function_id"]:
+            if value := restored_state.get(key):
+                logger.debug(f"Updating FunctionIOManager.{key} = {value}")
+                setattr(self, key, restored_state[key])
+
+        # Env vars and global state.
+        for key, value in restored_state.items():
+            # Empty string indicates that value does not need to be updated.
+            if value != "":
+                config.override_locally(key, value)
+
+        # Restore input to default state.
+        self.current_input_id = None
+        self.current_input_started_at = None
+
+        self._client = await _Client.from_env()
+        self._waiting_for_checkpoint = False
+
+    async def checkpoint(self) -> None:
+        """Message server indicating that function is ready to be checkpointed."""
+        if self.checkpoint_id:
+            logger.debug(f"Checkpoint ID: {self.checkpoint_id}")
+
+        await self._client.stub.ContainerCheckpoint(
+            api_pb2.ContainerCheckpointRequest(checkpoint_id=self.checkpoint_id)
+        )
+
+        self._waiting_for_checkpoint = True
+        await self._client._close()
+
+        logger.debug("Checkpointing request sent. Connection closed.")
+        await self.restore()
+
+    async def volume_commit(self, volume_ids: List[str]) -> None:
+        """
+        Perform volume commit for given `volume_ids`.
+        Only used on container exit to persist uncommitted changes on behalf of user.
+        """
+        if not volume_ids:
+            return
+        await asyncify(os.sync)()
+        results = await asyncio.gather(
+            *[
+                retry_transient_errors(
+                    self._client.stub.VolumeCommit,
+                    api_pb2.VolumeCommitRequest(volume_id=v_id),
+                    max_retries=9,
+                    base_delay=0.25,
+                    max_delay=256,
+                    delay_factor=2,
+                )
+                for v_id in volume_ids
+            ],
+            return_exceptions=True,
+        )
+        for volume_id, res in zip(volume_ids, results):
+            if isinstance(res, Exception):
+                logger.error(f"modal.Volume background commit failed for {volume_id}. Exception: {res}")
+            else:
+                logger.debug(f"modal.Volume background commit success for {volume_id}.")
+
+
+FunctionIOManager = synchronize_api(_FunctionIOManager)
+
+
 def call_function_sync(
-    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
+    function_io_manager,  #: FunctionIOManager,  TODO: this type is generated at runtime
     imp_fun: ImportedFunction,
 ):
     def run_input(input_id: str, function_call_id: str, args: Any, kwargs: Any) -> None:
         started_at = time.time()
         reset_context = _set_current_context_ids(input_id, function_call_id)
-        with container_io_manager.handle_input_exception(input_id, started_at):
+        with function_io_manager.handle_input_exception(input_id, started_at):
             logger.debug(f"Starting input {input_id} (sync)")
             res = imp_fun.fun(*args, **kwargs)
             logger.debug(f"Finished input {input_id} (sync)")
 
             # TODO(erikbern): any exception below shouldn't be considered a user exception
             if imp_fun.is_generator:
                 if not inspect.isgenerator(res):
                     raise InvalidError(f"Generator function returned value of type {type(res)}")
 
                 # Send up to this many outputs at a time.
-                generator_queue: asyncio.Queue[Any] = container_io_manager._queue_create(1024)
-                generator_output_task = container_io_manager.generator_output_task(
+                generator_queue: asyncio.Queue[Any] = function_io_manager._queue_create(1024)
+                generator_output_task = function_io_manager.generator_output_task(
                     function_call_id,
                     imp_fun.data_format,
                     generator_queue,
                     _future=True,  # Synchronicity magic to return a future.
                 )
 
                 item_count = 0
                 for value in res:
-                    container_io_manager._queue_put(generator_queue, value)
+                    function_io_manager._queue_put(generator_queue, value)
                     item_count += 1
 
-                container_io_manager._queue_put(generator_queue, _ContainerIOManager._GENERATOR_STOP_SENTINEL)
+                function_io_manager._queue_put(generator_queue, _FunctionIOManager._GENERATOR_STOP_SENTINEL)
                 generator_output_task.result()  # Wait to finish sending generator outputs.
                 message = api_pb2.GeneratorDone(items_total=item_count)
-                container_io_manager.push_output(input_id, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE)
+                function_io_manager.push_output(input_id, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE)
             else:
                 if inspect.iscoroutine(res) or inspect.isgenerator(res) or inspect.isasyncgen(res):
                     raise InvalidError(
                         f"Sync (non-generator) function return value of type {type(res)}."
                         " You might need to use @stub.function(..., is_generator=True)."
                     )
-                container_io_manager.push_output(input_id, started_at, res, imp_fun.data_format)
+                function_io_manager.push_output(input_id, started_at, res, imp_fun.data_format)
         reset_context()
 
     if imp_fun.input_concurrency > 1:
         # We can't use `concurrent.futures.ThreadPoolExecutor` here because in Python 3.11+, this
         # class has no workaround that allows us to exit the Python interpreter process without
         # waiting for the worker threads to finish. We need this behavior on SIGINT.
 
@@ -182,109 +713,122 @@
                     run_input(*args)
                 except BaseException:
                     # This should basically never happen, since only KeyboardInterrupt is the only error that can
                     # bubble out of from handle_input_exception and those wouldn't be raised outside the main thread
                     pass
                 inputs.task_done()
 
-        for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs(
+        for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs(
             imp_fun.input_concurrency
         ):
             if spawned_workers < imp_fun.input_concurrency:
                 threading.Thread(target=worker_thread, daemon=True).start()
                 spawned_workers += 1
             inputs.put((input_id, function_call_id, args, kwargs))
 
         finished.set()
         inputs.join()
 
     else:
-        for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs(
+        for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs(
             imp_fun.input_concurrency
         ):
             try:
                 run_input(input_id, function_call_id, args, kwargs)
             except:
                 raise
 
 
 async def call_function_async(
-    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
+    function_io_manager,  #: FunctionIOManager,  TODO: this type is generated at runtime
     imp_fun: ImportedFunction,
 ):
     async def run_input(input_id: str, function_call_id: str, args: Any, kwargs: Any) -> None:
         started_at = time.time()
         reset_context = _set_current_context_ids(input_id, function_call_id)
-        async with container_io_manager.handle_input_exception.aio(input_id, started_at):
+        async with function_io_manager.handle_input_exception.aio(input_id, started_at):
             logger.debug(f"Starting input {input_id} (async)")
             res = imp_fun.fun(*args, **kwargs)
             logger.debug(f"Finished input {input_id} (async)")
 
             # TODO(erikbern): any exception below shouldn't be considered a user exception
             if imp_fun.is_generator:
                 if not inspect.isasyncgen(res):
                     raise InvalidError(f"Async generator function returned value of type {type(res)}")
 
                 # Send up to this many outputs at a time.
-                generator_queue: asyncio.Queue[Any] = await container_io_manager._queue_create.aio(1024)
+                generator_queue: asyncio.Queue[Any] = await function_io_manager._queue_create.aio(1024)
                 generator_output_task = asyncio.create_task(
-                    container_io_manager.generator_output_task.aio(
+                    function_io_manager.generator_output_task.aio(
                         function_call_id,
                         imp_fun.data_format,
                         generator_queue,
                     )
                 )
 
                 item_count = 0
                 async for value in res:
-                    await container_io_manager._queue_put.aio(generator_queue, value)
+                    await function_io_manager._queue_put.aio(generator_queue, value)
                     item_count += 1
 
-                await container_io_manager._queue_put.aio(generator_queue, _ContainerIOManager._GENERATOR_STOP_SENTINEL)
+                await function_io_manager._queue_put.aio(generator_queue, _FunctionIOManager._GENERATOR_STOP_SENTINEL)
                 await generator_output_task  # Wait to finish sending generator outputs.
                 message = api_pb2.GeneratorDone(items_total=item_count)
-                await container_io_manager.push_output.aio(
+                await function_io_manager.push_output.aio(
                     input_id, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE
                 )
             else:
                 if not inspect.iscoroutine(res) or inspect.isgenerator(res) or inspect.isasyncgen(res):
                     raise InvalidError(
                         f"Async (non-generator) function returned value of type {type(res)}"
                         " You might need to use @stub.function(..., is_generator=True)."
                     )
                 value = await res
-                await container_io_manager.push_output.aio(input_id, started_at, value, imp_fun.data_format)
+                await function_io_manager.push_output.aio(input_id, started_at, value, imp_fun.data_format)
         reset_context()
 
     if imp_fun.input_concurrency > 1:
         # all run_input coroutines will have completed by the time we leave the execution context
         # but the wrapping *tasks* may not yet have been resolved, so we add a 0.01s
         # for them to resolve gracefully:
         async with TaskContext(0.01) as execution_context:
-            async for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs.aio(
+            async for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs.aio(
                 imp_fun.input_concurrency
             ):
                 # Note that run_inputs_outputs will not return until the concurrency semaphore has
                 # released all its slots so that they can be acquired by the run_inputs_outputs finalizer
                 # This prevents leaving the execution_context before outputs have been created
                 # TODO: refactor to make this a bit more easy to follow?
                 execution_context.create_task(run_input(input_id, function_call_id, args, kwargs))
     else:
-        async for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs.aio(
+        async for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs.aio(
             imp_fun.input_concurrency
         ):
             await run_input(input_id, function_call_id, args, kwargs)
 
 
+@dataclass
+class ImportedFunction:
+    obj: Any
+    fun: Callable
+    stub: Optional[_Stub]
+    is_async: bool
+    is_generator: bool
+    data_format: int  # api_pb2.DataFormat
+    input_concurrency: int
+    is_auto_snapshot: bool
+    function: _Function
+
+
 def import_function(
     function_def: api_pb2.Function,
     ser_cls,
     ser_fun,
     ser_params: Optional[bytes],
-    container_io_manager,
+    function_io_manager,
     client: Client,
 ) -> ImportedFunction:
     """Imports a function dynamically, and locates the stub.
 
     This is somewhat complex because we're dealing with 3 quite different type of functions:
     1. Functions defined in global scope and decorated in global scope (Function objects)
     2. Functions defined in global scope but decorated elsewhere (these will be raw callables)
@@ -305,15 +849,15 @@
     thread. This is so that any user code running in global scope (which executes as a part of
     the import) runs on the right thread.
     """
     module: Optional[ModuleType] = None
     cls: Optional[Type] = None
     fun: Callable
     function: Optional[_Function] = None
-    active_app: Optional[_App] = None
+    active_stub: Optional[_Stub] = None
     pty_info: api_pb2.PTYInfo = function_def.pty_info
 
     if ser_fun is not None:
         # This is a serialized function we already fetched from the server
         cls, fun = ser_cls, ser_fun
     else:
         # Load the module dynamically
@@ -327,49 +871,49 @@
         if len(parts) == 1:
             # This is a function
             cls = None
             f = getattr(module, qual_name)
             if isinstance(f, Function):
                 function = synchronizer._translate_in(f)
                 fun = function.get_raw_f()
-                active_app = function._stub
+                active_stub = function._stub
             else:
                 fun = f
         elif len(parts) == 2:
             # This is a method on a class
             cls_name, fun_name = parts
             cls = getattr(module, cls_name)
             if isinstance(cls, Cls):
                 # The cls decorator is in global scope
                 _cls = synchronizer._translate_in(cls)
                 fun = _cls._callables[fun_name]
                 function = _cls._functions.get(fun_name)
-                active_app = _cls._stub
+                active_stub = _cls._stub
             else:
                 # This is a raw class
                 fun = getattr(cls, fun_name)
         else:
             raise InvalidError(f"Invalid function qualname {qual_name}")
 
     # If the cls/function decorator was applied in local scope, but the stub is global, we can look it up
-    if active_app is None:
+    if active_stub is None:
         # This branch is reached in the special case that the imported function is 1) not serialized, and 2) isn't a FunctionHandle - i.e, not decorated at definition time
         # Look at all instantiated stubs - if there is only one with the indicated name, use that one
         stub_name: Optional[str] = function_def.stub_name or None  # coalesce protobuf field to None
-        matching_stubs = _App._all_apps.get(stub_name, [])
+        matching_stubs = _Stub._all_stubs.get(stub_name, [])
         if len(matching_stubs) > 1:
             if stub_name is not None:
                 warning_sub_message = f"stub with the same name ('{stub_name}')"
             else:
                 warning_sub_message = "unnamed stub"
             logger.warning(
                 f"You have more than one {warning_sub_message}. It's recommended to name all your Stubs uniquely when using multiple stubs"
             )
         elif len(matching_stubs) == 1:
-            (active_app,) = matching_stubs
+            (active_stub,) = matching_stubs
         # there could also technically be zero found stubs, but that should probably never be an issue since that would mean user won't use is_inside or other function handles anyway
 
     # Check this property before we turn it into a method (overriden by webhooks)
     is_async = get_is_async(fun)
 
     # Use the function definition for whether this is a generator (overriden by webhooks)
     is_generator = function_def.function_type == api_pb2.Function.FUNCTION_TYPE_GENERATOR
@@ -402,133 +946,122 @@
     if function_def.webhook_config.type:
         is_async = True
         is_generator = True
         data_format = api_pb2.DATA_FORMAT_ASGI
 
         if function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP:
             # Function returns an asgi_app, which we can use as a callable.
-            fun = asgi_app_wrapper(fun(), container_io_manager)
+            fun = asgi_app_wrapper(fun(), function_io_manager)
 
         elif function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP:
             # Function returns an wsgi_app, which we can use as a callable.
-            fun = wsgi_app_wrapper(fun(), container_io_manager)
+            fun = wsgi_app_wrapper(fun(), function_io_manager)
 
         elif function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION:
             # Function is a webhook without an ASGI app. Create one for it.
             fun = asgi_app_wrapper(
                 webhook_asgi_app(fun, function_def.webhook_config.method),
-                container_io_manager,
+                function_io_manager,
             )
 
         elif function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_WEB_SERVER:
             # Function spawns an HTTP web server listening at a port.
             fun()
 
             # We intentionally try to connect to the external interface instead of the loopback
             # interface here so users are forced to expose the server. This allows us to potentially
             # change the implementation to use an external bridge in the future.
             host = get_ip_address(b"eth0")
             port = function_def.webhook_config.web_server_port
             startup_timeout = function_def.webhook_config.web_server_startup_timeout
             wait_for_web_server(host, port, timeout=startup_timeout)
-            fun = asgi_app_wrapper(web_server_proxy(host, port), container_io_manager)
+            fun = asgi_app_wrapper(web_server_proxy(host, port), function_io_manager)
 
         else:
             raise InvalidError(f"Unrecognized web endpoint type {function_def.webhook_config.type}")
 
     return ImportedFunction(
         obj,
         fun,
-        active_app,
+        active_stub,
         is_async,
         is_generator,
         data_format,
         input_concurrency,
         function_def.is_auto_snapshot,
         function,
     )
 
 
 def call_lifecycle_functions(
     event_loop: UserCodeEventLoop,
-    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
-    funcs: Sequence[Callable],
+    function_io_manager,  #: FunctionIOManager,  TODO: this type is generated at runtime
+    funcs: Iterable[Callable],
 ) -> None:
     """Call function(s), can be sync or async, but any return values are ignored."""
-    with container_io_manager.handle_user_exception():
+    with function_io_manager.handle_user_exception():
         for func in funcs:
             # We are deprecating parameterized exit methods but want to gracefully handle old code.
             # We can remove this once the deprecation in the actual @exit decorator is enforced.
             args = (None, None, None) if method_has_params(func) else ()
             res = func(
                 *args
             )  # in case func is non-async, it's executed here and sigint will by default interrupt it using a KeyboardInterrupt exception
             if inspect.iscoroutine(res):
                 # if however func is async, we have to jump through some hoops
                 event_loop.run(res)
 
 
 def main(container_args: api_pb2.ContainerArguments, client: Client):
-    # This is a bit weird but we need both the blocking and async versions of ContainerIOManager.
+    # This is a bit weird but we need both the blocking and async versions of FunctionIOManager.
     # At some point, we should fix that by having built-in support for running "user code"
-    container_io_manager = ContainerIOManager(container_args, client)
+    function_io_manager = FunctionIOManager(container_args, client)
+
+    # Define a global app (need to do this before imports).
+    container_app: ContainerApp = function_io_manager.initialize_app()
 
-    with container_io_manager.heartbeats(), UserCodeEventLoop() as event_loop:
+    with function_io_manager.heartbeats(), UserCodeEventLoop() as event_loop:
         # If this is a serialized function, fetch the definition from the server
         if container_args.function_def.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED:
-            ser_cls, ser_fun = container_io_manager.get_serialized_function()
+            ser_cls, ser_fun = function_io_manager.get_serialized_function()
         else:
             ser_cls, ser_fun = None, None
 
         # Initialize the function, importing user code.
-        with container_io_manager.handle_user_exception():
+        with function_io_manager.handle_user_exception():
             imp_fun = import_function(
                 container_args.function_def,
                 ser_cls,
                 ser_fun,
                 container_args.serialized_params,
-                container_io_manager,
+                function_io_manager,
                 client,
             )
 
-        # Get ids and metadata for objects (primarily functions and classes) on the app
-        container_app: RunningApp = container_io_manager.get_app_objects()
-
         # Initialize objects on the stub.
-        # This is basically only functions and classes - anything else is deprecated and will be unsupported soon
-        if imp_fun.app is not None:
-            stub: App = synchronizer._translate_out(imp_fun.app, Interface.BLOCKING)
-            stub._init_container(client, container_app)
+        if imp_fun.stub is not None:
+            container_app.associate_stub_container(imp_fun.stub)
 
         # Hydrate all function dependencies.
         # TODO(erikbern): we an remove this once we
         # 1. Enable lazy hydration for all objects
         # 2. Fully deprecate .new() objects
         if imp_fun.function:
-            _client: _Client = synchronizer._translate_in(client)  # TODO(erikbern): ugly
             dep_object_ids: List[str] = [dep.object_id for dep in container_args.function_def.object_dependencies]
-            function_deps = imp_fun.function.deps(only_explicit_mounts=True)
-            if len(function_deps) != len(dep_object_ids):
-                raise ExecutionError(
-                    f"Function has {len(function_deps)} dependencies"
-                    f" but container got {len(dep_object_ids)} object ids."
-                )
-            for object_id, obj in zip(dep_object_ids, function_deps):
-                metadata: Message = container_app.object_handle_metadata[object_id]
-                obj._hydrate(object_id, _client, metadata)
+            container_app.hydrate_function_deps(imp_fun.function, dep_object_ids)
 
         # Identify all "enter" methods that need to run before we checkpoint.
         if imp_fun.obj is not None and not imp_fun.is_auto_snapshot:
             pre_checkpoint_methods = _find_callables_for_obj(imp_fun.obj, _PartialFunctionFlags.ENTER_PRE_CHECKPOINT)
-            call_lifecycle_functions(event_loop, container_io_manager, list(pre_checkpoint_methods.values()))
+            call_lifecycle_functions(event_loop, function_io_manager, pre_checkpoint_methods.values())
 
         # If this container is being used to create a checkpoint, checkpoint the container after
         # global imports and innitialization. Checkpointed containers run from this point onwards.
         if container_args.function_def.is_checkpointing_function:
-            container_io_manager.checkpoint()
+            function_io_manager.checkpoint()
 
         # Install hooks for interactive functions.
         if container_args.function_def.pty_info.pty_type != api_pb2.PTYInfo.PTY_TYPE_UNSPECIFIED:
 
             def breakpoint_wrapper():
                 # note: it would be nice to not have breakpoint_wrapper() included in the backtrace
                 interact()
@@ -537,45 +1070,45 @@
                 pdb.set_trace()
 
             sys.breakpointhook = breakpoint_wrapper
 
         # Identify the "enter" methods to run after resuming from a checkpoint.
         if imp_fun.obj is not None and not imp_fun.is_auto_snapshot:
             post_checkpoint_methods = _find_callables_for_obj(imp_fun.obj, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
-            call_lifecycle_functions(event_loop, container_io_manager, list(post_checkpoint_methods.values()))
+            call_lifecycle_functions(event_loop, function_io_manager, post_checkpoint_methods.values())
 
         # Execute the function.
         try:
             if imp_fun.is_async:
-                event_loop.run(call_function_async(container_io_manager, imp_fun))
+                event_loop.run(call_function_async(function_io_manager, imp_fun))
             else:
                 # Set up a signal handler for `SIGUSR1`, which gets translated to an InputCancellation
                 # during function execution. This is sent to cancel inputs from the user.
                 def _cancel_input_signal_handler(signum, stackframe):
                     raise InputCancellation("Input was cancelled by user")
 
                 signal.signal(signal.SIGUSR1, _cancel_input_signal_handler)
 
-                call_function_sync(container_io_manager, imp_fun)
+                call_function_sync(function_io_manager, imp_fun)
         finally:
             # Run exit handlers. From this point onward, ignore all SIGINT signals that come from
             # graceful shutdowns originating on the worker, as well as stray SIGUSR1 signals that
             # may have been sent to cancel inputs.
             int_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
             usr1_handler = signal.signal(signal.SIGUSR1, signal.SIG_IGN)
 
             try:
                 # Identify "exit" methods and run them.
                 if imp_fun.obj is not None and not imp_fun.is_auto_snapshot:
                     exit_methods = _find_callables_for_obj(imp_fun.obj, _PartialFunctionFlags.EXIT)
-                    call_lifecycle_functions(event_loop, container_io_manager, list(exit_methods.values()))
+                    call_lifecycle_functions(event_loop, function_io_manager, exit_methods.values())
 
                 # Finally, commit on exit to catch uncommitted volume changes and surface background
                 # commit errors.
-                container_io_manager.volume_commit(
+                function_io_manager.volume_commit(
                     [v.volume_id for v in container_args.function_def.volume_mounts if v.allow_background_commits]
                 )
             finally:
                 # Restore the original signal handler, needed for container_test hygiene since the
                 # test runs `main()` multiple times in the same process.
                 signal.signal(signal.SIGINT, int_handler)
                 signal.signal(signal.SIGUSR1, usr1_handler)
```

## modal/_resolver.py

```diff
@@ -1,27 +1,31 @@
 # Copyright Modal Labs 2023
 import asyncio
 import contextlib
 from asyncio import Future
-from typing import TYPE_CHECKING, Dict, Hashable, List, Optional
+from typing import TYPE_CHECKING, Dict, Hashable, List, Optional, TypeVar
 
 from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
-from .exception import ExecutionError, NotFoundError
-
 if TYPE_CHECKING:
+    from rich.spinner import Spinner
     from rich.tree import Tree
 
     from modal.object import _Object
+else:
+    Spinner = TypeVar("Spinner")
+    Tree = TypeVar("Tree")
+
+from modal.exception import ExecutionError, NotFoundError
 
 
 class StatusRow:
-    def __init__(self, progress: "Optional[Tree]"):
+    def __init__(self, progress: Optional[Tree]):
         from ._output import (
             step_progress,
         )
 
         self._spinner = None
         self._step_node = None
         if progress is not None:
@@ -39,14 +43,15 @@
 
         if self._step_node is not None:
             step_progress_update(self._spinner, message)
             self._step_node.label = step_completed(message, is_substep=True)
 
 
 class Resolver:
+    _tree: Tree
     _local_uuid_to_future: Dict[str, Future]
     _environment_name: Optional[str]
     _app_id: Optional[str]
     _deduplication_cache: Dict[Hashable, Future]
 
     def __init__(
         self,
```

## modal/_serialization.py

```diff
@@ -290,14 +290,17 @@
         assert isinstance(obj, api_pb2.GeneratorDone)
         return obj.SerializeToString(deterministic=True)
     else:
         raise InvalidError(f"Unknown data format {data_format!r}")
 
 
 def deserialize_data_format(s: bytes, data_format: int, client) -> Any:
+    if data_format == api_pb2.DATA_FORMAT_UNSPECIFIED:
+        # TODO: Remove this after Modal client version 0.52, when the data_format field is always set.
+        return deserialize(s, client)
     if data_format == api_pb2.DATA_FORMAT_PICKLE:
         return deserialize(s, client)
     elif data_format == api_pb2.DATA_FORMAT_ASGI:
         return _deserialize_asgi(api_pb2.Asgi.FromString(s))
     elif data_format == api_pb2.DATA_FORMAT_GENERATOR_DONE:
         return api_pb2.GeneratorDone.FromString(s)
     else:
```

## modal/app.py

```diff
@@ -1,816 +1,396 @@
 # Copyright Modal Labs 2022
-import inspect
-import typing
-from pathlib import PurePosixPath
-from typing import Any, AsyncGenerator, Callable, ClassVar, Dict, List, Optional, Sequence, Tuple, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, TypeVar
 
+from google.protobuf.empty_pb2 import Empty
 from google.protobuf.message import Message
-from synchronicity.async_wrap import asynccontextmanager
+from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
-from ._ipython import is_notebook
 from ._output import OutputManager
 from ._resolver import Resolver
 from ._utils.async_utils import synchronize_api
-from ._utils.function_utils import FunctionInfo
-from ._utils.mount_utils import validate_volumes
-from .app_utils import (  # noqa: F401
-    _list_apps,
-    list_apps,
-)
+from ._utils.grpc_utils import get_proto_oneof, retry_transient_errors
 from .client import _Client
-from .cloud_bucket_mount import _CloudBucketMount
-from .cls import _Cls
 from .config import logger
-from .exception import InvalidError, deprecation_error, deprecation_warning
-from .functions import _Function
-from .gpu import GPU_T
-from .image import _Image
-from .mount import _Mount
-from .network_file_system import _NetworkFileSystem
+from .exception import ExecutionError, InvalidError
 from .object import _Object
-from .partial_function import PartialFunction, _find_callables_for_cls, _PartialFunction, _PartialFunctionFlags
-from .proxy import _Proxy
-from .retries import Retries
-from .runner import _run_stub
-from .running_app import RunningApp
-from .sandbox import _Sandbox
-from .schedule import Schedule
-from .scheduler_placement import SchedulerPlacement
-from .secret import _Secret
-from .volume import _Volume
-
-_default_image: _Image = _Image.debian_slim()
-
-
-class _LocalEntrypoint:
-    _info: FunctionInfo
-    _stub: "_App"
-
-    def __init__(self, info, stub):
-        self._info = info  # type: ignore
-        self._stub = stub
 
-    def __call__(self, *args, **kwargs):
-        return self._info.raw_f(*args, **kwargs)
+if TYPE_CHECKING:
+    from .functions import _Function
 
-    @property
-    def info(self) -> FunctionInfo:
-        return self._info
-
-    @property
-    def stub(self) -> "_App":
-        return self._stub
-
-
-LocalEntrypoint = synchronize_api(_LocalEntrypoint)
-
-
-def check_sequence(items: typing.Sequence[typing.Any], item_type: typing.Type[typing.Any], error_msg: str):
-    if not isinstance(items, (list, tuple)):
-        raise InvalidError(error_msg)
-    if not all(isinstance(v, item_type) for v in items):
-        raise InvalidError(error_msg)
-
-
-CLS_T = typing.TypeVar("CLS_T", bound=typing.Type)
-
-
-class _App:
-    """A Modal app (formerly known as "stub") is a group of functions and classes
-    deployed together.
-
-    The stub object principally describes Modal objects (`Function`, `Image`,
-    `Secret`, etc.) associated with the application. It has three responsibilities:
-
-    * Syncing of identities across processes (your local Python interpreter and
-      every Modal worker active in your application).
-    * Making Objects stay alive and not be garbage collected for as long as the
-      app lives (see App lifetime below).
-    * Manage log collection for everything that happens inside your code.
+else:
+    _Function = TypeVar("_Function")
 
-    **Registering functions with an app**
 
-    The most common way to explicitly register an Object with an app is through the
-    `@stub.function()` decorator. It both registers the annotated function itself and
-    other passed objects, like schedules and secrets, with the app:
-
-    ```python
-    import modal
-
-    stub = modal.Stub()
-
-    @stub.function(
-        secrets=[modal.Secret.from_name("some_secret")],
-        schedule=modal.Period(days=1),
-    )
-    def foo():
-        pass
-    ```
-
-    In this example, the secret and schedule are registered with the app.
-    """
-
-    _name: Optional[str]
-    _description: Optional[str]
-    _indexed_objects: Dict[str, _Object]
-    _function_mounts: Dict[str, _Mount]
-    _mounts: Sequence[_Mount]
-    _secrets: Sequence[_Secret]
-    _volumes: Dict[Union[str, PurePosixPath], _Volume]
-    _web_endpoints: List[str]  # Used by the CLI
-    _local_entrypoints: Dict[str, _LocalEntrypoint]
-    _running_app: Optional[RunningApp]
-    _client: Optional[_Client]
-    _all_apps: ClassVar[Dict[Optional[str], List["_App"]]] = {}
+class _LocalApp:
+    _tag_to_object_id: Dict[str, str]
+    _client: _Client
+    _app_id: str
+    _app_page_url: str
+    _environment_name: str
+    _interactive: bool
 
     def __init__(
         self,
-        name: Optional[str] = None,
-        *,
-        image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)
-        mounts: Sequence[_Mount] = [],  # default mounts for all functions
-        secrets: Sequence[_Secret] = [],  # default secrets for all functions
-        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # default volumes for all functions
-        **kwargs: _Object,  # DEPRECATED: passing additional objects to the stub as kwargs is no longer supported
-    ) -> None:
-        """Construct a new app stub, optionally with default image, mounts, secrets, or volumes.
-
-        ```python notest
-        image = modal.Image.debian_slim().pip_install(...)
-        mount = modal.Mount.from_local_dir("./config")
-        secret = modal.Secret.from_name("my-secret")
-        volume = modal.Volume.from_name("my-data")
-        stub = modal.Stub(image=image, mounts=[mount], secrets=[secret], volumes={"/mnt/data": volume})
-        ```
-        """
-
-        self._name = name
-        self._description = name
-
-        check_sequence(mounts, _Mount, "`mounts=` has to be a list or tuple of Mount objects")
-        check_sequence(secrets, _Secret, "`secrets=` has to be a list or tuple of Secret objects")
-        validate_volumes(volumes)
-
-        if image is not None and not isinstance(image, _Image):
-            raise InvalidError("image has to be a modal Image or AioImage object")
-
-        if kwargs:
-            deprecation_error(
-                (2023, 12, 13),
-                "Passing additional objects to the stub constructor is deprecated."
-                f" Please remove the following parameters from your stub definition: {', '.join(kwargs)}."
-                " In most cases, persistent (named) objects can just be defined in the global scope.",
-            )
-
-        for k, v in kwargs.items():
-            self._validate_blueprint_value(k, v)
-
-        self._indexed_objects = kwargs
-        if image is not None:
-            self._indexed_objects["image"] = image  # backward compatibility since "image" used to be on the blueprint
-
-        self._mounts = mounts
-
-        self._secrets = secrets
-        self._volumes = volumes
-        self._local_entrypoints = {}
-        self._web_endpoints = []
-        self._running_app = None  # Set inside container, OR during the time an app is running locally
-        self._client = None
-
-        # Register this stub. This is used to look up the stub in the container, when we can't get it from the function
-        _App._all_apps.setdefault(self._name, []).append(self)
-
-    @property
-    def name(self) -> Optional[str]:
-        """The user-provided name of the Stub."""
-        return self._name
-
-    @property
-    def is_interactive(self) -> bool:
-        """Whether the current app for the stub is running in interactive mode."""
-        # return self._name
-        if self._running_app:
-            return self._running_app.interactive
-        else:
-            return False
+        client: _Client,
+        app_id: str,
+        app_page_url: str,
+        tag_to_object_id: Optional[Dict[str, str]] = None,
+        stub_name: Optional[str] = None,
+        environment_name: Optional[str] = None,
+        interactive: bool = False,
+    ):
+        """mdmd:hidden This is the app constructor. Users should not call this directly."""
+        self._app_id = app_id
+        self._app_page_url = app_page_url
+        self._client = client
+        self._tag_to_object_id = tag_to_object_id or {}
+        self._stub_name = stub_name
+        self._environment_name = environment_name
+        self._interactive = interactive
 
     @property
-    def app_id(self) -> Optional[str]:
-        """Return the app_id, if the stub is running."""
-        if self._running_app:
-            return self._running_app.app_id
-        else:
-            return None
+    def client(self) -> _Client:
+        """A reference to the running App's server client."""
+        return self._client
 
     @property
-    def description(self) -> Optional[str]:
-        """The Stub's `name`, if available, or a fallback descriptive identifier."""
-        return self._description
-
-    def set_description(self, description: str):
-        self._description = description
-
-    def _validate_blueprint_value(self, key: str, value: Any):
-        if not isinstance(value, _Object):
-            raise InvalidError(f"Stub attribute `{key}` with value {value!r} is not a valid Modal object")
-
-    def _add_object(self, tag, obj):
-        if self._running_app:
-            # If this is inside a container, then objects can be defined after app initialization.
-            # So we may have to initialize objects once they get bound to the stub.
-            if tag in self._running_app.tag_to_object_id:
-                object_id: str = self._running_app.tag_to_object_id[tag]
-                metadata: Message = self._running_app.object_handle_metadata[object_id]
-                obj._hydrate(object_id, self._client, metadata)
-
-        self._indexed_objects[tag] = obj
-
-    def __getitem__(self, tag: str):
-        """Stub assignments of the form `stub.x` or `stub["x"]` are deprecated!
-
-        The only use cases for these assignments is in conjunction with `.new()`, which is now
-        in itself deprecated. If you are constructing objects with `.from_name(...)`, there is no
-        need to assign those objects to the stub. Example:
-
-        ```python
-        d = modal.Dict.from_name("my-dict", create_if_missing=True)
-
-        @stub.function()
-        def f(x, y):
-            d[x] = y  # Refer to d in global scope
-        ```
-        """
-        deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-        return self._indexed_objects[tag]
-
-    def __setitem__(self, tag: str, obj: _Object):
-        deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-        self._validate_blueprint_value(tag, obj)
-        # Deprecated ?
-        self._add_object(tag, obj)
-
-    def __getattr__(self, tag: str) -> _Object:
-        # TODO(erikbern): remove this method later
-        assert isinstance(tag, str)
-        if tag.startswith("__"):
-            # Hacky way to avoid certain issues, e.g. pickle will try to look this up
-            raise AttributeError(f"App has no member {tag}")
-        if tag not in self._indexed_objects:
-            # Primarily to make hasattr work
-            raise AttributeError(f"App has no member {tag}")
-        obj: _Object = self._indexed_objects[tag]
-        deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-        return obj
-
-    def __setattr__(self, tag: str, obj: _Object):
-        # TODO(erikbern): remove this method later
-        # Note that only attributes defined in __annotations__ are set on the object itself,
-        # everything else is registered on the indexed_objects
-        if tag in self.__annotations__:
-            object.__setattr__(self, tag, obj)
-        elif tag == "image":
-            self._indexed_objects["image"] = obj
-        else:
-            self._validate_blueprint_value(tag, obj)
-            deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-            self._add_object(tag, obj)
+    def app_id(self) -> str:
+        """A unique identifier for this running App."""
+        return self._app_id
 
     @property
-    def image(self) -> _Image:
-        # Exists to get the type inference working for `stub.image`
-        # Will also keep this one after we remove [get/set][item/attr]
-        return self._indexed_objects["image"]
-
-    @image.setter
-    def image(self, value):
-        self._indexed_objects["image"] = value
-
-    def _uncreate_all_objects(self):
-        # TODO(erikbern): this doesn't unhydrate objects that aren't tagged
-        for obj in self._indexed_objects.values():
-            obj._unhydrate()
-
-    def is_inside(self, image: Optional[_Image] = None):
-        """Deprecated: use `Image.imports()` instead! Usage:
-        ```
-        my_image = modal.Image.debian_slim().pip_install("torch")
-        with my_image.imports():
-            import torch
-        ```
-        """
-        deprecation_error((2023, 11, 8), _App.is_inside.__doc__)
-
-    @asynccontextmanager
-    async def _set_local_app(self, client: _Client, app: RunningApp) -> AsyncGenerator[None, None]:
-        self._client = client
-        self._running_app = app
-        try:
-            yield
-        finally:
-            self._client = None
-            self._running_app = None
+    def is_interactive(self) -> bool:
+        return self._interactive
 
-    @asynccontextmanager
-    async def run(
+    async def _create_all_objects(
         self,
-        client: Optional[_Client] = None,
-        stdout=None,
-        show_progress: bool = True,
-        detach: bool = False,
+        indexed_objects: Dict[str, _Object],
+        new_app_state: int,
+        environment_name: str,
         output_mgr: Optional[OutputManager] = None,
-    ) -> AsyncGenerator["_App", None]:
-        """Context manager that runs an app on Modal.
+    ):  # api_pb2.AppState.V
+        """Create objects that have been defined but not created on the server."""
+        resolver = Resolver(
+            self._client,
+            output_mgr=output_mgr,
+            environment_name=environment_name,
+            app_id=self.app_id,
+        )
+        with resolver.display():
+            # Get current objects, and reset all objects
+            tag_to_object_id = self._tag_to_object_id
+            self._tag_to_object_id = {}
+
+            # Assign all objects
+            for tag, obj in indexed_objects.items():
+                # Reset object_id in case the app runs twice
+                # TODO(erikbern): clean up the interface
+                obj._unhydrate()
+
+            # Preload all functions to make sure they have ids assigned before they are loaded.
+            # This is important to make sure any enclosed function handle references in serialized
+            # functions have ids assigned to them when the function is serialized.
+            # Note: when handles/objs are merged, all objects will need to get ids pre-assigned
+            # like this in order to be referrable within serialized functions
+            for tag, obj in indexed_objects.items():
+                existing_object_id = tag_to_object_id.get(tag)
+                # Note: preload only currently implemented for Functions, returns None otherwise
+                # this is to ensure that directly referenced functions from the global scope has
+                # ids associated with them when they are serialized into other functions
+                await resolver.preload(obj, existing_object_id)
+                if obj.object_id is not None:
+                    tag_to_object_id[tag] = obj.object_id
+
+            for tag, obj in indexed_objects.items():
+                existing_object_id = tag_to_object_id.get(tag)
+                await resolver.load(obj, existing_object_id)
+                self._tag_to_object_id[tag] = obj.object_id
+
+        # Create the app (and send a list of all tagged obs)
+        # TODO(erikbern): we should delete objects from a previous version that are no longer needed
+        # We just delete them from the app, but the actual objects will stay around
+        indexed_object_ids = self._tag_to_object_id
+        assert indexed_object_ids == self._tag_to_object_id
+        all_objects = resolver.objects()
+
+        unindexed_object_ids = list(set(obj.object_id for obj in all_objects) - set(self._tag_to_object_id.values()))
+        req_set = api_pb2.AppSetObjectsRequest(
+            app_id=self._app_id,
+            indexed_object_ids=indexed_object_ids,
+            unindexed_object_ids=unindexed_object_ids,
+            new_app_state=new_app_state,  # type: ignore
+        )
+        await retry_transient_errors(self._client.stub.AppSetObjects, req_set)
 
-        Use this as the main entry point for your Modal application. All calls
-        to Modal functions should be made within the scope of this context
-        manager, and they will correspond to the current app.
-
-        Note that this method used to return a separate "App" object. This is
-        no longer useful since you can use the stub itself for access to all
-        objects. For backwards compatibility reasons, it returns the same stub.
-        """
-        # TODO(erikbern): deprecate this one too?
-        async with _run_stub(self, client, stdout, show_progress, detach, output_mgr):
-            yield self
-
-    def _get_default_image(self):
-        if "image" in self._indexed_objects:
-            return self._indexed_objects["image"]
-        else:
-            return _default_image
+    async def disconnect(
+        self, reason: "Optional[api_pb2.AppDisconnectReason.ValueType]" = None, exc_str: Optional[str] = None
+    ):
+        """Tell the server the client has disconnected for this app. Terminates all running tasks
+        for ephemeral apps."""
+
+        if exc_str:
+            exc_str = exc_str[:1000]  # Truncate to 1000 chars
+
+        logger.debug("Sending app disconnect/stop request")
+        req_disconnect = api_pb2.AppClientDisconnectRequest(app_id=self._app_id, reason=reason, exception=exc_str)
+        await retry_transient_errors(self._client.stub.AppClientDisconnect, req_disconnect)
+        logger.debug("App disconnected")
+
+    async def stop(self):
+        """Tell the server to stop this app, terminating all running tasks."""
+        req_disconnect = api_pb2.AppStopRequest(app_id=self._app_id, source=api_pb2.APP_STOP_SOURCE_PYTHON_CLIENT)
+        await retry_transient_errors(self._client.stub.AppStop, req_disconnect)
+
+    def log_url(self):
+        """URL link to a running app's logs page in the Modal dashboard."""
+        return self._app_page_url
+
+    @staticmethod
+    async def _init_existing(client: _Client, existing_app_id: str) -> "_LocalApp":
+        # Get all the objects first
+        obj_req = api_pb2.AppGetObjectsRequest(app_id=existing_app_id)
+        obj_resp = await retry_transient_errors(client.stub.AppGetObjects, obj_req)
+        app_page_url = f"https://modal.com/apps/{existing_app_id}"  # TODO (elias): this should come from the backend
+        object_ids = {item.tag: item.object.object_id for item in obj_resp.items}
+        return _LocalApp(client, existing_app_id, app_page_url, tag_to_object_id=object_ids)
+
+    @staticmethod
+    async def _init_new(
+        client: _Client,
+        description: str,
+        app_state: int,
+        environment_name: str = "",
+        interactive=False,
+    ) -> "_LocalApp":
+        app_req = api_pb2.AppCreateRequest(
+            description=description,
+            environment_name=environment_name,
+            app_state=app_state,
+        )
+        app_resp = await retry_transient_errors(client.stub.AppCreate, app_req)
+        app_page_url = app_resp.app_logs_url
+        logger.debug(f"Created new app with id {app_resp.app_id}")
+        return _LocalApp(
+            client, app_resp.app_id, app_page_url, environment_name=environment_name, interactive=interactive
+        )
 
-    def _get_watch_mounts(self):
-        all_mounts = [
-            *self._mounts,
-        ]
-        for function in self.registered_functions.values():
-            all_mounts.extend(function._all_mounts)
-
-        return [m for m in all_mounts if m.is_local()]
-
-    def _add_function(self, function: _Function):
-        if function.tag in self._indexed_objects:
-            old_function = self._indexed_objects[function.tag]
-            if isinstance(old_function, _Function):
-                if not is_notebook():
-                    logger.warning(
-                        f"Warning: Tag '{function.tag}' collision!"
-                        f" Overriding existing function [{old_function._info.module_name}].{old_function._info.function_name}"
-                        f" with new function [{function._info.module_name}].{function._info.function_name}"
-                    )
-            else:
-                logger.warning(f"Warning: tag {function.tag} exists but is overridden by function")
+    @staticmethod
+    async def _init_from_name(
+        client: _Client,
+        name: str,
+        namespace,
+        environment_name: str = "",
+    ):
+        # Look up any existing deployment
+        app_req = api_pb2.AppGetByDeploymentNameRequest(
+            name=name,
+            namespace=namespace,
+            environment_name=environment_name,
+        )
+        app_resp = await retry_transient_errors(client.stub.AppGetByDeploymentName, app_req)
+        existing_app_id = app_resp.app_id or None
 
-        self._add_object(function.tag, function)
+        # Grab the app
+        if existing_app_id is not None:
+            return await _LocalApp._init_existing(client, existing_app_id)
+        else:
+            return await _LocalApp._init_new(
+                client, name, api_pb2.APP_STATE_INITIALIZING, environment_name=environment_name
+            )
 
-    def _init_container(self, client: _Client, running_app: RunningApp):
-        self._client = client
-        self._running_app = running_app
+    async def deploy(self, name: str, namespace, public: bool) -> str:
+        """`App.deploy` is deprecated in favor of `modal.runner.deploy_stub`."""
 
-        # Hydrate objects on stub
-        for tag, object_id in running_app.tag_to_object_id.items():
-            if tag in self._indexed_objects:
-                obj = self._indexed_objects[tag]
-                handle_metadata = running_app.object_handle_metadata[object_id]
-                obj._hydrate(object_id, client, handle_metadata)
+        deploy_req = api_pb2.AppDeployRequest(
+            app_id=self.app_id,
+            name=name,
+            namespace=namespace,
+            object_entity="ap",
+            visibility=(api_pb2.APP_DEPLOY_VISIBILITY_PUBLIC if public else api_pb2.APP_DEPLOY_VISIBILITY_WORKSPACE),
+        )
+        try:
+            deploy_response = await retry_transient_errors(self._client.stub.AppDeploy, deploy_req)
+        except GRPCError as exc:
+            if exc.status == Status.INVALID_ARGUMENT:
+                raise InvalidError(exc.message)
+            if exc.status == Status.FAILED_PRECONDITION:
+                raise InvalidError(exc.message)
+            raise
+        return deploy_response.url
 
-    @property
-    def registered_functions(self) -> Dict[str, _Function]:
-        """All modal.Function objects registered on the stub."""
-        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Function)}
 
-    @property
-    def registered_classes(self) -> Dict[str, _Function]:
-        """All modal.Cls objects registered on the stub."""
-        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Cls)}
+class _ContainerApp:
+    _client: Optional[_Client]
+    _app_id: Optional[str]
+    _associated_stub: Optional[Any]  # TODO(erikbern): type
+    _environment_name: Optional[str]
+    _tag_to_object_id: Dict[str, str]
+    _object_handle_metadata: Dict[str, Optional[Message]]
+    _stub_name: Optional[str]
+    # if true, there's an active PTY shell session connected to this process.
+    _is_interactivity_enabled: bool
+    _function_def: Optional[api_pb2.Function]
+    _fetching_inputs: bool
 
-    @property
-    def registered_entrypoints(self) -> Dict[str, _LocalEntrypoint]:
-        """All local CLI entrypoints registered on the stub."""
-        return self._local_entrypoints
+    def __init__(self):
+        self._client = None
+        self._app_id = None
+        self._associated_stub = None
+        self._stub_name = None
+        self._environment_name = None
+        self._tag_to_object_id = {}
+        self._object_handle_metadata = {}
+        self._is_interactivity_enabled = False
+        self._fetching_inputs = True
 
     @property
-    def indexed_objects(self) -> Dict[str, _Object]:
-        return self._indexed_objects
+    def client(self) -> Optional[_Client]:
+        """A reference to the running App's server client."""
+        return self._client
 
     @property
-    def registered_web_endpoints(self) -> List[str]:
-        """Names of web endpoint (ie. webhook) functions registered on the stub."""
-        return self._web_endpoints
-
-    def local_entrypoint(
-        self, _warn_parentheses_missing=None, *, name: Optional[str] = None
-    ) -> Callable[[Callable[..., Any]], None]:
-        """Decorate a function to be used as a CLI entrypoint for a Modal App.
-
-        These functions can be used to define code that runs locally to set up the app,
-        and act as an entrypoint to start Modal functions from. Note that regular
-        Modal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,
-        those functions are executed remotely directly.
-
-        **Example**
-
-        ```python
-        @stub.local_entrypoint()
-        def main():
-            some_modal_function.remote()
-        ```
-
-        You can call the function using `modal run` directly from the CLI:
-
-        ```shell
-        modal run stub_module.py
-        ```
-
-        Note that an explicit [`stub.run()`](/docs/reference/modal.Stub#run) is not needed, as an
-        [app](/docs/guide/apps) is automatically created for you.
-
-        **Multiple Entrypoints**
-
-        If you have multiple `local_entrypoint` functions, you can qualify the name of your stub and function:
-
-        ```shell
-        modal run stub_module.py::stub.some_other_function
-        ```
-
-        **Parsing Arguments**
-
-        If your entrypoint function take arguments with primitive types, `modal run` automatically parses them as
-        CLI options. For example, the following function can be called with `modal run stub_module.py --foo 1 --bar "hello"`:
-
-        ```python
-        @stub.local_entrypoint()
-        def main(foo: int, bar: str):
-            some_modal_function.call(foo, bar)
-        ```
-
-        Currently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported. Use `modal run stub_module.py --help` for more
-        information on usage.
-
-        """
-        if _warn_parentheses_missing:
-            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.local_entrypoint()`.")
-        if name is not None and not isinstance(name, str):
-            raise InvalidError("Invalid value for `name`: Must be string.")
-
-        def wrapped(raw_f: Callable[..., Any]) -> None:
-            info = FunctionInfo(raw_f)
-            tag = name if name is not None else raw_f.__qualname__
-            if tag in self._local_entrypoints:
-                # TODO: get rid of this limitation.
-                raise InvalidError(f"Duplicate local entrypoint name: {tag}. Local entrypoint names must be unique.")
-            entrypoint = self._local_entrypoints[tag] = _LocalEntrypoint(info, self)
-            return entrypoint
-
-        return wrapped
-
-    def function(
-        self,
-        _warn_parentheses_missing=None,
-        *,
-        image: Optional[_Image] = None,  # The image to run as the container for the function
-        schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
-        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
-        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
-        serialized: bool = False,  # Whether to send the function over using cloudpickle.
-        mounts: Sequence[_Mount] = (),  # Modal Mounts added to the container
-        network_file_systems: Dict[
-            Union[str, PurePosixPath], _NetworkFileSystem
-        ] = {},  # Mountpoints for Modal NetworkFileSystems
-        volumes: Dict[
-            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
-        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
-        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
-        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
-        memory: Optional[
-            Union[int, Tuple[int, int]]
-        ] = None,  # Specify, in MiB, a memory request which is the minimum memory required. Or, pass (request, limit) to additionally specify a hard limit in MiB.
-        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
-        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
-        concurrency_limit: Optional[
-            int
-        ] = None,  # An optional maximum number of concurrent containers running the function (use keep_warm for minimum).
-        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
-        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
-        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
-        keep_warm: Optional[
-            int
-        ] = None,  # An optional minimum number of containers to always keep warm (use concurrency_limit for maximum).
-        name: Optional[str] = None,  # Sets the Modal name of the function within the stub
-        is_generator: Optional[
-            bool
-        ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
-        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
-        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
-        checkpointing_enabled: Optional[bool] = None,  # Deprecated
-        block_network: bool = False,  # Whether to block network access
-        max_inputs: Optional[
-            int
-        ] = None,  # Maximum number of inputs a container should handle before shutting down. With `max_inputs = 1`, containers will be single-use.
-        # The next group of parameters are deprecated; do not use in any new code
-        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
-        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
-        # Parameters below here are experimental. Use with caution!
-        _allow_background_volume_commits: bool = False,  # Experimental flag
-        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
-        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
-        _experimental_scheduler_placement: Optional[
-            SchedulerPlacement
-        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
-    ) -> Callable[..., _Function]:
-        """Decorator to register a new Modal function with this stub."""
-        if isinstance(_warn_parentheses_missing, _Image):
-            # Handle edge case where maybe (?) some users passed image as a positional arg
-            raise InvalidError("`image` needs to be a keyword argument: `@stub.function(image=image)`.")
-        if _warn_parentheses_missing:
-            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.function()`.")
-
-        if interactive:
-            deprecation_error(
-                (2024, 2, 29), "interactive=True has been deprecated. Set MODAL_INTERACTIVE_FUNCTIONS=1 instead."
-            )
-
-        if image is None:
-            image = self._get_default_image()
+    def app_id(self) -> Optional[str]:
+        """A unique identifier for this running App."""
+        return self._app_id
 
-        secrets = [*self._secrets, *secrets]
+    @property
+    def fetching_inputs(self) -> bool:
+        return self._fetching_inputs
 
-        def wrapped(
-            f: Union[_PartialFunction, Callable[..., Any]],
-            _cls: Optional[type] = None,  # Used for methods only
-        ) -> _Function:
-            nonlocal keep_warm, is_generator
-
-            if isinstance(f, _PartialFunction):
-                f.wrapped = True
-                info = FunctionInfo(f.raw_f, serialized=serialized, name_override=name, cls=_cls)
-                raw_f = f.raw_f
-                webhook_config = f.webhook_config
-                is_generator = f.is_generator
-                keep_warm = f.keep_warm or keep_warm
-
-                if webhook_config:
-                    if interactive:
-                        raise InvalidError("interactive=True is not supported with web endpoint functions")
-                    self._web_endpoints.append(info.get_tag())
-            else:
-                info = FunctionInfo(f, serialized=serialized, name_override=name, cls=_cls)
-                webhook_config = None
-                raw_f = f
-
-            if not _cls and not info.is_serialized() and "." in info.function_name:  # This is a method
-                raise InvalidError(
-                    "`stub.function` on methods is not allowed. See https://modal.com/docs/guide/lifecycle-functions instead"
-                )
-
-            if is_generator is None:
-                is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)
-
-            function = _Function.from_args(
-                info,
-                stub=self,
-                image=image,
-                secret=secret,
-                secrets=secrets,
-                schedule=schedule,
-                is_generator=is_generator,
-                gpu=gpu,
-                mounts=[*self._mounts, *mounts],
-                network_file_systems=network_file_systems,
-                allow_cross_region_volumes=allow_cross_region_volumes,
-                volumes={**self._volumes, **volumes},
-                memory=memory,
-                proxy=proxy,
-                retries=retries,
-                concurrency_limit=concurrency_limit,
-                allow_concurrent_inputs=allow_concurrent_inputs,
-                container_idle_timeout=container_idle_timeout,
-                timeout=timeout,
-                cpu=cpu,
-                keep_warm=keep_warm,
-                cloud=cloud,
-                webhook_config=webhook_config,
-                enable_memory_snapshot=enable_memory_snapshot,
-                checkpointing_enabled=checkpointing_enabled,
-                allow_background_volume_commits=_allow_background_volume_commits,
-                block_network=block_network,
-                max_inputs=max_inputs,
-                _experimental_boost=_experimental_boost,
-                _experimental_scheduler=_experimental_scheduler,
-                _experimental_scheduler_placement=_experimental_scheduler_placement,
+    def associate_stub_container(self, stub):
+        # TODO(erikbern): the fact that we need to set two-way references strongly indicate that
+        # we should just merge these two objects!
+        self._associated_stub = stub
+        stub._container_app = self
+
+        # Initialize objects on stub
+        stub_objects: dict[str, _Object] = dict(stub.get_objects())
+        for tag, object_id in self._tag_to_object_id.items():
+            obj = stub_objects.get(tag)
+            if obj is not None:
+                handle_metadata = self._object_handle_metadata[object_id]
+                obj._hydrate(object_id, self._client, handle_metadata)
+
+    def _has_object(self, tag: str) -> bool:
+        return tag in self._tag_to_object_id
+
+    def _hydrate_object(self, obj, tag: str):
+        object_id: str = self._tag_to_object_id[tag]
+        metadata: Message = self._object_handle_metadata[object_id]
+        obj._hydrate(object_id, self._client, metadata)
+
+    def hydrate_function_deps(self, function: _Function, dep_object_ids: List[str]):
+        function_deps = function.deps(only_explicit_mounts=True)
+        if len(function_deps) != len(dep_object_ids):
+            raise ExecutionError(
+                f"Function has {len(function_deps)} dependencies"
+                f" but container got {len(dep_object_ids)} object ids."
             )
+        for object_id, obj in zip(dep_object_ids, function_deps):
+            metadata: Message = self._object_handle_metadata[object_id]
+            obj._hydrate(object_id, self._client, metadata)
 
-            self._add_function(function)
-            return function
-
-        return wrapped
-
-    def cls(
-        self,
-        _warn_parentheses_missing=None,
-        *,
-        image: Optional[_Image] = None,  # The image to run as the container for the function
-        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
-        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
-        serialized: bool = False,  # Whether to send the function over using cloudpickle.
-        mounts: Sequence[_Mount] = (),
-        network_file_systems: Dict[
-            Union[str, PurePosixPath], _NetworkFileSystem
-        ] = {},  # Mountpoints for Modal NetworkFileSystems
-        volumes: Dict[
-            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
-        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
-        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
-        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
-        memory: Optional[
-            Union[int, Tuple[int, int]]
-        ] = None,  # Specify, in MiB, a memory request which is the minimum memory required. Or, pass (request, limit) to additionally specify a hard limit in MiB.
-        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
-        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
-        concurrency_limit: Optional[int] = None,  # Limit for max concurrent containers running the function.
-        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
-        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
-        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
-        keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm.
-        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
-        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
-        checkpointing_enabled: Optional[bool] = None,  # Deprecated
-        block_network: bool = False,  # Whether to block network access
-        _allow_background_volume_commits: bool = False,
-        max_inputs: Optional[
-            int
-        ] = None,  # Limits the number of inputs a container handles before shutting down. Use `max_inputs = 1` for single-use containers.
-        # The next group of parameters are deprecated; do not use in any new code
-        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
-        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
-        # Parameters below here are experimental. Use with caution!
-        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
-        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
-        _experimental_scheduler_placement: Optional[
-            SchedulerPlacement
-        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
-    ) -> Callable[[CLS_T], _Cls]:
-        if _warn_parentheses_missing:
-            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.cls()`.")
-
-        decorator: Callable[[PartialFunction, type], _Function] = self.function(
-            image=image,
-            secret=secret,
-            secrets=secrets,
-            gpu=gpu,
-            serialized=serialized,
-            mounts=mounts,
-            network_file_systems=network_file_systems,
-            allow_cross_region_volumes=allow_cross_region_volumes,
-            volumes=volumes,
-            cpu=cpu,
-            memory=memory,
-            proxy=proxy,
-            retries=retries,
-            concurrency_limit=concurrency_limit,
-            allow_concurrent_inputs=allow_concurrent_inputs,
-            container_idle_timeout=container_idle_timeout,
-            timeout=timeout,
-            interactive=interactive,
-            keep_warm=keep_warm,
-            cloud=cloud,
-            enable_memory_snapshot=enable_memory_snapshot,
-            checkpointing_enabled=checkpointing_enabled,
-            block_network=block_network,
-            _allow_background_volume_commits=_allow_background_volume_commits,
-            max_inputs=max_inputs,
-            _experimental_boost=_experimental_boost,
-            _experimental_scheduler=_experimental_scheduler,
-            _experimental_scheduler_placement=_experimental_scheduler_placement,
-        )
-
-        def wrapper(user_cls: CLS_T) -> _Cls:
-            cls: _Cls = _Cls.from_local(user_cls, self, decorator)
-
-            if (
-                _find_callables_for_cls(user_cls, _PartialFunctionFlags.ENTER_PRE_CHECKPOINT)
-                and not enable_memory_snapshot
-            ):
-                raise InvalidError("A class must have `enable_memory_snapshot=True` to use `snap=True` on its methods.")
-
-            if len(cls._functions) > 1 and keep_warm is not None:
-                deprecation_warning(
-                    (2023, 10, 20),
-                    "`@stub.cls(keep_warm=...)` is deprecated when there is more than 1 method."
-                    " Use `@method(keep_warm=...)` on each method instead!",
-                )
-
-            tag: str = user_cls.__name__
-            self._add_object(tag, cls)
-            return cls
-
-        return wrapper
-
-    async def spawn_sandbox(
+    async def init(
         self,
-        *entrypoint_args: str,
-        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
-        mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.
-        secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
-        network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
-        timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
-        workdir: Optional[str] = None,  # Working directory of the sandbox.
-        gpu: GPU_T = None,
-        cloud: Optional[str] = None,
-        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
-        memory: Optional[
-            Union[int, Tuple[int, int]]
-        ] = None,  # Specify, in MiB, a memory request which is the minimum memory required. Or, pass (request, limit) to additionally specify a hard limit in MiB.
-        block_network: bool = False,  # Whether to block network access
-        volumes: Dict[
-            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
-        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
-        _allow_background_volume_commits: bool = False,
-        pty_info: Optional[api_pb2.PTYInfo] = None,
-    ) -> _Sandbox:
-        """Sandboxes are a way to run arbitrary commands in dynamically defined environments.
-
-        This function returns a [SandboxHandle](/docs/reference/modal.Sandbox#modalsandboxsandbox), which can be used to interact with the running sandbox.
-
-        Refer to the [docs](/docs/guide/sandbox) on how to spawn and use sandboxes.
-        """
-        if self._running_app:
-            app_id = self._running_app.app_id
-            environment_name = self._running_app.environment_name
-            client = self._client
-        else:
-            raise InvalidError("`stub.spawn_sandbox` requires a running app.")
-
-        # TODO(erikbern): pulling a lot of app internals here, let's clean up shortly
-        resolver = Resolver(client, environment_name=environment_name, app_id=app_id)
-        obj = _Sandbox._new(
-            entrypoint_args,
-            image=image or _default_image,
-            mounts=mounts,
-            secrets=secrets,
-            timeout=timeout,
-            workdir=workdir,
-            gpu=gpu,
-            cloud=cloud,
-            cpu=cpu,
-            memory=memory,
-            network_file_systems=network_file_systems,
-            block_network=block_network,
-            volumes=volumes,
-            allow_background_volume_commits=_allow_background_volume_commits,
-            pty_info=pty_info,
-        )
-        await resolver.load(obj)
-        return obj
-
-    def include(self, /, other_stub: "_App"):
-        """Include another stub's objects in this one.
-
-        Useful splitting up Modal apps across different self-contained files
+        client: _Client,
+        app_id: str,
+        stub_name: str = "",
+        environment_name: str = "",
+        function_def: Optional[api_pb2.Function] = None,
+    ):
+        """Used by the container to bootstrap the app and all its objects. Not intended to be called by Modal users."""
+        global _is_container_app
+        _is_container_app = True
 
-        ```python
-        stub_a = modal.Stub("a")
-        @stub.function()
-        def foo():
-            ...
-
-        stub_b = modal.Stub("b")
-        @stub.function()
-        def bar():
-            ...
-
-        stub_a.include(stub_b)
+        self._client = client
+        self._app_id = app_id
+        self._stub_name = stub_name
+        self._environment_name = environment_name
+        self._function_def = function_def
+        self._tag_to_object_id = {}
+        self._object_handle_metadata = {}
+        req = api_pb2.AppGetObjectsRequest(app_id=app_id, include_unindexed=True)
+        resp = await retry_transient_errors(client.stub.AppGetObjects, req)
+        logger.debug(f"AppGetObjects received {len(resp.items)} objects for app {app_id}")
+        for item in resp.items:
+            handle_metadata: Optional[Message] = get_proto_oneof(item.object, "handle_metadata_oneof")
+            self._object_handle_metadata[item.object.object_id] = handle_metadata
+            logger.debug(f"Setting metadata for {item.object.object_id} ({item.tag})")
+            if item.tag:
+                self._tag_to_object_id[item.tag] = item.object.object_id
+
+    @staticmethod
+    def _reset_container():
+        # Just used for tests
+        global _is_container_app, _container_app
+        _is_container_app = False
+        _container_app.__init__()  # type: ignore
+
+    def stop_fetching_inputs(self):
+        self._fetching_inputs = False
+
+
+LocalApp = synchronize_api(_LocalApp)
+ContainerApp = synchronize_api(_ContainerApp)
+
+_is_container_app = False
+_container_app = _ContainerApp()
+container_app = synchronize_api(_container_app)
+assert isinstance(container_app, ContainerApp)
+
+
+async def _interact(client: Optional[_Client] = None) -> None:
+    if _container_app._is_interactivity_enabled:
+        # Currently, interactivity is enabled forever
+        return
+    _container_app._is_interactivity_enabled = True
+
+    if not client:
+        client = await _Client.from_env()
+
+    if client.client_type != api_pb2.CLIENT_TYPE_CONTAINER:
+        raise InvalidError("Interactivity only works inside a Modal Container.")
+
+    if _container_app._function_def is not None:
+        if not _container_app._function_def.pty_info:
+            raise InvalidError(
+                "Interactivity is not enabled in this function. Use MODAL_INTERACTIVE_FUNCTIONS=1 to enable interactivity."
+            )
 
-        @stub_a.local_entrypoint()
-        def main():
-            # use function declared on the included stub
-            bar.remote()
-        ```
-        """
-        for tag, object in other_stub._indexed_objects.items():
-            existing_object = self._indexed_objects.get(tag)
-            if existing_object and existing_object != object:
-                logger.warning(
-                    f"Named app object {tag} with existing value {existing_object} is being overwritten by a different object {object}"
-                )
+        if _container_app._function_def.concurrency_limit > 1:
+            print(
+                "Warning: Interactivity is not supported on functions with concurrency > 1. You may experience unexpected behavior."
+            )
 
-            self._add_object(tag, object)
+    # todo(nathan): add warning if concurrency limit > 1. but idk how to check this here
+    # todo(nathan): check if function interactivity is enabled
+    try:
+        await client.stub.FunctionStartPtyShell(Empty())
+    except Exception as e:
+        print("Error: Failed to start PTY shell.")
+        raise e
 
 
-App = synchronize_api(_App)
+interact = synchronize_api(_interact)
 
 
-class _Stub(_App):
-    """This enables using an "Stub" class instead of "App".
+def is_local() -> bool:
+    """Returns if we are currently on the machine launching/deploying a Modal app
 
-    For most of Modal's history, the app class was called "Stub", so this exists for
-    backwards compatibility, in order to facilitate moving from "Stub" to "App".
+    Returns `True` when executed locally on the user's machine.
+    Returns `False` when executed from a Modal container in the cloud.
     """
+    return not _is_container_app
+
 
-    pass
+async def _list_apps(env: str, client: Optional[_Client] = None) -> List[api_pb2.AppStats]:
+    """List apps in a given Modal environment."""
+    if client is None:
+        client = await _Client.from_env()
+    resp: api_pb2.AppListResponse = await client.stub.AppList(api_pb2.AppListRequest(environment_name=env))
+    return list(resp.apps)
 
 
-Stub = synchronize_api(_Stub)
+list_apps = synchronize_api(_list_apps)
```

## modal/app.pyi

```diff
@@ -1,340 +1,291 @@
+import google.protobuf.message
 import modal._output
-import modal._utils.function_utils
 import modal.client
-import modal.cloud_bucket_mount
-import modal.cls
-import modal.functions
-import modal.gpu
-import modal.image
-import modal.mount
-import modal.network_file_system
 import modal.object
-import modal.proxy
-import modal.retries
-import modal.running_app
-import modal.sandbox
-import modal.schedule
-import modal.scheduler_placement
-import modal.secret
-import modal.volume
 import modal_proto.api_pb2
-import pathlib
-import synchronicity.combined_types
 import typing
 import typing_extensions
 
-class _LocalEntrypoint:
-    _info: modal._utils.function_utils.FunctionInfo
-    _stub: _App
+_Function = typing.TypeVar("_Function")
 
-    def __init__(self, info, stub):
-        ...
+class _LocalApp:
+    _tag_to_object_id: typing.Dict[str, str]
+    _client: modal.client._Client
+    _app_id: str
+    _app_page_url: str
+    _environment_name: str
+    _interactive: bool
 
-    def __call__(self, *args, **kwargs):
+    def __init__(self, client: modal.client._Client, app_id: str, app_page_url: str, tag_to_object_id: typing.Union[typing.Dict[str, str], None] = None, stub_name: typing.Union[str, None] = None, environment_name: typing.Union[str, None] = None, interactive: bool = False):
         ...
 
     @property
-    def info(self) -> modal._utils.function_utils.FunctionInfo:
+    def client(self) -> modal.client._Client:
         ...
 
     @property
-    def stub(self) -> _App:
+    def app_id(self) -> str:
         ...
 
+    @property
+    def is_interactive(self) -> bool:
+        ...
 
-class LocalEntrypoint:
-    _info: modal._utils.function_utils.FunctionInfo
-    _stub: App
+    async def _create_all_objects(self, indexed_objects: typing.Dict[str, modal.object._Object], new_app_state: int, environment_name: str, output_mgr: typing.Union[modal._output.OutputManager, None] = None):
+        ...
 
-    def __init__(self, info, stub):
+    async def disconnect(self, reason: typing.Union[int, None] = None, exc_str: typing.Union[str, None] = None):
         ...
 
-    def __call__(self, *args, **kwargs):
+    async def stop(self):
         ...
 
-    @property
-    def info(self) -> modal._utils.function_utils.FunctionInfo:
+    def log_url(self):
         ...
 
-    @property
-    def stub(self) -> App:
+    @staticmethod
+    async def _init_existing(client: modal.client._Client, existing_app_id: str) -> _LocalApp:
         ...
 
+    @staticmethod
+    async def _init_new(client: modal.client._Client, description: str, app_state: int, environment_name: str = '', interactive=False) -> _LocalApp:
+        ...
 
-def check_sequence(items: typing.Sequence[typing.Any], item_type: typing.Type[typing.Any], error_msg: str):
-    ...
+    @staticmethod
+    async def _init_from_name(client: modal.client._Client, name: str, namespace, environment_name: str = ''):
+        ...
 
+    async def deploy(self, name: str, namespace, public: bool) -> str:
+        ...
 
-CLS_T = typing.TypeVar("CLS_T", bound="typing.Type")
 
-class _App:
-    _name: typing.Union[str, None]
-    _description: typing.Union[str, None]
-    _indexed_objects: typing.Dict[str, modal.object._Object]
-    _function_mounts: typing.Dict[str, modal.mount._Mount]
-    _mounts: typing.Sequence[modal.mount._Mount]
-    _secrets: typing.Sequence[modal.secret._Secret]
-    _volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume._Volume]
-    _web_endpoints: typing.List[str]
-    _local_entrypoints: typing.Dict[str, _LocalEntrypoint]
-    _running_app: typing.Union[modal.running_app.RunningApp, None]
+class _ContainerApp:
     _client: typing.Union[modal.client._Client, None]
-    _all_apps: typing.ClassVar[typing.Dict[typing.Union[str, None], typing.List[_App]]]
-
-    def __init__(self, name: typing.Union[str, None] = None, *, image: typing.Union[modal.image._Image, None] = None, mounts: typing.Sequence[modal.mount._Mount] = [], secrets: typing.Sequence[modal.secret._Secret] = [], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume._Volume] = {}, **kwargs: modal.object._Object) -> None:
-        ...
+    _app_id: typing.Union[str, None]
+    _associated_stub: typing.Union[typing.Any, None]
+    _environment_name: typing.Union[str, None]
+    _tag_to_object_id: typing.Dict[str, str]
+    _object_handle_metadata: typing.Dict[str, typing.Union[google.protobuf.message.Message, None]]
+    _stub_name: typing.Union[str, None]
+    _is_interactivity_enabled: bool
+    _function_def: typing.Union[modal_proto.api_pb2.Function, None]
+    _fetching_inputs: bool
 
-    @property
-    def name(self) -> typing.Union[str, None]:
+    def __init__(self):
         ...
 
     @property
-    def is_interactive(self) -> bool:
+    def client(self) -> typing.Union[modal.client._Client, None]:
         ...
 
     @property
     def app_id(self) -> typing.Union[str, None]:
         ...
 
     @property
-    def description(self) -> typing.Union[str, None]:
+    def fetching_inputs(self) -> bool:
         ...
 
-    def set_description(self, description: str):
+    def associate_stub_container(self, stub):
         ...
 
-    def _validate_blueprint_value(self, key: str, value: typing.Any):
+    def _has_object(self, tag: str) -> bool:
         ...
 
-    def _add_object(self, tag, obj):
+    def _hydrate_object(self, obj, tag: str):
         ...
 
-    def __getitem__(self, tag: str):
+    def hydrate_function_deps(self, function: _Function, dep_object_ids: typing.List[str]):
         ...
 
-    def __setitem__(self, tag: str, obj: modal.object._Object):
+    async def init(self, client: modal.client._Client, app_id: str, stub_name: str = '', environment_name: str = '', function_def: typing.Union[modal_proto.api_pb2.Function, None] = None):
         ...
 
-    def __getattr__(self, tag: str) -> modal.object._Object:
+    @staticmethod
+    def _reset_container():
         ...
 
-    def __setattr__(self, tag: str, obj: modal.object._Object):
+    def stop_fetching_inputs(self):
         ...
 
-    @property
-    def image(self) -> modal.image._Image:
-        ...
 
-    @image.setter
-    def image(self, value):
-        ...
-
-    def _uncreate_all_objects(self):
-        ...
+class LocalApp:
+    _tag_to_object_id: typing.Dict[str, str]
+    _client: modal.client.Client
+    _app_id: str
+    _app_page_url: str
+    _environment_name: str
+    _interactive: bool
 
-    def is_inside(self, image: typing.Union[modal.image._Image, None] = None):
+    def __init__(self, client: modal.client.Client, app_id: str, app_page_url: str, tag_to_object_id: typing.Union[typing.Dict[str, str], None] = None, stub_name: typing.Union[str, None] = None, environment_name: typing.Union[str, None] = None, interactive: bool = False):
         ...
 
-    def _set_local_app(self, client: modal.client._Client, app: modal.running_app.RunningApp) -> typing.AsyncContextManager[None]:
+    @property
+    def client(self) -> modal.client.Client:
         ...
 
-    def run(self, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None) -> typing.AsyncContextManager[_App]:
+    @property
+    def app_id(self) -> str:
         ...
 
-    def _get_default_image(self):
+    @property
+    def is_interactive(self) -> bool:
         ...
 
-    def _get_watch_mounts(self):
-        ...
+    class ___create_all_objects_spec(typing_extensions.Protocol):
+        def __call__(self, indexed_objects: typing.Dict[str, modal.object.Object], new_app_state: int, environment_name: str, output_mgr: typing.Union[modal._output.OutputManager, None] = None):
+            ...
 
-    def _add_function(self, function: modal.functions._Function):
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    def _init_container(self, client: modal.client._Client, running_app: modal.running_app.RunningApp):
-        ...
+    _create_all_objects: ___create_all_objects_spec
 
-    @property
-    def registered_functions(self) -> typing.Dict[str, modal.functions._Function]:
-        ...
+    class __disconnect_spec(typing_extensions.Protocol):
+        def __call__(self, reason: typing.Union[int, None] = None, exc_str: typing.Union[str, None] = None):
+            ...
 
-    @property
-    def registered_classes(self) -> typing.Dict[str, modal.functions._Function]:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    @property
-    def registered_entrypoints(self) -> typing.Dict[str, _LocalEntrypoint]:
-        ...
+    disconnect: __disconnect_spec
 
-    @property
-    def indexed_objects(self) -> typing.Dict[str, modal.object._Object]:
-        ...
+    class __stop_spec(typing_extensions.Protocol):
+        def __call__(self):
+            ...
 
-    @property
-    def registered_web_endpoints(self) -> typing.List[str]:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    def local_entrypoint(self, _warn_parentheses_missing=None, *, name: typing.Union[str, None] = None) -> typing.Callable[[typing.Callable[..., typing.Any]], None]:
-        ...
+    stop: __stop_spec
 
-    def function(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image._Image, None] = None, schedule: typing.Union[modal.schedule.Schedule, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, name: typing.Union[str, None] = None, is_generator: typing.Union[bool, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret._Secret, None] = None, _allow_background_volume_commits: bool = False, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[..., modal.functions._Function]:
+    def log_url(self):
         ...
 
-    def cls(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image._Image, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, _allow_background_volume_commits: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret._Secret, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[[CLS_T], modal.cls._Cls]:
-        ...
+    class ___init_existing_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, existing_app_id: str) -> LocalApp:
+            ...
 
-    async def spawn_sandbox(self, *entrypoint_args: str, image: typing.Union[modal.image._Image, None] = None, mounts: typing.Sequence[modal.mount._Mount] = (), secrets: typing.Sequence[modal.secret._Secret] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, block_network: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, _allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> modal.sandbox._Sandbox:
-        ...
+        async def aio(self, *args, **kwargs) -> LocalApp:
+            ...
 
-    def include(self, /, other_stub: _App):
-        ...
+    _init_existing: ___init_existing_spec
 
+    class ___init_new_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, description: str, app_state: int, environment_name: str = '', interactive=False) -> LocalApp:
+            ...
 
-class App:
-    _name: typing.Union[str, None]
-    _description: typing.Union[str, None]
-    _indexed_objects: typing.Dict[str, modal.object.Object]
-    _function_mounts: typing.Dict[str, modal.mount.Mount]
-    _mounts: typing.Sequence[modal.mount.Mount]
-    _secrets: typing.Sequence[modal.secret.Secret]
-    _volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume.Volume]
-    _web_endpoints: typing.List[str]
-    _local_entrypoints: typing.Dict[str, LocalEntrypoint]
-    _running_app: typing.Union[modal.running_app.RunningApp, None]
-    _client: typing.Union[modal.client.Client, None]
-    _all_apps: typing.ClassVar[typing.Dict[typing.Union[str, None], typing.List[App]]]
+        async def aio(self, *args, **kwargs) -> LocalApp:
+            ...
 
-    def __init__(self, name: typing.Union[str, None] = None, *, image: typing.Union[modal.image.Image, None] = None, mounts: typing.Sequence[modal.mount.Mount] = [], secrets: typing.Sequence[modal.secret.Secret] = [], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume.Volume] = {}, **kwargs: modal.object.Object) -> None:
-        ...
+    _init_new: ___init_new_spec
 
-    @property
-    def name(self) -> typing.Union[str, None]:
-        ...
+    class ___init_from_name_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, name: str, namespace, environment_name: str = ''):
+            ...
 
-    @property
-    def is_interactive(self) -> bool:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    @property
-    def app_id(self) -> typing.Union[str, None]:
-        ...
+    _init_from_name: ___init_from_name_spec
 
-    @property
-    def description(self) -> typing.Union[str, None]:
-        ...
+    class __deploy_spec(typing_extensions.Protocol):
+        def __call__(self, name: str, namespace, public: bool) -> str:
+            ...
 
-    def set_description(self, description: str):
-        ...
+        async def aio(self, *args, **kwargs) -> str:
+            ...
 
-    def _validate_blueprint_value(self, key: str, value: typing.Any):
-        ...
+    deploy: __deploy_spec
 
-    def _add_object(self, tag, obj):
-        ...
 
-    def __getitem__(self, tag: str):
-        ...
+class ContainerApp:
+    _client: typing.Union[modal.client.Client, None]
+    _app_id: typing.Union[str, None]
+    _associated_stub: typing.Union[typing.Any, None]
+    _environment_name: typing.Union[str, None]
+    _tag_to_object_id: typing.Dict[str, str]
+    _object_handle_metadata: typing.Dict[str, typing.Union[google.protobuf.message.Message, None]]
+    _stub_name: typing.Union[str, None]
+    _is_interactivity_enabled: bool
+    _function_def: typing.Union[modal_proto.api_pb2.Function, None]
+    _fetching_inputs: bool
 
-    def __setitem__(self, tag: str, obj: modal.object.Object):
+    def __init__(self):
         ...
 
-    def __getattr__(self, tag: str) -> modal.object.Object:
+    @property
+    def client(self) -> typing.Union[modal.client.Client, None]:
         ...
 
-    def __setattr__(self, tag: str, obj: modal.object.Object):
+    @property
+    def app_id(self) -> typing.Union[str, None]:
         ...
 
     @property
-    def image(self) -> modal.image.Image:
+    def fetching_inputs(self) -> bool:
         ...
 
-    @image.setter
-    def image(self, value):
+    def associate_stub_container(self, stub):
         ...
 
-    def _uncreate_all_objects(self):
+    def _has_object(self, tag: str) -> bool:
         ...
 
-    def is_inside(self, image: typing.Union[modal.image.Image, None] = None):
+    def _hydrate_object(self, obj, tag: str):
         ...
 
-    class ___set_local_app_spec(typing_extensions.Protocol):
-        def __call__(self, client: modal.client.Client, app: modal.running_app.RunningApp) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
-            ...
-
-        def aio(self, client: modal.client.Client, app: modal.running_app.RunningApp) -> typing.AsyncContextManager[None]:
-            ...
-
-    _set_local_app: ___set_local_app_spec
+    def hydrate_function_deps(self, function: _Function, dep_object_ids: typing.List[str]):
+        ...
 
-    class __run_spec(typing_extensions.Protocol):
-        def __call__(self, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[App]:
+    class __init_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, app_id: str, stub_name: str = '', environment_name: str = '', function_def: typing.Union[modal_proto.api_pb2.Function, None] = None):
             ...
 
-        def aio(self, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None) -> typing.AsyncContextManager[App]:
+        async def aio(self, *args, **kwargs):
             ...
 
-    run: __run_spec
+    init: __init_spec
 
-    def _get_default_image(self):
+    @staticmethod
+    def _reset_container():
         ...
 
-    def _get_watch_mounts(self):
+    def stop_fetching_inputs(self):
         ...
 
-    def _add_function(self, function: modal.functions.Function):
-        ...
 
-    def _init_container(self, client: modal.client.Client, running_app: modal.running_app.RunningApp):
-        ...
+_container_app: _ContainerApp
 
-    @property
-    def registered_functions(self) -> typing.Dict[str, modal.functions.Function]:
-        ...
+container_app: ContainerApp
 
-    @property
-    def registered_classes(self) -> typing.Dict[str, modal.functions.Function]:
-        ...
+async def _interact(client: typing.Union[modal.client._Client, None] = None) -> None:
+    ...
 
-    @property
-    def registered_entrypoints(self) -> typing.Dict[str, LocalEntrypoint]:
-        ...
 
-    @property
-    def indexed_objects(self) -> typing.Dict[str, modal.object.Object]:
+class __interact_spec(typing_extensions.Protocol):
+    def __call__(self, client: typing.Union[modal.client.Client, None] = None) -> None:
         ...
 
-    @property
-    def registered_web_endpoints(self) -> typing.List[str]:
+    async def aio(self, *args, **kwargs) -> None:
         ...
 
-    def local_entrypoint(self, _warn_parentheses_missing=None, *, name: typing.Union[str, None] = None) -> typing.Callable[[typing.Callable[..., typing.Any]], None]:
-        ...
+interact: __interact_spec
 
-    def function(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image.Image, None] = None, schedule: typing.Union[modal.schedule.Schedule, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, name: typing.Union[str, None] = None, is_generator: typing.Union[bool, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret.Secret, None] = None, _allow_background_volume_commits: bool = False, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[..., modal.functions.Function]:
-        ...
 
-    def cls(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image.Image, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, _allow_background_volume_commits: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret.Secret, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[[CLS_T], modal.cls.Cls]:
-        ...
+def is_local() -> bool:
+    ...
 
-    class __spawn_sandbox_spec(typing_extensions.Protocol):
-        def __call__(self, *entrypoint_args: str, image: typing.Union[modal.image.Image, None] = None, mounts: typing.Sequence[modal.mount.Mount] = (), secrets: typing.Sequence[modal.secret.Secret] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, block_network: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, _allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> modal.sandbox.Sandbox:
-            ...
 
-        async def aio(self, *args, **kwargs) -> modal.sandbox.Sandbox:
-            ...
+async def _list_apps(env: str, client: typing.Union[modal.client._Client, None] = None) -> typing.List[modal_proto.api_pb2.AppStats]:
+    ...
 
-    spawn_sandbox: __spawn_sandbox_spec
 
-    def include(self, other_stub: App):
+class __list_apps_spec(typing_extensions.Protocol):
+    def __call__(self, env: str, client: typing.Union[modal.client.Client, None] = None) -> typing.List[modal_proto.api_pb2.AppStats]:
         ...
 
-
-class _Stub(_App):
-    ...
-
-class Stub(App):
-    def __init__(self, name: typing.Union[str, None] = None, *, image: typing.Union[modal.image.Image, None] = None, mounts: typing.Sequence[modal.mount.Mount] = [], secrets: typing.Sequence[modal.secret.Secret] = [], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume.Volume] = {}, **kwargs: modal.object.Object) -> None:
+    async def aio(self, *args, **kwargs) -> typing.List[modal_proto.api_pb2.AppStats]:
         ...
 
-
-_default_image: modal.image._Image
+list_apps: __list_apps_spec
```

## modal/client.py

```diff
@@ -1,18 +1,16 @@
 # Copyright Modal Labs 2022
 import asyncio
 import platform
 import warnings
-from typing import AsyncIterator, Awaitable, Callable, ClassVar, Dict, Optional, Tuple
+from typing import Awaitable, Callable, Dict, Optional, Tuple
 
-import grpclib.client
 from aiohttp import ClientConnectorError, ClientResponseError
 from google.protobuf import empty_pb2
 from grpclib import GRPCError, Status
-from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_grpc, api_pb2
 from modal_version import __version__
 
 from ._utils import async_utils
 from ._utils.async_utils import synchronize_api
 from ._utils.grpc_utils import create_channel, retry_transient_errors
@@ -73,53 +71,54 @@
         return f"HTTP exception: {exc.os_error.__class__.__name__}"
     except Exception as exc:
         return f"HTTP exception: {exc.__class__.__name__}"
 
 
 async def _grpc_exc_string(exc: GRPCError, method_name: str, server_url: str, timeout: float) -> str:
     http_status = await _http_check(server_url, timeout=timeout)
-    return f"{method_name}: {exc.message} [gRPC status: {exc.status.name}, {http_status}]"
+    return f"{method_name}: {exc.message} [GRPC status: {exc.status.name}, {http_status}]"
 
 
 class _Client:
-    _client_from_env: ClassVar[Optional["_Client"]] = None
-    _client_from_env_lock: ClassVar[asyncio.Lock] = asyncio.Lock()
+    _client_from_env = None
+    _client_from_env_lock = None
+
+    client_type: int
 
     def __init__(
         self,
-        server_url: str,
-        client_type: int,
-        credentials: Optional[Tuple[str, str]],
-        version: str = __version__,
+        server_url,
+        client_type,
+        credentials,
+        version=__version__,
+        *,
+        no_verify=False,
     ):
         """The Modal client object is not intended to be instantiated directly by users."""
         self.server_url = server_url
         self.client_type = client_type
         self.credentials = credentials
         self.version = version
-        self._authenticated = False
-        self.image_builder_version: Optional[str] = None
+        self.no_verify = no_verify
         self._pre_stop: Optional[Callable[[], Awaitable[None]]] = None
-        self._channel: Optional[grpclib.client.Channel] = None
+        self._channel = None
         self._stub: Optional[api_grpc.ModalClientStub] = None
 
     @property
     def stub(self) -> Optional[api_grpc.ModalClientStub]:
         """mdmd:hidden"""
         return self._stub
 
-    @property
-    def authenticated(self) -> bool:
-        """mdmd:hidden"""
-        return self._authenticated
-
     async def _open(self):
         assert self._stub is None
         metadata = _get_metadata(self.client_type, self.credentials, self.version)
-        self._channel = create_channel(self.server_url, metadata=metadata)
+        self._channel = create_channel(
+            self.server_url,
+            metadata=metadata,
+        )
         self._stub = api_grpc.ModalClientStub(self._channel)  # type: ignore
 
     async def _close(self):
         if self._pre_stop is not None:
             logger.debug("Client: running pre-stop coroutine before shutting down")
             await self._pre_stop()  # type: ignore
 
@@ -135,82 +134,77 @@
         # teardown when an interrupt signal is received (eg. KeyboardInterrupt).
         # By registering a pre-stop fn stub.serve() can have its teardown
         # performed before the client is disconnected.
         #
         # ref: github.com/modal-labs/modal-client/pull/108
         self._pre_stop = pre_stop
 
-    async def _init(self):
-        """Connect to server and retrieve version information; raise appropriate error for various failures."""
+    async def _verify(self):
         logger.debug("Client: Starting")
         _check_config()
         try:
             req = empty_pb2.Empty()
             resp = await retry_transient_errors(
                 self.stub.ClientHello,
                 req,
                 attempt_timeout=CLIENT_CREATE_ATTEMPT_TIMEOUT,
                 total_timeout=CLIENT_CREATE_TOTAL_TIMEOUT,
             )
             if resp.warning:
                 ALARM_EMOJI = chr(0x1F6A8)
                 warnings.warn(f"{ALARM_EMOJI} {resp.warning} {ALARM_EMOJI}", DeprecationError)
-            self._authenticated = True
-            self.image_builder_version = resp.image_builder_version
         except GRPCError as exc:
             if exc.status == Status.FAILED_PRECONDITION:
                 raise VersionError(
-                    f"The client version ({self.version}) is too old. Please update (pip install --upgrade modal)."
+                    f"The client version {self.version} is too old. Please update to the latest package on PyPi: https://pypi.org/project/modal"
                 )
             elif exc.status == Status.UNAUTHENTICATED:
                 raise AuthError(exc.message)
             else:
                 exc_string = await _grpc_exc_string(exc, "ClientHello", self.server_url, CLIENT_CREATE_TOTAL_TIMEOUT)
                 raise ConnectionError(exc_string)
         except (OSError, asyncio.TimeoutError) as exc:
             raise ConnectionError(str(exc))
 
     async def __aenter__(self):
         await self._open()
-        try:
-            await self._init()
-        except BaseException:
-            await self._close()
-            raise
+        if not self.no_verify:
+            try:
+                await self._verify()
+            except BaseException:
+                await self._close()
+                raise
         return self
 
     async def __aexit__(self, exc_type, exc, tb):
         await self._close()
 
     @classmethod
-    @asynccontextmanager
-    async def anonymous(cls, server_url: str) -> AsyncIterator["_Client"]:
-        """mdmd:hidden
-        Create a connection with no credentials; to be used for token creation.
-        """
-        logger.debug("Client: Starting client without authentication")
-        client = cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials=None)
-        try:
-            await client._open()
-            # Skip client._init
-            yield client
-        finally:
-            await client._close()
+    async def verify(cls, server_url, credentials):
+        """mdmd:hidden"""
+        async with _Client(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials):
+            pass  # Will call ClientHello
+
+    @classmethod
+    async def unauthenticated_client(cls, server_url: str):
+        """mdmd:hidden"""
+        # Create a connection with no credentials
+        # To be used with the token flow
+        return _Client(server_url, api_pb2.CLIENT_TYPE_CLIENT, None, no_verify=True)
 
     @classmethod
     async def from_env(cls, _override_config=None) -> "_Client":
-        """mdmd:hidden
-        Singleton that is instantiated from the Modal config and reused on subsequent calls.
-        """
+        """mdmd:hidden"""
         if _override_config:
             # Only used for testing
             c = _override_config
         else:
             c = config
 
+        # Sets server_url to socket file path if proxy is available.
         server_url = c["server_url"]
 
         token_id = c["token_id"]
         token_secret = c["token_secret"]
         task_id = c["task_id"]
         task_secret = c["task_secret"]
 
@@ -220,63 +214,52 @@
         elif token_id and token_secret:
             client_type = api_pb2.CLIENT_TYPE_CLIENT
             credentials = (token_id, token_secret)
         else:
             client_type = api_pb2.CLIENT_TYPE_CLIENT
             credentials = None
 
+        if cls._client_from_env_lock is None:
+            cls._client_from_env_lock = asyncio.Lock()
+
         async with cls._client_from_env_lock:
             if cls._client_from_env:
                 return cls._client_from_env
             else:
                 client = _Client(server_url, client_type, credentials)
                 await client._open()
                 async_utils.on_shutdown(client._close())
                 try:
-                    await client._init()
+                    await client._verify()
                 except AuthError:
                     if not credentials:
                         creds_missing_msg = (
-                            "Token missing. Could not authenticate client."
-                            " If you have token credentials, see modal.com/docs/reference/modal.config for setup help."
-                            " If you are a new user, register an account at modal.com, then run `modal token new`."
+                            "Token missing. Could not authenticate client. "
+                            "If you have token credentials, see modal.com/docs/reference/modal.config for setup help. "
+                            "If you are a new user, register an account at modal.com, then run `modal token new`."
                         )
                         raise AuthError(creds_missing_msg)
                     else:
                         raise
                 cls._client_from_env = client
                 return client
 
     @classmethod
     async def from_credentials(cls, token_id: str, token_secret: str) -> "_Client":
-        """mdmd:hidden
-        Constructor based on token credentials; useful for managing Modal on behalf of third-party users.
-        """
-        server_url = config["server_url"]
+        """mdmd:hidden"""
         client_type = api_pb2.CLIENT_TYPE_CLIENT
         credentials = (token_id, token_secret)
+        server_url = config["server_url"]
+
         client = _Client(server_url, client_type, credentials)
         await client._open()
-        try:
-            await client._init()
-        except BaseException:
-            await client._close()
-            raise
         async_utils.on_shutdown(client._close())
         return client
 
     @classmethod
-    async def verify(cls, server_url: str, credentials: Tuple[str, str]) -> None:
-        """mdmd:hidden
-        Check whether can the client can connect to this server with these credentials; raise if not.
-        """
-        async with cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials):
-            pass  # Will call ClientHello RPC and possibly raise AuthError or ConnectionError
-
-    @classmethod
     def set_env_client(cls, client: Optional["_Client"]):
         """mdmd:hidden"""
         # Just used from tests.
         cls._client_from_env = client
 
 
 Client = synchronize_api(_Client)
```

## modal/cloud_bucket_mount.py

```diff
@@ -1,150 +1,89 @@
 # Copyright Modal Labs 2022
 from dataclasses import dataclass
-from typing import List, Optional, Tuple
-from urllib.parse import urlparse
+from enum import Enum
+from typing import List, Optional, Tuple, Union
 
 from modal_proto import api_pb2
 
 from ._utils.async_utils import synchronize_api
 from .secret import _Secret
 
 
+class BucketType(Enum):
+    S3 = "s3"
+
+    @property
+    def proto(self):
+        if self.value == "s3":
+            return api_pb2.CloudBucketMount.BucketType.S3
+
+
 @dataclass
 class _CloudBucketMount:
     """Mounts a cloud bucket to your container. Currently supports AWS S3 buckets.
 
-    S3 buckets are mounted using [AWS S3 Mountpoint](https://github.com/awslabs/mountpoint-s3).
+    S3 buckets are mounted using [AWS' S3 Mountpoint](https://github.com/awslabs/mountpoint-s3).
     S3 mounts are optimized for reading large files sequentially. It does not support every file operation; consult
-    [the AWS S3 Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md) for more information.
-
-    **AWS S3 Usage**
-
-    ```python
-    import subprocess
-
-    stub = modal.Stub()
-    secret = modal.Secret.from_dict({
-        "AWS_ACCESS_KEY_ID": "...",
-        "AWS_SECRET_ACCESS_KEY": "...",
-    })
-    @stub.function(
-        volumes={
-            "/my-mount": modal.CloudBucketMount(
-                bucket_name="s3-bucket-name",
-                secret=secret,
-                read_only=True
-            )
-        }
-    )
-    def f():
-        subprocess.run(["ls", "/my-mount"], check=True)
-    ```
-
-    **Cloudflare R2 Usage**
+    [the AWS S3 Mountpoin documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md) for more information.
 
-    Cloudflare R2 is [S3-compatible](https://developers.cloudflare.com/r2/api/s3/api/) so its setup looks very similar to S3.
-    But additionally the `bucket_endpoint_url` argument must be passed.
+    **Usage**
 
     ```python
+    import modal
     import subprocess
 
     stub = modal.Stub()
-    secret = modal.Secret.from_dict({
-        "AWS_ACCESS_KEY_ID": "...",
-        "AWS_SECRET_ACCESS_KEY": "...",
-    })
-    @stub.function(
-        volumes={
-            "/my-mount": modal.CloudBucketMount(
-                bucket_name="my-r2-bucket",
-                bucket_endpoint_url="https://<ACCOUNT ID>.r2.cloudflarestorage.com",
-                secret=secret,
-                read_only=True
-            )
-        }
-    )
-    def f():
-        subprocess.run(["ls", "/my-mount"], check=True)
-    ```
-
-    **Google GCS Usage**
 
-    Google Cloud Storage (GCS) is partially [S3-compatible](https://cloud.google.com/storage/docs/interoperability).
-    Currently **only `read_only=True`** is supported for GCS buckets. GCS Buckets also require a secret with Google-specific
-    key names (see below) populated with a [HMAC key](https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create).
-
-    ```python
-    import subprocess
-
-    stub = modal.Stub()
-    gcp_hmac_secret = modal.Secret.from_dict({
-        "GOOGLE_ACCESS_KEY_ID": "GOOG1ERM12345...",
-        "GOOGLE_ACCESS_KEY_SECRET": "HTJ123abcdef...",
-    })
     @stub.function(
         volumes={
-            "/my-mount": modal.CloudBucketMount(
-                bucket_name="my-gcs-bucket",
-                bucket_endpoint_url="https://storage.googleapis.com",
-                secret=gcp_hmac_secret,
-                read_only=True,  # writing to bucket currently unsupported
-            )
+            "/my-mount": modal.CloudBucketMount("s3-bucket-name", secret=modal.Secret.from_dict({
+                "AWS_ACCESS_KEY_ID": "...",
+                "AWS_SECRET_ACCESS_KEY": "...",
+            }), read_only=True)
         }
     )
     def f():
-        subprocess.run(["ls", "/my-mount"], check=True)
+        subprocess.run(["ls", "/my-mount"])
     ```
     """
 
     bucket_name: str
-    # Endpoint URL is used to support Cloudflare R2 and Google Cloud Platform GCS.
-    bucket_endpoint_url: Optional[str] = None
 
-    # Credentials used to access a cloud bucket.
-    # If the bucket is private, the secret **must** contain AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
-    # If the bucket is publicly accessible, the secret is unnecessary and can be omitted.
+    # Credentials used to access a cloud bucket. When
+    # The given secret can contain AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
+    # AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY can be omitted if the bucket is publicly accessible.
     secret: Optional[_Secret] = None
 
     read_only: bool = False
     requester_pays: bool = False
+    bucket_type: Union[
+        BucketType, str
+    ] = BucketType.S3.value  # S3 is the default bucket type until other cloud buckets are supported
 
 
 def cloud_bucket_mounts_to_proto(mounts: List[Tuple[str, _CloudBucketMount]]) -> List[api_pb2.CloudBucketMount]:
     """Helper function to convert `CloudBucketMount` to a list of protobufs that can be passed to the server."""
     cloud_bucket_mounts: List[api_pb2.CloudBucketMount] = []
 
     for path, mount in mounts:
-        # crude mapping from mount arguments to type.
-        if mount.bucket_endpoint_url:
-            parse_result = urlparse(mount.bucket_endpoint_url)
-            if parse_result.hostname.endswith("r2.cloudflarestorage.com"):
-                bucket_type = api_pb2.CloudBucketMount.BucketType.R2
-            elif parse_result.hostname.endswith("storage.googleapis.com"):
-                bucket_type = api_pb2.CloudBucketMount.BucketType.GCP
-                if not mount.read_only:
-                    raise ValueError(
-                        f"CloudBucketMount of '{mount.bucket_name}' is invalid. Writing to GCP buckets with modal.CloudBucketMount in currently unsupported."
-                    )
-            else:
-                raise ValueError(f"Unsupported bucket endpoint hostname '{parse_result.hostname}'")
+        if isinstance(mount.bucket_type, str):
+            bucket_type = BucketType(mount.bucket_type)
         else:
-            # just assume S3; this is backwards and forwards compatible.
-            bucket_type = api_pb2.CloudBucketMount.BucketType.S3
+            bucket_type = mount.bucket_type
 
         if mount.requester_pays and not mount.secret:
             raise ValueError("Credentials required in order to use Requester Pays.")
 
         cloud_bucket_mount = api_pb2.CloudBucketMount(
             bucket_name=mount.bucket_name,
-            bucket_endpoint_url=mount.bucket_endpoint_url,
             mount_path=path,
             credentials_secret_id=mount.secret.object_id if mount.secret else "",
             read_only=mount.read_only,
-            bucket_type=bucket_type,
+            bucket_type=bucket_type.proto,
             requester_pays=mount.requester_pays,
         )
         cloud_bucket_mounts.append(cloud_bucket_mount)
 
     return cloud_bucket_mounts
```

## modal/cloud_bucket_mount.pyi

```diff
@@ -1,19 +1,32 @@
+import enum
 import modal.secret
 import modal_proto.api_pb2
 import typing
 
+class BucketType(enum.Enum):
+    def _generate_next_value_(name, start, count, last_values):
+        ...
+
+    @property
+    def proto(self):
+        ...
+
+    def __new__(cls, value):
+        ...
+
+
 class _CloudBucketMount:
     bucket_name: str
-    bucket_endpoint_url: typing.Union[str, None]
     secret: typing.Union[modal.secret._Secret, None]
     read_only: bool
     requester_pays: bool
+    bucket_type: typing.Union[BucketType, str]
 
-    def __init__(self, bucket_name: str, bucket_endpoint_url: typing.Union[str, None] = None, secret: typing.Union[modal.secret._Secret, None] = None, read_only: bool = False, requester_pays: bool = False) -> None:
+    def __init__(self, bucket_name: str, secret: typing.Union[modal.secret._Secret, None] = None, read_only: bool = False, requester_pays: bool = False, bucket_type: typing.Union[BucketType, str] = 's3') -> None:
         ...
 
     def __repr__(self):
         ...
 
     def __eq__(self, other):
         ...
@@ -21,20 +34,20 @@
 
 def cloud_bucket_mounts_to_proto(mounts: typing.List[typing.Tuple[str, _CloudBucketMount]]) -> typing.List[modal_proto.api_pb2.CloudBucketMount]:
     ...
 
 
 class CloudBucketMount:
     bucket_name: str
-    bucket_endpoint_url: typing.Union[str, None]
     secret: typing.Union[modal.secret.Secret, None]
     read_only: bool
     requester_pays: bool
+    bucket_type: typing.Union[BucketType, str]
 
-    def __init__(self, bucket_name: str, bucket_endpoint_url: typing.Union[str, None] = None, secret: typing.Union[modal.secret.Secret, None] = None, read_only: bool = False, requester_pays: bool = False) -> None:
+    def __init__(self, bucket_name: str, secret: typing.Union[modal.secret.Secret, None] = None, read_only: bool = False, requester_pays: bool = False, bucket_type: typing.Union[BucketType, str] = 's3') -> None:
         ...
 
     def __repr__(self):
         ...
 
     def __eq__(self, other):
         ...
```

## modal/cls.py

```diff
@@ -1,30 +1,29 @@
 # Copyright Modal Labs 2022
 import os
 import typing
-from typing import Any, Callable, Collection, Dict, List, Optional, Tuple, Type, TypeVar, Union
+from typing import Any, Callable, Collection, Dict, List, Optional, Type, TypeVar, Union
 
 from google.protobuf.message import Message
 from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
 from ._output import OutputManager
 from ._resolver import Resolver
-from ._resources import convert_fn_config_to_resources_config
 from ._serialization import check_valid_cls_constructor_arg
 from ._utils.async_utils import synchronize_api, synchronizer
 from ._utils.grpc_utils import retry_transient_errors
 from ._utils.mount_utils import validate_volumes
 from .client import _Client
 from .exception import InvalidError, NotFoundError
 from .functions import (
     _parse_retries,
 )
-from .gpu import GPU_T
+from .gpu import GPU_T, parse_gpu_config
 from .object import _get_environment_name, _Object
 from .partial_function import (
     PartialFunction,
     _find_callables_for_cls,
     _find_partial_methods_for_cls,
     _Function,
     _PartialFunctionFlags,
@@ -33,15 +32,15 @@
 from .secret import _Secret
 from .volume import _Volume
 
 T = TypeVar("T")
 
 
 if typing.TYPE_CHECKING:
-    import modal.app
+    import modal.stub
 
 
 class _Obj:
     """An instance of a `Cls`, i.e. `Cls("foo", 42)` returns an `Obj`.
 
     All this class does is to return `Function` objects."""
 
@@ -134,15 +133,15 @@
 
 class _Cls(_Object, type_prefix="cs"):
     _user_cls: Optional[type]
     _functions: Dict[str, _Function]
     _options: Optional[api_pb2.FunctionOptions]
     _callables: Dict[str, Callable]
     _from_other_workspace: Optional[bool]  # Functions require FunctionBindParams before invocation.
-    _stub: Optional["modal.app._App"] = None  # not set for lookups
+    _stub: Optional["modal.stub._Stub"] = None  # not set for lookups
 
     def _initialize_from_empty(self):
         self._user_cls = None
         self._functions = {}
         self._options = None
         self._callables = {}
         self._from_other_workspace = None
@@ -255,29 +254,28 @@
         cls = cls._from_loader(_load_remote, rep, is_another_app=True)
         cls._from_other_workspace = bool(workspace is not None)
         return cls
 
     def with_options(
         self: "_Cls",
         cpu: Optional[float] = None,
-        memory: Optional[Union[int, Tuple[int, int]]] = None,
+        memory: Optional[int] = None,
         gpu: GPU_T = None,
         secrets: Collection[_Secret] = (),
         volumes: Dict[Union[str, os.PathLike], _Volume] = {},
         retries: Optional[Union[int, Retries]] = None,
         timeout: Optional[int] = None,
         concurrency_limit: Optional[int] = None,
         allow_concurrent_inputs: Optional[int] = None,
         container_idle_timeout: Optional[int] = None,
         allow_background_volume_commits: bool = False,
     ) -> "_Cls":
         """
-        Beta: Allows for the runtime modification of a modal.Cls's configuration.
-
-        This is a beta feature and may be unstable.
+        Allows for the runtime modification of a modal.Cls's configuration.
+        Designed for usage in the [MK1 Flywheel](/docs/guide/mk1).
 
         **Usage:**
 
         ```python notest
         import modal
         Model = modal.Cls.lookup(
             "flywheel-generic", "Model", workspace="mk-1"
@@ -287,15 +285,17 @@
             volumes={"/models": models_vol}
         )
         Model2().generate.remote(42)
         ```
         """
         retry_policy = _parse_retries(retries)
         if gpu or cpu or memory:
-            resources = convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu)
+            milli_cpu = int(1000 * cpu) if cpu is not None else None
+            gpu_config = parse_gpu_config(gpu)
+            resources = api_pb2.Resources(milli_cpu=milli_cpu, gpu_config=gpu_config, memory_mb=memory)
         else:
             resources = None
 
         volume_mounts = [
             api_pb2.VolumeMount(
                 mount_path=path,
                 volume_id=volume.object_id,
```

## modal/cls.pyi

```diff
@@ -1,17 +1,17 @@
 import google.protobuf.message
 import modal._output
-import modal.app
 import modal.client
 import modal.functions
 import modal.gpu
 import modal.object
 import modal.partial_function
 import modal.retries
 import modal.secret
+import modal.stub
 import modal.volume
 import modal_proto.api_pb2
 import os
 import typing
 import typing_extensions
 
 T = typing.TypeVar("T")
@@ -86,15 +86,15 @@
 
 class _Cls(modal.object._Object):
     _user_cls: typing.Union[type, None]
     _functions: typing.Dict[str, modal.functions._Function]
     _options: typing.Union[modal_proto.api_pb2.FunctionOptions, None]
     _callables: typing.Dict[str, typing.Callable]
     _from_other_workspace: typing.Union[bool, None]
-    _stub: typing.Union[modal.app._App, None]
+    _stub: typing.Union[modal.stub._Stub, None]
 
     def _initialize_from_empty(self):
         ...
 
     def _initialize_from_other(self, other: _Cls):
         ...
 
@@ -111,15 +111,15 @@
     def from_local(user_cls, stub, decorator: typing.Callable[[modal.partial_function.PartialFunction, type], modal.functions._Function]) -> _Cls:
         ...
 
     @classmethod
     def from_name(cls: typing.Type[_Cls], app_name: str, tag: typing.Union[str, None] = None, namespace=1, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> _Cls:
         ...
 
-    def with_options(self: _Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret._Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume._Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> _Cls:
+    def with_options(self: _Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret._Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume._Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> _Cls:
         ...
 
     @staticmethod
     async def lookup(app_name: str, tag: typing.Union[str, None] = None, namespace=1, client: typing.Union[modal.client._Client, None] = None, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> _Cls:
         ...
 
     def __call__(self, *args, **kwargs) -> _Obj:
@@ -131,15 +131,15 @@
 
 class Cls(modal.object.Object):
     _user_cls: typing.Union[type, None]
     _functions: typing.Dict[str, modal.functions.Function]
     _options: typing.Union[modal_proto.api_pb2.FunctionOptions, None]
     _callables: typing.Dict[str, typing.Callable]
     _from_other_workspace: typing.Union[bool, None]
-    _stub: typing.Union[modal.app.App, None]
+    _stub: typing.Union[modal.stub.Stub, None]
 
     def __init__(self, *args, **kwargs):
         ...
 
     def _initialize_from_empty(self):
         ...
 
@@ -159,15 +159,15 @@
     def from_local(user_cls, stub, decorator: typing.Callable[[modal.partial_function.PartialFunction, type], modal.functions.Function]) -> Cls:
         ...
 
     @classmethod
     def from_name(cls: typing.Type[Cls], app_name: str, tag: typing.Union[str, None] = None, namespace=1, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> Cls:
         ...
 
-    def with_options(self: Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret.Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume.Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> Cls:
+    def with_options(self: Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret.Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume.Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> Cls:
         ...
 
     class __lookup_spec(typing_extensions.Protocol):
         def __call__(self, app_name: str, tag: typing.Union[str, None] = None, namespace=1, client: typing.Union[modal.client.Client, None] = None, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> Cls:
             ...
 
         async def aio(self, *args, **kwargs) -> Cls:
```

## modal/config.py

```diff
@@ -169,18 +169,14 @@
 
 
 _profile = os.environ.get("MODAL_PROFILE") or _config_active_profile()
 
 # Define settings
 
 
-def _to_boolean(x: object) -> bool:
-    return str(x).lower() not in {"", "0", "false"}
-
-
 class _Setting(typing.NamedTuple):
     default: typing.Any = None
     transform: typing.Callable[[str], typing.Any] = lambda x: x  # noqa: E731
 
 
 _SETTINGS = {
     "loglevel": _Setting("WARNING", lambda s: s.upper()),
@@ -190,26 +186,25 @@
     "token_secret": _Setting(),
     "task_id": _Setting(),
     "task_secret": _Setting(),
     "serve_timeout": _Setting(transform=float),
     "sync_entrypoint": _Setting(),
     "logs_timeout": _Setting(10, float),
     "image_id": _Setting(),
-    "automount": _Setting(True, transform=_to_boolean),
-    "profiling_enabled": _Setting(False, transform=_to_boolean),
+    "automount": _Setting(True, transform=lambda x: x not in ("", "0")),
+    "profiling_enabled": _Setting(False, transform=lambda x: x not in ("", "0")),
     "heartbeat_interval": _Setting(15, float),
     "function_runtime": _Setting(),
-    "function_runtime_debug": _Setting(False, transform=_to_boolean),  # For internal debugging use.
+    "function_runtime_debug": _Setting(False, transform=lambda x: x not in ("", "0")),  # For internal debugging use.
     "environment": _Setting(),
     "default_cloud": _Setting(None, transform=lambda x: x if x else None),
     "worker_id": _Setting(),  # For internal debugging use.
     "restore_state_path": _Setting("/__modal/restore-state.json"),
-    "force_build": _Setting(False, transform=_to_boolean),
-    "traceback": _Setting(False, transform=_to_boolean),
-    "image_builder_version": _Setting(),
+    "force_build": _Setting(False, transform=lambda x: x not in ("", "0")),
+    "traceback": _Setting(False, transform=lambda x: x not in ("", "0")),
 }
 
 
 class Config:
     """Singleton that holds configuration used by Modal internally."""
 
     def __init__(self):
```

## modal/dict.py

```diff
@@ -18,46 +18,40 @@
 def _serialize_dict(data):
     return [api_pb2.DictEntry(key=serialize(k), value=serialize(v)) for k, v in data.items()]
 
 
 class _Dict(_Object, type_prefix="di"):
     """Distributed dictionary for storage in Modal apps.
 
-    Keys and values can be essentially any object, so long as they can be serialized by
-    `cloudpickle`, which includes other Modal objects.
+    Keys and values can be essentially any object, so long as they can be
+    serialized by `cloudpickle`, including Modal objects.
 
     **Lifetime of a Dict and its items**
 
     An individual dict entry will expire 30 days after it was last added to its Dict object.
-    Additionally, data are stored in memory on the Modal server and could be lost due to
-    unexpected server restarts. Because of this, `Dict` is best suited for storing short-term
-    state and is not recommended for durable storage.
+    Because of this, `Dict`s are best not used for
+    long-term storage. All data is deleted when the app is stopped.
 
     **Usage**
 
     ```python
-    import modal
+    from modal import Dict, Stub
 
-    stub = modal.Stub()
-    my_dict = modal.Dict.from_name("my-persisted_dict", create_if_missing=True)
+    stub = Stub()
+    my_dict = Dict.from_name("my-persisted_dict", create_if_missing=True)
 
     @stub.local_entrypoint()
     def main():
         my_dict["some key"] = "some value"
         my_dict[123] = 456
 
         assert my_dict["some key"] == "some value"
         assert my_dict[123] == 456
     ```
 
-    The `Dict` class offers a few methods for operations that are usually accomplished
-    in Python with operators, such as `Dict.put` and `Dict.contains`. The advantage of
-    these methods is that they can be safely called in an asynchronous context, whereas
-    their operator-based analogues will block the event loop.
-
     For more examples, see the [guide](/docs/guide/dicts-and-queues#modal-dicts).
     """
 
     @staticmethod
     def new(data: Optional[dict] = None) -> "_Dict":
         """`Dict.new` is deprecated.
 
@@ -146,15 +140,15 @@
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
                 data=serialized,
             )
             response = await resolver.client.stub.DictGetOrCreate(req)
             logger.debug(f"Created dict with id {response.dict_id}")
             self._hydrate(response.dict_id, resolver.client, None)
 
-        return _Dict._from_loader(_load, "Dict()", is_another_app=True, hydrate_lazily=True)
+        return _Dict._from_loader(_load, "Dict()", is_another_app=True)
 
     @staticmethod
     def persisted(
         label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None
     ) -> "_Dict":
         """Deprecated! Use `Dict.from_name(name, create_if_missing=True)`."""
         deprecation_warning((2024, 3, 1), _Dict.persisted.__doc__)
@@ -221,15 +215,15 @@
         resp = await retry_transient_errors(self._client.stub.DictLen, req)
         return resp.len
 
     @live_method
     async def __getitem__(self, key: Any) -> Any:
         """Get the value associated with a key.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         NOT_FOUND = object()
         value = await self.get(key, NOT_FOUND)
         if value is NOT_FOUND:
             raise KeyError(f"{key} not in dict {self.object_id}")
 
         return value
@@ -249,15 +243,15 @@
         req = api_pb2.DictUpdateRequest(dict_id=self.object_id, updates=serialized)
         await retry_transient_errors(self._client.stub.DictUpdate, req)
 
     @live_method
     async def __setitem__(self, key: Any, value: Any) -> None:
         """Set a specific key-value pair to the dictionary.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         return await self.put(key, value)
 
     @live_method
     async def pop(self, key: Any) -> Any:
         """Remove a key from the dictionary, returning the value if it exists."""
         req = api_pb2.DictPopRequest(dict_id=self.object_id, key=serialize(key))
@@ -266,21 +260,21 @@
             raise KeyError(f"{key} not in dict {self.object_id}")
         return deserialize(resp.value, self._client)
 
     @live_method
     async def __delitem__(self, key: Any) -> Any:
         """Delete a key from the dictionary.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         return await self.pop(key)
 
     @live_method
     async def __contains__(self, key: Any) -> bool:
         """Return if a key is present.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         return await self.contains(key)
 
 
 Dict = synchronize_api(_Dict)
```

## modal/exception.py

```diff
@@ -117,15 +117,15 @@
 
 
 def deprecation_error(deprecated_on: Tuple[int, int, int], msg: str):
     raise DeprecationError(f"Deprecated on {date(*deprecated_on)}: {msg}")
 
 
 def deprecation_warning(
-    deprecated_on: Tuple[int, int, int], msg: str, *, pending: bool = False, show_source: bool = True
+    deprecated_on: Tuple[int, int, int], msg: str, pending: bool = False, show_source: bool = True
 ) -> None:
     """Utility for getting the proper stack entry.
 
     See the implementation of the built-in [warnings.warn](https://docs.python.org/3/library/warnings.html#available-functions).
     """
     filename, lineno = "<unknown>", 0
     if show_source:
```

## modal/experimental.py

```diff
@@ -1,9 +1,10 @@
 # Copyright Modal Labs 2022
-from ._container_io_manager import _ContainerIOManager
 
 
 def stop_fetching_inputs():
     """Don't fetch any more inputs from the server, after the current one.
     The container will exit gracefully after the current input is processed."""
 
-    _ContainerIOManager.stop_fetching_inputs()
+    from .app import _container_app
+
+    _container_app.stop_fetching_inputs()
```

## modal/functions.py

```diff
@@ -1,67 +1,63 @@
 # Copyright Modal Labs 2023
 import asyncio
 import inspect
 import time
-import typing
 import warnings
 from contextvars import ContextVar
 from dataclasses import dataclass
 from pathlib import PurePosixPath
 from typing import (
     TYPE_CHECKING,
     Any,
     AsyncGenerator,
     AsyncIterable,
     AsyncIterator,
     Callable,
     Collection,
     Dict,
-    Iterable,
     List,
+    Literal,
     Optional,
     Sequence,
     Set,
     Sized,
     Tuple,
     Type,
     Union,
 )
 
 from aiostream import pipe, stream
 from google.protobuf.message import Message
 from grpclib import GRPCError, Status
-from synchronicity.combined_types import MethodWithAio
+from grpclib.exceptions import StreamTerminatedError
 from synchronicity.exceptions import UserCodeException
 
+from modal import _pty, is_local
 from modal_proto import api_grpc, api_pb2
 
-from ._container_io_manager import is_local
 from ._location import parse_cloud_provider
 from ._output import OutputManager
-from ._pty import get_pty_info
 from ._resolver import Resolver
-from ._resources import convert_fn_config_to_resources_config
 from ._serialization import deserialize, deserialize_data_format, serialize
 from ._traceback import append_modal_tb
 from ._utils.async_utils import (
-    AsyncOrSyncIteratable,
     queue_batch_iterator,
     synchronize_api,
     synchronizer,
     warn_if_generator_is_not_consumed,
 )
 from ._utils.blob_utils import (
     BLOB_MAX_PARALLELISM,
     MAX_OBJECT_SIZE_BYTES,
     blob_download,
     blob_upload,
 )
-from ._utils.function_utils import FunctionInfo, _stream_function_call_data, get_referred_objects, is_async
-from ._utils.grpc_utils import retry_transient_errors
+from ._utils.function_utils import FunctionInfo, get_referred_objects, is_async
+from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors, unary_stream
 from ._utils.mount_utils import validate_mount_points, validate_volumes
 from .call_graph import InputInfo, _reconstruct_call_graph
 from .client import _Client
 from .cloud_bucket_mount import _CloudBucketMount, cloud_bucket_mounts_to_proto
 from .config import config, logger
 from .exception import (
     ExecutionError,
@@ -84,36 +80,15 @@
 from .volume import _Volume
 
 OUTPUTS_TIMEOUT = 55.0  # seconds
 ATTEMPT_TIMEOUT_GRACE_PERIOD = 5  # seconds
 
 
 if TYPE_CHECKING:
-    import modal.app
-
-
-class _SynchronizedQueue:
-    """mdmd:hidden"""
-
-    # small wrapper around asyncio.Queue to make it cross-thread compatible through synchronicity
-    async def init(self):
-        # in Python 3.8 the asyncio.Queue is bound to the event loop on creation
-        # so it needs to be created in a synchronicity-wrapped init method
-        self.q = asyncio.Queue()
-
-    @synchronizer.no_io_translation
-    async def put(self, item):
-        await self.q.put(item)
-
-    @synchronizer.no_io_translation
-    async def get(self):
-        return await self.q.get()
-
-
-SynchronizedQueue = synchronize_api(_SynchronizedQueue)
+    import modal.stub
 
 
 def exc_with_hints(exc: BaseException):
     """mdmd:hidden"""
     if isinstance(exc, ImportError) and exc.msg == "attempted relative import with no known parent package":
         exc.msg += """\n
 HINT: For relative imports to work, you might need to run your modal app as a module. Try:
@@ -200,14 +175,53 @@
     else:
         return api_pb2.FunctionPutInputsItem(
             input=api_pb2.FunctionInput(args=args_serialized, data_format=api_pb2.DATA_FORMAT_PICKLE),
             idx=idx,
         )
 
 
+async def _stream_function_call_data(
+    client, function_call_id: str, variant: Literal["data_in", "data_out"]
+) -> AsyncIterator[Any]:
+    """Read from the `data_in` or `data_out` stream of a function call."""
+    last_index = 0
+    retries_remaining = 10
+
+    if variant == "data_in":
+        stub_fn = client.stub.FunctionCallGetDataIn
+    elif variant == "data_out":
+        stub_fn = client.stub.FunctionCallGetDataOut
+    else:
+        raise ValueError(f"Invalid variant {variant}")
+
+    while True:
+        req = api_pb2.FunctionCallGetDataRequest(function_call_id=function_call_id, last_index=last_index)
+        try:
+            async for chunk in unary_stream(stub_fn, req):
+                if chunk.index <= last_index:
+                    continue
+                last_index = chunk.index
+                if chunk.data_blob_id:
+                    message_bytes = await blob_download(chunk.data_blob_id, client.stub)
+                else:
+                    message_bytes = chunk.data
+                message = deserialize_data_format(message_bytes, chunk.data_format, client)
+                yield message
+        except (GRPCError, StreamTerminatedError) as exc:
+            if retries_remaining > 0:
+                retries_remaining -= 1
+                if isinstance(exc, GRPCError):
+                    if exc.status in RETRYABLE_GRPC_STATUS_CODES:
+                        await asyncio.sleep(1.0)
+                        continue
+                elif isinstance(exc, StreamTerminatedError):
+                    continue
+            raise
+
+
 @dataclass
 class _OutputValue:
     # box class for distinguishing None results from non-existing/None markers
     value: Any
 
 
 class _Invocation:
@@ -323,15 +337,16 @@
 
 
 MAP_INVOCATION_CHUNK_SIZE = 49
 
 
 async def _map_invocation(
     function_id: str,
-    raw_input_queue: _SynchronizedQueue,
+    input_stream: AsyncIterable[Any],
+    kwargs: Dict[str, Any],
     client: _Client,
     order_outputs: bool,
     return_exceptions: bool,
     count_update_callback: Optional[Callable[[int, int], None]],
 ):
     assert client.stub
     request = api_pb2.FunctionMapRequest(
@@ -343,71 +358,58 @@
     response = await retry_transient_errors(client.stub.FunctionMap, request)
 
     function_call_id = response.function_call_id
 
     have_all_inputs = False
     num_inputs = 0
     num_outputs = 0
-
-    def count_update():
-        if count_update_callback is not None:
-            count_update_callback(num_outputs, num_inputs)
-
     pending_outputs: Dict[str, int] = {}  # Map input_id -> next expected gen_index value
     completed_outputs: Set[str] = set()  # Set of input_ids whose outputs are complete (expecting no more values)
 
     input_queue: asyncio.Queue = asyncio.Queue()
 
-    async def create_input(argskwargs):
+    async def create_input(arg: Any) -> api_pb2.FunctionPutInputsItem:
         nonlocal num_inputs
         idx = num_inputs
         num_inputs += 1
-        (args, kwargs) = argskwargs
-        return await _create_input(args, kwargs, client, idx)
-
-    async def input_iter():
-        while 1:
-            raw_input = await raw_input_queue.get()
-            if raw_input is None:  # end of input sentinel
-                return
-            yield raw_input  # args, kwargs
+        item = await _create_input(arg, kwargs, client, idx=idx)
+        return item
 
     async def drain_input_generator():
         # Parallelize uploading blobs
-        proto_input_stream = stream.iterate(input_iter()) | pipe.map(
+        proto_input_stream = stream.iterate(input_stream) | pipe.map(
             create_input,  # type: ignore[reportArgumentType]
             ordered=True,
             task_limit=BLOB_MAX_PARALLELISM,
         )
         async with proto_input_stream.stream() as streamer:
             async for item in streamer:
                 await input_queue.put(item)
 
         # close queue iterator
         await input_queue.put(None)
         yield
 
     async def pump_inputs():
         assert client.stub
-        nonlocal have_all_inputs, num_inputs
+        nonlocal have_all_inputs
         async for items in queue_batch_iterator(input_queue, MAP_INVOCATION_CHUNK_SIZE):
             request = api_pb2.FunctionPutInputsRequest(
                 function_id=function_id, inputs=items, function_call_id=function_call_id
             )
             logger.debug(
                 f"Pushing {len(items)} inputs to server. Num queued inputs awaiting push is {input_queue.qsize()}."
             )
             resp = await retry_transient_errors(
                 client.stub.FunctionPutInputs,
                 request,
                 max_retries=None,
                 max_delay=10,
                 additional_status_codes=[Status.RESOURCE_EXHAUSTED],
             )
-            count_update()
             for item in resp.inputs:
                 pending_outputs.setdefault(item.input_id, 0)
             logger.debug(
                 f"Successfully pushed {len(items)} inputs to server. Num queued inputs awaiting push is {input_queue.qsize()}."
             )
 
         have_all_inputs = True
@@ -476,15 +478,16 @@
 
         # map to store out-of-order outputs received
         received_outputs = {}
         output_idx = 0
 
         async with outputs_fetched.stream() as streamer:
             async for idx, output in streamer:
-                count_update()
+                if count_update_callback is not None:
+                    count_update_callback(num_outputs, num_inputs)
                 if not order_outputs:
                     yield _OutputValue(output)
                 else:
                     # hold on to outputs for function maps, so we can reorder them correctly.
                     received_outputs[idx] = output
                     while output_idx in received_outputs:
                         output = received_outputs.pop(output_idx)
@@ -529,49 +532,48 @@
         err_object = f"Function {raw_f}" if raw_f else "Function"
         raise InvalidError(
             f"{err_object} retries must be an integer or instance of modal.Retries. Found: {type(retries)}"
         )
 
 
 @dataclass
-class _FunctionSpec:
+class FunctionEnv:
     """
-    Stores information about a Function specification.
-    This is used for `modal shell` to support running shells with
-    the same configuration as a user-defined Function.
+    Stores information about the function environment. This is used for `modal shell` to support
+    running shells in the same environment as a user-defined function.
     """
 
     image: Optional[_Image]
     mounts: Sequence[_Mount]
     secrets: Sequence[_Secret]
     network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem]
     volumes: Dict[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]]
     gpu: GPU_T
     cloud: Optional[str]
     cpu: Optional[float]
-    memory: Optional[Union[int, Tuple[int, int]]]
+    memory: Optional[int]
 
 
 class _Function(_Object, type_prefix="fu"):
     """Functions are the basic units of serverless execution on Modal.
 
     Generally, you will not construct a `Function` directly. Instead, use the
     `@stub.function()` decorator on the `Stub` object for your application.
     """
 
     # TODO: more type annotations
     _info: Optional[FunctionInfo]
     _all_mounts: Collection[_Mount]
-    _stub: "modal.app._App"
+    _stub: "modal.stub._Stub"
     _obj: Any
     _web_url: Optional[str]
     _is_remote_cls_method: bool = False  # TODO(erikbern): deprecated
     _function_name: Optional[str]
     _is_method: bool
-    _spec: _FunctionSpec
+    _env: FunctionEnv
     _tag: str
     _raw_f: Callable[..., Any]
     _build_args: dict
     _parent: "_Function"
 
     @staticmethod
     def from_args(
@@ -585,15 +587,15 @@
         gpu: GPU_T = None,
         # TODO: maybe break this out into a separate decorator for notebooks.
         mounts: Collection[_Mount] = (),
         network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
         allow_cross_region_volumes: bool = False,
         volumes: Dict[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]] = {},
         webhook_config: Optional[api_pb2.WebhookConfig] = None,
-        memory: Optional[Union[int, Tuple[int, int]]] = None,
+        memory: Optional[int] = None,
         proxy: Optional[_Proxy] = None,
         retries: Optional[Union[int, Retries]] = None,
         timeout: Optional[int] = None,
         concurrency_limit: Optional[int] = None,
         allow_concurrent_inputs: Optional[int] = None,
         container_idle_timeout: Optional[int] = None,
         cpu: Optional[float] = None,
@@ -659,15 +661,15 @@
         gpu_config = parse_gpu_config(gpu)
 
         if proxy:
             # HACK: remove this once we stop using ssh tunnels for this.
             if image:
                 image = image.apt_install("autossh")
 
-        function_spec = _FunctionSpec(
+        function_env = FunctionEnv(
             mounts=all_mounts,
             secrets=secrets,
             gpu=gpu,
             network_file_systems=network_file_systems,
             volumes=volumes,
             image=image,
             cloud=cloud,
@@ -796,18 +798,22 @@
             status_row.message(f"Creating {tag}...")
 
             if is_generator:
                 function_type = api_pb2.Function.FUNCTION_TYPE_GENERATOR
             else:
                 function_type = api_pb2.Function.FUNCTION_TYPE_FUNCTION
 
+            if cpu is not None and cpu < 0.25:
+                raise InvalidError(f"Invalid fractional CPU value {cpu}. Cannot have less than 0.25 CPU resources.")
+            milli_cpu = int(1000 * cpu) if cpu is not None else 0
+
             timeout_secs = timeout
 
             if stub and stub.is_interactive and not is_builder_function:
-                pty_info = get_pty_info(shell=False)
+                pty_info = _pty.get_pty_info(shell=False)
             else:
                 pty_info = None
 
             if info.is_serialized():
                 # Use cloudpickle. Used when working w/ Jupyter notebooks.
                 # serialize at _load time, not function decoration time
                 # otherwise we can't capture a surrounding class for lifetime methods etc.
@@ -861,15 +867,15 @@
                 mount_ids=loaded_mount_ids,
                 secret_ids=[secret.object_id for secret in secrets],
                 image_id=(image.object_id if image else ""),
                 definition_type=info.definition_type,
                 function_serialized=function_serialized or b"",
                 class_serialized=class_serialized or b"",
                 function_type=function_type,
-                resources=convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu),
+                resources=api_pb2.Resources(milli_cpu=milli_cpu, gpu_config=gpu_config, memory_mb=memory or 0),
                 webhook_config=webhook_config,
                 shared_volume_mounts=network_file_system_mount_protos(
                     validated_network_file_systems, allow_cross_region_volumes
                 ),
                 volume_mounts=volume_mounts,
                 proxy_id=(proxy.object_id if proxy else None),
                 retry_policy=retry_policy,
@@ -950,15 +956,15 @@
         obj._info = info
         obj._tag = tag
         obj._all_mounts = all_mounts  # needed for modal.serve file watching
         obj._stub = stub  # needed for CLI right now
         obj._obj = None
         obj._is_generator = is_generator
         obj._is_method = bool(info.cls)
-        obj._spec = function_spec  # needed for modal shell
+        obj._env = function_env  # needed for modal shell
 
         # Used to check whether we should rebuild an image using run_function
         # Plaintext source and arg definition for the function, so it's part of the image
         # hash. We can't use the cloudpickle hash because it's not very stable.
         obj._build_args = dict(  # See get_build_def
             secrets=repr(secrets),
             gpu_config=repr(gpu_config),
@@ -1094,28 +1100,28 @@
     @property
     def tag(self) -> str:
         """mdmd:hidden"""
         assert self._tag
         return self._tag
 
     @property
-    def stub(self) -> "modal.app._App":
+    def stub(self) -> "modal.stub._Stub":
         """mdmd:hidden"""
         return self._stub
 
     @property
     def info(self) -> FunctionInfo:
         """mdmd:hidden"""
         assert self._info
         return self._info
 
     @property
-    def spec(self) -> _FunctionSpec:
+    def env(self) -> FunctionEnv:
         """mdmd:hidden"""
-        return self._spec
+        return self._env
 
     def get_build_def(self) -> str:
         """mdmd:hidden"""
         assert hasattr(self, "_raw_f") and hasattr(self, "_build_args")
         return f"{inspect.getsource(self._raw_f)}\n{repr(self._build_args)}"
 
     # Live handle methods
@@ -1170,26 +1176,15 @@
 
     @property
     def is_generator(self) -> bool:
         """mdmd:hidden"""
         assert self._is_generator is not None
         return self._is_generator
 
-    @live_method_gen
-    async def _map(
-        self, input_queue: _SynchronizedQueue, order_outputs: bool, return_exceptions: bool
-    ) -> AsyncGenerator[Any, None]:
-        """mdmd:hidden
-
-        Synchronicity-wrapped map implementation. To be safe against invocations of user code in the synchronicity thread
-        it doesn't accept an [async]iterator, and instead takes a _SynchronizedQueue instance.
-
-        _SynchronizedQueue is used instead of asyncio.Queue so that the main thread can put
-        items in the queue safely.
-        """
+    async def _map(self, input_stream: AsyncIterable[Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
         if self._web_url:
             raise InvalidError(
                 "A web endpoint function cannot be directly invoked for parallel remote execution. "
                 f"Invoke this function via its web url '{self._web_url}' or call it locally: {self._function_name}()."
             )
         if self._is_generator:
             raise InvalidError("A generator function cannot be called with `.map(...)`.")
@@ -1197,15 +1192,16 @@
         assert self._function_name
         count_update_callback = (
             self._output_mgr.function_progress_callback(self._function_name, total=None) if self._output_mgr else None
         )
 
         async for item in _map_invocation(
             self.object_id,
-            input_queue,
+            input_stream,
+            kwargs,
             self._client,
             order_outputs,
             return_exceptions,
             count_update_callback,
         ):
             yield item
 
@@ -1217,38 +1213,36 @@
             # this can happen if the user terminates a program, triggering a cancellation cascade
             if not self._mute_cancellation:
                 raise
 
     async def _call_function_nowait(self, args, kwargs) -> _Invocation:
         return await _Invocation.create(self.object_id, args, kwargs, self._client)
 
-    @warn_if_generator_is_not_consumed()
+    @warn_if_generator_is_not_consumed
     @live_method_gen
     @synchronizer.no_input_translation
     async def _call_generator(self, args, kwargs):
         invocation = await _Invocation.create(self.object_id, args, kwargs, self._client)
         async for res in invocation.run_generator():
             yield res
 
     @synchronizer.no_io_translation
     async def _call_generator_nowait(self, args, kwargs):
         return await _Invocation.create(self.object_id, args, kwargs, self._client)
 
-    # note that `map()` is not synchronicity-wrapped, since it accepts executable code in the form of
-    # iterators that we don't want to run inside the synchronicity thread. We delegate to `._map()` with
-    # a safer Queue as input
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.map")
-    def _map_sync(
+    @warn_if_generator_is_not_consumed
+    @live_method_gen
+    @synchronizer.no_input_translation
+    async def map(
         self,
-        *input_iterators: Iterable[Any],  # one input iterator per argument in the mapped-over function/generator
+        *input_iterators,  # one input iterator per argument in the mapped-over function/generator
         kwargs={},  # any extra keyword arguments for the function
         order_outputs: bool = True,  # return outputs in order
-        return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
-    ) -> AsyncOrSyncIteratable:
+        return_exceptions: bool = False,  # propogate exceptions (False) or aggregate them in the results list (True)
+    ) -> AsyncGenerator[Any, None]:
         """Parallel map over a set of inputs.
 
         Takes one iterator argument per argument in the function being mapped over.
 
         Example:
         ```python
         @stub.function()
@@ -1278,113 +1272,38 @@
         @stub.local_entrypoint()
         def main():
             # [0, 1, UserCodeException(Exception('ohno'))]
             print(list(my_func.map(range(3), return_exceptions=True)))
         ```
         """
 
-        return AsyncOrSyncIteratable(
-            self._map_async(
-                *input_iterators, kwargs=kwargs, order_outputs=order_outputs, return_exceptions=return_exceptions
-            ),
-            nested_async_message="You can't iter(Function.map()) or Function.for_each() from an async function. Use async for ... Function.map.aio() or Function.for_each.aio() instead.",
-        )
-
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.map.aio")
-    async def _map_async(
-        self,
-        *input_iterators: Union[
-            Iterable[Any], AsyncIterable[Any]
-        ],  # one input iterator per argument in the mapped-over function/generator
-        kwargs={},  # any extra keyword arguments for the function
-        order_outputs: bool = True,  # return outputs in order
-        return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
-    ) -> AsyncGenerator[Any, None]:
-        """mdmd:hidden
-        This runs in an event loop on the main thread
-
-        It concurrently feeds new input to the input queue and yields available outputs
-        to the caller.
-        Note that since the iterator(s) can block, it's a bit opaque how often the event
-        loop decides to get a new input vs how often it will emit a new output.
-        We could make this explicit as an improvement or even let users decide what they
-        prefer: throughput (prioritize queueing inputs) or latency (prioritize yielding results)
-        """
-        raw_input_queue: Any = SynchronizedQueue()  # type: ignore
-        raw_input_queue.init()
-
-        async def feed_queue():
-            # This runs in a main thread event loop, so it doesn't block the synchronizer loop
-            async with stream.zip(*[stream.iterate(it) for it in input_iterators]).stream() as streamer:
-                async for args in streamer:
-                    await raw_input_queue.put.aio((args, kwargs))
-            await raw_input_queue.put.aio(None)  # end-of-input sentinel
-
-        feed_input_task = asyncio.create_task(feed_queue())
-
-        try:
-            async for output in self._map.aio(raw_input_queue, order_outputs, return_exceptions):  # type: ignore[reportFunctionMemberAccess]
-                yield output
-        finally:
-            feed_input_task.cancel()  # should only be needed in case of exceptions
+        input_stream = stream.zip(*(stream.iterate(it) for it in input_iterators))
+        async for item in self._map(input_stream, order_outputs, return_exceptions, kwargs):
+            yield item
 
-    def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
+    @synchronizer.no_input_translation
+    async def for_each(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
         """Execute function for all inputs, ignoring outputs.
 
         Convenient alias for `.map()` in cases where the function just needs to be called.
         as the caller doesn't have to consume the generator to process the inputs.
         """
         # TODO(erikbern): it would be better if this is more like a map_spawn that immediately exits
         # rather than iterating over the result
-        for _ in self.map(*input_iterators, kwargs=kwargs, order_outputs=False, return_exceptions=ignore_exceptions):
-            pass
-
-    @synchronizer.nowrap
-    async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        async for _ in self.map.aio(  # type: ignore
+        async for _ in self.map(
             *input_iterators, kwargs=kwargs, order_outputs=False, return_exceptions=ignore_exceptions
         ):
             pass
 
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.starmap")
-    async def _starmap_async(
-        self,
-        input_iterator: Union[Iterable[Sequence[Any]], AsyncIterable[Sequence[Any]]],
-        kwargs={},
-        order_outputs: bool = True,
-        return_exceptions: bool = False,
-    ) -> typing.AsyncIterable[Any]:
-        raw_input_queue: Any = SynchronizedQueue()  # type: ignore
-        raw_input_queue.init()
-
-        async def feed_queue():
-            # This runs in a main thread event loop, so it doesn't block the synchronizer loop
-            async with stream.iterate(input_iterator).stream() as streamer:
-                async for args in streamer:
-                    await raw_input_queue.put.aio((args, kwargs))
-            await raw_input_queue.put.aio(None)  # end-of-input sentinel
-
-        feed_input_task = asyncio.create_task(feed_queue())
-        try:
-            async for output in self._map.aio(raw_input_queue, order_outputs, return_exceptions):  # type: ignore[reportFunctionMemberAccess]
-                yield output
-        finally:
-            feed_input_task.cancel()  # should only be needed in case of exceptions
-
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.starmap.aio")
-    def _starmap_sync(
-        self,
-        input_iterator: Iterable[Sequence[Any]],
-        kwargs={},
-        order_outputs: bool = True,
-        return_exceptions: bool = False,
-    ) -> AsyncOrSyncIteratable:
+    @warn_if_generator_is_not_consumed
+    @live_method_gen
+    @synchronizer.no_input_translation
+    async def starmap(
+        self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False
+    ) -> AsyncGenerator[Any, None]:
         """Like `map`, but spreads arguments over multiple function arguments.
 
         Assumes every input is a sequence (e.g. a tuple).
 
         Example:
         ```python
         @stub.function()
@@ -1393,20 +1312,17 @@
 
 
         @stub.local_entrypoint()
         def main():
             assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]
         ```
         """
-        return AsyncOrSyncIteratable(
-            self._starmap_async(
-                input_iterator, kwargs=kwargs, order_outputs=order_outputs, return_exceptions=return_exceptions
-            ),
-            nested_async_message="You can't run Function.map() or Function.for_each() from an async function. Use Function.map.aio()/Function.for_each.aio() instead.",
-        )
+        input_stream = stream.iterate(input_iterator)
+        async for item in self._map(input_stream, order_outputs, return_exceptions, kwargs):
+            yield item
 
     @synchronizer.no_io_translation
     @live_method
     async def remote(self, *args, **kwargs) -> Any:
         """
         Calls the function remotely, executing it with the given arguments and returning the execution's result.
         """
@@ -1466,17 +1382,15 @@
         else:
             return self._obj
 
     @synchronizer.nowrap
     def local(self, *args, **kwargs) -> Any:
         """
         Calls the function locally, executing it with the given arguments and returning the execution's result.
-
-        The function will execute in the same environment as the caller, just like calling the underlying function
-        directly in Python. In particular, secrets will not be available through environment variables.
+        This method allows a caller to execute the standard Python function wrapped by Modal.
         """
         # TODO(erikbern): it would be nice to remove the nowrap thing, but right now that would cause
         # "user code" to run on the synchronicity thread, which seems bad
         info = self._get_info()
         if not info:
             msg = (
                 "The definition for this function is missing so it is not possible to invoke it locally. "
@@ -1537,22 +1451,14 @@
         resp = await self._client.stub.FunctionGetCurrentStats(
             api_pb2.FunctionGetCurrentStatsRequest(function_id=self.object_id)
         )
         return FunctionStats(
             backlog=resp.backlog, num_active_runners=resp.num_active_tasks, num_total_runners=resp.num_total_tasks
         )
 
-    # A bit hacky - but the map-style functions need to not be synchronicity-wrapped
-    # in order to not execute their input iterators on the synchronicity event loop.
-    # We still need to wrap them using MethodWithAio to maintain a synchronicity-like
-    # api with `.aio` and get working type-stubs and reference docs generation:
-    map = MethodWithAio(_map_sync, _map_async, synchronizer)
-    starmap = MethodWithAio(_starmap_sync, _starmap_async, synchronizer)
-    for_each = MethodWithAio(_for_each_sync, _for_each_async, synchronizer)
-
 
 Function = synchronize_api(_Function)
 
 
 class _FunctionCall(_Object, type_prefix="fc"):
     """A reference to an executed function call.
```

## modal/functions.pyi

```diff
@@ -1,88 +1,48 @@
 import _contextvars
 import google.protobuf.message
 import modal._output
-import modal._utils.async_utils
 import modal._utils.function_utils
-import modal.app
 import modal.call_graph
 import modal.client
 import modal.cloud_bucket_mount
 import modal.gpu
 import modal.image
 import modal.mount
 import modal.network_file_system
 import modal.object
 import modal.proxy
 import modal.retries
 import modal.schedule
 import modal.scheduler_placement
 import modal.secret
+import modal.stub
 import modal.volume
 import modal_proto.api_grpc
 import modal_proto.api_pb2
 import pathlib
 import typing
 import typing_extensions
 
-class _SynchronizedQueue:
-    async def init(self):
-        ...
-
-    async def put(self, item):
-        ...
-
-    async def get(self):
-        ...
-
-
-class SynchronizedQueue:
-    def __init__(self, /, *args, **kwargs):
-        ...
-
-    class __init_spec(typing_extensions.Protocol):
-        def __call__(self):
-            ...
-
-        async def aio(self, *args, **kwargs):
-            ...
-
-    init: __init_spec
-
-    class __put_spec(typing_extensions.Protocol):
-        def __call__(self, item):
-            ...
-
-        async def aio(self, *args, **kwargs):
-            ...
-
-    put: __put_spec
-
-    class __get_spec(typing_extensions.Protocol):
-        def __call__(self):
-            ...
-
-        async def aio(self, *args, **kwargs):
-            ...
-
-    get: __get_spec
-
-
 def exc_with_hints(exc: BaseException):
     ...
 
 
 async def _process_result(result: modal_proto.api_pb2.GenericResult, data_format: int, stub, client=None):
     ...
 
 
 async def _create_input(args, kwargs, client, idx: typing.Union[int, None] = None) -> modal_proto.api_pb2.FunctionPutInputsItem:
     ...
 
 
+def _stream_function_call_data(client, function_call_id: str, variant: typing.Literal['data_in', 'data_out']) -> typing.AsyncIterator[typing.Any]:
+    ...
+
+
 class _OutputValue:
     value: typing.Any
 
     def __init__(self, value: typing.Any) -> None:
         ...
 
     def __repr__(self):
@@ -109,15 +69,15 @@
     async def poll_function(self, timeout: typing.Union[float, None] = None):
         ...
 
     def run_generator(self):
         ...
 
 
-def _map_invocation(function_id: str, raw_input_queue: _SynchronizedQueue, client: modal.client._Client, order_outputs: bool, return_exceptions: bool, count_update_callback: typing.Union[typing.Callable[[int, int], None], None]):
+def _map_invocation(function_id: str, input_stream: typing.AsyncIterable[typing.Any], kwargs: typing.Dict[str, typing.Any], client: modal.client._Client, order_outputs: bool, return_exceptions: bool, count_update_callback: typing.Union[typing.Callable[[int, int], None], None]):
     ...
 
 
 class FunctionStats:
     backlog: int
     num_active_runners: int
     num_total_runners: int
@@ -141,52 +101,52 @@
         ...
 
 
 def _parse_retries(retries: typing.Union[int, modal.retries.Retries, None], raw_f: typing.Union[typing.Callable, None] = None) -> typing.Union[modal_proto.api_pb2.FunctionRetryPolicy, None]:
     ...
 
 
-class _FunctionSpec:
+class FunctionEnv:
     image: typing.Union[modal.image._Image, None]
     mounts: typing.Sequence[modal.mount._Mount]
     secrets: typing.Sequence[modal.secret._Secret]
     network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem]
     volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]]
     gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig]
     cloud: typing.Union[str, None]
     cpu: typing.Union[float, None]
-    memory: typing.Union[int, typing.Tuple[int, int], None]
+    memory: typing.Union[int, None]
 
-    def __init__(self, image: typing.Union[modal.image._Image, None], mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig], cloud: typing.Union[str, None], cpu: typing.Union[float, None], memory: typing.Union[int, typing.Tuple[int, int], None]) -> None:
+    def __init__(self, image: typing.Union[modal.image._Image, None], mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig], cloud: typing.Union[str, None], cpu: typing.Union[float, None], memory: typing.Union[int, None]) -> None:
         ...
 
     def __repr__(self):
         ...
 
     def __eq__(self, other):
         ...
 
 
 class _Function(modal.object._Object):
     _info: typing.Union[modal._utils.function_utils.FunctionInfo, None]
     _all_mounts: typing.Collection[modal.mount._Mount]
-    _stub: modal.app._App
+    _stub: modal.stub._Stub
     _obj: typing.Any
     _web_url: typing.Union[str, None]
     _is_remote_cls_method: bool
     _function_name: typing.Union[str, None]
     _is_method: bool
-    _spec: _FunctionSpec
+    _env: FunctionEnv
     _tag: str
     _raw_f: typing.Callable[..., typing.Any]
     _build_args: dict
     _parent: _Function
 
     @staticmethod
-    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image._Image, secret: typing.Union[modal.secret._Secret, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
+    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image._Image, secret: typing.Union[modal.secret._Secret, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
         ...
 
     def from_parametrized(self, obj, from_other_workspace: bool, options: typing.Union[modal_proto.api_pb2.FunctionOptions, None], args: typing.Sized, kwargs: typing.Dict[str, typing.Any]) -> _Function:
         ...
 
     async def keep_warm(self, warm_pool_size: int) -> None:
         ...
@@ -200,23 +160,23 @@
         ...
 
     @property
     def tag(self) -> str:
         ...
 
     @property
-    def stub(self) -> modal.app._App:
+    def stub(self) -> modal.stub._Stub:
         ...
 
     @property
     def info(self) -> modal._utils.function_utils.FunctionInfo:
         ...
 
     @property
-    def spec(self) -> _FunctionSpec:
+    def env(self) -> FunctionEnv:
         ...
 
     def get_build_def(self) -> str:
         ...
 
     def _initialize_from_empty(self):
         ...
@@ -237,45 +197,36 @@
     def web_url(self) -> str:
         ...
 
     @property
     def is_generator(self) -> bool:
         ...
 
-    def _map(self, input_queue: _SynchronizedQueue, order_outputs: bool, return_exceptions: bool) -> typing.AsyncGenerator[typing.Any, None]:
+    def _map(self, input_stream: typing.AsyncIterable[typing.Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
         ...
 
     async def _call_function(self, args, kwargs):
         ...
 
     async def _call_function_nowait(self, args, kwargs) -> _Invocation:
         ...
 
     def _call_generator(self, args, kwargs):
         ...
 
     async def _call_generator_nowait(self, args, kwargs):
         ...
 
-    def _map_sync(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-        ...
-
-    def _map_async(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
+    def map(self, *input_iterators, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
         ...
 
-    def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
+    async def for_each(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
         ...
 
-    async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        ...
-
-    def _starmap_async(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-        ...
-
-    def _starmap_sync(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
+    def starmap(self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
         ...
 
     async def remote(self, *args, **kwargs) -> typing.Any:
         ...
 
     def remote_gen(self, *args, **kwargs) -> typing.AsyncGenerator[typing.Any, None]:
         ...
@@ -300,62 +251,35 @@
 
     def get_raw_f(self) -> typing.Callable[..., typing.Any]:
         ...
 
     async def get_current_stats(self) -> FunctionStats:
         ...
 
-    class __map_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
-            ...
-
-    map: __map_spec
-
-    class __starmap_spec(typing_extensions.Protocol):
-        def __call__(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-            ...
-
-    starmap: __starmap_spec
-
-    class __for_each_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-        async def aio(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-    for_each: __for_each_spec
-
 
 class Function(modal.object.Object):
     _info: typing.Union[modal._utils.function_utils.FunctionInfo, None]
     _all_mounts: typing.Collection[modal.mount.Mount]
-    _stub: modal.app.App
+    _stub: modal.stub.Stub
     _obj: typing.Any
     _web_url: typing.Union[str, None]
     _is_remote_cls_method: bool
     _function_name: typing.Union[str, None]
     _is_method: bool
-    _spec: _FunctionSpec
+    _env: FunctionEnv
     _tag: str
     _raw_f: typing.Callable[..., typing.Any]
     _build_args: dict
     _parent: Function
 
     def __init__(self, *args, **kwargs):
         ...
 
     @staticmethod
-    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image.Image, secret: typing.Union[modal.secret.Secret, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
+    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image.Image, secret: typing.Union[modal.secret.Secret, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
         ...
 
     def from_parametrized(self, obj, from_other_workspace: bool, options: typing.Union[modal_proto.api_pb2.FunctionOptions, None], args: typing.Sized, kwargs: typing.Dict[str, typing.Any]) -> Function:
         ...
 
     class __keep_warm_spec(typing_extensions.Protocol):
         def __call__(self, warm_pool_size: int) -> None:
@@ -380,23 +304,23 @@
     lookup: __lookup_spec
 
     @property
     def tag(self) -> str:
         ...
 
     @property
-    def stub(self) -> modal.app.App:
+    def stub(self) -> modal.stub.Stub:
         ...
 
     @property
     def info(self) -> modal._utils.function_utils.FunctionInfo:
         ...
 
     @property
-    def spec(self) -> _FunctionSpec:
+    def env(self) -> FunctionEnv:
         ...
 
     def get_build_def(self) -> str:
         ...
 
     def _initialize_from_empty(self):
         ...
@@ -418,18 +342,18 @@
         ...
 
     @property
     def is_generator(self) -> bool:
         ...
 
     class ___map_spec(typing_extensions.Protocol):
-        def __call__(self, input_queue: SynchronizedQueue, order_outputs: bool, return_exceptions: bool) -> typing.Generator[typing.Any, None, None]:
+        def __call__(self, input_stream: typing.Iterable[typing.Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
             ...
 
-        def aio(self, input_queue: SynchronizedQueue, order_outputs: bool, return_exceptions: bool) -> typing.AsyncGenerator[typing.Any, None]:
+        def aio(self, input_stream: typing.AsyncIterable[typing.Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
             ...
 
     _map: ___map_spec
 
     class ___call_function_spec(typing_extensions.Protocol):
         def __call__(self, args, kwargs):
             ...
@@ -456,31 +380,40 @@
             ...
 
         async def aio(self, *args, **kwargs):
             ...
 
     _call_generator_nowait: ___call_generator_nowait_spec
 
-    def _map_sync(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-        ...
+    class __map_spec(typing_extensions.Protocol):
+        def __call__(self, *input_iterators, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.Generator[typing.Any, None, None]:
+            ...
 
-    def _map_async(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
-        ...
+        def aio(self, *input_iterators, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
+            ...
 
-    def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        ...
+    map: __map_spec
 
-    async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        ...
+    class __for_each_spec(typing_extensions.Protocol):
+        def __call__(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
+            ...
 
-    def _starmap_async(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    def _starmap_sync(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-        ...
+    for_each: __for_each_spec
+
+    class __starmap_spec(typing_extensions.Protocol):
+        def __call__(self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.Generator[typing.Any, None, None]:
+            ...
+
+        def aio(self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
+            ...
+
+    starmap: __starmap_spec
 
     class __remote_spec(typing_extensions.Protocol):
         def __call__(self, *args, **kwargs) -> typing.Any:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.Any:
             ...
@@ -534,41 +467,14 @@
             ...
 
         async def aio(self, *args, **kwargs) -> FunctionStats:
             ...
 
     get_current_stats: __get_current_stats_spec
 
-    class __map_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
-            ...
-
-    map: __map_spec
-
-    class __starmap_spec(typing_extensions.Protocol):
-        def __call__(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-            ...
-
-    starmap: __starmap_spec
-
-    class __for_each_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-        async def aio(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-    for_each: __for_each_spec
-
 
 class _FunctionCall(modal.object._Object):
     def _invocation(self):
         ...
 
     async def get(self, timeout: typing.Union[float, None] = None):
         ...
```

## modal/image.py

```diff
@@ -1,148 +1,95 @@
 # Copyright Modal Labs 2022
 import contextlib
 import os
-import re
 import shlex
 import sys
 import typing
 import warnings
 from dataclasses import dataclass
 from inspect import isfunction
 from pathlib import Path, PurePosixPath
-from typing import Any, Callable, Dict, List, Literal, Optional, Sequence, Set, Tuple, Union, get_args
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 
 from google.protobuf.message import Message
 from grpclib.exceptions import GRPCError, StreamTerminatedError
 
 from modal_proto import api_pb2
 
 from ._resolver import Resolver
 from ._serialization import serialize
 from ._utils.async_utils import synchronize_api
 from ._utils.blob_utils import MAX_OBJECT_SIZE_BYTES
 from ._utils.function_utils import FunctionInfo
 from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors, unary_stream
-from .config import config, logger, user_config_path
-from .exception import InvalidError, NotFoundError, RemoteError, VersionError, deprecation_error, deprecation_warning
+from .config import config, logger
+from .exception import InvalidError, NotFoundError, RemoteError, deprecation_warning
 from .gpu import GPU_T, parse_gpu_config
 from .mount import _Mount, python_standalone_mount_name
 from .network_file_system import _NetworkFileSystem
 from .object import _Object
 from .secret import _Secret
 
-if typing.TYPE_CHECKING:
-    import modal.functions
-
-
-# This is used for both type checking and runtime validation
-ImageBuilderVersion = Literal["2023.12", "2024.04"]
 
-# Note: we also define supported Python versions via logic at the top of the package __init__.py
-# so that we fail fast / clearly in unsupported containers. Additionally, we enumerate the supported
-# Python versions in mount.py where we specify the "standalone Python versions" we create mounts for.
-# Consider consolidating these multiple sources of truth?
-SUPPORTED_PYTHON_SERIES: Set[str] = {"3.8", "3.9", "3.10", "3.11", "3.12"}
-
-CONTAINER_REQUIREMENTS_PATH = "/modal_requirements.txt"
-
-
-def _validate_python_version(version: Optional[str], allow_micro_granularity: bool = True) -> str:
-    if version is None:
-        # If Python version is unspecified, match the local version, up to the minor component
-        version = series_version = "{0}.{1}".format(*sys.version_info)
-    elif not isinstance(version, str):
-        raise InvalidError(f"Python version must be specified as a string, not {type(version).__name__}")
-    elif not re.match(r"^3(?:\.\d{1,2}){1,2}$", version):
-        raise InvalidError(f"Invalid Python version: {version!r}")
-    else:
-        components = version.split(".")
-        if len(components) == 3 and not allow_micro_granularity:
-            raise InvalidError(
-                "Python version must be specified as 'major.minor' for this interface;"
-                f" micro-level specification ({version!r}) is not valid."
-            )
-        series_version = "{0}.{1}".format(*components)
-
-    if series_version not in SUPPORTED_PYTHON_SERIES:
+def _validate_python_version(version: str) -> None:
+    components = version.split(".")
+    supported_versions = {"3.12", "3.11", "3.10", "3.9", "3.8"}
+    if len(components) == 2 and version in supported_versions:
+        return
+    elif len(components) == 3:
         raise InvalidError(
-            f"Unsupported Python version: {version!r}."
-            f" Modal supports versions in the following series: {SUPPORTED_PYTHON_SERIES!r}."
+            f"major.minor.patch version specification not valid. Supported major.minor versions are {supported_versions}."
         )
-    return version
+    raise InvalidError(f"Unsupported version {version}. Supported versions are {supported_versions}.")
+
 
+def _dockerhub_python_version(python_version=None):
+    if python_version is None:
+        python_version = "%d.%d" % sys.version_info[:2]
 
-def _dockerhub_python_version(builder_version: ImageBuilderVersion, python_version: Optional[str] = None) -> str:
-    python_version = _validate_python_version(python_version)
-    components = python_version.split(".")
+    parts = python_version.split(".")
 
-    # When user specifies a full Python version, use that
-    if len(components) > 2:
+    if len(parts) > 2:
         return python_version
 
-    # Otherwise, use the same series, but a specific micro version, corresponding to the latest
-    # available from https://hub.docker.com/_/python at the time of each image builder release.
+    # We use the same major/minor version, but the highest micro version
+    # See https://hub.docker.com/_/python
     latest_micro_version = {
-        "2023.12": {
-            "3.12": "1",
-            "3.11": "0",
-            "3.10": "8",
-            "3.9": "15",
-            "3.8": "15",
-        },
-        "2024.04": {
-            "3.12": "2",
-            "3.11": "8",
-            "3.10": "14",
-            "3.9": "19",
-            "3.8": "19",
-        },
+        "3.12": "1",
+        "3.11": "0",
+        "3.10": "8",
+        "3.9": "15",
+        "3.8": "15",
     }
-    python_series = "{0}.{1}".format(*components)
-    micro_version = latest_micro_version[builder_version][python_series]
-    python_version = f"{python_series}.{micro_version}"
+    major_minor_version = ".".join(parts[:2])
+    python_version = major_minor_version + "." + latest_micro_version[major_minor_version]
     return python_version
 
 
-def _dockerhub_debian_codename(builder_version: ImageBuilderVersion) -> str:
-    return {"2023.12": "bullseye", "2024.04": "bookworm"}[builder_version]
-
-
-def _get_modal_requirements_path(builder_version: ImageBuilderVersion, python_version: Optional[str] = None) -> str:
-    # Locate Modal client requirements data
+def _get_client_requirements_path(python_version: Optional[str] = None) -> str:
+    # Locate Modal client requirements.txt
     import modal
 
-    modal_path = Path(modal.__path__[0])
-
-    # When we added Python 3.12 support, we needed to update a few dependencies but did not yet
-    # support versioned builds, so we put them in a separate 3.12-specific requirements file.
-    # When the python_version is not specified in the Image API, we fall back to the local version.
-    # Note that this is buggy if you're using a registry or dockerfile Image that (implicitly) contains 3.12
-    # and have a different local version. We can't really fix that; but users can update their image builder.
-    # We can get rid of this complexity entirely when we drop support for 2023.12.
-    python_version = python_version or sys.version
-    suffix = ".312" if builder_version == "2023.12" and python_version.startswith("3.12") else ""
-
-    return str(modal_path / "requirements" / f"{builder_version}{suffix}.txt")
-
-
-def _get_modal_requirements_command(version: ImageBuilderVersion) -> str:
-    command = "pip install"
-    if version <= "2023.12":
-        args = f"-r {CONTAINER_REQUIREMENTS_PATH}"
+    modal_path = modal.__path__[0]
+    if python_version is None:
+        major, minor, *_ = sys.version_info
     else:
-        args = f"--no-cache --no-deps -r {CONTAINER_REQUIREMENTS_PATH}"
-    return f"{command} {args}"
+        major, minor = python_version.split("-")[0].split(".")[:2]
+    suffix = {(3, 12): ".312"}.get((int(major), int(minor)), "")
+    return os.path.join(modal_path, f"requirements{suffix}.txt")
 
 
 def _flatten_str_args(function_name: str, arg_name: str, args: Tuple[Union[str, List[str]], ...]) -> List[str]:
     """Takes a tuple of strings, or string lists, and flattens it.
 
     Raises an error if any of the elements are not strings or string lists.
     """
+    # TODO(erikbern): maybe we can just build somthing intelligent that checks
+    # based on type annotations in real time?
+    # Or use something like this? https://github.com/FelixTheC/strongtyping
 
     def is_str_list(x):
         return isinstance(x, list) and all(isinstance(y, str) for y in x)
 
     ret: List[str] = []
     for x in args:
         if isinstance(x, str):
@@ -162,50 +109,21 @@
 ) -> str:
     flags = [
         ("--find-links", find_links),  # TODO(erikbern): allow multiple?
         ("--index-url", index_url),
         ("--extra-index-url", extra_index_url),  # TODO(erikbern): allow multiple?
     ]
 
-    args = " ".join(f"{flag} {shlex.quote(value)}" for flag, value in flags if value is not None)
+    args = " ".join(flag + " " + shlex.quote(value) for flag, value in flags if value is not None)
     if pre:
         args += " --pre"
 
     return args
 
 
-def _get_image_builder_version(client_version: str) -> ImageBuilderVersion:
-    if config_version := config.get("image_builder_version"):
-        version = config_version
-        if (env_var := "MODAL_IMAGE_BUILDER_VERSION") in os.environ:
-            version_source = f" (based on your `{env_var}` environment variable)"
-        else:
-            version_source = f" (based on your local config file at `{user_config_path}`)"
-    else:
-        version = client_version
-        version_source = ""
-
-    supported_versions: Set[ImageBuilderVersion] = set(get_args(ImageBuilderVersion))
-    if version not in supported_versions:
-        if config_version is not None:
-            update_suggestion = "or remove your local configuration"
-        elif version < min(supported_versions):
-            update_suggestion = "your image builder version using the Modal dashboard"
-        else:
-            update_suggestion = "your client library (pip install --upgrade modal)"
-        raise VersionError(
-            "This version of the modal client supports the following image builder versions:"
-            f" {supported_versions!r}."
-            f"\n\nYou are using {version!r}{version_source}."
-            f" Please update {update_suggestion}."
-        )
-
-    return version
-
-
 class _ImageRegistryConfig:
     """mdmd:hidden"""
 
     def __init__(
         self,
         # TODO: change to _PUBLIC after worker starts handling it.
         registry_auth_type: int = api_pb2.REGISTRY_AUTH_TYPE_UNSPECIFIED,
@@ -217,14 +135,18 @@
     def get_proto(self) -> api_pb2.ImageRegistryConfig:
         return api_pb2.ImageRegistryConfig(
             registry_auth_type=self.registry_auth_type,
             secret_id=(self.secret.object_id if self.secret else None),
         )
 
 
+if typing.TYPE_CHECKING:
+    import modal.functions
+
+
 @dataclass
 class DockerfileSpec:
     # Ideally we would use field() with default_factory=, but doesn't work with synchronicity type-stub gen
     commands: List[str]
     context_files: Dict[str, str]
 
 
@@ -247,15 +169,15 @@
             for exc in self.inside_exceptions:
                 raise exc
 
     @staticmethod
     def _from_args(
         *,
         base_images: Optional[Dict[str, "_Image"]] = None,
-        dockerfile_function: Optional[Callable[[ImageBuilderVersion], DockerfileSpec]] = None,
+        dockerfile_function: Optional[Callable[[], DockerfileSpec]] = None,
         secrets: Optional[Sequence[_Secret]] = None,
         gpu_config: Optional[api_pb2.GPUConfig] = None,
         build_function: Optional["modal.functions._Function"] = None,
         build_function_input: Optional[api_pb2.FunctionInput] = None,
         image_registry_config: Optional[_ImageRegistryConfig] = None,
         context_mount: Optional[_Mount] = None,
         force_build: bool = False,
@@ -285,20 +207,18 @@
             if context_mount:
                 deps.append(context_mount)
             if image_registry_config.secret:
                 deps.append(image_registry_config.secret)
             return deps
 
         async def _load(self: _Image, resolver: Resolver, existing_object_id: Optional[str]):
-            builder_version = _get_image_builder_version(resolver.client.image_builder_version)
-
             if dockerfile_function is None:
                 dockerfile = DockerfileSpec(commands=[], context_files={})
             else:
-                dockerfile = dockerfile_function(builder_version)
+                dockerfile = dockerfile_function()
 
             if not dockerfile.commands and not build_function:
                 raise InvalidError(
                     "No commands were provided for the image — have you tried using modal.Image.debian_slim()?"
                 )
             if dockerfile.commands and build_function:
                 raise InvalidError("Cannot provide both a build function and Dockerfile commands!")
@@ -363,15 +283,14 @@
             req = api_pb2.ImageGetOrCreateRequest(
                 app_id=resolver.app_id,
                 image=image_definition,
                 existing_image_id=existing_object_id,  # TODO: ignored
                 build_function_id=build_function_id,
                 force_build=config.get("force_build") or force_build,
                 namespace=_namespace,
-                builder_version=builder_version,
             )
             resp = await retry_transient_errors(resolver.client.stub.ImageGetOrCreate, req)
             image_id = resp.image_id
 
             logger.debug("Waiting for image %s" % image_id)
             last_entry_id: Optional[str] = None
             result: Optional[api_pb2.GenericResult] = None
@@ -428,15 +347,15 @@
     def extend(self, **kwargs) -> "_Image":
         """Deprecated! This is a low-level method not intended to be part of the public API."""
         deprecation_warning(
             (2024, 3, 7),
             "`Image.extend` is deprecated; please use a higher-level method, such as `Image.dockerfile_commands`.",
         )
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile():
             return DockerfileSpec(
                 commands=kwargs.pop("dockerfile_commands", []),
                 context_files=kwargs.pop("context_files", {}),
             )
 
         return _Image._from_args(base_images={"base": self}, dockerfile_function=build_dockerfile, **kwargs)
 
@@ -454,15 +373,15 @@
         # place mount's contents into /static directory of image.
         image = modal.Image.debian_slim().copy_mount(mount, remote_path="/static")
         ```
         """
         if not isinstance(mount, _Mount):
             raise InvalidError("The mount argument to copy has to be a Modal Mount object")
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = ["FROM base", f"COPY . {remote_path}"]  # copy everything from the supplied mount
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             context_mount=mount,
@@ -472,15 +391,15 @@
         """Copy a file into the image as a part of building it.
 
         This works in a similar way to [`COPY`](https://docs.docker.com/engine/reference/builder/#copy) in a `Dockerfile`.
         """
         basename = str(Path(local_path).name)
         mount = _Mount.from_local_file(local_path, remote_path=f"/{basename}")
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base", f"COPY {basename} {remote_path}"], context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             context_mount=mount,
         )
@@ -488,15 +407,15 @@
     def copy_local_dir(self, local_path: Union[str, Path], remote_path: Union[str, Path] = ".") -> "_Image":
         """Copy a directory into the image as a part of building the image.
 
         This works in a similar way to [`COPY`](https://docs.docker.com/engine/reference/builder/#copy) in a `Dockerfile`.
         """
         mount = _Mount.from_local_dir(local_path, remote_path="/")
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base", f"COPY . {remote_path}"], context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             context_mount=mount,
         )
@@ -520,27 +439,32 @@
         image = modal.Image.debian_slim().pip_install("click", "httpx~=0.23.3")
         ```
         """
         pkgs = _flatten_str_args("pip_install", "packages", packages)
         if not pkgs:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            package_args = " ".join(shlex.quote(pkg) for pkg in sorted(pkgs))
+        def build_dockerfile() -> DockerfileSpec:
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
-            commands = ["FROM base", f"RUN python -m pip install {package_args} {extra_args}"]
-            if version > "2023.12":  # Back-compat for legacy trailing space with empty extra_args
-                commands = [cmd.strip() for cmd in commands]
+            package_args = " ".join(shlex.quote(pkg) for pkg in sorted(pkgs))
+
+            commands = [
+                "FROM base",
+                f"RUN python -m pip install {package_args} {extra_args}",
+                # TODO(erikbern): if extra_args is empty, we add a superfluous space at the end.
+                # However removing it at this point would cause image hashes to change.
+                # Maybe let's remove it later when/if client requirements change.
+            ]
             return DockerfileSpec(commands=commands, context_files={})
 
         gpu_config = parse_gpu_config(gpu)
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
-            force_build=self.force_build or force_build,
+            force_build=self.force_build or force_build,  # TODO shouldn't forcing upstream build always rerun this?
             gpu_config=gpu_config,
             secrets=secrets,
         )
 
     def pip_install_private_repos(
         self,
         *repositories: str,
@@ -606,30 +530,28 @@
             raise InvalidError(
                 f"{len(invalid_repos)} out of {len(repositories)} given repository refs are invalid. "
                 f"Invalid refs: {invalid_repos}. "
             )
 
         secret_names = ",".join([s.app_name if hasattr(s, "app_name") else str(s) for s in secrets])  # type: ignore
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = ["FROM base"]
             if any(r.startswith("github") for r in repositories):
                 commands.append(
                     f"RUN bash -c \"[[ -v GITHUB_TOKEN ]] || (echo 'GITHUB_TOKEN env var not set by provided modal.Secret(s): {secret_names}' && exit 1)\"",
                 )
             if any(r.startswith("gitlab") for r in repositories):
                 commands.append(
                     f"RUN bash -c \"[[ -v GITLAB_TOKEN ]] || (echo 'GITLAB_TOKEN env var not set by provided modal.Secret(s): {secret_names}' && exit 1)\"",
                 )
 
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
             commands.extend(["RUN apt-get update && apt-get install -y git"])
             commands.extend([f'RUN python3 -m pip install "{url}" {extra_args}' for url in install_urls])
-            if version > "2023.12":  # Back-compat for legacy trailing space with empty extra_args
-                commands = [cmd.strip() for cmd in commands]
             return DockerfileSpec(commands=commands, context_files={})
 
         gpu_config = parse_gpu_config(gpu)
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
@@ -648,29 +570,26 @@
         pre: bool = False,  # Passes --pre (allow pre-releases) to pip install
         force_build: bool = False,
         secrets: Sequence[_Secret] = [],
         gpu: GPU_T = None,
     ) -> "_Image":
         """Install a list of Python packages from a local `requirements.txt` file."""
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             requirements_txt_path = os.path.expanduser(requirements_txt)
             context_files = {"/.requirements.txt": requirements_txt_path}
 
-            null_find_links_arg = " " if version == "2023.12" else ""
-            find_links_arg = f" -f {find_links}" if find_links else null_find_links_arg
+            find_links_arg = f"-f {find_links}" if find_links else ""
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
 
             commands = [
                 "FROM base",
                 "COPY /.requirements.txt /.requirements.txt",
-                f"RUN python -m pip install -r /.requirements.txt{find_links_arg} {extra_args}",
+                f"RUN python -m pip install -r /.requirements.txt {find_links_arg} {extra_args}",
             ]
-            if version > "2023.12":  # Back-compat for legacy whitespace with empty find_link / extra args
-                commands = [cmd.strip() for cmd in commands]
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             force_build=self.force_build or force_build,
             gpu_config=parse_gpu_config(gpu),
@@ -694,15 +613,15 @@
 
         `optional_dependencies` is a list of the keys of the
         optional-dependencies section(s) of the `pyproject.toml` file
         (e.g. test, doc, experiment, etc). When provided,
         all of the packages in each listed section are installed as well.
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             # Defer toml import so we don't need it in the container runtime environment
             import toml
 
             config = toml.load(os.path.expanduser(pyproject_toml))
 
             dependencies = []
             if "project" not in config or "dependencies" not in config["project"]:
@@ -718,18 +637,22 @@
                 optionals = config["project"]["optional-dependencies"]
                 for dep_group_name in optional_dependencies:
                     if dep_group_name in optionals:
                         dependencies.extend(optionals[dep_group_name])
 
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
             package_args = " ".join(shlex.quote(pkg) for pkg in sorted(dependencies))
-            commands = ["FROM base", f"RUN python -m pip install {package_args} {extra_args}"]
-            if version > "2023.12":  # Back-compat for legacy trailing space
-                commands = [cmd.strip() for cmd in commands]
 
+            commands = [
+                "FROM base",
+                f"RUN python -m pip install {package_args} {extra_args}",
+                # TODO(erikbern): if extra_args is empty, we add a superfluous space at the end.
+                # However removing it at this point would cause image hashes to change.
+                # Maybe let's remove it later when/if client requirements change.
+            ]
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             force_build=self.force_build or force_build,
             secrets=secrets,
@@ -761,15 +684,15 @@
         If not provided as argument the path to the lockfile is inferred. However, the
         file has to exist, unless `ignore_lockfile` is set to `True`.
 
         Note that the root project of the poetry project is not installed,
         only the dependencies. For including local packages see `modal.Mount.from_local_python_packages`
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             context_files = {"/.pyproject.toml": os.path.expanduser(poetry_pyproject_toml)}
 
             commands = ["FROM base", "RUN python -m pip install poetry~=1.7"]
 
             if old_installer:
                 commands += ["RUN poetry config experimental.new-installer false"]
 
@@ -781,18 +704,16 @@
                         raise NotFoundError(
                             f"poetry.lock not found at inferred location: {p.absolute()}. If a lockfile is not needed, `ignore_lockfile=True` can be used."
                         )
                     poetry_lockfile = p.as_posix()
                 context_files["/.poetry.lock"] = poetry_lockfile
                 commands += ["COPY /.poetry.lock /tmp/poetry/poetry.lock"]
 
-            install_cmd = "poetry install --no-root"
-            if version == "2023.12":
-                # Backwards compatability for previous string, which started with whitespace
-                install_cmd = "  " + install_cmd
+            # Indentation for back-compat TODO: fix when we update image_builder_version
+            install_cmd = "  poetry install --no-root"
 
             if with_:
                 install_cmd += f" --with {','.join(with_)}"
 
             if without:
                 install_cmd += f" --without {','.join(without)}"
 
@@ -827,15 +748,15 @@
         force_build: bool = False,
     ) -> "_Image":
         """Extend an image with arbitrary Dockerfile-like commands."""
         cmds = _flatten_str_args("dockerfile_commands", "dockerfile_commands", dockerfile_commands)
         if not cmds:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base", *cmds], context_files=context_files)
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu, raise_on_true=False),
@@ -851,46 +772,42 @@
         force_build: bool = False,
     ) -> "_Image":
         """Extend an image with a list of shell commands to run."""
         cmds = _flatten_str_args("run_commands", "commands", commands)
         if not cmds:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base"] + [f"RUN {cmd}" for cmd in cmds], context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu, raise_on_true=False),
             force_build=self.force_build or force_build,
         )
 
     @staticmethod
-    def conda(python_version: Optional[str] = None, force_build: bool = False) -> "_Image":
+    def conda(python_version: str = "3.9", force_build: bool = False) -> "_Image":
         """
         A Conda base image, using miniconda3 and derived from the official Docker Hub image.
         In most cases, using [`Image.micromamba()`](/docs/reference/modal.Image#micromamba) with [`micromamba_install`](/docs/reference/modal.Image#micromamba_install) is recommended over `Image.conda()`, as it leads to significantly faster image build times.
         """
+        _validate_python_version(python_version)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            nonlocal python_version
-            if version == "2023.12" and python_version is None:
-                python_version = "3.9"  # Backcompat for old hardcoded default param
-            validated_python_version = _validate_python_version(python_version)
-            debian_codename = _dockerhub_debian_codename(version)
-            requirements_path = _get_modal_requirements_path(version, python_version)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: requirements_path}
+        def build_dockerfile() -> DockerfileSpec:
+            requirements_path = _get_client_requirements_path(python_version)
+            context_files = {"/modal_requirements.txt": requirements_path}
 
             # Doesn't use the official continuumio/miniconda3 image as a base. That image has maintenance
             # issues (https://github.com/ContinuumIO/docker-images/issues) and building our own is more flexible.
             conda_install_script = "https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
             commands = [
-                f"FROM debian:{debian_codename}",  # the -slim images lack files required by Conda.
+                "FROM debian:bullseye",  # the -slim images lack files required by Conda.
                 # Temporarily add utility packages for conda installation.
                 "RUN apt-get --quiet update && apt-get --quiet --yes install curl bzip2 \\",
                 f"&& curl --silent --show-error --location {conda_install_script} --output /tmp/miniconda.sh \\",
                 # Install miniconda to a filesystem location on the $PATH of Modal container tasks.
                 # -b = install in batch mode w/o manual intervention.
                 # -f = allow install prefix to already exist.
                 # -p = the install prefix location.
@@ -898,41 +815,39 @@
                 "&& rm -rf /tmp/miniconda.sh",
                 # Biggest and most stable community-led Conda channel.
                 "RUN conda config --add channels conda-forge \\ ",
                 # softlinking can put conda in a broken state, surfacing error on uninstall like:
                 # `No such device or address: '/usr/local/lib/libz.so' -> '/usr/local/lib/libz.so.c~'`
                 "&& conda config --set allow_softlinks false \\ ",
                 # Install requested Python version from conda-forge channel; base debian image has only 3.7.
-                f"&& conda install --yes --channel conda-forge python={validated_python_version} \\ ",
+                f"&& conda install --yes --channel conda-forge python={python_version} \\ ",
                 "&& conda update conda \\ ",
                 # Remove now unneeded packages and files.
                 "&& apt-get --quiet --yes remove curl bzip2 \\ ",
                 "&& apt-get --quiet --yes autoremove \\ ",
                 "&& apt-get autoclean \\ ",
                 "&& rm -rf /var/lib/apt/lists/* /var/log/dpkg.log \\ ",
                 "&& conda clean --all --yes",
                 # Setup .bashrc for conda.
                 "RUN conda init bash --verbose",
-                f"COPY {CONTAINER_REQUIREMENTS_PATH} {CONTAINER_REQUIREMENTS_PATH}",
+                "COPY /modal_requirements.txt /modal_requirements.txt",
                 # .bashrc is explicitly sourced because RUN is a non-login shell and doesn't run bash.
                 "RUN . /root/.bashrc && conda activate base \\ ",
-                # Ensure that packaging tools are up to date and install client dependenices
-                f"&& python -m pip install --upgrade {'pip' if version == '2023.12' else 'pip wheel'} \\ ",
-                f"&& python -m {_get_modal_requirements_command(version)}",
+                "&& python -m pip install --upgrade pip \\ ",
+                "&& python -m pip install -r /modal_requirements.txt",
             ]
-            if version > "2023.12":
-                commands.append(f"RUN rm {CONTAINER_REQUIREMENTS_PATH}")
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         base = _Image._from_args(
             dockerfile_function=build_dockerfile,
             force_build=force_build,
             _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
         )
 
+        # TODO include these in the base image once we version the build?
         return base.dockerfile_commands(
             [
                 "ENV CONDA_EXE=/usr/local/bin/conda",
                 "ENV CONDA_PREFIX=/usr/local",
                 "ENV CONDA_PROMPT_MODIFIER=(base)",
                 "ENV CONDA_SHLVL=1",
                 "ENV CONDA_PYTHON_EXE=/usr/local/bin/python",
@@ -951,15 +866,15 @@
         """Install a list of additional packages using Conda. Note that in most cases, using [`Image.micromamba()`](/docs/reference/modal.Image#micromamba) with [`micromamba_install`](/docs/reference/modal.Image#micromamba_install)
         is recommended over `conda_install`, as it leads to significantly faster image build times."""
 
         pkgs = _flatten_str_args("conda_install", "packages", packages)
         if not pkgs:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             package_args = " ".join(shlex.quote(pkg) for pkg in pkgs)
             channel_args = "".join(f" -c {channel}" for channel in channels)
 
             commands = [
                 "FROM base",
                 f"RUN conda install {package_args}{channel_args} --yes \\ ",
                 "&& conda clean --yes --index-cache --tarballs --tempfiles --logfiles",
@@ -980,15 +895,15 @@
         force_build: bool = False,
         *,
         secrets: Sequence[_Secret] = [],
         gpu: GPU_T = None,
     ) -> "_Image":
         """Update a Conda environment using dependencies from a given environment.yml file."""
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             context_files = {"/environment.yml": os.path.expanduser(environment_yml)}
 
             commands = [
                 "FROM base",
                 "COPY /environment.yml /environment.yml",
                 "RUN conda env update --name base -f /environment.yml \\ ",
                 "&& conda clean --yes --index-cache --tarballs --tempfiles --logfiles",
@@ -1001,37 +916,32 @@
             force_build=self.force_build or force_build,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu),
         )
 
     @staticmethod
     def micromamba(
-        python_version: Optional[str] = None,
+        python_version: str = "3.9",
         force_build: bool = False,
     ) -> "_Image":
         """
         A Micromamba base image. Micromamba allows for fast building of small Conda-based containers.
         In most cases it will be faster than using [`Image.conda()`](/docs/reference/modal.Image#conda).
         """
+        _validate_python_version(python_version)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            nonlocal python_version
-            if version == "2023.12" and python_version is None:
-                python_version = "3.9"  # Backcompat for old hardcoded default param
-            validated_python_version = _validate_python_version(python_version)
-            micromamba_version = {"2023.12": "1.3.1", "2024.04": "1.5.8"}[version]
-            debian_codename = _dockerhub_debian_codename(version)
-            tag = f"mambaorg/micromamba:{micromamba_version}-{debian_codename}-slim"
+        def build_dockerfile() -> DockerfileSpec:
+            tag = "mambaorg/micromamba:1.3.1-bullseye-slim"
             setup_commands = [
                 'SHELL ["/usr/local/bin/_dockerfile_shell.sh"]',
                 "ENV MAMBA_DOCKERFILE_ACTIVATE=1",
-                f"RUN micromamba install -n base -y python={validated_python_version} pip -c conda-forge",
+                f"RUN micromamba install -n base -y python={python_version} pip -c conda-forge",
             ]
-            commands = _Image._registry_setup_commands(tag, version, setup_commands)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: _get_modal_requirements_path(version, python_version)}
+            commands = _Image._registry_setup_commands(tag, setup_commands, add_python=None)
+            context_files = {"/modal_requirements.txt": _get_client_requirements_path(python_version)}
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             dockerfile_function=build_dockerfile,
             force_build=force_build,
             _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
         )
@@ -1048,15 +958,15 @@
     ) -> "_Image":
         """Install a list of additional packages using micromamba."""
 
         pkgs = _flatten_str_args("micromamba_install", "packages", packages)
         if not pkgs:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             package_args = " ".join(shlex.quote(pkg) for pkg in pkgs)
             channel_args = "".join(f" -c {channel}" for channel in channels)
 
             commands = [
                 "FROM base",
                 f"RUN micromamba install {package_args}{channel_args} --yes",
             ]
@@ -1067,42 +977,33 @@
             dockerfile_function=build_dockerfile,
             force_build=self.force_build or force_build,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu),
         )
 
     @staticmethod
-    def _registry_setup_commands(
-        tag: str,
-        builder_version: ImageBuilderVersion,
-        setup_commands: List[str],
-        add_python: Optional[str] = None,
-    ) -> List[str]:
+    def _registry_setup_commands(tag: str, setup_commands: List[str], add_python: Optional[str]) -> List[str]:
         add_python_commands: List[str] = []
         if add_python:
-            _validate_python_version(add_python, allow_micro_granularity=False)
             add_python_commands = [
                 "COPY /python/. /usr/local",
                 "RUN ln -s /usr/local/bin/python3 /usr/local/bin/python",
                 "ENV TERMINFO_DIRS=/etc/terminfo:/lib/terminfo:/usr/share/terminfo:/usr/lib/terminfo",
             ]
-
-        modal_requirements_commands = [
-            f"COPY {CONTAINER_REQUIREMENTS_PATH} {CONTAINER_REQUIREMENTS_PATH}",
-            f"RUN python -m pip install --upgrade {'pip' if builder_version == '2023.12' else 'pip wheel'}",
-            f"RUN python -m {_get_modal_requirements_command(builder_version)}",
-        ]
-        if builder_version > "2023.12":
-            modal_requirements_commands.append(f"RUN rm {CONTAINER_REQUIREMENTS_PATH}")
-
         return [
             f"FROM {tag}",
             *add_python_commands,
             *setup_commands,
-            *modal_requirements_commands,
+            "COPY /modal_requirements.txt /modal_requirements.txt",
+            "RUN python -m pip install --upgrade pip",
+            "RUN python -m pip install -r /modal_requirements.txt",
+            # TODO: We should add this next line at some point to clean up the image, but it would
+            # trigger a hash change, so batch it with the next rebuild-triggering change.
+            #
+            # "RUN rm /modal_requirements.txt",
         ]
 
     @staticmethod
     def from_registry(
         tag: str,
         *,
         secret: Optional[_Secret] = None,
@@ -1144,17 +1045,17 @@
                 python_standalone_mount_name(add_python),
                 namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
             )
 
         if "image_registry_config" not in kwargs and secret is not None:
             kwargs["image_registry_config"] = _ImageRegistryConfig(api_pb2.REGISTRY_AUTH_TYPE_STATIC_CREDS, secret)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            commands = _Image._registry_setup_commands(tag, version, setup_dockerfile_commands, add_python)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: _get_modal_requirements_path(version, add_python)}
+        def build_dockerfile() -> DockerfileSpec:
+            commands = _Image._registry_setup_commands(tag, setup_dockerfile_commands, add_python)
+            context_files = {"/modal_requirements.txt": _get_client_requirements_path(add_python)}
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             dockerfile_function=build_dockerfile,
             context_mount=context_mount,
             force_build=force_build,
             **kwargs,
@@ -1270,22 +1171,22 @@
         ```python
         image = modal.Image.from_dockerfile("./Dockerfile", add_python="3.12")
         ```
         """
 
         # --- Build the base dockerfile
 
-        def build_dockerfile_base(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_base_dockerfile() -> DockerfileSpec:
             with open(os.path.expanduser(path)) as f:
                 commands = f.read().split("\n")
             return DockerfileSpec(commands=commands, context_files={})
 
         gpu_config = parse_gpu_config(gpu)
         base_image = _Image._from_args(
-            dockerfile_function=build_dockerfile_base,
+            dockerfile_function=build_base_dockerfile,
             context_mount=context_mount,
             gpu_config=gpu_config,
             secrets=secrets,
         )
 
         # --- Now add in the modal dependencies, and, optionally a Python distribution
         # This happening in two steps is probably a vestigial consequence of previous limitations,
@@ -1295,50 +1196,67 @@
             context_mount = _Mount.from_name(
                 python_standalone_mount_name(add_python),
                 namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
             )
         else:
             context_mount = None
 
-        def build_dockerfile_python(version: ImageBuilderVersion) -> DockerfileSpec:
-            commands = _Image._registry_setup_commands("base", version, [], add_python)
-            requirements_path = _get_modal_requirements_path(version, add_python)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: requirements_path}
+        def enhance_dockerfile() -> DockerfileSpec:
+            requirements_path = _get_client_requirements_path(add_python)
+
+            add_python_commands = []
+            if add_python:
+                add_python_commands = [
+                    "COPY /python/. /usr/local",
+                    "RUN ln -s /usr/local/bin/python3 /usr/local/bin/python",
+                    "ENV TERMINFO_DIRS=/etc/terminfo:/lib/terminfo:/usr/share/terminfo:/usr/lib/terminfo",
+                ]
+
+            commands = [
+                "FROM base",
+                *add_python_commands,
+                "COPY /modal_requirements.txt /modal_requirements.txt",
+                "RUN python -m pip install --upgrade pip",
+                "RUN python -m pip install -r /modal_requirements.txt",
+            ]
+
+            context_files = {"/modal_requirements.txt": requirements_path}
+
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             base_images={"base": base_image},
-            dockerfile_function=build_dockerfile_python,
+            dockerfile_function=enhance_dockerfile,
             context_mount=context_mount,
             force_build=force_build,
         )
 
     @staticmethod
     def debian_slim(python_version: Optional[str] = None, force_build: bool = False) -> "_Image":
-        """Default image, based on the official `python` Docker images."""
+        """Default image, based on the official `python:X.Y.Z-slim-bullseye` Docker images."""
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            requirements_path = _get_modal_requirements_path(version, python_version)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: requirements_path}
-            full_python_version = _dockerhub_python_version(version, python_version)
-            debian_codename = _dockerhub_debian_codename(version)
+        def build_dockerfile() -> DockerfileSpec:
+            full_python_version = _dockerhub_python_version(python_version)
 
+            requirements_path = _get_client_requirements_path(full_python_version)
             commands = [
-                f"FROM python:{full_python_version}-slim-{debian_codename}",
-                f"COPY {CONTAINER_REQUIREMENTS_PATH} {CONTAINER_REQUIREMENTS_PATH}",
+                f"FROM python:{full_python_version}-slim-bullseye",
+                "COPY /modal_requirements.txt /modal_requirements.txt",
                 "RUN apt-get update",
                 "RUN apt-get install -y gcc gfortran build-essential",
-                f"RUN pip install --upgrade {'pip' if version == '2023.12' else 'pip wheel'}",
-                f"RUN {_get_modal_requirements_command(version)}",
-                # Set debian front-end to non-interactive to avoid users getting stuck with input prompts.
+                "RUN pip install --upgrade pip",
+                "RUN pip install -r /modal_requirements.txt",
+                # Set debian front-end to non-interactive to avoid users getting stuck with input
+                # prompts.
                 "RUN echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections",
             ]
-            if version > "2023.12":
-                commands.append(f"RUN rm {CONTAINER_REQUIREMENTS_PATH}")
-            return DockerfileSpec(commands=commands, context_files=context_files)
+            return DockerfileSpec(
+                commands=commands,
+                context_files={"/modal_requirements.txt": requirements_path},
+            )
 
         return _Image._from_args(
             dockerfile_function=build_dockerfile,
             force_build=force_build,
             _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
         )
 
@@ -1359,15 +1277,15 @@
         """
         pkgs = _flatten_str_args("apt_install", "packages", packages)
         if not pkgs:
             return self
 
         package_args = " ".join(shlex.quote(pkg) for pkg in pkgs)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = [
                 "FROM base",
                 "RUN apt-get update",
                 f"RUN apt-get install -y {package_args}",
             ]
             return DockerfileSpec(commands=commands, context_files={})
 
@@ -1472,15 +1390,15 @@
                 .env({"CONDA_OVERRIDE_CUDA": "11.2"})
                 .conda_install("jax", "cuda-nvcc", channels=["conda-forge", "nvidia"])
                 .pip_install("dm-haiku", "optax")
         )
         ```
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = ["FROM base"] + [f"ENV {key}={shlex.quote(val)}" for (key, val) in vars.items()]
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
         )
@@ -1496,16 +1414,16 @@
             .run_commands("git clone https://xyz app")
             .workdir("/app")
             .run_commands("yarn install")
         )
         ```
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            commands = ["FROM base", f"WORKDIR {shlex.quote(path)}"]
+        def build_dockerfile() -> DockerfileSpec:
+            commands = ["FROM base"] + [f"WORKDIR {shlex.quote(path)}"]
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
         )
 
@@ -1545,11 +1463,12 @@
         **Usage:**
 
         ```python notest
         with image.imports():
             import torch
         ```
         """
-        deprecation_error((2023, 12, 15), Image.run_inside.__doc__)
+        deprecation_warning((2023, 12, 15), Image.run_inside.__doc__)
+        return self.imports()
 
 
 Image = synchronize_api(_Image)
```

## modal/image.pyi

```diff
@@ -5,48 +5,34 @@
 import modal.network_file_system
 import modal.object
 import modal.secret
 import modal_proto.api_pb2
 import pathlib
 import typing
 
-ImageBuilderVersion = typing.Literal['2023.12', '2024.04']
-
-def _validate_python_version(version: typing.Union[str, None], allow_micro_granularity: bool = True) -> str:
-    ...
-
-
-def _dockerhub_python_version(builder_version: typing.Literal['2023.12', '2024.04'], python_version: typing.Union[str, None] = None) -> str:
-    ...
-
-
-def _dockerhub_debian_codename(builder_version: typing.Literal['2023.12', '2024.04']) -> str:
+def _validate_python_version(version: str) -> None:
     ...
 
 
-def _get_modal_requirements_path(builder_version: typing.Literal['2023.12', '2024.04'], python_version: typing.Union[str, None] = None) -> str:
+def _dockerhub_python_version(python_version=None):
     ...
 
 
-def _get_modal_requirements_command(version: typing.Literal['2023.12', '2024.04']) -> str:
+def _get_client_requirements_path(python_version: typing.Union[str, None] = None) -> str:
     ...
 
 
 def _flatten_str_args(function_name: str, arg_name: str, args: typing.Tuple[typing.Union[str, typing.List[str]], ...]) -> typing.List[str]:
     ...
 
 
 def _make_pip_install_args(find_links: typing.Union[str, None] = None, index_url: typing.Union[str, None] = None, extra_index_url: typing.Union[str, None] = None, pre: bool = False) -> str:
     ...
 
 
-def _get_image_builder_version(client_version: str) -> typing.Literal['2023.12', '2024.04']:
-    ...
-
-
 class _ImageRegistryConfig:
     def __init__(self, registry_auth_type: int = 0, secret: typing.Union[modal.secret._Secret, None] = None):
         ...
 
     def get_proto(self) -> modal_proto.api_pb2.ImageRegistryConfig:
         ...
 
@@ -72,15 +58,15 @@
     def _initialize_from_empty(self):
         ...
 
     def _hydrate_metadata(self, message: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     @staticmethod
-    def _from_args(*, base_images: typing.Union[typing.Dict[str, _Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[typing.Literal['2023.12', '2024.04']], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret._Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions._Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False, _namespace: int = 1):
+    def _from_args(*, base_images: typing.Union[typing.Dict[str, _Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret._Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions._Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False, _namespace: int = 1):
         ...
 
     def extend(self, *, secrets: typing.Union[typing.Sequence[modal.secret._Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions._Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False, _namespace: int = 1) -> _Image:
         ...
 
     def copy_mount(self, mount: modal.mount._Mount, remote_path: typing.Union[str, pathlib.Path] = '.') -> _Image:
         ...
@@ -109,32 +95,32 @@
     def dockerfile_commands(self, *dockerfile_commands: typing.Union[str, typing.List[str]], context_files: typing.Dict[str, str] = {}, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False) -> _Image:
         ...
 
     def run_commands(self, *commands: typing.Union[str, typing.List[str]], secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, force_build: bool = False) -> _Image:
         ...
 
     @staticmethod
-    def conda(python_version: typing.Union[str, None] = None, force_build: bool = False) -> _Image:
+    def conda(python_version: str = '3.9', force_build: bool = False) -> _Image:
         ...
 
     def conda_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> _Image:
         ...
 
     def conda_update_from_environment(self, environment_yml: str, force_build: bool = False, *, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> _Image:
         ...
 
     @staticmethod
-    def micromamba(python_version: typing.Union[str, None] = None, force_build: bool = False) -> _Image:
+    def micromamba(python_version: str = '3.9', force_build: bool = False) -> _Image:
         ...
 
     def micromamba_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> _Image:
         ...
 
     @staticmethod
-    def _registry_setup_commands(tag: str, builder_version: typing.Literal['2023.12', '2024.04'], setup_commands: typing.List[str], add_python: typing.Union[str, None] = None) -> typing.List[str]:
+    def _registry_setup_commands(tag: str, setup_commands: typing.List[str], add_python: typing.Union[str, None]) -> typing.List[str]:
         ...
 
     @staticmethod
     def from_registry(tag: str, *, secret: typing.Union[modal.secret._Secret, None] = None, setup_dockerfile_commands: typing.List[str] = [], force_build: bool = False, add_python: typing.Union[str, None] = None, **kwargs) -> _Image:
         ...
 
     @staticmethod
@@ -182,15 +168,15 @@
     def _initialize_from_empty(self):
         ...
 
     def _hydrate_metadata(self, message: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     @staticmethod
-    def _from_args(*, base_images: typing.Union[typing.Dict[str, Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[typing.Literal['2023.12', '2024.04']], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret.Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions.Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount.Mount, None] = None, force_build: bool = False, _namespace: int = 1):
+    def _from_args(*, base_images: typing.Union[typing.Dict[str, Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret.Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions.Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount.Mount, None] = None, force_build: bool = False, _namespace: int = 1):
         ...
 
     def extend(self, **kwargs) -> Image:
         ...
 
     def copy_mount(self, mount: modal.mount.Mount, remote_path: typing.Union[str, pathlib.Path] = '.') -> Image:
         ...
@@ -219,32 +205,32 @@
     def dockerfile_commands(self, *dockerfile_commands: typing.Union[str, typing.List[str]], context_files: typing.Dict[str, str] = {}, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, context_mount: typing.Union[modal.mount.Mount, None] = None, force_build: bool = False) -> Image:
         ...
 
     def run_commands(self, *commands: typing.Union[str, typing.List[str]], secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, force_build: bool = False) -> Image:
         ...
 
     @staticmethod
-    def conda(python_version: typing.Union[str, None] = None, force_build: bool = False) -> Image:
+    def conda(python_version: str = '3.9', force_build: bool = False) -> Image:
         ...
 
     def conda_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> Image:
         ...
 
     def conda_update_from_environment(self, environment_yml: str, force_build: bool = False, *, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> Image:
         ...
 
     @staticmethod
-    def micromamba(python_version: typing.Union[str, None] = None, force_build: bool = False) -> Image:
+    def micromamba(python_version: str = '3.9', force_build: bool = False) -> Image:
         ...
 
     def micromamba_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> Image:
         ...
 
     @staticmethod
-    def _registry_setup_commands(tag: str, builder_version: typing.Literal['2023.12', '2024.04'], setup_commands: typing.List[str], add_python: typing.Union[str, None] = None) -> typing.List[str]:
+    def _registry_setup_commands(tag: str, setup_commands: typing.List[str], add_python: typing.Union[str, None]) -> typing.List[str]:
         ...
 
     @staticmethod
     def from_registry(tag: str, *, secret: typing.Union[modal.secret.Secret, None] = None, setup_dockerfile_commands: typing.List[str] = [], force_build: bool = False, add_python: typing.Union[str, None] = None, **kwargs) -> Image:
         ...
 
     @staticmethod
@@ -276,10 +262,7 @@
         ...
 
     def imports(self):
         ...
 
     def run_inside(self):
         ...
-
-
-SUPPORTED_PYTHON_SERIES: typing.Set[str]
```

## modal/mount.py

```diff
@@ -50,15 +50,15 @@
     """Get the deployed name of the python-build-standalone mount."""
     if "-" in version:  # default to glibc
         version, libc = version.split("-")
     else:
         libc = "gnu"
     if version not in PYTHON_STANDALONE_VERSIONS:
         raise modal.exception.InvalidError(
-            f"Unsupported standalone python version: {version!r}, supported values are {list(PYTHON_STANDALONE_VERSIONS)}"
+            f"Unsupported standalone python version: {version}, supported values are {list(PYTHON_STANDALONE_VERSIONS.keys())}"
         )
     if libc != "gnu":
         raise modal.exception.InvalidError(f"Unsupported libc identifier: {libc}")
     release, full_version = PYTHON_STANDALONE_VERSIONS[version]
     return f"python-build-standalone.{release}.{full_version}-{libc}"
```

## modal/network_file_system.py

```diff
@@ -20,15 +20,14 @@
 from .object import (
     EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
     _get_environment_name,
     _Object,
     live_method,
     live_method_gen,
 )
-from .volume import FileEntry
 
 NETWORK_FILE_SYSTEM_PUT_FILE_CLIENT_TIMEOUT = (
     10 * 60
 )  # 10 min max for transferring files (does not include upload time to s3)
 
 
 def network_file_system_mount_protos(
@@ -139,15 +138,15 @@
                 namespace=namespace,
                 environment_name=_get_environment_name(environment_name, resolver),
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
             )
             response = await resolver.client.stub.SharedVolumeGetOrCreate(req)
             self._hydrate(response.shared_volume_id, resolver.client, None)
 
-        return _NetworkFileSystem._from_loader(_load, "NetworkFileSystem()", hydrate_lazily=True)
+        return _NetworkFileSystem._from_loader(_load, "NetworkFileSystem()")
 
     @classmethod
     @asynccontextmanager
     async def ephemeral(
         cls: Type["_NetworkFileSystem"],
         client: Optional[_Client] = None,
         environment_name: Optional[str] = None,
@@ -290,25 +289,25 @@
         if response.WhichOneof("data_oneof") == "data":
             yield response.data
         else:
             async for data in blob_iter(response.data_blob_id, self._client.stub):
                 yield data
 
     @live_method_gen
-    async def iterdir(self, path: str) -> AsyncIterator[FileEntry]:
+    async def iterdir(self, path: str) -> AsyncIterator[api_pb2.SharedVolumeListFilesEntry]:
         """Iterate over all files in a directory in the network file system.
 
         * Passing a directory path lists all files in the directory (names are relative to the directory)
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         req = api_pb2.SharedVolumeListFilesRequest(shared_volume_id=self.object_id, path=path)
         async for batch in unary_stream(self._client.stub.SharedVolumeListFilesStream, req):
             for entry in batch.entries:
-                yield FileEntry._from_proto(entry)
+                yield entry
 
     @live_method
     async def add_local_file(
         self, local_path: Union[Path, str], remote_path: Optional[Union[str, PurePosixPath, None]] = None
     ):
         local_path = Path(local_path)
         if remote_path is None:
@@ -339,15 +338,15 @@
                     continue
                 relpath_str = subpath.relative_to(_local_path).as_posix()
                 yield self.add_local_file(subpath, PurePosixPath(remote_path, relpath_str))
 
         await ConcurrencyPool(20).run_coros(gen_transfers(), return_exceptions=True)
 
     @live_method
-    async def listdir(self, path: str) -> List[FileEntry]:
+    async def listdir(self, path: str) -> List[api_pb2.SharedVolumeListFilesEntry]:
         """List all files in a directory in the network file system.
 
         * Passing a directory path lists all files in the directory (names are relative to the directory)
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         return [entry async for entry in self.iterdir(path)]
```

## modal/network_file_system.pyi

```diff
@@ -1,10 +1,9 @@
 import modal.client
 import modal.object
-import modal.volume
 import modal_proto.api_pb2
 import pathlib
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
 def network_file_system_mount_protos(validated_network_file_systems: typing.List[typing.Tuple[str, _NetworkFileSystem]], allow_cross_region_volumes: bool) -> typing.List[modal_proto.api_pb2.SharedVolumeMount]:
@@ -41,24 +40,24 @@
 
     async def write_file(self, remote_path: str, fp: typing.BinaryIO) -> int:
         ...
 
     def read_file(self, path: str) -> typing.AsyncIterator[bytes]:
         ...
 
-    def iterdir(self, path: str) -> typing.AsyncIterator[modal.volume.FileEntry]:
+    def iterdir(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
         ...
 
     async def add_local_file(self, local_path: typing.Union[pathlib.Path, str], remote_path: typing.Union[str, pathlib.PurePosixPath, None] = None):
         ...
 
     async def add_local_dir(self, local_path: typing.Union[pathlib.Path, str], remote_path: typing.Union[str, pathlib.PurePosixPath, None] = None):
         ...
 
-    async def listdir(self, path: str) -> typing.List[modal.volume.FileEntry]:
+    async def listdir(self, path: str) -> typing.List[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
         ...
 
     async def remove_file(self, path: str, recursive=False):
         ...
 
 
 class NetworkFileSystem(modal.object.Object):
@@ -117,18 +116,18 @@
 
         def aio(self, path: str) -> typing.AsyncIterator[bytes]:
             ...
 
     read_file: __read_file_spec
 
     class __iterdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.Iterator[modal.volume.FileEntry]:
+        def __call__(self, path: str) -> typing.Iterator[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
-        def aio(self, path: str) -> typing.AsyncIterator[modal.volume.FileEntry]:
+        def aio(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
     iterdir: __iterdir_spec
 
     class __add_local_file_spec(typing_extensions.Protocol):
         def __call__(self, local_path: typing.Union[pathlib.Path, str], remote_path: typing.Union[str, pathlib.PurePosixPath, None] = None):
             ...
@@ -144,18 +143,18 @@
 
         async def aio(self, *args, **kwargs):
             ...
 
     add_local_dir: __add_local_dir_spec
 
     class __listdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.List[modal.volume.FileEntry]:
+        def __call__(self, path: str) -> typing.List[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
-        async def aio(self, *args, **kwargs) -> typing.List[modal.volume.FileEntry]:
+        async def aio(self, *args, **kwargs) -> typing.List[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
     listdir: __listdir_spec
 
     class __remove_file_spec(typing_extensions.Protocol):
         def __call__(self, path: str, recursive=False):
             ...
```

## modal/object.py

```diff
@@ -207,16 +207,15 @@
             raise ExecutionError(
                 "Object has not been hydrated and doesn't support lazy hydration."
                 " This might happen if an object is defined on a different stub,"
                 " or if it's on the same stub but it didn't get created because it"
                 " wasn't defined in global scope."
             )
         else:
-            # TODO: this client and/or resolver can't be changed by a caller to X.from_name()
-            resolver = Resolver(await _Client.from_env())
+            resolver = Resolver()  # TODO: this resolver has no attached Client!
             await resolver.load(self)
 
 
 Object = synchronize_api(_Object, target_module=__name__)
 
 
 def live_method(method):
```

## modal/partial_function.pyi

```diff
@@ -1,18 +1,24 @@
 import enum
 import modal.functions
 import modal_proto.api_pb2
 import typing
 
 class _PartialFunctionFlags(enum.IntFlag):
-    FUNCTION: int = 1
-    BUILD: int = 2
-    ENTER_PRE_CHECKPOINT: int = 4
-    ENTER_POST_CHECKPOINT: int = 8
-    EXIT: int = 16
+    FUNCTION: int
+    BUILD: int
+    ENTER_PRE_CHECKPOINT: int
+    ENTER_POST_CHECKPOINT: int
+    EXIT: int
+
+    def _generate_next_value_(name, start, count, last_values):
+        ...
+
+    def __new__(cls, value):
+        ...
 
 
 class _PartialFunction:
     raw_f: typing.Callable[..., typing.Any]
     flags: _PartialFunctionFlags
     webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None]
     is_generator: typing.Union[bool, None]
```

## modal/queue.py

```diff
@@ -1,57 +1,37 @@
 # Copyright Modal Labs 2022
 import queue  # The system library
 import time
 import warnings
-from typing import Any, AsyncGenerator, AsyncIterator, List, Optional, Type
+from typing import Any, AsyncIterator, List, Optional, Type
 
 from grpclib import GRPCError, Status
 from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_pb2
 
 from ._resolver import Resolver
 from ._serialization import deserialize, serialize
-from ._utils.async_utils import TaskContext, synchronize_api, warn_if_generator_is_not_consumed
+from ._utils.async_utils import TaskContext, synchronize_api
 from ._utils.grpc_utils import retry_transient_errors
 from .client import _Client
-from .exception import InvalidError, deprecation_warning
-from .object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
+from .exception import deprecation_warning
+from .object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method
 
 
 class _Queue(_Object, type_prefix="qu"):
     """Distributed, FIFO queue for data flow in Modal apps.
 
     The queue can contain any object serializable by `cloudpickle`, including Modal objects.
 
-    By default, the `Queue` object acts as a single FIFO queue which supports puts and gets (blocking and non-blocking).
+    **Lifetime of a queue and its contents**
 
-    **Queue partitions (beta)**
-
-    Specifying partition keys gives access to other independent FIFO partitions within the same `Queue` object.
-    Across any two partitions, puts and gets are completely independent. For example, a put in one partition does not affect
-    a get in any other partition.
-
-    When no partition key is specified (by default), puts and gets will operate on a default partition. This default partition
-    is also isolated from all other partitions. Please see the Usage section below for an example using partitions.
-
-    **Lifetime of a queue and its partitions**
-
-    By default, each partition is cleared 24 hours after the last `put` operation. A lower TTL can be specified by the `partition_ttl`
-    argument in the `put` or `put_many` methods. Each partition's expiry is handled independently.
-
-    As such, `Queue`s are best used for communication between active functions and not relied on for persistent storage.
-
-    On app completion or after stopping an app any associated `Queue` objects are cleaned up. All its partitions will be cleared.
-
-    **Limits**
-
-    A single `Queue` can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 256 KiB.
-
-    Partition keys must be non-empty and must not exceed 64 bytes.
+    A `Queue`'s lifetime matches the lifetime of the app it's attached to, but the contents expire after 30 days.
+    Because of this, `Queues`s are best used for communication between active functions and not relied on for
+    persistent storage. On app completion or after stopping an app any associated `Queue` objects are cleaned up.
 
     **Usage**
 
     ```python
     from modal import Queue, Stub
 
     stub = Stub()
@@ -60,29 +40,14 @@
     @stub.local_entrypoint()
     def main():
         my_queue.put("some value")
         my_queue.put(123)
 
         assert my_queue.get() == "some value"
         assert my_queue.get() == 123
-
-        my_queue.put(0)
-        my_queue.put(1, partition_key="foo")
-        my_queue.put(2, partition_key="bar")
-
-        # Default and "foo" partition are ignored by the get operation.
-        assert my_queue.get(partition_key="bar") == 2
-
-        # Set custom 10s expiration time on "foo" partition.
-        my_queue.put(3, partition_key="foo", partition_ttl=10)
-
-        # (beta feature) Iterate through items in place (read immutably)
-        my_queue.put(1)
-        assert [v for v in my_queue.iterate()] == [0, 1]
-
     ```
 
     For more examples, see the [guide](/docs/guide/dicts-and-queues#modal-queues).
     """
 
     @staticmethod
     def new():
@@ -99,25 +64,14 @@
 
         return _Queue._from_loader(_load, "Queue()")
 
     def __init__(self):
         """mdmd:hidden"""
         raise RuntimeError("Queue() is not allowed. Please use `Queue.from_name(...)` or `Queue.ephemeral()` instead.")
 
-    @staticmethod
-    def validate_partition_key(partition: Optional[str]) -> bytes:
-        if partition is not None:
-            partition_key = partition.encode("utf-8")
-            if len(partition_key) == 0 or len(partition_key) > 64:
-                raise InvalidError("Queue partition key must be between 1 and 64 characters.")
-        else:
-            partition_key = b""
-
-        return partition_key
-
     @classmethod
     @asynccontextmanager
     async def ephemeral(
         cls: Type["_Queue"],
         client: Optional[_Client] = None,
         environment_name: Optional[str] = None,
         _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
@@ -171,15 +125,15 @@
                 namespace=namespace,
                 environment_name=_get_environment_name(environment_name, resolver),
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
             )
             response = await resolver.client.stub.QueueGetOrCreate(req)
             self._hydrate(response.queue_id, resolver.client, None)
 
-        return _Queue._from_loader(_load, "Queue()", is_another_app=True, hydrate_lazily=True)
+        return _Queue._from_loader(_load, "Queue()", is_another_app=True)
 
     @staticmethod
     def persisted(
         label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None
     ) -> "_Queue":
         """Deprecated! Use `Queue.from_name(name, create_if_missing=True)`."""
         deprecation_warning((2024, 3, 1), _Queue.persisted.__doc__)
@@ -205,43 +159,41 @@
         )
         if client is None:
             client = await _Client.from_env()
         resolver = Resolver(client=client)
         await resolver.load(obj)
         return obj
 
-    async def _get_nonblocking(self, partition: Optional[str], n_values: int) -> List[Any]:
+    async def _get_nonblocking(self, n_values: int) -> List[Any]:
         request = api_pb2.QueueGetRequest(
             queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
             timeout=0,
             n_values=n_values,
         )
 
         response = await retry_transient_errors(self._client.stub.QueueGet, request)
         if response.values:
             return [deserialize(value, self._client) for value in response.values]
         else:
             return []
 
-    async def _get_blocking(self, partition: Optional[str], timeout: Optional[float], n_values: int) -> List[Any]:
+    async def _get_blocking(self, timeout: Optional[float], n_values: int) -> List[Any]:
         if timeout is not None:
             deadline = time.time() + timeout
         else:
             deadline = None
 
         while True:
             request_timeout = 50.0  # We prevent longer ones in order to keep the connection alive
 
             if deadline is not None:
                 request_timeout = min(request_timeout, deadline - time.time())
 
             request = api_pb2.QueueGetRequest(
                 queue_id=self.object_id,
-                partition_key=self.validate_partition_key(partition),
                 timeout=request_timeout,
                 n_values=n_values,
             )
 
             response = await retry_transient_errors(self._client.stub.QueueGet, request)
 
             if response.values:
@@ -249,183 +201,119 @@
 
             if deadline is not None and time.time() > deadline:
                 break
 
         raise queue.Empty()
 
     @live_method
-    async def get(
-        self, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
-    ) -> Optional[Any]:
+    async def get(self, block: bool = True, timeout: Optional[float] = None) -> Optional[Any]:
         """Remove and return the next object in the queue.
 
         If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
         an object, or until `timeout` if specified. Raises a native `queue.Empty` exception
         if the `timeout` is reached.
 
         If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
         ignored in this case.
         """
 
         if block:
-            values = await self._get_blocking(partition, timeout, 1)
+            values = await self._get_blocking(timeout, 1)
         else:
             if timeout is not None:
                 warnings.warn("Timeout is ignored for non-blocking get.")
-            values = await self._get_nonblocking(partition, 1)
+            values = await self._get_nonblocking(1)
 
         if values:
             return values[0]
         else:
             return None
 
     @live_method
-    async def get_many(
-        self, n_values: int, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
-    ) -> List[Any]:
+    async def get_many(self, n_values: int, block: bool = True, timeout: Optional[float] = None) -> List[Any]:
         """Remove and return up to `n_values` objects from the queue.
 
         If there are fewer than `n_values` items in the queue, return all of them.
 
         If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
         at least 1 object to be present, or until `timeout` if specified. Raises the stdlib's `queue.Empty`
         exception if the `timeout` is reached.
 
         If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
         ignored in this case.
         """
 
         if block:
-            return await self._get_blocking(partition, timeout, n_values)
+            return await self._get_blocking(timeout, n_values)
         else:
             if timeout is not None:
                 warnings.warn("Timeout is ignored for non-blocking get.")
-            return await self._get_nonblocking(partition, n_values)
+            return await self._get_nonblocking(n_values)
 
     @live_method
-    async def put(
-        self,
-        v: Any,
-        block: bool = True,
-        timeout: Optional[float] = None,
-        *,
-        partition: Optional[str] = None,
-        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
-    ) -> None:
+    async def put(self, v: Any, block: bool = True, timeout: Optional[float] = None) -> None:
         """Add an object to the end of the queue.
 
         If `block` is `True` and the queue is full, this method will retry indefinitely or
         until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
         If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.
 
         If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
         ignored in this case."""
-        await self.put_many([v], block, timeout, partition=partition, partition_ttl=partition_ttl)
+        await self.put_many([v], block, timeout)
 
     @live_method
-    async def put_many(
-        self,
-        vs: List[Any],
-        block: bool = True,
-        timeout: Optional[float] = None,
-        *,
-        partition: Optional[str] = None,
-        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
-    ) -> None:
+    async def put_many(self, vs: List[Any], block: bool = True, timeout: Optional[float] = None) -> None:
         """Add several objects to the end of the queue.
 
         If `block` is `True` and the queue is full, this method will retry indefinitely or
         until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
         If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.
 
         If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
-        ignored in this case.
-        """
+        ignored in this case."""
         if block:
-            await self._put_many_blocking(partition, partition_ttl, vs, timeout)
+            await self._put_many_blocking(vs, timeout)
         else:
             if timeout is not None:
                 warnings.warn("`timeout` argument is ignored for non-blocking put.")
-            await self._put_many_nonblocking(partition, partition_ttl, vs)
+            await self._put_many_nonblocking(vs)
 
-    async def _put_many_blocking(
-        self, partition: Optional[str], partition_ttl: int, vs: List[Any], timeout: Optional[float] = None
-    ):
+    async def _put_many_blocking(self, vs: List[Any], timeout: Optional[float] = None):
         vs_encoded = [serialize(v) for v in vs]
 
         request = api_pb2.QueuePutRequest(
             queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
             values=vs_encoded,
-            partition_ttl_seconds=partition_ttl,
         )
         try:
             await retry_transient_errors(
                 self._client.stub.QueuePut,
                 request,
                 # A full queue will return this status.
                 additional_status_codes=[Status.RESOURCE_EXHAUSTED],
                 max_delay=30.0,
                 total_timeout=timeout,
             )
         except GRPCError as exc:
             raise queue.Full(str(exc)) if exc.status == Status.RESOURCE_EXHAUSTED else exc
 
-    async def _put_many_nonblocking(self, partition: Optional[str], partition_ttl: int, vs: List[Any]):
+    async def _put_many_nonblocking(self, vs: List[Any]):
         vs_encoded = [serialize(v) for v in vs]
         request = api_pb2.QueuePutRequest(
             queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
             values=vs_encoded,
-            partition_ttl_seconds=partition_ttl,
         )
         try:
             await retry_transient_errors(self._client.stub.QueuePut, request)
         except GRPCError as exc:
             raise queue.Full(exc.message) if exc.status == Status.RESOURCE_EXHAUSTED else exc
 
     @live_method
-    async def len(self, *, partition: Optional[str] = None) -> int:
-        """Return the number of objects in the queue partition."""
-        request = api_pb2.QueueLenRequest(
-            queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
-        )
+    async def len(self) -> int:
+        """Return the number of objects in the queue."""
+        request = api_pb2.QueueLenRequest(queue_id=self.object_id)
         response = await retry_transient_errors(self._client.stub.QueueLen, request)
         return response.len
 
-    @warn_if_generator_is_not_consumed()
-    @live_method_gen
-    async def iterate(
-        self, *, partition: Optional[str] = None, item_poll_timeout: float = 0.0
-    ) -> AsyncGenerator[Any, None]:
-        """(Beta feature) Iterate through items in the queue without mutation.
-
-        Specify `item_poll_timeout` to control how long the iterator should wait for the next time before giving up.
-        """
-        last_entry_id: Optional[str] = None
-        validated_partition_key = self.validate_partition_key(partition)
-        fetch_deadline = time.time() + item_poll_timeout
-
-        MAX_POLL_DURATION = 30.0
-        while True:
-            poll_duration = max(0.0, min(MAX_POLL_DURATION, fetch_deadline - time.time()))
-            request = api_pb2.QueueNextItemsRequest(
-                queue_id=self.object_id,
-                partition_key=validated_partition_key,
-                last_entry_id=last_entry_id,
-                item_poll_timeout=poll_duration,
-            )
-
-            response: api_pb2.QueueNextItemsResponse = await retry_transient_errors(
-                self._client.stub.QueueNextItems, request
-            )
-            if response.items:
-                for item in response.items:
-                    yield deserialize(item.value, self._client)
-                    last_entry_id = item.entry_id
-                fetch_deadline = time.time() + item_poll_timeout
-            elif time.time() > fetch_deadline:
-                break
-
 
 Queue = synchronize_api(_Queue)
```

## modal/queue.pyi

```diff
@@ -8,18 +8,14 @@
     @staticmethod
     def new():
         ...
 
     def __init__(self):
         ...
 
-    @staticmethod
-    def validate_partition_key(partition: typing.Union[str, None]) -> bytes:
-        ...
-
     @classmethod
     def ephemeral(cls: typing.Type[_Queue], client: typing.Union[modal.client._Client, None] = None, environment_name: typing.Union[str, None] = None, _heartbeat_sleep: float = 300) -> typing.AsyncContextManager[_Queue]:
         ...
 
     @staticmethod
     def from_name(label: str, namespace=1, environment_name: typing.Union[str, None] = None, create_if_missing: bool = False) -> _Queue:
         ...
@@ -28,57 +24,50 @@
     def persisted(label: str, namespace=1, environment_name: typing.Union[str, None] = None) -> _Queue:
         ...
 
     @staticmethod
     async def lookup(label: str, namespace=1, client: typing.Union[modal.client._Client, None] = None, environment_name: typing.Union[str, None] = None, create_if_missing: bool = False) -> _Queue:
         ...
 
-    async def _get_nonblocking(self, partition: typing.Union[str, None], n_values: int) -> typing.List[typing.Any]:
+    async def _get_nonblocking(self, n_values: int) -> typing.List[typing.Any]:
         ...
 
-    async def _get_blocking(self, partition: typing.Union[str, None], timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
+    async def _get_blocking(self, timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
         ...
 
-    async def get(self, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.Union[typing.Any, None]:
+    async def get(self, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.Union[typing.Any, None]:
         ...
 
-    async def get_many(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.List[typing.Any]:
+    async def get_many(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.List[typing.Any]:
         ...
 
-    async def put(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+    async def put(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None) -> None:
         ...
 
-    async def put_many(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+    async def put_many(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None) -> None:
         ...
 
-    async def _put_many_blocking(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
+    async def _put_many_blocking(self, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
         ...
 
-    async def _put_many_nonblocking(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any]):
+    async def _put_many_nonblocking(self, vs: typing.List[typing.Any]):
         ...
 
-    async def len(self, *, partition: typing.Union[str, None] = None) -> int:
-        ...
-
-    def iterate(self, *, partition: typing.Union[str, None] = None, item_poll_timeout: float = 0.0) -> typing.AsyncGenerator[typing.Any, None]:
+    async def len(self) -> int:
         ...
 
 
 class Queue(modal.object.Object):
     def __init__(self):
         ...
 
     @staticmethod
     def new():
         ...
 
-    @staticmethod
-    def validate_partition_key(partition: typing.Union[str, None]) -> bytes:
-        ...
-
     @classmethod
     def ephemeral(cls: typing.Type[Queue], client: typing.Union[modal.client.Client, None] = None, environment_name: typing.Union[str, None] = None, _heartbeat_sleep: float = 300) -> synchronicity.combined_types.AsyncAndBlockingContextManager[Queue]:
         ...
 
     @staticmethod
     def from_name(label: str, namespace=1, environment_name: typing.Union[str, None] = None, create_if_missing: bool = False) -> Queue:
         ...
@@ -93,95 +82,86 @@
 
         async def aio(self, *args, **kwargs) -> Queue:
             ...
 
     lookup: __lookup_spec
 
     class ___get_nonblocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], n_values: int) -> typing.List[typing.Any]:
+        def __call__(self, n_values: int) -> typing.List[typing.Any]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.List[typing.Any]:
             ...
 
     _get_nonblocking: ___get_nonblocking_spec
 
     class ___get_blocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
+        def __call__(self, timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.List[typing.Any]:
             ...
 
     _get_blocking: ___get_blocking_spec
 
     class __get_spec(typing_extensions.Protocol):
-        def __call__(self, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.Union[typing.Any, None]:
+        def __call__(self, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.Union[typing.Any, None]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.Union[typing.Any, None]:
             ...
 
     get: __get_spec
 
     class __get_many_spec(typing_extensions.Protocol):
-        def __call__(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.List[typing.Any]:
+        def __call__(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.List[typing.Any]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.List[typing.Any]:
             ...
 
     get_many: __get_many_spec
 
     class __put_spec(typing_extensions.Protocol):
-        def __call__(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+        def __call__(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None) -> None:
             ...
 
         async def aio(self, *args, **kwargs) -> None:
             ...
 
     put: __put_spec
 
     class __put_many_spec(typing_extensions.Protocol):
-        def __call__(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+        def __call__(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None) -> None:
             ...
 
         async def aio(self, *args, **kwargs) -> None:
             ...
 
     put_many: __put_many_spec
 
     class ___put_many_blocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
+        def __call__(self, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
             ...
 
         async def aio(self, *args, **kwargs):
             ...
 
     _put_many_blocking: ___put_many_blocking_spec
 
     class ___put_many_nonblocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any]):
+        def __call__(self, vs: typing.List[typing.Any]):
             ...
 
         async def aio(self, *args, **kwargs):
             ...
 
     _put_many_nonblocking: ___put_many_nonblocking_spec
 
     class __len_spec(typing_extensions.Protocol):
-        def __call__(self, *, partition: typing.Union[str, None] = None) -> int:
+        def __call__(self) -> int:
             ...
 
         async def aio(self, *args, **kwargs) -> int:
             ...
 
     len: __len_spec
-
-    class __iterate_spec(typing_extensions.Protocol):
-        def __call__(self, *, partition: typing.Union[str, None] = None, item_poll_timeout: float = 0.0) -> typing.Generator[typing.Any, None, None]:
-            ...
-
-        def aio(self, *, partition: typing.Union[str, None] = None, item_poll_timeout: float = 0.0) -> typing.AsyncGenerator[typing.Any, None]:
-            ...
-
-    iterate: __iterate_spec
```

## modal/runner.py

```diff
@@ -1,207 +1,67 @@
 # Copyright Modal Labs 2022
 import asyncio
 import dataclasses
 import os
 from multiprocessing.synchronize import Event
-from typing import TYPE_CHECKING, AsyncGenerator, Dict, List, Optional, TypeVar
+from typing import TYPE_CHECKING, AsyncGenerator, List, Optional, TypeVar
 
-from grpclib import GRPCError, Status
 from rich.console import Console
 from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_pb2
 
-from ._container_io_manager import is_local
 from ._output import OutputManager, get_app_logs_loop, step_completed, step_progress
 from ._pty import get_pty_info
-from ._resolver import Resolver
 from ._sandbox_shell import connect_to_sandbox
 from ._utils.app_utils import is_valid_app_name
 from ._utils.async_utils import TaskContext, synchronize_api
 from ._utils.grpc_utils import retry_transient_errors
+from .app import _LocalApp, is_local
 from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
-from .config import config, logger
-from .exception import ExecutionError, InteractiveTimeoutError, InvalidError, _CliUserExecutionError
-from .object import _Object
-from .running_app import RunningApp
+from .config import config
+from .exception import InteractiveTimeoutError, InvalidError, _CliUserExecutionError
 
 if TYPE_CHECKING:
-    from .stub import _App
+    from .stub import _Stub
 else:
-    _App = TypeVar("_App")
+    _Stub = TypeVar("_Stub")
 
 
 async def _heartbeat(client, app_id):
     request = api_pb2.AppHeartbeatRequest(app_id=app_id)
     # TODO(erikbern): we should capture exceptions here
     # * if request fails: destroy the client
     # * if server says the app is gone: print a helpful warning about detaching
     await retry_transient_errors(client.stub.AppHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT)
 
 
-async def _init_local_app_existing(client: _Client, existing_app_id: str) -> "RunningApp":
-    # Get all the objects first
-    obj_req = api_pb2.AppGetObjectsRequest(app_id=existing_app_id)
-    obj_resp = await retry_transient_errors(client.stub.AppGetObjects, obj_req)
-    app_page_url = f"https://modal.com/apps/{existing_app_id}"  # TODO (elias): this should come from the backend
-    object_ids = {item.tag: item.object.object_id for item in obj_resp.items}
-    return RunningApp(existing_app_id, app_page_url=app_page_url, tag_to_object_id=object_ids)
-
-
-async def _init_local_app_new(
-    client: _Client,
-    description: str,
-    app_state: int,
-    environment_name: str = "",
-    interactive=False,
-) -> "RunningApp":
-    app_req = api_pb2.AppCreateRequest(
-        description=description,
-        environment_name=environment_name,
-        app_state=app_state,
-    )
-    app_resp = await retry_transient_errors(client.stub.AppCreate, app_req)
-    app_page_url = app_resp.app_logs_url
-    logger.debug(f"Created new app with id {app_resp.app_id}")
-    return RunningApp(
-        app_resp.app_id, app_page_url=app_page_url, environment_name=environment_name, interactive=interactive
-    )
-
-
-async def _init_local_app_from_name(
-    client: _Client,
-    name: str,
-    namespace,
-    environment_name: str = "",
-):
-    # Look up any existing deployment
-    app_req = api_pb2.AppGetByDeploymentNameRequest(
-        name=name,
-        namespace=namespace,
-        environment_name=environment_name,
-    )
-    app_resp = await retry_transient_errors(client.stub.AppGetByDeploymentName, app_req)
-    existing_app_id = app_resp.app_id or None
-
-    # Grab the app
-    if existing_app_id is not None:
-        return await _init_local_app_existing(client, existing_app_id)
-    else:
-        return await _init_local_app_new(
-            client, name, api_pb2.APP_STATE_INITIALIZING, environment_name=environment_name
-        )
-
-
-async def _create_all_objects(
-    client: _Client,
-    app: RunningApp,
-    indexed_objects: Dict[str, _Object],
-    new_app_state: int,
-    environment_name: str,
-    output_mgr: Optional[OutputManager] = None,
-):  # api_pb2.AppState.V
-    """Create objects that have been defined but not created on the server."""
-    if not client.authenticated:
-        raise ExecutionError("Objects cannot be created with an unauthenticated client")
-
-    resolver = Resolver(
-        client,
-        output_mgr=output_mgr,
-        environment_name=environment_name,
-        app_id=app.app_id,
-    )
-    with resolver.display():
-        # Get current objects, and reset all objects
-        tag_to_object_id = app.tag_to_object_id
-        app.tag_to_object_id = {}
-
-        # Assign all objects
-        for tag, obj in indexed_objects.items():
-            # Reset object_id in case the app runs twice
-            # TODO(erikbern): clean up the interface
-            obj._unhydrate()
-
-        # Preload all functions to make sure they have ids assigned before they are loaded.
-        # This is important to make sure any enclosed function handle references in serialized
-        # functions have ids assigned to them when the function is serialized.
-        # Note: when handles/objs are merged, all objects will need to get ids pre-assigned
-        # like this in order to be referrable within serialized functions
-        for tag, obj in indexed_objects.items():
-            existing_object_id = tag_to_object_id.get(tag)
-            # Note: preload only currently implemented for Functions, returns None otherwise
-            # this is to ensure that directly referenced functions from the global scope has
-            # ids associated with them when they are serialized into other functions
-            await resolver.preload(obj, existing_object_id)
-            if obj.object_id is not None:
-                tag_to_object_id[tag] = obj.object_id
-
-        for tag, obj in indexed_objects.items():
-            existing_object_id = tag_to_object_id.get(tag)
-            await resolver.load(obj, existing_object_id)
-            app.tag_to_object_id[tag] = obj.object_id
-
-    # Create the app (and send a list of all tagged obs)
-    # TODO(erikbern): we should delete objects from a previous version that are no longer needed
-    # We just delete them from the app, but the actual objects will stay around
-    indexed_object_ids = app.tag_to_object_id
-    assert indexed_object_ids == app.tag_to_object_id
-    all_objects = resolver.objects()
-
-    unindexed_object_ids = list(set(obj.object_id for obj in all_objects) - set(app.tag_to_object_id.values()))
-    req_set = api_pb2.AppSetObjectsRequest(
-        app_id=app.app_id,
-        indexed_object_ids=indexed_object_ids,
-        unindexed_object_ids=unindexed_object_ids,
-        new_app_state=new_app_state,  # type: ignore
-    )
-    await retry_transient_errors(client.stub.AppSetObjects, req_set)
-
-
-async def _disconnect(
-    client: _Client,
-    app_id: str,
-    reason: "Optional[api_pb2.AppDisconnectReason.ValueType]" = None,
-    exc_str: Optional[str] = None,
-):
-    """Tell the server the client has disconnected for this app. Terminates all running tasks
-    for ephemeral apps."""
-
-    if exc_str:
-        exc_str = exc_str[:1000]  # Truncate to 1000 chars
-
-    logger.debug("Sending app disconnect/stop request")
-    req_disconnect = api_pb2.AppClientDisconnectRequest(app_id=app_id, reason=reason, exception=exc_str)
-    await retry_transient_errors(client.stub.AppClientDisconnect, req_disconnect)
-    logger.debug("App disconnected")
-
-
 @asynccontextmanager
-async def _run_app(
-    stub: _App,
+async def _run_stub(
+    stub: _Stub,
     client: Optional[_Client] = None,
     stdout=None,
     show_progress: bool = True,
     detach: bool = False,
     output_mgr: Optional[OutputManager] = None,
     environment_name: Optional[str] = None,
     shell=False,
     interactive=False,
-) -> AsyncGenerator[_App, None]:
+) -> AsyncGenerator[_Stub, None]:
     """mdmd:hidden"""
     if environment_name is None:
         environment_name = config.get("environment")
 
     if not is_local():
         raise InvalidError(
             "Can not run an app from within a container."
             " Are you calling stub.run() directly?"
             " Consider using the `modal run` shell command."
         )
-    if stub._running_app:
+    if stub._local_app:
         raise InvalidError(
             "App is already running and can't be started again.\n"
             "You should not use `stub.run` or `run_stub` within a Modal `local_entrypoint`"
         )
 
     if stub.description is None:
         import __main__
@@ -216,40 +76,38 @@
     if client is None:
         client = await _Client.from_env()
     if output_mgr is None:
         output_mgr = OutputManager(stdout, show_progress, "Running app...")
     if shell:
         output_mgr._visible_progress = False
     app_state = api_pb2.APP_STATE_DETACHED if detach else api_pb2.APP_STATE_EPHEMERAL
-    app = await _init_local_app_new(
+    app = await _LocalApp._init_new(
         client,
         stub.description,
         environment_name=environment_name,
         app_state=app_state,
         interactive=interactive,
     )
-    async with stub._set_local_app(client, app), TaskContext(grace=config["logs_timeout"]) as tc:
+    async with stub._set_local_app(app), TaskContext(grace=config["logs_timeout"]) as tc:
         # Start heartbeats loop to keep the client alive
         tc.infinite_loop(lambda: _heartbeat(client, app.app_id), sleep=HEARTBEAT_INTERVAL)
 
         with output_mgr.ctx_if_visible(output_mgr.make_live(step_progress("Initializing..."))):
-            initialized_msg = f"Initialized. [grey70]View run at [underline]{app.app_page_url}[/underline][/grey70]"
+            initialized_msg = f"Initialized. [grey70]View run at [underline]{app.log_url()}[/underline][/grey70]"
             output_mgr.print_if_visible(step_completed(initialized_msg))
-            output_mgr.update_app_page_url(app.app_page_url)
+            output_mgr.update_app_page_url(app.log_url())
 
         # Start logs loop
         if not shell:
             logs_loop = tc.create_task(get_app_logs_loop(app.app_id, client, output_mgr))
 
         exc_info: Optional[BaseException] = None
         try:
             # Create all members
-            await _create_all_objects(
-                client, app, stub._indexed_objects, app_state, environment_name, output_mgr=output_mgr
-            )
+            await app._create_all_objects(stub._indexed_objects, app_state, environment_name, output_mgr=output_mgr)
 
             # Update all functions client-side to have the output mgr
             for obj in stub.registered_functions.values():
                 obj._set_output_mgr(output_mgr)
 
             # Update all the classes client-side to propagate output manager to their methods.
             for obj in stub.registered_classes.values():
@@ -270,23 +128,21 @@
             # mute cancellation errors on all function handles to prevent exception spam
             for obj in stub.registered_functions.values():
                 obj._set_mute_cancellation(True)
 
             if detach:
                 output_mgr.print_if_visible(step_completed("Shutting down Modal client."))
                 output_mgr.print_if_visible(
-                    f"""The detached app keeps running. You can track its progress at: [magenta]{app.app_page_url}[/magenta]"""
+                    f"""The detached app keeps running. You can track its progress at: [magenta]{app.log_url()}[/magenta]"""
                 )
                 if not shell:
                     logs_loop.cancel()
             else:
                 output_mgr.print_if_visible(
-                    step_completed(
-                        f"App aborted. [grey70]View run at [underline]{app.app_page_url}[/underline][/grey70]"
-                    )
+                    step_completed(f"App aborted. [grey70]View run at [underline]{app.log_url()}[/underline][/grey70]")
                 )
                 output_mgr.print_if_visible(
                     "Disconnecting from Modal - This will terminate your Modal app in a few seconds.\n"
                 )
         except BaseException as e:
             exc_info = e
             raise e
@@ -301,38 +157,38 @@
             if isinstance(exc_info, _CliUserExecutionError):
                 exc_str = repr(exc_info.__cause__)
             elif exc_info:
                 exc_str = repr(exc_info)
             else:
                 exc_str = ""
 
-            await _disconnect(client, app.app_id, reason, exc_str)
+            await app.disconnect(reason, exc_str)
             stub._uncreate_all_objects()
 
     output_mgr.print_if_visible(
-        step_completed(f"App completed. [grey70]View run at [underline]{app.app_page_url}[/underline][/grey70]")
+        step_completed(f"App completed. [grey70]View run at [underline]{app.log_url()}[/underline][/grey70]")
     )
 
 
 async def _serve_update(
     stub,
     existing_app_id: str,
     is_ready: Event,
     environment_name: str,
 ) -> None:
     """mdmd:hidden"""
     # Used by child process to reinitialize a served app
     client = await _Client.from_env()
     try:
-        app = await _init_local_app_existing(client, existing_app_id)
+        app = await _LocalApp._init_existing(client, existing_app_id)
 
         # Create objects
         output_mgr = OutputManager(None, True)
-        await _create_all_objects(
-            client, app, stub._indexed_objects, api_pb2.APP_STATE_UNSPECIFIED, environment_name, output_mgr=output_mgr
+        await app._create_all_objects(
+            stub._indexed_objects, api_pb2.APP_STATE_UNSPECIFIED, environment_name, output_mgr=output_mgr
         )
 
         # Communicate to the parent process
         is_ready.set()
     except asyncio.exceptions.CancelledError:
         # Stopped by parent process
         pass
@@ -341,16 +197,16 @@
 @dataclasses.dataclass(frozen=True)
 class DeployResult:
     """Dataclass representing the result of deploying an app."""
 
     app_id: str
 
 
-async def _deploy_app(
-    stub: _App,
+async def _deploy_stub(
+    stub: _Stub,
     name: str = None,
     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
     client=None,
     stdout=None,
     show_progress=True,
     environment_name: Optional[str] = None,
     public: bool = False,
@@ -397,65 +253,43 @@
         )
 
     if client is None:
         client = await _Client.from_env()
 
     output_mgr = OutputManager(stdout, show_progress)
 
-    app = await _init_local_app_from_name(client, name, namespace, environment_name=environment_name)
+    app = await _LocalApp._init_from_name(client, name, namespace, environment_name=environment_name)
 
     async with TaskContext(0) as tc:
         # Start heartbeats loop to keep the client alive
         tc.infinite_loop(lambda: _heartbeat(client, app.app_id), sleep=HEARTBEAT_INTERVAL)
 
         # Don't change the app state - deploy state is set by AppDeploy
         post_init_state = api_pb2.APP_STATE_UNSPECIFIED
 
         try:
             # Create all members
-            await _create_all_objects(
-                client,
-                app,
-                stub._indexed_objects,
-                post_init_state,
-                environment_name=environment_name,
-                output_mgr=output_mgr,
+            await app._create_all_objects(
+                stub._indexed_objects, post_init_state, environment_name=environment_name, output_mgr=output_mgr
             )
 
             # Deploy app
             # TODO(erikbern): not needed if the app already existed
-            deploy_req = api_pb2.AppDeployRequest(
-                app_id=app.app_id,
-                name=name,
-                namespace=namespace,
-                object_entity="ap",
-                visibility=(
-                    api_pb2.APP_DEPLOY_VISIBILITY_PUBLIC if public else api_pb2.APP_DEPLOY_VISIBILITY_WORKSPACE
-                ),
-            )
-            try:
-                deploy_response = await retry_transient_errors(client.stub.AppDeploy, deploy_req)
-            except GRPCError as exc:
-                if exc.status == Status.INVALID_ARGUMENT:
-                    raise InvalidError(exc.message)
-                if exc.status == Status.FAILED_PRECONDITION:
-                    raise InvalidError(exc.message)
-                raise
-            url = deploy_response.url
+            url = await app.deploy(name, namespace, public)
         except Exception as e:
             # Note that AppClientDisconnect only stops the app if it's still initializing, and is a no-op otherwise.
-            await _disconnect(client, app.app_id, reason=api_pb2.APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION)
+            await app.disconnect(reason=api_pb2.APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION)
             raise e
 
     output_mgr.print_if_visible(step_completed("App deployed! 🎉"))
     output_mgr.print_if_visible(f"\nView Deployment: [magenta]{url}[/magenta]")
     return DeployResult(app_id=app.app_id)
 
 
-async def _interactive_shell(_stub: _App, cmd: List[str], environment_name: str = "", **kwargs):
+async def _interactive_shell(_stub: _Stub, cmd: List[str], environment_name: str = "", **kwargs):
     """Run an interactive shell (like `bash`) within the image for this app.
 
     This is useful for online debugging and interactive exploration of the
     contents of this image. If `cmd` is optionally provided, it will be run
     instead of the default shell inside this image.
 
     **Example**
@@ -492,17 +326,11 @@
             loading_status.stop()
             raise InteractiveTimeoutError("Timed out while waiting for sandbox to start")
 
         loading_status.stop()
         await connect_to_sandbox(sb)
 
 
-run_app = synchronize_api(_run_app)
+run_stub = synchronize_api(_run_stub)
 serve_update = synchronize_api(_serve_update)
-deploy_app = synchronize_api(_deploy_app)
+deploy_stub = synchronize_api(_deploy_stub)
 interactive_shell = synchronize_api(_interactive_shell)
-
-# Soon-to-be-deprecated ones, add warning soon
-_run_stub = _run_app
-run_stub = run_app
-_deploy_stub = _deploy_app
-deploy_stub = deploy_app
```

## modal/runner.pyi

```diff
@@ -1,43 +1,21 @@
 import modal._output
 import modal.client
-import modal.object
-import modal.running_app
 import multiprocessing.synchronize
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
-_App = typing.TypeVar("_App")
+_Stub = typing.TypeVar("_Stub")
 
 async def _heartbeat(client, app_id):
     ...
 
 
-async def _init_local_app_existing(client: modal.client._Client, existing_app_id: str) -> modal.running_app.RunningApp:
-    ...
-
-
-async def _init_local_app_new(client: modal.client._Client, description: str, app_state: int, environment_name: str = '', interactive=False) -> modal.running_app.RunningApp:
-    ...
-
-
-async def _init_local_app_from_name(client: modal.client._Client, name: str, namespace, environment_name: str = ''):
-    ...
-
-
-async def _create_all_objects(client: modal.client._Client, app: modal.running_app.RunningApp, indexed_objects: typing.Dict[str, modal.object._Object], new_app_state: int, environment_name: str, output_mgr: typing.Union[modal._output.OutputManager, None] = None):
-    ...
-
-
-async def _disconnect(client: modal.client._Client, app_id: str, reason: typing.Union[int, None] = None, exc_str: typing.Union[str, None] = None):
-    ...
-
-
-def _run_app(stub: _App, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
+def _run_stub(stub: _Stub, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_Stub]:
     ...
 
 
 async def _serve_update(stub, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str) -> None:
     ...
 
 
@@ -59,81 +37,53 @@
     def __delattr__(self, name):
         ...
 
     def __hash__(self):
         ...
 
 
-async def _deploy_app(stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
+async def _deploy_stub(stub: _Stub, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
     ...
 
 
-async def _interactive_shell(_stub: _App, cmd: typing.List[str], environment_name: str = '', **kwargs):
+async def _interactive_shell(_stub: _Stub, cmd: typing.List[str], environment_name: str = '', **kwargs):
     ...
 
 
-class __run_app_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
+class __run_stub_spec(typing_extensions.Protocol):
+    def __call__(self, stub: _Stub, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_Stub]:
         ...
 
-    def aio(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
+    def aio(self, stub: _Stub, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_Stub]:
         ...
 
-run_app: __run_app_spec
+run_stub: __run_stub_spec
 
 
 class __serve_update_spec(typing_extensions.Protocol):
     def __call__(self, stub, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str) -> None:
         ...
 
     async def aio(self, *args, **kwargs) -> None:
         ...
 
 serve_update: __serve_update_spec
 
 
-class __deploy_app_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
+class __deploy_stub_spec(typing_extensions.Protocol):
+    def __call__(self, stub: _Stub, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
         ...
 
     async def aio(self, *args, **kwargs) -> DeployResult:
         ...
 
-deploy_app: __deploy_app_spec
+deploy_stub: __deploy_stub_spec
 
 
 class __interactive_shell_spec(typing_extensions.Protocol):
-    def __call__(self, _stub: _App, cmd: typing.List[str], environment_name: str = '', **kwargs):
+    def __call__(self, _stub: _Stub, cmd: typing.List[str], environment_name: str = '', **kwargs):
         ...
 
     async def aio(self, *args, **kwargs):
         ...
 
 interactive_shell: __interactive_shell_spec
-
-
-def _run_stub(stub: _App, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
-    ...
-
-
-class __run_stub_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
-        ...
-
-    def aio(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
-        ...
-
-run_stub: __run_stub_spec
-
-
-async def _deploy_stub(stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
-    ...
-
-
-class __deploy_stub_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
-        ...
-
-    async def aio(self, *args, **kwargs) -> DeployResult:
-        ...
-
-deploy_stub: __deploy_stub_spec
```

## modal/sandbox.py

```diff
@@ -1,29 +1,28 @@
 # Copyright Modal Labs 2022
 import asyncio
 import os
-from typing import AsyncIterator, Dict, List, Optional, Sequence, Tuple, Union
+from typing import AsyncIterator, Dict, List, Optional, Sequence, Union
 
 from google.protobuf.message import Message
 from grpclib.exceptions import GRPCError, StreamTerminatedError
 
 from modal.cloud_bucket_mount import _CloudBucketMount, cloud_bucket_mounts_to_proto
 from modal.exception import InvalidError, SandboxTerminatedError, SandboxTimeoutError
 from modal.volume import _Volume
 from modal_proto import api_pb2
 
 from ._location import parse_cloud_provider
 from ._resolver import Resolver
-from ._resources import convert_fn_config_to_resources_config
 from ._utils.async_utils import synchronize_api
 from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors, unary_stream
 from ._utils.mount_utils import validate_mount_points, validate_volumes
 from .client import _Client
 from .config import config
-from .gpu import GPU_T
+from .gpu import GPU_T, parse_gpu_config
 from .image import _Image
 from .mount import _Mount
 from .network_file_system import _NetworkFileSystem, network_file_system_mount_protos
 from .object import _Object
 from .secret import _Secret
 
 
@@ -238,15 +237,15 @@
         mounts: Sequence[_Mount],
         secrets: Sequence[_Secret],
         timeout: Optional[int] = None,
         workdir: Optional[str] = None,
         gpu: GPU_T = None,
         cloud: Optional[str] = None,
         cpu: Optional[float] = None,
-        memory: Optional[Union[int, Tuple[int, int]]] = None,
+        memory: Optional[int] = None,
         network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
         block_network: bool = False,
         volumes: Dict[Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]] = {},
         allow_background_volume_commits: bool = False,
         pty_info: Optional[api_pb2.PTYInfo] = None,
     ) -> "_Sandbox":
         """mdmd:hidden"""
@@ -271,14 +270,22 @@
                 deps.append(vol)
             for _, cloud_bucket_mount in cloud_bucket_mounts:
                 if cloud_bucket_mount.secret:
                     deps.append(cloud_bucket_mount.secret)
             return deps
 
         async def _load(self: _Sandbox, resolver: Resolver, _existing_object_id: Optional[str]):
+            gpu_config = parse_gpu_config(gpu)
+
+            cloud_provider = parse_cloud_provider(cloud) if cloud else None
+
+            if cpu is not None and cpu < 0.25:
+                raise InvalidError(f"Invalid fractional CPU value {cpu}. Cannot have less than 0.25 CPU resources.")
+            milli_cpu = int(1000 * cpu) if cpu is not None else None
+
             # Relies on dicts being ordered (true as of Python 3.6).
             volume_mounts = [
                 api_pb2.VolumeMount(
                     mount_path=path,
                     volume_id=volume.object_id,
                     allow_background_commits=allow_background_volume_commits,
                 )
@@ -288,16 +295,16 @@
             definition = api_pb2.Sandbox(
                 entrypoint_args=entrypoint_args,
                 image_id=image.object_id,
                 mount_ids=[mount.object_id for mount in mounts],
                 secret_ids=[secret.object_id for secret in secrets],
                 timeout_secs=timeout,
                 workdir=workdir,
-                resources=convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu),
-                cloud_provider=parse_cloud_provider(cloud) if cloud else None,
+                resources=api_pb2.Resources(gpu_config=gpu_config, milli_cpu=milli_cpu, memory_mb=memory),
+                cloud_provider=cloud_provider,
                 nfs_mounts=network_file_system_mount_protos(validated_network_file_systems, False),
                 runtime_debug=config.get("function_runtime_debug"),
                 block_network=block_network,
                 cloud_bucket_mounts=cloud_bucket_mounts_to_proto(cloud_bucket_mounts),
                 volume_mounts=volume_mounts,
                 pty_info=pty_info,
             )
```

## modal/sandbox.pyi

```diff
@@ -108,15 +108,15 @@
 class _Sandbox(modal.object._Object):
     _result: typing.Union[modal_proto.api_pb2.GenericResult, None]
     _stdout: _LogsReader
     _stderr: _LogsReader
     _stdin: _StreamWriter
 
     @staticmethod
-    def _new(entrypoint_args: typing.Sequence[str], image: modal.image._Image, mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system._NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> _Sandbox:
+    def _new(entrypoint_args: typing.Sequence[str], image: modal.image._Image, mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system._NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> _Sandbox:
         ...
 
     def _hydrate_metadata(self, handle_metadata: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     @staticmethod
     async def from_id(sandbox_id: str, client: typing.Union[modal.client._Client, None] = None) -> _Sandbox:
@@ -154,15 +154,15 @@
     _stderr: LogsReader
     _stdin: StreamWriter
 
     def __init__(self, *args, **kwargs):
         ...
 
     @staticmethod
-    def _new(entrypoint_args: typing.Sequence[str], image: modal.image.Image, mounts: typing.Sequence[modal.mount.Mount], secrets: typing.Sequence[modal.secret.Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system.NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> Sandbox:
+    def _new(entrypoint_args: typing.Sequence[str], image: modal.image.Image, mounts: typing.Sequence[modal.mount.Mount], secrets: typing.Sequence[modal.secret.Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system.NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> Sandbox:
         ...
 
     def _hydrate_metadata(self, handle_metadata: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     class __from_id_spec(typing_extensions.Protocol):
         def __call__(self, sandbox_id: str, client: typing.Union[modal.client.Client, None] = None) -> Sandbox:
```

## modal/secret.py

```diff
@@ -2,18 +2,18 @@
 import os
 from typing import Dict, List, Optional, Union
 
 from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
-from ._container_io_manager import is_local
 from ._resolver import Resolver
 from ._utils.async_utils import synchronize_api
 from ._utils.grpc_utils import retry_transient_errors
+from .app import is_local
 from .client import _Client
 from .exception import InvalidError, NotFoundError
 from .object import _get_environment_name, _Object
 
 ENV_DICT_WRONG_TYPE_ERR = "the env_dict argument to Secret has to be a dict[str, Union[str, None]]"
```

## modal/serving.py

```diff
@@ -10,38 +10,38 @@
 from synchronicity import Interface
 from synchronicity.async_wrap import asynccontextmanager
 
 from ._output import OutputManager
 from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
 from ._utils.logger import logger
 from ._watcher import watch
-from .cli.import_refs import import_app
+from .cli.import_refs import import_stub
 from .client import _Client
 from .config import config
-from .runner import _disconnect, _run_app, serve_update
+from .runner import _run_stub, serve_update
 
 if TYPE_CHECKING:
-    from .stub import _App
+    from .stub import _Stub
 else:
-    _App = TypeVar("_App")
+    _Stub = TypeVar("_Stub")
 
 
-def _run_serve(app_ref: str, existing_app_id: str, is_ready: Event, environment_name: str):
+def _run_serve(stub_ref: str, existing_app_id: str, is_ready: Event, environment_name: str):
     # subprocess entrypoint
-    _app = import_app(app_ref)
-    blocking_app = synchronizer._translate_out(_app, Interface.BLOCKING)
-    serve_update(blocking_app, existing_app_id, is_ready, environment_name)
+    _stub = import_stub(stub_ref)
+    blocking_stub = synchronizer._translate_out(_stub, Interface.BLOCKING)
+    serve_update(blocking_stub, existing_app_id, is_ready, environment_name)
 
 
 async def _restart_serve(
-    app_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0
+    stub_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0
 ) -> SpawnProcess:
     ctx = multiprocessing.get_context("spawn")  # Needed to reload the interpreter
     is_ready = ctx.Event()
-    p = ctx.Process(target=_run_serve, args=(app_ref, existing_app_id, is_ready, environment_name))
+    p = ctx.Process(target=_run_serve, args=(stub_ref, existing_app_id, is_ready, environment_name))
     p.start()
     await asyncify(is_ready.wait)(timeout)
     # TODO(erikbern): we don't fail if the above times out, but that's somewhat intentional, since
     # the child process might build a huge image or similar
     return p
 
 
@@ -59,15 +59,15 @@
             )
             proc.kill()
     except ProcessLookupError:
         pass  # Child process already finished
 
 
 async def _run_watch_loop(
-    app_ref: str,
+    stub_ref: str,
     app_id: str,
     output_mgr: OutputManager,
     watcher: AsyncGenerator[Set[str], None],
     environment_name: str,
 ):
     unsupported_msg = None
     if platform.system() == "Windows":
@@ -79,56 +79,51 @@
             output_mgr.print_if_visible(unsupported_msg)
     else:
         curr_proc = None
         try:
             async for trigger_files in watcher:
                 logger.debug(f"The following files triggered an app update: {', '.join(trigger_files)}")
                 await _terminate(curr_proc, output_mgr)
-                curr_proc = await _restart_serve(app_ref, existing_app_id=app_id, environment_name=environment_name)
+                curr_proc = await _restart_serve(stub_ref, existing_app_id=app_id, environment_name=environment_name)
         finally:
             await _terminate(curr_proc, output_mgr)
 
 
-def _get_clean_app_description(app_ref: str) -> str:
+def _get_clean_stub_description(stub_ref: str) -> str:
     # If possible, consider the 'ref' argument the start of the app's args. Everything
     # before it Modal CLI cruft (eg. `modal serve --timeout 1.0`).
     try:
-        func_ref_arg_idx = sys.argv.index(app_ref)
+        func_ref_arg_idx = sys.argv.index(stub_ref)
         return " ".join(sys.argv[func_ref_arg_idx:])
     except ValueError:
         return " ".join(sys.argv)
 
 
 @asynccontextmanager
-async def _serve_app(
-    app: "_App",
-    app_ref: str,
+async def _serve_stub(
+    stub: "_Stub",
+    stub_ref: str,
     stdout: Optional[io.TextIOWrapper] = None,
     show_progress: bool = True,
     _watcher: Optional[AsyncGenerator[Set[str], None]] = None,  # for testing
     environment_name: Optional[str] = None,
-) -> AsyncGenerator["_App", None]:
+) -> AsyncGenerator["_Stub", None]:
     if environment_name is None:
         environment_name = config.get("environment")
 
     client = await _Client.from_env()
 
     output_mgr = OutputManager(stdout, show_progress, "Running app...")
     if _watcher is not None:
         watcher = _watcher  # Only used by tests
     else:
-        mounts_to_watch = app._get_watch_mounts()
+        mounts_to_watch = stub._get_watch_mounts()
         watcher = watch(mounts_to_watch, output_mgr)
 
-    async with _run_app(app, client=client, output_mgr=output_mgr, environment_name=environment_name):
-        app_id: str = app.app_id
-        client.set_pre_stop(lambda: _disconnect(client, app_id))
+    async with _run_stub(stub, client=client, output_mgr=output_mgr, environment_name=environment_name):
+        client.set_pre_stop(stub._local_app.disconnect)
         async with TaskContext(grace=0.1) as tc:
-            tc.create_task(_run_watch_loop(app_ref, app.app_id, output_mgr, watcher, environment_name))
-            yield app
+            tc.create_task(_run_watch_loop(stub_ref, stub.app_id, output_mgr, watcher, environment_name))
+            yield stub
 
 
-serve_app = synchronize_api(_serve_app)
-
-# Soon-to-be-deprecated ones, add warning soon
-_serve_stub = _serve_app
-serve_stub = serve_app
+serve_stub = synchronize_api(_serve_stub)
```

## modal/serving.pyi

```diff
@@ -2,55 +2,41 @@
 import modal._output
 import multiprocessing.context
 import multiprocessing.synchronize
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
-_App = typing.TypeVar("_App")
+_Stub = typing.TypeVar("_Stub")
 
-def _run_serve(app_ref: str, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str):
+def _run_serve(stub_ref: str, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str):
     ...
 
 
-async def _restart_serve(app_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0) -> multiprocessing.context.SpawnProcess:
+async def _restart_serve(stub_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0) -> multiprocessing.context.SpawnProcess:
     ...
 
 
 async def _terminate(proc: typing.Union[multiprocessing.context.SpawnProcess, None], output_mgr: modal._output.OutputManager, timeout: float = 5.0):
     ...
 
 
-async def _run_watch_loop(app_ref: str, app_id: str, output_mgr: modal._output.OutputManager, watcher: typing.AsyncGenerator[typing.Set[str], None], environment_name: str):
+async def _run_watch_loop(stub_ref: str, app_id: str, output_mgr: modal._output.OutputManager, watcher: typing.AsyncGenerator[typing.Set[str], None], environment_name: str):
     ...
 
 
-def _get_clean_app_description(app_ref: str) -> str:
+def _get_clean_stub_description(stub_ref: str) -> str:
     ...
 
 
-def _serve_app(app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
-    ...
-
-
-class __serve_app_spec(typing_extensions.Protocol):
-    def __call__(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.Generator[typing.Set[str], None, None], None] = None, environment_name: typing.Union[str, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
-        ...
-
-    def aio(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
-        ...
-
-serve_app: __serve_app_spec
-
-
-def _serve_stub(app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
+def _serve_stub(stub: _Stub, stub_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_Stub]:
     ...
 
 
 class __serve_stub_spec(typing_extensions.Protocol):
-    def __call__(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.Generator[typing.Set[str], None, None], None] = None, environment_name: typing.Union[str, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
+    def __call__(self, stub: _Stub, stub_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.Generator[typing.Set[str], None, None], None] = None, environment_name: typing.Union[str, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_Stub]:
         ...
 
-    def aio(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
+    def aio(self, stub: _Stub, stub_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_Stub]:
         ...
 
 serve_stub: __serve_stub_spec
```

## modal/token_flow.py

```diff
@@ -74,15 +74,16 @@
     next_url: Optional[str] = None,
 ):
     server_url = config.get("server_url", profile=profile)
 
     console = Console()
 
     result: Optional[api_pb2.TokenFlowWaitResponse] = None
-    async with _Client.anonymous(server_url) as client:
+    client = await _Client.unauthenticated_client(server_url)
+    async with client:
         token_flow = _TokenFlow(client)
 
         async with token_flow.start(source, next_url) as (_, web_url, code):
             with console.status("Waiting for authentication in the web browser", spinner="dots"):
                 # Open the web url in the browser
                 if _open_url(web_url):
                     console.print(
```

## modal/volume.py

```diff
@@ -1,17 +1,12 @@
 # Copyright Modal Labs 2023
 import asyncio
 import concurrent.futures
-import enum
-import os
-import platform
-import re
 import time
 from contextlib import nullcontext
-from dataclasses import dataclass
 from pathlib import Path, PurePosixPath
 from typing import (
     IO,
     AsyncGenerator,
     AsyncIterator,
     BinaryIO,
     Callable,
@@ -38,56 +33,21 @@
     blob_upload_file,
     get_file_upload_spec_from_fileobj,
     get_file_upload_spec_from_path,
 )
 from ._utils.grpc_utils import retry_transient_errors, unary_stream
 from .client import _Client
 from .config import logger
-from .exception import deprecation_error
 from .object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
 
 # Max duration for uploading to volumes files
 # As a guide, files >40GiB will take >10 minutes to upload.
 VOLUME_PUT_FILE_CLIENT_TIMEOUT = 30 * 60
 
 
-class FileEntryType(enum.IntEnum):
-    """Type of a file entry listed from a Modal volume."""
-
-    UNSPECIFIED = 0
-    FILE = 1
-    DIRECTORY = 2
-    SYMLINK = 3
-
-
-@dataclass(frozen=True)
-class FileEntry:
-    """A file or directory entry listed from a Modal volume."""
-
-    path: str
-    type: FileEntryType
-    mtime: int
-    size: int
-
-    @classmethod
-    def _from_proto(cls, proto: api_pb2.FileEntry) -> "FileEntry":
-        return cls(
-            path=proto.path,
-            type=FileEntryType(proto.type),
-            mtime=proto.mtime,
-            size=proto.size,
-        )
-
-    def __getattr__(self, name: str):
-        deprecation_error(
-            (2024, 4, 15),
-            f"The FileEntry dataclass was introduced to replace a private Protobuf message. This dataclass does not have the {name} attribute.",
-        )
-
-
 class _Volume(_Object, type_prefix="vo"):
     """A writeable volume that can be used to share files between one or more Modal functions.
 
     The contents of a volume is exposed as a filesystem. You can use it to share data between different functions, or
     to persist durable state across several instances of the same function.
 
     Unlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was mounted.
@@ -189,15 +149,15 @@
                 namespace=namespace,
                 environment_name=_get_environment_name(environment_name, resolver),
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
             )
             response = await resolver.client.stub.VolumeGetOrCreate(req)
             self._hydrate(response.volume_id, resolver.client, None)
 
-        return _Volume._from_loader(_load, "Volume()", hydrate_lazily=True)
+        return _Volume._from_loader(_load, "Volume()")
 
     @classmethod
     @asynccontextmanager
     async def ephemeral(
         cls: Type["_Volume"],
         client: Optional[_Client] = None,
         environment_name: Optional[str] = None,
@@ -312,44 +272,31 @@
         reloading.
 
         Reloading will fail if there are open files for the volume.
         """
         try:
             await self._do_reload()
         except GRPCError as exc:
-            # TODO(staffan): This is brittle and janky, as it relies on specific paths and error messages which can
-            #  change server-side at any time. Consider returning the open files directly in the error emitted from the
-            #  server.
-            if exc.message == "there are open files preventing the operation":
-                # Attempt to identify what open files are problematic and include information about the first (to avoid
-                # really verbose errors) open file in the error message to help troubleshooting.
-                # This is best-effort and not necessarily bulletproof, as the view of open files inside the container
-                # might differ from that outside - but it will at least catch common errors.
-                vol_path = f"/__modal/volumes/{self.object_id}"
-                annotation = _open_files_error_annotation(vol_path)
-                if annotation:
-                    raise RuntimeError(f"{exc.message}: {annotation}")
-
             raise RuntimeError(exc.message) if exc.status in (Status.FAILED_PRECONDITION, Status.NOT_FOUND) else exc
 
     @live_method_gen
-    async def iterdir(self, path: str) -> AsyncIterator[FileEntry]:
+    async def iterdir(self, path: str) -> AsyncIterator[api_pb2.VolumeListFilesEntry]:
         """Iterate over all files in a directory in the volume.
 
         * Passing a directory path lists all files in the directory (names are relative to the directory)
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         req = api_pb2.VolumeListFilesRequest(volume_id=self.object_id, path=path)
         async for batch in unary_stream(self._client.stub.VolumeListFiles, req):
             for entry in batch.entries:
-                yield FileEntry._from_proto(entry)
+                yield entry
 
     @live_method
-    async def listdir(self, path: str) -> List[FileEntry]:
+    async def listdir(self, path: str) -> List[api_pb2.VolumeListFilesEntry]:
         """List all files under a path prefix in the modal.Volume.
 
         * Passing a directory path lists all files in the directory
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         return [entry async for entry in self.iterdir(path)]
@@ -643,62 +590,7 @@
             sha256_hex=file_spec.sha256_hex,
             mode=file_spec.mode,
         )
 
 
 Volume = synchronize_api(_Volume)
 VolumeUploadContextManager = synchronize_api(_VolumeUploadContextManager)
-
-
-def _open_files_error_annotation(mount_path: str) -> Optional[str]:
-    if platform.system() != "Linux":
-        return None
-
-    self_pid = os.readlink("/proc/self")
-
-    def find_open_file_for_pid(pid: str) -> Optional[str]:
-        # /proc/{pid}/cmdline is null separated
-        with open(f"/proc/{pid}/cmdline", "rb") as f:
-            raw = f.read()
-            parts = raw.split(b"\0")
-            cmdline = " ".join([part.decode() for part in parts]).rstrip(" ")
-
-        cwd = PurePosixPath(os.readlink(f"/proc/{pid}/cwd"))
-        # NOTE(staffan): Python 3.8 doesn't have is_relative_to(), so we're stuck with catching ValueError until
-        # we drop Python 3.8 support.
-        try:
-            _rel_cwd = cwd.relative_to(mount_path)
-            if pid == self_pid:
-                return "cwd is inside volume"
-            else:
-                return f"cwd of '{cmdline}' is inside volume"
-        except ValueError:
-            pass
-
-        for fd in os.listdir(f"/proc/{pid}/fd"):
-            try:
-                path = PurePosixPath(os.readlink(f"/proc/{pid}/fd/{fd}"))
-                try:
-                    rel_path = path.relative_to(mount_path)
-                    if pid == self_pid:
-                        return f"path {rel_path} is open"
-                    else:
-                        return f"path {rel_path} is open from '{cmdline}'"
-                except ValueError:
-                    pass
-
-            except FileNotFoundError:
-                # File was closed
-                pass
-        return None
-
-    pid_re = re.compile("^[1-9][0-9]*$")
-    for dirent in os.listdir("/proc/"):
-        if pid_re.match(dirent):
-            try:
-                annotation = find_open_file_for_pid(dirent)
-                if annotation:
-                    return annotation
-            except (FileNotFoundError, PermissionError):
-                pass
-
-    return None
```

## modal/volume.pyi

```diff
@@ -1,59 +1,17 @@
 import asyncio.locks
-import enum
 import modal._utils.blob_utils
 import modal.client
 import modal.object
 import modal_proto.api_pb2
 import pathlib
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
-class FileEntryType(enum.IntEnum):
-    """Type of a file entry listed from a Modal volume."""
-
-    UNSPECIFIED = 0
-    FILE = 1
-    DIRECTORY = 2
-    SYMLINK = 3
-
-
-class FileEntry:
-    path: str
-    type: FileEntryType
-    mtime: int
-    size: int
-
-    @classmethod
-    def _from_proto(cls, proto: modal_proto.api_pb2.FileEntry) -> FileEntry:
-        ...
-
-    def __getattr__(self, name: str):
-        ...
-
-    def __init__(self, path: str, type: FileEntryType, mtime: int, size: int) -> None:
-        ...
-
-    def __repr__(self):
-        ...
-
-    def __eq__(self, other):
-        ...
-
-    def __setattr__(self, name, value):
-        ...
-
-    def __delattr__(self, name):
-        ...
-
-    def __hash__(self):
-        ...
-
-
 class _Volume(modal.object._Object):
     _lock: asyncio.locks.Lock
 
     def _initialize_from_empty(self):
         ...
 
     @staticmethod
@@ -85,18 +43,18 @@
 
     async def commit(self):
         ...
 
     async def reload(self):
         ...
 
-    def iterdir(self, path: str) -> typing.AsyncIterator[FileEntry]:
+    def iterdir(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.VolumeListFilesEntry]:
         ...
 
-    async def listdir(self, path: str) -> typing.List[FileEntry]:
+    async def listdir(self, path: str) -> typing.List[modal_proto.api_pb2.VolumeListFilesEntry]:
         ...
 
     def read_file(self, path: typing.Union[str, bytes]) -> typing.AsyncIterator[bytes]:
         ...
 
     async def read_file_into_fileobj(self, path: typing.Union[str, bytes], fileobj: typing.IO[bytes], progress: bool = False) -> int:
         ...
@@ -206,27 +164,27 @@
 
         async def aio(self, *args, **kwargs):
             ...
 
     reload: __reload_spec
 
     class __iterdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.Iterator[FileEntry]:
+        def __call__(self, path: str) -> typing.Iterator[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
-        def aio(self, path: str) -> typing.AsyncIterator[FileEntry]:
+        def aio(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
     iterdir: __iterdir_spec
 
     class __listdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.List[FileEntry]:
+        def __call__(self, path: str) -> typing.List[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
-        async def aio(self, *args, **kwargs) -> typing.List[FileEntry]:
+        async def aio(self, *args, **kwargs) -> typing.List[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
     listdir: __listdir_spec
 
     class __read_file_spec(typing_extensions.Protocol):
         def __call__(self, path: typing.Union[str, bytes]) -> typing.Iterator[bytes]:
             ...
@@ -313,11 +271,7 @@
         def __call__(self, file_spec: modal._utils.blob_utils.FileUploadSpec) -> modal_proto.api_pb2.MountFile:
             ...
 
         async def aio(self, *args, **kwargs) -> modal_proto.api_pb2.MountFile:
             ...
 
     _upload_file: ___upload_file_spec
-
-
-def _open_files_error_annotation(mount_path: str) -> typing.Union[str, None]:
-    ...
```

## modal/_utils/async_utils.py

```diff
@@ -8,15 +8,14 @@
 import typing
 from contextlib import asynccontextmanager
 from typing import Any, AsyncGenerator, Callable, Iterator, List, Optional, Set, TypeVar, cast
 
 import synchronicity
 from typing_extensions import ParamSpec
 
-from ..exception import InvalidError
 from .logger import logger
 
 synchronizer = synchronicity.Synchronizer()
 # atexit.register(synchronizer.close)
 
 
 def synchronize_api(obj, target_module=None):
@@ -231,16 +230,14 @@
         fut = executor.submit(asyncio.run, coro)
         return fut.result()
 
 
 async def queue_batch_iterator(q: asyncio.Queue, max_batch_size=100, debounce_time=0.015):
     """
     Read from a queue but return lists of items when queue is large
-
-    Treats a None value as end of queue items
     """
     item_list: List[Any] = []
 
     while True:
         if q.empty() and len(item_list) > 0:
             yield item_list
             item_list = []
@@ -256,102 +253,52 @@
             if len(item_list) > 0:
                 yield item_list
             break
         item_list.append(res)
 
 
 class _WarnIfGeneratorIsNotConsumed:
-    def __init__(self, gen, function_name: str):
+    def __init__(self, gen, gen_f):
         self.gen = gen
-        self.function_name = function_name
+        self.gen_f = gen_f
         self.iterated = False
         self.warned = False
 
     def __aiter__(self):
         self.iterated = True
-        return self.gen.__aiter__()
+        return self.gen
 
     async def __anext__(self):
         self.iterated = True
         return await self.gen.__anext__()
 
-    async def asend(self, value):
-        self.iterated = True
-        return await self.gen.asend(value)
-
     def __repr__(self):
         return repr(self.gen)
 
     def __del__(self):
         if not self.iterated and not self.warned:
             self.warned = True
+            name = self.gen_f.__name__
             logger.warning(
-                f"Warning: the results of a call to {self.function_name} was not consumed, so the call will never be executed."
-                f" Consider a for-loop like `for x in {self.function_name}(...)` or unpacking the generator using `list(...)`"
+                f"Warning: the results of a call to {name} was not consumed, so the call will never be executed."
+                f" Consider a for-loop like `for x in {name}(...)` or unpacking the generator using `list(...)`"
             )
 
 
 synchronize_api(_WarnIfGeneratorIsNotConsumed)
 
 
-class _WarnIfNonWrappedGeneratorIsNotConsumed(_WarnIfGeneratorIsNotConsumed):
-    # used for non-synchronicity-wrapped generators and iterators
-    def __iter__(self):
-        self.iterated = True
-        return iter(self.gen)
-
-    def __next__(self):
-        self.iterated = True
-        return self.gen.__next__()
-
-    def send(self, value):
-        self.iterated = True
-        return self.gen.send(value)
-
-
-def warn_if_generator_is_not_consumed(function_name: Optional[str] = None):
+def warn_if_generator_is_not_consumed(gen_f):
     # https://gist.github.com/erikbern/01ae78d15f89edfa7f77e5c0a827a94d
-    def decorator(gen_f):
-        presented_func_name = function_name if function_name is not None else gen_f.__name__
-
-        @functools.wraps(gen_f)
-        def f_wrapped(*args, **kwargs):
-            gen = gen_f(*args, **kwargs)
-            if inspect.isasyncgen(gen):
-                return _WarnIfGeneratorIsNotConsumed(gen, presented_func_name)
-            else:
-                return _WarnIfNonWrappedGeneratorIsNotConsumed(gen, presented_func_name)
-
-        return f_wrapped
-
-    return decorator
-
-
-class AsyncOrSyncIteratable:
-    """Compatibility class for non-synchronicity wrapped async iterables to get
-    both async and sync interfaces in the same way that synchronicity does (but on the main thread)
-    so they can be "lazily" iterated using either `for _ in x` or `async for _ in x`
-
-    nested_async_message is raised as an InvalidError if the async variant is called
-    from an already async context, since that would otherwise deadlock the event loop
-    """
-
-    def __init__(self, async_iterable: typing.AsyncIterable[Any], nested_async_message):
-        self._async_iterable = async_iterable
-        self.nested_async_message = nested_async_message
+    @functools.wraps(gen_f)
+    def f_wrapped(*args, **kwargs):
+        gen = gen_f(*args, **kwargs)
+        return _WarnIfGeneratorIsNotConsumed(gen, gen_f)
 
-    def __aiter__(self):
-        return self._async_iterable
-
-    def __iter__(self):
-        try:
-            for output in run_generator_sync(self._async_iterable):  # type: ignore
-                yield output
-        except NestedAsyncCalls:
-            raise InvalidError(self.nested_async_message)
+    return f_wrapped
 
 
 _shutdown_tasks = []
 
 
 def on_shutdown(coro):
     # hook into event loop shutdown when all active tasks get cancelled
@@ -433,46 +380,7 @@
     Note that for Python 3.10+ you can use contextlib.nullcontext() instead.
 
     Usage:
     async with asyncnullcontext():
         pass
     """
     yield
-
-
-YIELD_TYPE = typing.TypeVar("YIELD_TYPE")
-SEND_TYPE = typing.TypeVar("SEND_TYPE")
-
-
-class NestedAsyncCalls(Exception):
-    pass
-
-
-def run_generator_sync(
-    gen: typing.AsyncGenerator[YIELD_TYPE, SEND_TYPE],
-) -> typing.Generator[YIELD_TYPE, SEND_TYPE, None]:
-    try:
-        asyncio.get_running_loop()
-    except RuntimeError:
-        pass  # no event loop - this is what we expect!
-    else:
-        raise NestedAsyncCalls()
-    loop = asyncio.new_event_loop()  # set up new event loop for the map so we can use async logic
-
-    # more or less copied from synchronicity's implementation:
-    next_send: typing.Union[SEND_TYPE, None] = None
-    next_yield: YIELD_TYPE
-    exc: Optional[BaseException] = None
-    while True:
-        try:
-            if exc:
-                next_yield = loop.run_until_complete(gen.athrow(exc))
-            else:
-                next_yield = loop.run_until_complete(gen.asend(next_send))  # type: ignore[arg-type]
-        except StopAsyncIteration:
-            break
-        try:
-            next_send = yield next_yield
-            exc = None
-        except BaseException as err:
-            exc = err
-    loop.close()
```

## modal/_utils/function_utils.py

```diff
@@ -1,32 +1,26 @@
 # Copyright Modal Labs 2022
-import asyncio
 import inspect
 import os
 import site
 import sys
 import sysconfig
 import typing
 from collections import deque
 from enum import Enum
 from pathlib import Path, PurePosixPath
-from typing import Any, AsyncIterator, Callable, List, Literal, Optional, Set, Type
-
-from grpclib import GRPCError
-from grpclib.exceptions import StreamTerminatedError
+from typing import Callable, List, Optional, Set, Type
 
 from modal_proto import api_pb2
 
-from .._serialization import deserialize_data_format, serialize
+from .._serialization import serialize
 from ..config import config, logger
 from ..exception import InvalidError, ModuleNotMountable
 from ..mount import ROOT_DIR, _Mount
 from ..object import Object
-from .blob_utils import blob_download
-from .grpc_utils import RETRYABLE_GRPC_STATUS_CODES, unary_stream
 
 SYS_PREFIXES = {
     Path(p)
     for p in (
         sys.prefix,
         sys.base_prefix,
         sys.exec_prefix,
@@ -335,46 +329,7 @@
     Used for deprecation of @exit() parameters.
     """
     num_params = len(inspect.signature(f).parameters)
     if hasattr(f, "__self__"):
         return num_params > 0
     else:
         return num_params > 1
-
-
-async def _stream_function_call_data(
-    client, function_call_id: str, variant: Literal["data_in", "data_out"]
-) -> AsyncIterator[Any]:
-    """Read from the `data_in` or `data_out` stream of a function call."""
-    last_index = 0
-    retries_remaining = 10
-
-    if variant == "data_in":
-        stub_fn = client.stub.FunctionCallGetDataIn
-    elif variant == "data_out":
-        stub_fn = client.stub.FunctionCallGetDataOut
-    else:
-        raise ValueError(f"Invalid variant {variant}")
-
-    while True:
-        req = api_pb2.FunctionCallGetDataRequest(function_call_id=function_call_id, last_index=last_index)
-        try:
-            async for chunk in unary_stream(stub_fn, req):
-                if chunk.index <= last_index:
-                    continue
-                last_index = chunk.index
-                if chunk.data_blob_id:
-                    message_bytes = await blob_download(chunk.data_blob_id, client.stub)
-                else:
-                    message_bytes = chunk.data
-                message = deserialize_data_format(message_bytes, chunk.data_format, client)
-                yield message
-        except (GRPCError, StreamTerminatedError) as exc:
-            if retries_remaining > 0:
-                retries_remaining -= 1
-                if isinstance(exc, GRPCError):
-                    if exc.status in RETRYABLE_GRPC_STATUS_CODES:
-                        await asyncio.sleep(1.0)
-                        continue
-                elif isinstance(exc, StreamTerminatedError):
-                    continue
-            raise
```

## modal/_utils/grpc_testing.py

```diff
@@ -171,18 +171,15 @@
         # dropping any preceding requests if there is a match
         # returns the payload of the request
         for i, (_method_name, msg) in enumerate(self.calls):
             if _method_name == method_name:
                 self.calls = self.calls[i + 1 :]
                 return msg
 
-        raise KeyError(f"No message of that type in call list: {self.calls}")
-
-    def get_requests(self, method_name: str) -> List[Any]:
-        return [msg for _method_name, msg in self.calls if _method_name == method_name]
+        raise Exception(f"No message of that type in call list: {self.calls}")
 
 
 class InterceptedStream:
     def __init__(self, interception_context, method_name, stream):
         self.interception_context = interception_context
         self.method_name = method_name
         self.stream = stream
```

## modal/_utils/grpc_utils.py

```diff
@@ -261,20 +261,15 @@
             n_retries += 1
 
             await asyncio.sleep(delay)
             delay = min(delay * delay_factor, max_delay)
 
 
 def find_free_port() -> int:
-    """
-    Find a free TCP port, useful for testing.
-
-    WARN: if a returned free port is not bound immediately by the caller, that same port
-    may be returned in subsequent calls to this function, potentially creating port collisions.
-    """
+    """Find a free TCP port, useful for testing."""
     with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
         s.bind(("", 0))
         s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
         return s.getsockname()[1]
 
 
 def get_proto_oneof(message: Message, oneof_group: str) -> Optional[Message]:
```

## modal/_utils/hash_utils.py

```diff
@@ -1,59 +1,58 @@
 # Copyright Modal Labs 2022
 import base64
 import dataclasses
 import hashlib
-from typing import BinaryIO, Callable, List, Union
+from typing import IO, Union
 
 HASH_CHUNK_SIZE = 4096
 
 
-def _update(hashers: List[Callable[[bytes], None]], data: Union[bytes, BinaryIO]) -> None:
+def _update(hashers, data: Union[bytes, IO[bytes]]):
     if isinstance(data, bytes):
         for hasher in hashers:
-            hasher(data)
+            hasher.update(data)
     else:
-        assert not isinstance(data, (bytearray, memoryview))  # https://github.com/microsoft/pyright/issues/5697
         pos = data.tell()
-        while True:
+        while 1:
             chunk = data.read(HASH_CHUNK_SIZE)
             if not isinstance(chunk, bytes):
                 raise ValueError(f"Only accepts bytes or byte buffer objects, not {type(chunk)} buffers")
             if not chunk:
                 break
             for hasher in hashers:
-                hasher(chunk)
+                hasher.update(chunk)
         data.seek(pos)
 
 
-def get_sha256_hex(data: Union[bytes, BinaryIO]) -> str:
+def get_sha256_hex(data: Union[bytes, IO[bytes]]) -> str:
     hasher = hashlib.sha256()
-    _update([hasher.update], data)
+    _update([hasher], data)
     return hasher.hexdigest()
 
 
-def get_sha256_base64(data: Union[bytes, BinaryIO]) -> str:
+def get_sha256_base64(data: Union[bytes, IO[bytes]]) -> str:
     hasher = hashlib.sha256()
-    _update([hasher.update], data)
+    _update([hasher], data)
     return base64.b64encode(hasher.digest()).decode("ascii")
 
 
-def get_md5_base64(data: Union[bytes, BinaryIO]) -> str:
+def get_md5_base64(data: Union[bytes, IO[bytes]]) -> str:
     hasher = hashlib.md5()
-    _update([hasher.update], data)
+    _update([hasher], data)
     return base64.b64encode(hasher.digest()).decode("utf-8")
 
 
 @dataclasses.dataclass
 class UploadHashes:
     md5_base64: str
     sha256_base64: str
 
 
-def get_upload_hashes(data: Union[bytes, BinaryIO]) -> UploadHashes:
+def get_upload_hashes(data: Union[bytes, IO[bytes]]) -> UploadHashes:
     md5 = hashlib.md5()
     sha256 = hashlib.sha256()
-    _update([md5.update, sha256.update], data)
+    _update([md5, sha256], data)
     return UploadHashes(
         md5_base64=base64.b64encode(md5.digest()).decode("ascii"),
         sha256_base64=base64.b64encode(sha256.digest()).decode("ascii"),
     )
```

## modal/_utils/mount_utils.py

```diff
@@ -1,12 +1,12 @@
 # Copyright Modal Labs 2022
 import posixpath
 import typing
 from pathlib import PurePath, PurePosixPath
-from typing import TYPE_CHECKING, Dict, List, Mapping, Sequence, Tuple, Union
+from typing import TYPE_CHECKING, Dict, List, Mapping, Tuple, Union
 
 from ..exception import InvalidError
 from ..volume import _Volume
 
 if TYPE_CHECKING:
     from ..cloud_bucket_mount import _CloudBucketMount
     from ..network_file_system import _NetworkFileSystem
@@ -36,15 +36,15 @@
             raise InvalidError(f"{display_name} {path} cannot be mounted at '/tmp'.")
         validated.append((path, vol))
     return validated
 
 
 def validate_volumes(
     volumes: Mapping[Union[str, PurePosixPath], Union["_Volume", "_CloudBucketMount"]],
-) -> Sequence[Tuple[str, Union["_Volume", "_NetworkFileSystem", "_CloudBucketMount"]]]:
+) -> List[Tuple[str, Union["_Volume", "_NetworkFileSystem", "_CloudBucketMount"]]]:
     if not isinstance(volumes, dict):
         raise InvalidError("volumes must be a dict[str, Volume] where the keys are paths")
 
     validated_volumes = validate_mount_points("Volume", volumes)
     # We don't support mounting a volume in more than one location
     volume_to_paths: Dict["_Volume", List[str]] = {}
     for path, volume in validated_volumes:
```

## modal/_utils/rand_pb_testing.py

```diff
@@ -54,18 +54,20 @@
                     element = msg_field.add()
                     _fill(element, field.message_type, rand)
             else:
                 _fill(msg_field, field.message_type, rand)
         else:
             if field.type == FieldDescriptor.TYPE_ENUM:
                 enum_values = [x.number for x in field.enum_type.values]
-                generator = lambda rand: rand.choice(enum_values)  # noqa: E731
+
+                def generator(rand):
+                    return rand.choice(enum_values)
 
             else:
-                generator = _FIELD_RANDOM_GENERATOR[field.type]
+                generator = _FIELD_RANDOM_GENERATOR.get(field.type)
             if is_repeated:
                 num = rand.randint(0, 2)
                 msg_field = getattr(msg, field.name)
                 for _ in range(num):
                     msg_field.append(generator(rand))
             else:
                 setattr(msg, field.name, generator(rand))
```

## modal/cli/_download.py

```diff
@@ -1,36 +1,62 @@
 # Copyright Modal Labs 2023
 import asyncio
 import os
 import shutil
 import sys
 from pathlib import Path
-from typing import Optional, Tuple, Union
+from typing import Callable, Optional, Tuple, Union, overload
 
 from click import UsageError
 
 from modal.network_file_system import _NetworkFileSystem
-from modal.volume import FileEntry, FileEntryType, _Volume
+from modal.volume import _Volume
+from modal_proto import api_pb2
+
+_Entry = Union[api_pb2.SharedVolumeListFilesEntry, api_pb2.VolumeListFilesEntry]
+
+
+@overload
+def _glob_download(
+    volume: _Volume,
+    is_file_fn: Callable[[api_pb2.VolumeListFilesEntry], bool],
+    remote_glob_path: str,
+    local_destination: Path,
+    overwrite: bool,
+):
+    ...
+
+
+@overload
+def _glob_download(
+    volume: _NetworkFileSystem,
+    is_file_fn: Callable[[api_pb2.SharedVolumeListFilesEntry], bool],
+    remote_glob_path: str,
+    local_destination: Path,
+    overwrite: bool,
+):
+    ...
 
 
 async def _glob_download(
-    volume: Union[_NetworkFileSystem, _Volume],
+    volume,
+    is_file_fn,
     remote_glob_path: str,
     local_destination: Path,
     overwrite: bool,
 ):
-    q: asyncio.Queue[Tuple[Optional[Path], Optional[FileEntry]]] = asyncio.Queue()
+    q: asyncio.Queue[Tuple[Optional[Path], Optional[_Entry]]] = asyncio.Queue()
     num_consumers = 10  # concurrency limit
 
     async def producer():
         async for entry in volume.iterdir(remote_glob_path):
             output_path = local_destination / entry.path
             if output_path.exists():
                 if overwrite:
-                    if output_path.is_file():
+                    if is_file_fn(entry):
                         os.remove(output_path)
                     else:
                         shutil.rmtree(output_path)
                 else:
                     raise UsageError(
                         f"Output path '{output_path}' already exists. Use --force to overwrite the output directory"
                     )
@@ -41,21 +67,20 @@
 
     async def consumer():
         while True:
             output_path, entry = await q.get()
             if output_path is None:
                 return
             try:
-                if entry.type == FileEntryType.FILE:
+                if is_file_fn(entry):
                     output_path.parent.mkdir(parents=True, exist_ok=True)
                     with output_path.open("wb") as fp:
                         b = 0
                         async for chunk in volume.read_file(entry.path):
                             b += fp.write(chunk)
+
                     print(f"Wrote {b} bytes to {output_path}", file=sys.stderr)
-                elif entry.type == FileEntryType.DIRECTORY:
-                    output_path.mkdir(parents=True, exist_ok=True)
             finally:
                 q.task_done()
 
     consumers = [consumer() for _ in range(num_consumers)]
     await asyncio.gather(producer(), *consumers)
```

## modal/cli/app.py

```diff
@@ -5,15 +5,15 @@
 import typer
 from click import UsageError
 from grpclib import GRPCError, Status
 from rich.text import Text
 
 from modal._output import OutputManager, get_app_logs_loop
 from modal._utils.async_utils import synchronizer
-from modal.app_utils import _list_apps
+from modal.app import _list_apps
 from modal.cli.utils import ENV_OPTION, display_table, timestamp_to_local
 from modal.client import _Client
 from modal.environments import ensure_env
 from modal_proto import api_pb2
 
 app_cli = typer.Typer(name="app", help="Manage deployed and running apps.", no_args_is_help=True)
 
@@ -28,19 +28,20 @@
 }
 
 
 @app_cli.command("list")
 @synchronizer.create_blocking
 async def list(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
     """List all running or recently running Modal apps for the current account"""
+    client = await _Client.from_env()
     env = ensure_env(env)
 
     column_names = ["App ID", "Name", "State", "Creation time", "Stop time"]
     rows: List[List[Union[Text, str]]] = []
-    apps: List[api_pb2.AppStats] = await _list_apps(env)
+    apps = await _list_apps(env=env, client=client)
     for app_stats in apps:
         state = APP_STATE_TO_MESSAGE.get(app_stats.state, Text("unknown", style="gray"))
 
         rows.append(
             [
                 app_stats.app_id,
                 app_stats.description,
```

## modal/cli/import_refs.py

```diff
@@ -15,17 +15,17 @@
 from typing import Any, Optional, Union
 
 import click
 from rich.console import Console
 from rich.markdown import Markdown
 
 import modal
-from modal.app import App, LocalEntrypoint
 from modal.exception import _CliUserExecutionError
 from modal.functions import Function
+from modal.stub import LocalEntrypoint, Stub
 
 
 @dataclasses.dataclass
 class ImportRef:
     file_or_module: str
     object_path: Optional[str]
 
@@ -37,16 +37,15 @@
         raise modal.exception.InvalidError(f"Invalid object reference: {object_ref}. Did you mean '::' instead of ':'?")
     else:
         file_or_module, object_path = object_ref, None
 
     return ImportRef(file_or_module, object_path)
 
 
-DEFAULT_APP_NAME = "stub"
-POSSIBLE_APP_NAMES = ["stub", "app"]
+DEFAULT_STUB_NAME = "stub"
 
 
 def import_file_or_module(file_or_module: str):
     if "" not in sys.path:
         # When running from a CLI like `modal run`
         # the current working directory isn't added to sys.path
         # so we add it in order to make module path specification possible
@@ -71,25 +70,25 @@
             module = importlib.import_module(file_or_module)
         except Exception as exc:
             raise _CliUserExecutionError(file_or_module) from exc
 
     return module
 
 
-def get_by_object_path(obj: Any, obj_path: Optional[str]) -> Optional[Any]:
+def get_by_object_path(obj: Any, obj_path: str) -> Optional[Any]:
     # Try to evaluate a `.`-delimited object path in a Modal context
     # With the caveat that some object names can actually have `.` in their name (lifecycled methods' tags)
 
     # Note: this is eager, so no backtracking is performed in case an
     # earlier match fails at some later point in the path expansion
     prefix = ""
     for segment in obj_path.split("."):
         attr = prefix + segment
         try:
-            if isinstance(obj, App):
+            if isinstance(obj, Stub):
                 if attr in obj.registered_entrypoints:
                     # local entrypoints are not on stub blueprint
                     obj = obj.registered_entrypoints[attr]
                     continue
             obj = getattr(obj, attr)
 
         except Exception:
@@ -99,116 +98,103 @@
 
     if prefix:
         return None
 
     return obj
 
 
-def get_by_object_path_try_possible_app_names(obj: Any, obj_path: Optional[str]) -> Optional[Any]:
-    """This just exists as a dumb workaround to support both "stub" and "app" """
-
-    if obj_path:
-        return get_by_object_path(obj, obj_path)
-    else:
-        for obj_path in POSSIBLE_APP_NAMES:
-            app = get_by_object_path(obj, obj_path)
-            if app is not None:
-                return app
-        else:
-            return None
-
-
 def _infer_function_or_help(
-    app: App, module, accept_local_entrypoint: bool, accept_webhook: bool
+    stub: Stub, module, accept_local_entrypoint: bool, accept_webhook: bool
 ) -> Union[Function, LocalEntrypoint]:
-    function_choices = set(app.registered_functions.keys())
+    function_choices = set(stub.registered_functions.keys())
     if not accept_webhook:
-        function_choices -= set(app.registered_web_endpoints)
+        function_choices -= set(stub.registered_web_endpoints)
     if accept_local_entrypoint:
-        function_choices |= set(app.registered_entrypoints.keys())
+        function_choices |= set(stub.registered_entrypoints.keys())
 
     sorted_function_choices = sorted(function_choices)
     registered_functions_str = "\n".join(sorted_function_choices)
     filtered_local_entrypoints = [
         name
-        for name, entrypoint in app.registered_entrypoints.items()
+        for name, entrypoint in stub.registered_entrypoints.items()
         if entrypoint.info.module_name == module.__name__
     ]
 
     if accept_local_entrypoint and len(filtered_local_entrypoints) == 1:
         # If there is just a single local entrypoint in the target module, use
         # that regardless of other functions.
         function_name = list(filtered_local_entrypoints)[0]
-    elif accept_local_entrypoint and len(app.registered_entrypoints) == 1:
+    elif accept_local_entrypoint and len(stub.registered_entrypoints) == 1:
         # Otherwise, if there is just a single local entrypoint in the stub as a whole,
         # use that one.
-        function_name = list(app.registered_entrypoints.keys())[0]
+        function_name = list(stub.registered_entrypoints.keys())[0]
     elif len(function_choices) == 1:
         function_name = sorted_function_choices[0]
     elif len(function_choices) == 0:
-        if app.registered_web_endpoints:
+        if stub.registered_web_endpoints:
             err_msg = "Modal stub has only web endpoints. Use `modal serve` instead of `modal run`."
         else:
             err_msg = "Modal stub has no registered functions. Nothing to run."
         raise click.UsageError(err_msg)
     else:
         help_text = f"""You need to specify a Modal function or local entrypoint to run, e.g.
 
 modal run app.py::my_function [...args]
 
 Registered functions and local entrypoints on the selected stub are:
 {registered_functions_str}
 """
         raise click.UsageError(help_text)
 
-    if function_name in app.registered_entrypoints:
+    if function_name in stub.registered_entrypoints:
         # entrypoint is in entrypoint registry, for now
-        return app.registered_entrypoints[function_name]
+        return stub.registered_entrypoints[function_name]
 
-    function = app.indexed_objects[function_name]  # functions are in blueprint
+    function = stub.indexed_objects[function_name]  # functions are in blueprint
     assert isinstance(function, Function)
     return function
 
 
-def _show_no_auto_detectable_app(app_ref: ImportRef) -> None:
-    object_path = app_ref.object_path
-    import_path = app_ref.file_or_module
+def _show_no_auto_detectable_stub(stub_ref: ImportRef) -> None:
+    object_path = stub_ref.object_path
+    import_path = stub_ref.file_or_module
     error_console = Console(stderr=True)
     error_console.print(f"[bold red]Could not find Modal stub '{object_path}' in {import_path}.[/bold red]")
 
     if object_path is None:
         guidance_msg = (
-            f"Expected to find a stub variable named **`{DEFAULT_APP_NAME}`** (the default stub name). If your `modal.Stub` is named differently, "
+            f"Expected to find a stub variable named **`{DEFAULT_STUB_NAME}`** (the default stub name). If your `modal.Stub` is named differently, "
             "you must specify it in the stub ref argument. "
             f"For example a stub variable `app_stub = modal.Stub()` in `{import_path}` would "
             f"be specified as `{import_path}::app_stub`."
         )
         md = Markdown(guidance_msg)
         error_console.print(md)
 
 
-def import_app(app_ref: str) -> App:
-    import_ref = parse_import_ref(app_ref)
+def import_stub(stub_ref: str) -> Stub:
+    import_ref = parse_import_ref(stub_ref)
 
     module = import_file_or_module(import_ref.file_or_module)
-    app = get_by_object_path_try_possible_app_names(module, import_ref.object_path)
+    obj_path = import_ref.object_path or DEFAULT_STUB_NAME  # get variable named "stub" by default
+    stub = get_by_object_path(module, obj_path)
 
-    if app is None:
-        _show_no_auto_detectable_app(import_ref)
+    if stub is None:
+        _show_no_auto_detectable_stub(import_ref)
         sys.exit(1)
 
-    if not isinstance(app, App):
-        raise click.UsageError(f"{app} is not a Modal Stub")
+    if not isinstance(stub, Stub):
+        raise click.UsageError(f"{stub} is not a Modal Stub")
 
-    return app
+    return stub
 
 
-def _show_function_ref_help(app_ref: ImportRef, base_cmd: str) -> None:
-    object_path = app_ref.object_path
-    import_path = app_ref.file_or_module
+def _show_function_ref_help(stub_ref: ImportRef, base_cmd: str) -> None:
+    object_path = stub_ref.object_path
+    import_path = stub_ref.file_or_module
     error_console = Console(stderr=True)
     if object_path:
         error_console.print(
             f"[bold red]Could not find Modal function or local entrypoint '{object_path}' in '{import_path}'.[/bold red]"
         )
     else:
         error_console.print(
@@ -232,33 +218,29 @@
 
 def import_function(
     func_ref: str, base_cmd: str, accept_local_entrypoint=True, accept_webhook=False
 ) -> Union[Function, LocalEntrypoint]:
     import_ref = parse_import_ref(func_ref)
 
     module = import_file_or_module(import_ref.file_or_module)
-    app_or_function = get_by_object_path_try_possible_app_names(module, import_ref.object_path)
+    obj_path = import_ref.object_path or DEFAULT_STUB_NAME  # get variable named "stub" by default
+    stub_or_function = get_by_object_path(module, obj_path)
 
-    if app_or_function is None:
+    if stub_or_function is None:
         _show_function_ref_help(import_ref, base_cmd)
         sys.exit(1)
 
-    if isinstance(app_or_function, App):
+    if isinstance(stub_or_function, Stub):
         # infer function or display help for how to select one
-        app = app_or_function
-        function_handle = _infer_function_or_help(app, module, accept_local_entrypoint, accept_webhook)
+        stub = stub_or_function
+        function_handle = _infer_function_or_help(stub, module, accept_local_entrypoint, accept_webhook)
         return function_handle
-    elif isinstance(app_or_function, Function):
-        return app_or_function
-    elif isinstance(app_or_function, LocalEntrypoint):
+    elif isinstance(stub_or_function, Function):
+        return stub_or_function
+    elif isinstance(stub_or_function, LocalEntrypoint):
         if not accept_local_entrypoint:
             raise click.UsageError(
                 f"{func_ref} is not a Modal Function (a Modal local_entrypoint can't be used in this context)"
             )
-        return app_or_function
+        return stub_or_function
     else:
-        raise click.UsageError(f"{app_or_function} is not a Modal entity (should be a App or Function)")
-
-
-# For backwards compatibility - delete soon
-# We use it in our internal intergration tests
-import_stub = import_app
+        raise click.UsageError(f"{stub_or_function} is not a Modal entity (should be a Stub or Function)")
```

## modal/cli/launch.py

```diff
@@ -1,52 +1,36 @@
 # Copyright Modal Labs 2023
-import asyncio
-import inspect
-import json
-import os
+import subprocess
+import tempfile
 from pathlib import Path
-from typing import Any, Dict, Optional
+from typing import Optional
 
 from typer import Typer
 
-from ..app import App
-from ..exception import _CliUserExecutionError
-from ..runner import run_app
-from .import_refs import import_function
-
 launch_cli = Typer(
     name="launch",
     no_args_is_help=True,
     help="""
     [Preview] Open a serverless app instance on Modal.
 
     This command is in preview and may change in the future.
     """,
 )
 
 
-def _launch_program(name: str, filename: str, args: Dict[str, Any]) -> None:
-    os.environ["MODAL_LAUNCH_LOCAL_ARGS"] = json.dumps(args)
-
-    program_path = str(Path(__file__).parent / "programs" / filename)
-    entrypoint = import_function(program_path, "modal launch")
-    app: App = entrypoint.stub
-    app.set_description(f"modal launch {name}")
-
-    # `launch/` scripts must have a `local_entrypoint()` with no args, for simplicity here.
-    func = entrypoint.info.raw_f
-    isasync = inspect.iscoroutinefunction(func)
-    with run_app(app):
-        try:
-            if isasync:
-                asyncio.run(func())
-            else:
-                func()
-        except Exception as exc:
-            raise _CliUserExecutionError(inspect.getsourcefile(func)) from exc
+def _launch_program(name: str, args) -> None:
+    contents = (Path(__file__).parent / "programs" / name).read_text()
+    contents = contents.replace("args: Dict[str, Any] = {}", f"args: Any = {repr(args)}")
+
+    # TODO: This is a big hack and can break for unexpected $PATH reasons. Make an actual code path
+    # for correctly setting up and running a program in the CLI.
+    with tempfile.TemporaryDirectory() as tmpdir:
+        f = Path(tmpdir) / name
+        f.write_text(contents)
+        subprocess.run(["modal", "run", f])
 
 
 @launch_cli.command(name="jupyter", help="Start Jupyter Lab on Modal.")
 def jupyter(
     cpu: int = 8,
     memory: int = 32768,
     gpu: Optional[str] = None,
@@ -58,15 +42,15 @@
         "cpu": cpu,
         "memory": memory,
         "gpu": gpu,
         "timeout": timeout,
         "image": image,
         "add_python": add_python,
     }
-    _launch_program("jupyter", "run_jupyter.py", args)
+    _launch_program("run_jupyter.py", args)
 
 
 @launch_cli.command(name="vscode", help="Start Visual Studio Code on Modal.")
 def vscode(
     cpu: int = 8,
     memory: int = 32768,
     gpu: Optional[str] = None,
@@ -74,8 +58,8 @@
 ):
     args = {
         "cpu": cpu,
         "memory": memory,
         "gpu": gpu,
         "timeout": timeout,
     }
-    _launch_program("vscode", "vscode.py", args)
+    _launch_program("vscode.py", args)
```

## modal/cli/network_file_system.py

```diff
@@ -25,14 +25,17 @@
 from modal.cli._download import _glob_download
 from modal.cli.utils import ENV_OPTION, display_table
 from modal.client import _Client
 from modal.environments import ensure_env
 from modal.network_file_system import _NetworkFileSystem
 from modal_proto import api_pb2
 
+FileType = api_pb2.SharedVolumeListFilesEntry.FileType
+
+
 nfs_cli = Typer(name="nfs", help="Read and edit `modal.NetworkFileSystem` file systems.", no_args_is_help=True)
 
 
 @nfs_cli.command(name="list", help="List the names of all network file systems.")
 @synchronizer.create_blocking
 async def list(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
     env = ensure_env(env)
@@ -107,15 +110,15 @@
         console.print(f"Directory listing of '{path}' in '{volume_name}'")
         table = Table()
 
         table.add_column("filename")
         table.add_column("type")
 
         for entry in entries:
-            filetype = "dir" if entry.type == api_pb2.FileEntry.FileType.DIRECTORY else "file"
+            filetype = "dir" if entry.type == FileType.DIRECTORY else "file"
             table.add_row(entry.path, filetype)
         console.print(table)
     else:
         for entry in entries:
             print(entry.path)
 
 
@@ -188,16 +191,25 @@
 
     Use "-" (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).
     """
     ensure_env(env)
     destination = Path(local_destination)
     volume = await _volume_from_name(volume_name)
 
+    def is_file_fn(entry):
+        return entry.type == FileType.FILE
+
     if "*" in remote_path:
-        await _glob_download(volume, remote_path, destination, force)
+        await _glob_download(
+            volume,
+            is_file_fn,
+            remote_path,
+            destination,
+            force,
+        )
         return
 
     if destination != PIPE_PATH:
         if destination.is_dir():
             destination = destination / remote_path.rsplit("/")[-1]
 
         if destination.exists() and not force:
```

## modal/cli/run.py

```diff
@@ -9,23 +9,23 @@
 from typing import Any, Callable, Dict, Optional, get_type_hints
 
 import click
 import typer
 from rich.console import Console
 from typing_extensions import TypedDict
 
-from ..app import App, LocalEntrypoint
 from ..config import config
 from ..environments import ensure_env
 from ..exception import ExecutionError, InvalidError, _CliUserExecutionError
-from ..functions import Function, _FunctionSpec
+from ..functions import Function, FunctionEnv
 from ..image import Image
-from ..runner import deploy_app, interactive_shell, run_app
-from ..serving import serve_app
-from .import_refs import import_app, import_function
+from ..runner import deploy_stub, interactive_shell, run_stub
+from ..serving import serve_stub
+from ..stub import LocalEntrypoint, Stub
+from .import_refs import import_function, import_stub
 from .utils import ENV_OPTION, ENV_OPTION_HELP
 
 
 class ParameterMetadata(TypedDict):
     name: str
     default: Any
     annotation: Any
@@ -114,26 +114,26 @@
         else:
             kwargs["required"] = True
 
         click.option(cli_name, **kwargs)(func)
     return func
 
 
-def _get_clean_app_description(func_ref: str) -> str:
+def _get_clean_stub_description(func_ref: str) -> str:
     # If possible, consider the 'ref' argument the start of the app's args. Everything
     # before it Modal CLI cruft (eg. `modal run --detach`).
     try:
         func_ref_arg_idx = sys.argv.index(func_ref)
         return " ".join(sys.argv[func_ref_arg_idx:])
     except ValueError:
         return " ".join(sys.argv)
 
 
-def _get_click_command_for_function(app: App, function_tag):
-    function = app.indexed_objects[function_tag]
+def _get_click_command_for_function(stub: Stub, function_tag):
+    function = stub.indexed_objects[function_tag]
     assert isinstance(function, Function)
 
     if function.is_generator:
         raise InvalidError("`modal run` is not supported for generator functions")
 
     signature: Dict[str, ParameterMetadata]
     if function.info.cls is not None:
@@ -142,16 +142,16 @@
         signature = dict(**cls_signature, **fun_signature)  # Pool all arguments
         # TODO(erikbern): assert there's no overlap?
     else:
         signature = _get_signature(function.info.raw_f)
 
     @click.pass_context
     def f(ctx, **kwargs):
-        with run_app(
-            app,
+        with run_stub(
+            stub,
             detach=ctx.obj["detach"],
             show_progress=ctx.obj["show_progress"],
             environment_name=ctx.obj["env"],
             interactive=ctx.obj["interactive"],
         ):
             if function.info.cls is None:
                 function.remote(**kwargs)
@@ -163,27 +163,27 @@
                 method = function.from_parametrized(None, False, None, tuple(), cls_kwargs)
                 method.remote(**fun_kwargs)
 
     with_click_options = _add_click_options(f, signature)
     return click.command(with_click_options)
 
 
-def _get_click_command_for_local_entrypoint(app: App, entrypoint: LocalEntrypoint):
+def _get_click_command_for_local_entrypoint(stub: Stub, entrypoint: LocalEntrypoint):
     func = entrypoint.info.raw_f
     isasync = inspect.iscoroutinefunction(func)
 
     @click.pass_context
     def f(ctx, *args, **kwargs):
         if ctx.obj["detach"]:
             print(
                 "Note that running a local entrypoint in detached mode only keeps the last triggered Modal function alive after the parent process has been killed or disconnected."
             )
 
-        with run_app(
-            app,
+        with run_stub(
+            stub,
             detach=ctx.obj["detach"],
             show_progress=ctx.obj["show_progress"],
             environment_name=ctx.obj["env"],
             interactive=ctx.obj["interactive"],
         ):
             try:
                 if isasync:
@@ -201,22 +201,22 @@
     def get_command(self, ctx, func_ref):
         # note: get_command here is run before the "group logic" in the `run` logic below
         # so to ensure that `env` has been globally populated before user code is loaded, it
         # needs to be handled here, and not in the `run` logic below
         ctx.ensure_object(dict)
         ctx.obj["env"] = ensure_env(ctx.params["env"])
         function_or_entrypoint = import_function(func_ref, accept_local_entrypoint=True, base_cmd="modal run")
-        app: App = function_or_entrypoint.stub
-        if app.description is None:
-            app.set_description(_get_clean_app_description(func_ref))
+        stub: Stub = function_or_entrypoint.stub
+        if stub.description is None:
+            stub.set_description(_get_clean_stub_description(func_ref))
         if isinstance(function_or_entrypoint, LocalEntrypoint):
-            click_command = _get_click_command_for_local_entrypoint(app, function_or_entrypoint)
+            click_command = _get_click_command_for_local_entrypoint(stub, function_or_entrypoint)
         else:
             tag = function_or_entrypoint.info.get_tag()
-            click_command = _get_click_command_for_function(app, tag)
+            click_command = _get_click_command_for_function(stub, tag)
 
         return click_command
 
 
 @click.group(
     cls=RunGroup,
     subcommand_metavar="FUNC_REF",
@@ -259,61 +259,61 @@
     ctx.ensure_object(dict)
     ctx.obj["detach"] = detach  # if subcommand would be a click command...
     ctx.obj["show_progress"] = False if quiet else True
     ctx.obj["interactive"] = interactive
 
 
 def deploy(
-    app_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
+    stub_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
     name: str = typer.Option(None, help="Name of the deployment."),
     env: str = ENV_OPTION,
     public: bool = typer.Option(
         False, help="[beta] Publicize the deployment so other workspaces can lookup the function."
     ),
     skip_confirm: bool = typer.Option(False, help="Skip public app confirmation dialog."),
 ):
     # this ensures that `modal.lookup()` without environment specification uses the same env as specified
     env = ensure_env(env)
 
-    app = import_app(app_ref)
+    stub = import_stub(stub_ref)
 
     if name is None:
-        name = app.name
+        name = stub.name
 
     if public and not skip_confirm:
         if not click.confirm(
             "⚠️ Public apps are a beta feature. ⚠️\n"
             "Making an app public will allow any user (including from outside your workspace) to look up and use your functions.\n"
             "Are you sure you want your app to be public?"
         ):
             return
 
-    deploy_app(app, name=name, environment_name=env, public=public)
+    deploy_stub(stub, name=name, environment_name=env, public=public)
 
 
 def serve(
-    app_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
+    stub_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
     timeout: Optional[float] = None,
     env: str = ENV_OPTION,
 ):
     """Run a web endpoint(s) associated with a Modal stub and hot-reload code.
 
     **Examples:**
 
     ```bash
     modal serve hello_world.py
     ```
     """
     env = ensure_env(env)
 
-    app = import_app(app_ref)
-    if app.description is None:
-        app.set_description(_get_clean_app_description(app_ref))
+    stub = import_stub(stub_ref)
+    if stub.description is None:
+        stub.set_description(_get_clean_stub_description(stub_ref))
 
-    with serve_app(app, app_ref, environment_name=env):
+    with serve_stub(stub, stub_ref, environment_name=env):
         if timeout is None:
             timeout = config["serve_timeout"]
         if timeout is None:
             timeout = float("inf")
         while timeout > 0:
             t = min(timeout, 3600)
             time.sleep(t)
@@ -371,31 +371,31 @@
     """
     env = ensure_env(env)
 
     console = Console()
     if not console.is_terminal:
         raise click.UsageError("`modal shell` can only be run from a terminal.")
 
-    app = App("modal shell")
+    stub = Stub("modal shell")
 
     if func_ref is not None:
         function = import_function(func_ref, accept_local_entrypoint=False, accept_webhook=True, base_cmd="modal shell")
         assert isinstance(function, Function)
-        function_spec: _FunctionSpec = function.spec
+        function_env: FunctionEnv = function.env
         start_shell = partial(
             interactive_shell,
-            image=function_spec.image,
-            mounts=function_spec.mounts,
-            secrets=function_spec.secrets,
-            network_file_systems=function_spec.network_file_systems,
-            gpu=function_spec.gpu,
-            cloud=function_spec.cloud,
-            cpu=function_spec.cpu,
-            memory=function_spec.memory,
-            volumes=function_spec.volumes,
+            image=function_env.image,
+            mounts=function_env.mounts,
+            secrets=function_env.secrets,
+            network_file_systems=function_env.network_file_systems,
+            gpu=function_env.gpu,
+            cloud=function_env.cloud,
+            cpu=function_env.cpu,
+            memory=function_env.memory,
+            volumes=function_env.volumes,
             _allow_background_volume_commits=True,
         )
     else:
         modal_image = Image.from_registry(image, add_python=add_python) if image else None
         start_shell = partial(interactive_shell, image=modal_image, cpu=cpu, memory=memory, gpu=gpu, cloud=cloud)
 
-    start_shell(app, cmd=[cmd], environment_name=env, timeout=3600)
+    start_shell(stub, cmd=[cmd], environment_name=env, timeout=3600)
```

## modal/cli/volume.py

```diff
@@ -24,14 +24,15 @@
 from modal.cli._download import _glob_download
 from modal.cli.utils import ENV_OPTION, display_table
 from modal.client import _Client
 from modal.environments import ensure_env
 from modal.volume import _Volume, _VolumeUploadContextManager
 from modal_proto import api_pb2
 
+FileType = api_pb2.VolumeListFilesEntry.FileType
 PIPE_PATH = Path("-")
 
 volume_cli = Typer(
     name="volume",
     no_args_is_help=True,
     help="""
     Read and edit `modal.Volume` volumes.
@@ -99,16 +100,19 @@
 
     Use "-" (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).
     """
     ensure_env(env)
     destination = Path(local_destination)
     volume = await _Volume.lookup(volume_name, environment_name=env)
 
+    def is_file_fn(entry):
+        return entry.type == FileType.FILE
+
     if "*" in remote_path:
-        await _glob_download(volume, remote_path, destination, force)
+        await _glob_download(volume, is_file_fn, remote_path, destination, force)
         return
 
     if destination != PIPE_PATH:
         if destination.is_dir():
             destination = destination / remote_path.rsplit("/")[-1]
 
         if destination.exists() and not force:
@@ -178,17 +182,17 @@
         console.print(f"Directory listing of '{path}' in '{volume_name}'")
         table = Table()
         for name in ["filename", "type", "created/modified", "size"]:
             table.add_column(name)
 
         locale_tz = datetime.now().astimezone().tzinfo
         for entry in entries:
-            if entry.type == api_pb2.FileEntry.FileType.DIRECTORY:
+            if entry.type == FileType.DIRECTORY:
                 filetype = "dir"
-            elif entry.type == api_pb2.FileEntry.FileType.SYMLINK:
+            elif entry.type == FileType.SYMLINK:
                 filetype = "link"
             else:
                 filetype = "file"
             table.add_row(
                 entry.path,
                 filetype,
                 str(datetime.fromtimestamp(entry.mtime, tz=locale_tz)),
```

## modal/cli/programs/run_jupyter.py

```diff
@@ -1,38 +1,35 @@
 # Copyright Modal Labs 2023
 # type: ignore
-import json
 import os
 import secrets
 import socket
 import subprocess
 import threading
 import time
 import webbrowser
 from typing import Any, Dict
 
 from modal import Image, Queue, Stub, forward
 
-# Passed by `modal launch` locally via CLI, empty on remote runner.
-args: Dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_LOCAL_ARGS", "{}"))
-
+args: Dict[str, Any] = {}
 
 stub = Stub()
 stub.image = Image.from_registry(args.get("image"), add_python=args.get("add_python")).pip_install("jupyterlab")
 
 
 def wait_for_port(url: str, q: Queue):
     start_time = time.monotonic()
     while True:
         try:
-            with socket.create_connection(("localhost", 8888), timeout=30.0):
+            with socket.create_connection(("localhost", 8888), timeout=15.0):
                 break
         except OSError as exc:
             time.sleep(0.01)
-            if time.monotonic() - start_time >= 30.0:
+            if time.monotonic() - start_time >= 15.0:
                 raise TimeoutError("Waited too long for port 8888 to accept connections") from exc
     q.put(url)
 
 
 @stub.function(cpu=args.get("cpu"), memory=args.get("memory"), gpu=args.get("gpu"), timeout=args.get("timeout"))
 def run_jupyter(q: Queue):
     os.mkdir("/lab")
@@ -57,14 +54,14 @@
         )
     q.put("done")
 
 
 @stub.local_entrypoint()
 def main():
     with Queue.ephemeral() as q:
-        run_jupyter.spawn(q)
+        stub.run_jupyter.spawn(q)
         url = q.get()
         time.sleep(1)  # Give Jupyter a chance to start up
         print("\nJupyter on Modal, opening in browser...")
         print(f"   -> {url}\n")
         webbrowser.open(url)
         assert q.get() == "done"
```

## modal/cli/programs/vscode.py

```diff
@@ -1,38 +1,35 @@
 # Copyright Modal Labs 2023
 # type: ignore
-import json
 import os
 import secrets
 import socket
 import subprocess
 import threading
 import time
 import webbrowser
 from typing import Any, Dict, Tuple
 
 from modal import Image, Queue, Stub, forward
 
-# Passed by `modal launch` locally via CLI, empty on remote runner.
-args: Dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_LOCAL_ARGS", "{}"))
-
+args: Dict[str, Any] = {}
 
 stub = Stub()
 stub.image = Image.from_registry("codercom/code-server", add_python="3.11").dockerfile_commands("ENTRYPOINT []")
 
 
 def wait_for_port(data: Tuple[str, str], q: Queue):
     start_time = time.monotonic()
     while True:
         try:
-            with socket.create_connection(("localhost", 8080), timeout=30.0):
+            with socket.create_connection(("localhost", 8080), timeout=15.0):
                 break
         except OSError as exc:
             time.sleep(0.01)
-            if time.monotonic() - start_time >= 30.0:
+            if time.monotonic() - start_time >= 15.0:
                 raise TimeoutError("Waited too long for port 8080 to accept connections") from exc
     q.put(data)
 
 
 @stub.function(cpu=args.get("cpu"), memory=args.get("memory"), gpu=args.get("gpu"), timeout=args.get("timeout"))
 def run_vscode(q: Queue):
     os.chdir("/home/coder")
@@ -46,15 +43,15 @@
         )
     q.put("done")
 
 
 @stub.local_entrypoint()
 def main():
     with Queue.ephemeral() as q:
-        run_vscode.spawn(q)
+        stub.run_vscode.spawn(q)
         url, token = q.get()
-        time.sleep(1)  # Give VS Code a chance to start up
+        time.sleep(1)  # Give Jupyter a chance to start up
         print("\nVS Code on Modal, opening in browser...")
         print(f"   -> {url}")
         print(f"   -> password: {token}\n")
         webbrowser.open(url)
         assert q.get() == "done"
```

## modal_docs/mdmd/signatures.py

```diff
@@ -1,11 +1,10 @@
 # Copyright Modal Labs 2023
 import ast
 import inspect
-import re
 import textwrap
 import warnings
 from typing import Tuple
 
 from synchronicity.synchronizer import FunctionWithAio
 
 
@@ -67,11 +66,8 @@
     ):
         # hack to "reset" signature to a blocking one if the underlying source definition is async
         # but the wrapper function isn't (like when synchronicity wraps an async function as a blocking one)
         definition_source = definition_source.replace("async def", "def")
         definition_source = definition_source.replace("asynccontextmanager", "contextmanager")
         definition_source = definition_source.replace("AsyncIterator", "Iterator")
 
-    # remove any synchronicity-internal decorators
-    definition_source, _ = re.subn(r"^\s*@synchronizer\..*\n", "", definition_source)
-
     return definition_source
```

## modal_proto/api.proto

```diff
@@ -80,42 +80,38 @@
   CLIENT_TYPE_WORKER = 2 [deprecated=true];
   CLIENT_TYPE_CONTAINER = 3;
   CLIENT_TYPE_SERVER = 4 [deprecated=true];
   CLIENT_TYPE_WEB_SERVER = 5;
 }
 
 message CloudBucketMount {
-  enum BucketType {
-    UNSPECIFIED = 0;
-    S3 = 1;
-    R2 = 2;
-    GCP = 3;
-  }
-
   string bucket_name = 1;
   string mount_path = 2;
   string credentials_secret_id = 3;
   bool read_only = 4;
-  BucketType bucket_type = 5;
   bool requester_pays = 6;
-  optional string bucket_endpoint_url = 7;
+
+  enum BucketType {
+    UNSPECIFIED = 0;
+    S3 = 1;
+  }
+  BucketType bucket_type = 5;
 }
 
 enum CloudProvider {
   CLOUD_PROVIDER_UNSPECIFIED = 0;
   CLOUD_PROVIDER_AWS = 1;
   CLOUD_PROVIDER_GCP = 2;
   CLOUD_PROVIDER_AUTO = 3;
   CLOUD_PROVIDER_OCI = 4;
-  CLOUD_PROVIDER_LAMBDA_LABS = 5;
 }
 
 // Which data format a binary message is encoded with.
 enum DataFormat {
-  DATA_FORMAT_UNSPECIFIED = 0;
+  DATA_FORMAT_UNSPECIFIED = 0; // Equivalent to PICKLE in client version 0.52 and earlier.
   DATA_FORMAT_PICKLE = 1; // Cloudpickle
   DATA_FORMAT_ASGI = 2; // "Asgi" protobuf message
   DATA_FORMAT_GENERATOR_DONE = 3; // "GeneratorDone" protobuf message
 }
 
 enum DeploymentNamespace {
   DEPLOYMENT_NAMESPACE_UNSPECIFIED = 0;
@@ -132,28 +128,14 @@
 enum FileDescriptor {
   FILE_DESCRIPTOR_UNSPECIFIED = 0;
   FILE_DESCRIPTOR_STDOUT = 1;
   FILE_DESCRIPTOR_STDERR = 2;
   FILE_DESCRIPTOR_INFO = 3;
 }
 
-// A file entry when listing files in a volume or network file system.
-message FileEntry {
-  enum FileType {
-    UNSPECIFIED = 0;
-    FILE = 1;
-    DIRECTORY = 2;
-    SYMLINK = 3;
-  }
-  string path = 1;
-  FileType type = 2;
-  uint64 mtime = 3;
-  uint64 size = 4;
-}
-
 enum FunctionCallType {
   FUNCTION_CALL_TYPE_UNSPECIFIED = 0;
   FUNCTION_CALL_TYPE_UNARY = 1;
   FUNCTION_CALL_TYPE_MAP = 2;
 }
 
 enum GPUType {
@@ -697,22 +679,14 @@
   optional bytes value = 2;
 }
 
 message DictHeartbeatRequest {
   string dict_id = 1;
 }
 
-message DictContentsRequest {
-  string dict_id = 1;
-  // Setting these to True will populate the corresponding field in the response, otherwise it will be null
-  // This lets us support the keys/values/items SDK API through one RPC without unnecessary data transfer
-  bool keys = 2;
-  bool values = 3;
-}
-
 message DictLenRequest {
   string dict_id = 1;
 }
 
 message DictLenResponse {
   int32 len = 1;
 }
@@ -1250,14 +1224,18 @@
   string runtime = 19;
   // Not included in image definition checksum as debug features do not affect built image.
   bool runtime_debug = 20;
 
   BuildFunction build_function = 21;
 }
 
+message ImageBuilderVersionLookupResponse {
+  string version = 1;
+}
+
 message ImageContextFile {
   string filename = 1;
   bytes data = 2;
 }
 
 message ImageGetOrCreateRequest {
   Image image = 2;
@@ -1429,67 +1407,46 @@
   string queue_id = 1;
 }
 
 message QueueGetRequest {
   string queue_id = 1;
   float timeout = 3;
   int32 n_values = 4;
-  bytes partition_key = 5;
 }
 
 message QueueGetResponse {
   repeated bytes values = 2;
 }
 
 message QueueHeartbeatRequest {
   string queue_id = 1;
 }
 
 message QueuePutRequest {
   string queue_id = 1;
   repeated bytes values = 4;
-  bytes partition_key = 5;
-  int32 partition_ttl_seconds = 6;
 }
 
 message QueueLenRequest {
   string queue_id = 1;
-  bytes partition_key = 2;
 }
 
 message QueueLenResponse {
   int32 len = 1;
 }
 
-message QueueNextItemsRequest {
-  string queue_id = 1;
-  bytes partition_key = 2;
-  string last_entry_id = 3;
-  float item_poll_timeout = 4; // seconds
-}
-
-message QueueItem {
-  bytes value = 1;
-  string entry_id = 2;
-}
-
-message QueueNextItemsResponse {
-  repeated QueueItem items = 1;
-}
-
 message RateLimit {
   int32 limit = 1;
   RateLimitInterval interval = 2;
 }
 
 message Resources {
-  uint32 memory_mb = 2; // MiB
-  uint32 milli_cpu = 3; // milli CPU cores
+  uint32 memory_mb = 2;
+  uint32 milli_cpu = 3;
   GPUConfig gpu_config = 4;
-  uint32 memory_mb_max = 5; // MiB
 }
 
 message S3Mount {
   string bucket_name = 1;
   string mount_path = 2;
   string credentials_secret_id = 3;
   bool read_only = 4;
@@ -1727,16 +1684,26 @@
 
 message SharedVolumeRemoveFileRequest {
   string shared_volume_id = 1 [ (modal.options.audit_target_attr) = true ];
   string path = 2;
   bool recursive = 3;
 }
 
+message SharedVolumeListFilesEntry {
+  enum FileType {
+    UNSPECIFIED = 0;
+    FILE = 1;
+    DIRECTORY = 2;
+  }
+  string path = 1;
+  FileType type = 2;
+}
+
 message SharedVolumeListFilesResponse {
-  repeated FileEntry entries = 1;
+  repeated SharedVolumeListFilesEntry entries = 1;
 }
 
 message SharedVolumeMount {
   string mount_path = 1;
   string shared_volume_id = 2;
   CloudProvider cloud_provider = 3;
   bool allow_cross_region = 4;
@@ -1899,22 +1866,35 @@
     string data_blob_id = 2;
   }
   uint64 size = 3; // total file size
   uint64 start = 4; // file position of first byte returned
   uint64 len = 5; // number of bytes returned
 }
 
+message VolumeListFilesEntry {
+  enum FileType {
+    UNSPECIFIED = 0;
+    FILE = 1;
+    DIRECTORY = 2;
+    SYMLINK = 3;
+  }
+  string path = 1;
+  FileType type = 2;
+  uint64 mtime = 3;
+  uint64 size = 4;
+}
+
 message VolumeListFilesRequest {
   string volume_id = 1;
   string path = 2;
   optional int32 max_entries = 3;
 }
 
 message VolumeListFilesResponse {
-  repeated FileEntry entries = 1;
+  repeated VolumeListFilesEntry entries = 1;
 }
 
 message VolumeListItem {
   string label = 1;  // app name of object entity app
   string volume_id = 2;
   double created_at = 3;
 }
@@ -1975,15 +1955,15 @@
 message WebUrlInfo {
   bool truncated = 1;
   bool has_unique_hash = 2;
   bool label_stolen = 3;
 }
 
 message WorkspaceNameLookupResponse {
-  string workspace_name = 1 [deprecated=true];
+  string workspace_name = 1;
   string username = 2;
 }
 
 // Used for `modal container exec`
 message RuntimeOutputMessage {
   // only stdout / stderr is used
   FileDescriptor file_descriptor = 1;
@@ -2041,15 +2021,14 @@
   rpc ContainerCheckpoint(ContainerCheckpointRequest) returns (google.protobuf.Empty);
 
   // Dicts
   rpc DictClear(DictClearRequest) returns (google.protobuf.Empty);
   rpc DictCreate(DictCreateRequest) returns (DictCreateResponse);  // Will be superseded by DictGetOrCreate
   rpc DictGetOrCreate(DictGetOrCreateRequest) returns (DictGetOrCreateResponse);
   rpc DictHeartbeat(DictHeartbeatRequest) returns (google.protobuf.Empty);
-  rpc DictContents(DictContentsRequest) returns (stream DictEntry);
   rpc DictUpdate(DictUpdateRequest) returns (DictUpdateResponse);
   rpc DictGet(DictGetRequest) returns (DictGetResponse);
   rpc DictPop(DictPopRequest) returns (DictPopResponse);
   rpc DictContains(DictContainsRequest) returns (DictContainsResponse);
   rpc DictLen(DictLenRequest) returns (DictLenResponse);
 
   // Domains
@@ -2087,14 +2066,15 @@
 
   // Interactive functions
   rpc FunctionStartPtyShell(google.protobuf.Empty) returns (google.protobuf.Empty);
 
   // Images
   rpc ImageGetOrCreate(ImageGetOrCreateRequest) returns (ImageGetOrCreateResponse);
   rpc ImageJoinStreaming(ImageJoinStreamingRequest) returns (stream ImageJoinStreamingResponse);
+  rpc ImageBuilderVersionLookup(google.protobuf.Empty) returns (ImageBuilderVersionLookupResponse);
 
   // Mounts
   rpc MountPutFile(MountPutFileRequest) returns (MountPutFileResponse);
   rpc MountBuild(MountBuildRequest) returns (MountBuildResponse);
   rpc MountGetOrCreate(MountGetOrCreateRequest) returns (MountGetOrCreateResponse);
 
   // Proxies
@@ -2103,15 +2083,14 @@
   // Queues
   rpc QueueCreate(QueueCreateRequest) returns (QueueCreateResponse);
   rpc QueueGetOrCreate(QueueGetOrCreateRequest) returns (QueueGetOrCreateResponse);
   rpc QueueGet(QueueGetRequest) returns (QueueGetResponse);
   rpc QueueHeartbeat(QueueHeartbeatRequest) returns (google.protobuf.Empty);
   rpc QueuePut(QueuePutRequest) returns (google.protobuf.Empty);
   rpc QueueLen(QueueLenRequest) returns (QueueLenResponse);
-  rpc QueueNextItems(QueueNextItemsRequest) returns (QueueNextItemsResponse);
 
   // Sandboxes
   rpc SandboxCreate(SandboxCreateRequest) returns (SandboxCreateResponse);
   rpc SandboxGetTaskId(SandboxGetTaskIdRequest) returns (SandboxGetTaskIdResponse); // needed for modal container exec
   rpc SandboxGetLogs(SandboxGetLogsRequest) returns (stream TaskLogsBatch);
   rpc SandboxWait(SandboxWaitRequest) returns (SandboxWaitResponse);
   rpc SandboxList(SandboxListRequest) returns (SandboxListResponse);
```

## modal_proto/api_grpc.py

```diff
@@ -126,18 +126,14 @@
         pass
 
     @abc.abstractmethod
     async def DictHeartbeat(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictHeartbeatRequest, google.protobuf.empty_pb2.Empty]') -> None:
         pass
 
     @abc.abstractmethod
-    async def DictContents(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictContentsRequest, modal_proto.api_pb2.DictEntry]') -> None:
-        pass
-
-    @abc.abstractmethod
     async def DictUpdate(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictUpdateRequest, modal_proto.api_pb2.DictUpdateResponse]') -> None:
         pass
 
     @abc.abstractmethod
     async def DictGet(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictGetRequest, modal_proto.api_pb2.DictGetResponse]') -> None:
         pass
 
@@ -262,14 +258,18 @@
         pass
 
     @abc.abstractmethod
     async def ImageJoinStreaming(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.ImageJoinStreamingRequest, modal_proto.api_pb2.ImageJoinStreamingResponse]') -> None:
         pass
 
     @abc.abstractmethod
+    async def ImageBuilderVersionLookup(self, stream: 'grpclib.server.Stream[google.protobuf.empty_pb2.Empty, modal_proto.api_pb2.ImageBuilderVersionLookupResponse]') -> None:
+        pass
+
+    @abc.abstractmethod
     async def MountPutFile(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.MountPutFileRequest, modal_proto.api_pb2.MountPutFileResponse]') -> None:
         pass
 
     @abc.abstractmethod
     async def MountBuild(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.MountBuildRequest, modal_proto.api_pb2.MountBuildResponse]') -> None:
         pass
 
@@ -302,18 +302,14 @@
         pass
 
     @abc.abstractmethod
     async def QueueLen(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.QueueLenRequest, modal_proto.api_pb2.QueueLenResponse]') -> None:
         pass
 
     @abc.abstractmethod
-    async def QueueNextItems(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.QueueNextItemsRequest, modal_proto.api_pb2.QueueNextItemsResponse]') -> None:
-        pass
-
-    @abc.abstractmethod
     async def SandboxCreate(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.SandboxCreateRequest, modal_proto.api_pb2.SandboxCreateResponse]') -> None:
         pass
 
     @abc.abstractmethod
     async def SandboxGetTaskId(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.SandboxGetTaskIdRequest, modal_proto.api_pb2.SandboxGetTaskIdResponse]') -> None:
         pass
 
@@ -631,20 +627,14 @@
             ),
             '/modal.client.ModalClient/DictHeartbeat': grpclib.const.Handler(
                 self.DictHeartbeat,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.DictHeartbeatRequest,
                 google.protobuf.empty_pb2.Empty,
             ),
-            '/modal.client.ModalClient/DictContents': grpclib.const.Handler(
-                self.DictContents,
-                grpclib.const.Cardinality.UNARY_STREAM,
-                modal_proto.api_pb2.DictContentsRequest,
-                modal_proto.api_pb2.DictEntry,
-            ),
             '/modal.client.ModalClient/DictUpdate': grpclib.const.Handler(
                 self.DictUpdate,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.DictUpdateRequest,
                 modal_proto.api_pb2.DictUpdateResponse,
             ),
             '/modal.client.ModalClient/DictGet': grpclib.const.Handler(
@@ -835,14 +825,20 @@
             ),
             '/modal.client.ModalClient/ImageJoinStreaming': grpclib.const.Handler(
                 self.ImageJoinStreaming,
                 grpclib.const.Cardinality.UNARY_STREAM,
                 modal_proto.api_pb2.ImageJoinStreamingRequest,
                 modal_proto.api_pb2.ImageJoinStreamingResponse,
             ),
+            '/modal.client.ModalClient/ImageBuilderVersionLookup': grpclib.const.Handler(
+                self.ImageBuilderVersionLookup,
+                grpclib.const.Cardinality.UNARY_UNARY,
+                google.protobuf.empty_pb2.Empty,
+                modal_proto.api_pb2.ImageBuilderVersionLookupResponse,
+            ),
             '/modal.client.ModalClient/MountPutFile': grpclib.const.Handler(
                 self.MountPutFile,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.MountPutFileRequest,
                 modal_proto.api_pb2.MountPutFileResponse,
             ),
             '/modal.client.ModalClient/MountBuild': grpclib.const.Handler(
@@ -895,20 +891,14 @@
             ),
             '/modal.client.ModalClient/QueueLen': grpclib.const.Handler(
                 self.QueueLen,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.QueueLenRequest,
                 modal_proto.api_pb2.QueueLenResponse,
             ),
-            '/modal.client.ModalClient/QueueNextItems': grpclib.const.Handler(
-                self.QueueNextItems,
-                grpclib.const.Cardinality.UNARY_UNARY,
-                modal_proto.api_pb2.QueueNextItemsRequest,
-                modal_proto.api_pb2.QueueNextItemsResponse,
-            ),
             '/modal.client.ModalClient/SandboxCreate': grpclib.const.Handler(
                 self.SandboxCreate,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.SandboxCreateRequest,
                 modal_proto.api_pb2.SandboxCreateResponse,
             ),
             '/modal.client.ModalClient/SandboxGetTaskId': grpclib.const.Handler(
@@ -1309,20 +1299,14 @@
         )
         self.DictHeartbeat = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/DictHeartbeat',
             modal_proto.api_pb2.DictHeartbeatRequest,
             google.protobuf.empty_pb2.Empty,
         )
-        self.DictContents = grpclib.client.UnaryStreamMethod(
-            channel,
-            '/modal.client.ModalClient/DictContents',
-            modal_proto.api_pb2.DictContentsRequest,
-            modal_proto.api_pb2.DictEntry,
-        )
         self.DictUpdate = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/DictUpdate',
             modal_proto.api_pb2.DictUpdateRequest,
             modal_proto.api_pb2.DictUpdateResponse,
         )
         self.DictGet = grpclib.client.UnaryUnaryMethod(
@@ -1513,14 +1497,20 @@
         )
         self.ImageJoinStreaming = grpclib.client.UnaryStreamMethod(
             channel,
             '/modal.client.ModalClient/ImageJoinStreaming',
             modal_proto.api_pb2.ImageJoinStreamingRequest,
             modal_proto.api_pb2.ImageJoinStreamingResponse,
         )
+        self.ImageBuilderVersionLookup = grpclib.client.UnaryUnaryMethod(
+            channel,
+            '/modal.client.ModalClient/ImageBuilderVersionLookup',
+            google.protobuf.empty_pb2.Empty,
+            modal_proto.api_pb2.ImageBuilderVersionLookupResponse,
+        )
         self.MountPutFile = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/MountPutFile',
             modal_proto.api_pb2.MountPutFileRequest,
             modal_proto.api_pb2.MountPutFileResponse,
         )
         self.MountBuild = grpclib.client.UnaryUnaryMethod(
@@ -1573,20 +1563,14 @@
         )
         self.QueueLen = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/QueueLen',
             modal_proto.api_pb2.QueueLenRequest,
             modal_proto.api_pb2.QueueLenResponse,
         )
-        self.QueueNextItems = grpclib.client.UnaryUnaryMethod(
-            channel,
-            '/modal.client.ModalClient/QueueNextItems',
-            modal_proto.api_pb2.QueueNextItemsRequest,
-            modal_proto.api_pb2.QueueNextItemsResponse,
-        )
         self.SandboxCreate = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/SandboxCreate',
             modal_proto.api_pb2.SandboxCreateRequest,
             modal_proto.api_pb2.SandboxCreateResponse,
         )
         self.SandboxGetTaskId = grpclib.client.UnaryUnaryMethod(
```

## modal_proto/api_pb2.py

```diff
@@ -14,15 +14,15 @@
 
 
 from modal_proto import options_pb2 as modal__proto_dot_options__pb2
 from google.protobuf import empty_pb2 as google_dot_protobuf_dot_empty__pb2
 from google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15modal_proto/api.proto\x12\x0cmodal.client\x1a\x19modal_proto/options.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1egoogle/protobuf/wrappers.proto\"\xb7\x02\n\x10\x43loudBucketMount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\x12>\n\x0b\x62ucket_type\x18\x05 \x01(\x0e\x32).modal.client.CloudBucketMount.BucketType\x12\x16\n\x0erequester_pays\x18\x06 \x01(\x08\x12 \n\x13\x62ucket_endpoint_url\x18\x07 \x01(\tH\x00\x88\x01\x01\"6\n\nBucketType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x06\n\x02S3\x10\x01\x12\x06\n\x02R2\x10\x02\x12\x07\n\x03GCP\x10\x03\x42\x16\n\x14_bucket_endpoint_url\"\xa9\x01\n\tFileEntry\x12\x0c\n\x04path\x18\x01 \x01(\t\x12.\n\x04type\x18\x02 \x01(\x0e\x32 .modal.client.FileEntry.FileType\x12\r\n\x05mtime\x18\x03 \x01(\x04\x12\x0c\n\x04size\x18\x04 \x01(\x04\"A\n\x08\x46ileType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x08\n\x04\x46ILE\x10\x01\x12\r\n\tDIRECTORY\x10\x02\x12\x0b\n\x07SYMLINK\x10\x03\"r\n\x1a\x41ppClientDisconnectRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x31\n\x06reason\x18\x02 \x01(\x0e\x32!.modal.client.AppDisconnectReason\x12\x11\n\texception\x18\x03 \x01(\t\"\xb3\x01\n\x10\x41ppCreateRequest\x12\x17\n\tclient_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x13\n\x0b\x64\x65scription\x18\x02 \x01(\t\x12\x12\n\x06\x64\x65tach\x18\x03 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x0cinitializing\x18\x04 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x10\x65nvironment_name\x18\x05 \x01(\t\x12)\n\tapp_state\x18\x06 \x01(\x0e\x32\x16.modal.client.AppState\"9\n\x11\x41ppCreateResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x14\n\x0c\x61pp_logs_url\x18\x02 \x01(\t\"S\n\x0e\x41ppStopRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12+\n\x06source\x18\x02 \x01(\x0e\x32\x1b.modal.client.AppStopSource\"\xba\x01\n\x10\x41ppDeployRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x15\n\robject_entity\x18\x04 \x01(\t\x12\x35\n\nvisibility\x18\x05 \x01(\x0e\x32!.modal.client.AppDeployVisibility\" \n\x11\x41ppDeployResponse\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x8f\x01\n\x1c\x41ppDeploySingleObjectRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12\x11\n\tobject_id\x18\x04 \x01(\t\"/\n\x1d\x41ppDeploySingleObjectResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"}\n\x1d\x41ppGetByDeploymentNameRequest\x12\x34\n\tnamespace\x18\x01 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"0\n\x1e\x41ppGetByDeploymentNameResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"\xba\x01\n\x11\x41ppGetLogsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x05 \x01(\t\x12\x10\n\x08input_id\x18\x06 \x01(\t\x12\x0f\n\x07task_id\x18\x07 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\"A\n\x14\x41ppGetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11include_unindexed\x18\x02 \x01(\x08\"F\n\x11\x41ppGetObjectsItem\x12\x0b\n\x03tag\x18\x01 \x01(\t\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\"G\n\x15\x41ppGetObjectsResponse\x12.\n\x05items\x18\x02 \x03(\x0b\x32\x1f.modal.client.AppGetObjectsItem\"%\n\x13\x41ppHeartbeatRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"*\n\x0e\x41ppListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"7\n\x0f\x41ppListResponse\x12$\n\x04\x61pps\x18\x01 \x03(\x0b\x32\x16.modal.client.AppStats\"\xb8\x01\n\x16\x41ppLookupObjectRequest\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x10\n\x08\x61pp_name\x18\x03 \x01(\t\x12\x12\n\nobject_tag\x18\x04 \x01(\t\x12\x11\n\tobject_id\x18\x05 \x01(\t\x12\x15\n\robject_entity\x18\x06 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x07 \x01(\t\"O\n\x17\x41ppLookupObjectResponse\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\x12\x0e\n\x06\x61pp_id\x18\x07 \x01(\t\"\xaf\x02\n\x14\x41ppSetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12T\n\x12indexed_object_ids\x18\x02 \x03(\x0b\x32\x38.modal.client.AppSetObjectsRequest.IndexedObjectIdsEntry\x12\x11\n\tclient_id\x18\x03 \x01(\t\x12\x1c\n\x14unindexed_object_ids\x18\x04 \x03(\t\x12-\n\rnew_app_state\x18\x05 \x01(\x0e\x32\x16.modal.client.AppState\x12\x18\n\x10single_object_id\x18\x06 \x01(\t\x1a\x37\n\x15IndexedObjectIdsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\xd1\x01\n\x08\x41ppStats\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64\x65scription\x18\x03 \x01(\t\x12%\n\x05state\x18\x04 \x01(\x0e\x32\x16.modal.client.AppState\x12\x12\n\ncreated_at\x18\x05 \x01(\x01\x12\x12\n\nstopped_at\x18\x06 \x01(\x01\x12\x17\n\x0fn_running_tasks\x18\x08 \x01(\x05\x12\x15\n\robject_entity\x18\t \x01(\t\x12\x0c\n\x04name\x18\n \x01(\t\x12\x13\n\x0b\x64\x65ployed_at\x18\x0b \x01(\x01\"\xa3\x0e\n\x04\x41sgi\x12\'\n\x04http\x18\x01 \x01(\x0b\x32\x17.modal.client.Asgi.HttpH\x00\x12\x36\n\x0chttp_request\x18\x02 \x01(\x0b\x32\x1e.modal.client.Asgi.HttpRequestH\x00\x12\x43\n\x13http_response_start\x18\x03 \x01(\x0b\x32$.modal.client.Asgi.HttpResponseStartH\x00\x12\x41\n\x12http_response_body\x18\x04 \x01(\x0b\x32#.modal.client.Asgi.HttpResponseBodyH\x00\x12I\n\x16http_response_trailers\x18\x05 \x01(\x0b\x32\'.modal.client.Asgi.HttpResponseTrailersH\x00\x12<\n\x0fhttp_disconnect\x18\x06 \x01(\x0b\x32!.modal.client.Asgi.HttpDisconnectH\x00\x12\x31\n\twebsocket\x18\x07 \x01(\x0b\x32\x1c.modal.client.Asgi.WebsocketH\x00\x12@\n\x11websocket_connect\x18\x08 \x01(\x0b\x32#.modal.client.Asgi.WebsocketConnectH\x00\x12>\n\x10websocket_accept\x18\t \x01(\x0b\x32\".modal.client.Asgi.WebsocketAcceptH\x00\x12@\n\x11websocket_receive\x18\n \x01(\x0b\x32#.modal.client.Asgi.WebsocketReceiveH\x00\x12:\n\x0ewebsocket_send\x18\x0b \x01(\x0b\x32 .modal.client.Asgi.WebsocketSendH\x00\x12\x46\n\x14websocket_disconnect\x18\x0c \x01(\x0b\x32&.modal.client.Asgi.WebsocketDisconnectH\x00\x12<\n\x0fwebsocket_close\x18\r \x01(\x0b\x32!.modal.client.Asgi.WebsocketCloseH\x00\x1a\xc5\x01\n\x04Http\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x0e\n\x06scheme\x18\x03 \x01(\t\x12\x0c\n\x04path\x18\x04 \x01(\t\x12\x14\n\x0cquery_string\x18\x05 \x01(\x0c\x12\x0f\n\x07headers\x18\x06 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x07 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x08 \x01(\rH\x01\x88\x01\x01\x42\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a.\n\x0bHttpRequest\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a\x46\n\x11HttpResponseStart\x12\x0e\n\x06status\x18\x01 \x01(\r\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x12\x10\n\x08trailers\x18\x03 \x01(\x08\x1a\x33\n\x10HttpResponseBody\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a>\n\x14HttpResponseTrailers\x12\x0f\n\x07headers\x18\x01 \x03(\x0c\x12\x15\n\rmore_trailers\x18\x02 \x01(\x08\x1a\x10\n\x0eHttpDisconnect\x1a\xd0\x01\n\tWebsocket\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06scheme\x18\x02 \x01(\t\x12\x0c\n\x04path\x18\x03 \x01(\t\x12\x14\n\x0cquery_string\x18\x04 \x01(\x0c\x12\x0f\n\x07headers\x18\x05 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x06 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x07 \x01(\rH\x01\x88\x01\x01\x12\x14\n\x0csubprotocols\x18\x08 \x03(\tB\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a\x12\n\x10WebsocketConnect\x1aL\n\x0fWebsocketAccept\x12\x18\n\x0bsubprotocol\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x42\x0e\n\x0c_subprotocol\x1a>\n\x10WebsocketReceive\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a;\n\rWebsocketSend\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a\x31\n\x13WebsocketDisconnect\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x42\x07\n\x05_code\x1a<\n\x0eWebsocketClose\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x12\x0e\n\x06reason\x18\x02 \x01(\tB\x07\n\x05_codeB\x06\n\x04type\"7\n\tBaseImage\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x12\n\ndocker_tag\x18\x02 \x01(\tJ\x04\x08\x04\x10\x05\"_\n\x11\x42lobCreateRequest\x12\x13\n\x0b\x63ontent_md5\x18\x01 \x01(\t\x12\x1d\n\x15\x63ontent_sha256_base64\x18\x02 \x01(\t\x12\x16\n\x0e\x63ontent_length\x18\x03 \x01(\x03\"\x84\x01\n\x12\x42lobCreateResponse\x12\x0f\n\x07\x62lob_id\x18\x02 \x01(\t\x12\x14\n\nupload_url\x18\x01 \x01(\tH\x00\x12\x32\n\tmultipart\x18\x03 \x01(\x0b\x32\x1d.modal.client.MultiPartUploadH\x00\x42\x13\n\x11upload_type_oneof\"!\n\x0e\x42lobGetRequest\x12\x0f\n\x07\x62lob_id\x18\x01 \x01(\t\"\'\n\x0f\x42lobGetResponse\x12\x14\n\x0c\x64ownload_url\x18\x01 \x01(\t\"i\n\x0e\x43heckpointInfo\x12\x10\n\x08\x63hecksum\x18\x01 \x01(\t\x12.\n\x06status\x18\x02 \x01(\x0e\x32\x1e.modal.client.CheckpointStatus\x12\x15\n\rcheckpoint_id\x18\x03 \x01(\t\"q\n\x12\x43lassCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_class_id\x18\x02 \x01(\t\x12*\n\x07methods\x18\x03 \x03(\x0b\x32\x19.modal.client.ClassMethod\"c\n\x13\x43lassCreateResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"\xb9\x01\n\x0f\x43lassGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\x12\x18\n\x10lookup_published\x18\x08 \x01(\x08\x12\x16\n\x0eworkspace_name\x18\t \x01(\t\"`\n\x10\x43lassGetResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"A\n\x13\x43lassHandleMetadata\x12*\n\x07methods\x18\x01 \x03(\x0b\x32\x19.modal.client.ClassMethod\"\x81\x01\n\x0b\x43lassMethod\x12\x15\n\rfunction_name\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x46\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"[\n\x13\x43lientCreateRequest\x12\x33\n\x0b\x63lient_type\x18\x01 \x01(\x0e\x32\x18.modal.client.ClientTypeB\x04\x80\xb5\x18\x01\x12\x0f\n\x07version\x18\x02 \x01(\t\"U\n\x14\x43lientCreateResponse\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12\x1b\n\x13\x64\x65precation_warning\x18\x03 \x01(\t\"E\n\x13\x43lientHelloResponse\x12\x0f\n\x07warning\x18\x01 \x01(\t\x12\x1d\n\x15image_builder_version\x18\x02 \x01(\t\"g\n\x16\x43lientHeartbeatRequest\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\x18\n\x10\x63urrent_input_id\x18\x03 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x04 \x01(\x01\"\x9f\x03\n\x12\x43ontainerArguments\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x04 \x01(\t\x12,\n\x0c\x66unction_def\x18\x07 \x01(\x0b\x32\x16.modal.client.Function\x12+\n\nproxy_info\x18\x08 \x01(\x0b\x32\x17.modal.client.ProxyInfo\x12M\n\x0ftracing_context\x18\t \x03(\x0b\x32\x34.modal.client.ContainerArguments.TracingContextEntry\x12\x19\n\x11serialized_params\x18\n \x01(\x0c\x12\x0f\n\x07runtime\x18\x0b \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\r \x01(\t\x12\x1a\n\rcheckpoint_id\x18\x0e \x01(\tH\x00\x88\x01\x01\x1a\x35\n\x13TracingContextEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x42\x10\n\x0e_checkpoint_id\"%\n\x10\x43\x61ncelInputEvent\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"t\n\x1a\x43ontainerHeartbeatResponse\x12?\n\x12\x63\x61ncel_input_event\x18\x01 \x01(\x0b\x32\x1e.modal.client.CancelInputEventH\x00\x88\x01\x01\x42\x15\n\x13_cancel_input_event\"\x85\x01\n\x19\x43ontainerHeartbeatRequest\x12\x18\n\x10\x63urrent_input_id\x18\x01 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x02 \x01(\x01\x12,\n$supports_graceful_input_cancellation\x18\x03 \x01(\x08\"3\n\x1a\x43ontainerCheckpointRequest\x12\x15\n\rcheckpoint_id\x18\x01 \x01(\t\"\x9d\x01\n\x14\x43ontainerExecRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0f\n\x07\x63ommand\x18\x02 \x03(\t\x12\'\n\x08pty_info\x18\x03 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12#\n\x1bterminate_container_on_exit\x18\x04 \x01(\x08\x12\x15\n\rruntime_debug\x18\x05 \x01(\x08\"[\n\x1d\x43ontainerExecGetOutputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x18\n\x10last_batch_index\x18\x03 \x01(\x04\"a\n\x1c\x43ontainerExecPutInputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x30\n\x05input\x18\x02 \x01(\x0b\x32!.modal.client.RuntimeInputMessage\"(\n\x15\x43ontainerExecResponse\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\"\"\n\x12\x43ustomDomainConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\"\x1f\n\x10\x43ustomDomainInfo\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x7f\n\tDataChunk\x12-\n\x0b\x64\x61ta_format\x18\x01 \x01(\x0e\x32\x18.modal.client.DataFormat\x12\x0e\n\x04\x64\x61ta\x18\x02 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x03 \x01(\tH\x00\x12\r\n\x05index\x18\x04 \x01(\x04\x42\x0c\n\ndata_oneof\"#\n\x10\x44ictClearRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"3\n\x13\x44ictContainsRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\"%\n\x14\x44ictContainsResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\"j\n\x11\x44ictCreateRequest\x12%\n\x04\x64\x61ta\x18\x01 \x03(\x0b\x32\x17.modal.client.DictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x18\n\x10\x65xisting_dict_id\x18\x03 \x01(\t\"%\n\x12\x44ictCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\xe8\x01\n\x16\x44ictGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12%\n\x04\x64\x61ta\x18\x05 \x03(\x0b\x32\x17.modal.client.DictEntry\"*\n\x17\x44ictGetOrCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\'\n\tDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\x0c\x12\r\n\x05value\x18\x02 \x01(\x0c\".\n\x0e\x44ictGetRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictGetResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"\'\n\x14\x44ictHeartbeatRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"D\n\x13\x44ictContentsRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0c\n\x04keys\x18\x02 \x01(\x08\x12\x0e\n\x06values\x18\x03 \x01(\x08\"!\n\x0e\x44ictLenRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\x1e\n\x0f\x44ictLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\".\n\x0e\x44ictPopRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictPopResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"N\n\x11\x44ictUpdateRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12(\n\x07updates\x18\x02 \x03(\x0b\x32\x17.modal.client.DictEntry\"\x14\n\x12\x44ictUpdateResponse\"S\n\tDNSRecord\x12)\n\x04type\x18\x01 \x01(\x0e\x32\x1b.modal.client.DNSRecordType\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\r\n\x05value\x18\x03 \x01(\t\"0\n\x13\x44omainCreateRequest\x12\x19\n\x0b\x64omain_name\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"W\n\x14\x44omainCreateResponse\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12,\n\x0b\x64ns_records\x18\x02 \x03(\x0b\x32\x17.modal.client.DNSRecord\"\x13\n\x11\x44omainListRequest\"\xaf\x01\n\x06\x44omain\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64omain_name\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12;\n\x12\x63\x65rtificate_status\x18\x04 \x01(\x0e\x32\x1f.modal.client.CertificateStatus\x12,\n\x0b\x64ns_records\x18\x05 \x03(\x0b\x32\x17.modal.client.DNSRecord\";\n\x12\x44omainListResponse\x12%\n\x07\x64omains\x18\x01 \x03(\x0b\x32\x14.modal.client.Domain\"3\n\x1e\x44omainCertificateVerifyRequest\x12\x11\n\tdomain_id\x18\x01 \x01(\t\"G\n\x1f\x44omainCertificateVerifyResponse\x12$\n\x06\x64omain\x18\x01 \x01(\x0b\x32\x14.modal.client.Domain\"(\n\x18\x45nvironmentCreateRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"(\n\x18\x45nvironmentDeleteRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\";\n\x13\x45nvironmentListItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x16\n\x0ewebhook_suffix\x18\x02 \x01(\t\"K\n\x17\x45nvironmentListResponse\x12\x30\n\x05items\x18\x02 \x03(\x0b\x32!.modal.client.EnvironmentListItem\"\x8e\x01\n\x18\x45nvironmentUpdateRequest\x12\x14\n\x0c\x63urrent_name\x18\x01 \x01(\t\x12*\n\x04name\x18\x02 \x01(\x0b\x32\x1c.google.protobuf.StringValue\x12\x30\n\nweb_suffix\x18\x03 \x01(\x0b\x32\x1c.google.protobuf.StringValue\"d\n\x1a\x46unctionCallPutDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12,\n\x0b\x64\x61ta_chunks\x18\x02 \x03(\x0b\x32\x17.modal.client.DataChunk\"J\n\x1a\x46unctionCallGetDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nlast_index\x18\x02 \x01(\x04\"\xf4\x0e\n\x08\x46unction\x12\x13\n\x0bmodule_name\x18\x01 \x01(\t\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12\x11\n\tmount_ids\x18\x03 \x03(\t\x12\x10\n\x08image_id\x18\x04 \x01(\t\x12\x1b\n\x13\x66unction_serialized\x18\x06 \x01(\x0c\x12>\n\x0f\x64\x65\x66inition_type\x18\x07 \x01(\x0e\x32%.modal.client.Function.DefinitionType\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12*\n\tresources\x18\t \x01(\x0b\x32\x17.modal.client.Resources\x12\x12\n\nsecret_ids\x18\n \x03(\t\x12+\n\nrate_limit\x18\x0b \x01(\x0b\x32\x17.modal.client.RateLimit\x12\x33\n\x0ewebhook_config\x18\x0f \x01(\x0b\x32\x1b.modal.client.WebhookConfig\x12=\n\x14shared_volume_mounts\x18\x10 \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\x08proxy_id\x18\x11 \x01(\tH\x00\x88\x01\x01\x12\x37\n\x0cretry_policy\x18\x12 \x01(\x0b\x32!.modal.client.FunctionRetryPolicy\x12\x19\n\x11\x63oncurrency_limit\x18\x13 \x01(\r\x12\x11\n\tkeep_warm\x18\x14 \x01(\x08\x12\x14\n\x0ctimeout_secs\x18\x15 \x01(\r\x12\'\n\x08pty_info\x18\x16 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12\x18\n\x10\x63lass_serialized\x18\x17 \x01(\x0c\x12\x1e\n\x16task_idle_timeout_secs\x18\x19 \x01(\r\x12\x38\n\x0e\x63loud_provider\x18\x1a \x01(\x0e\x32\x1b.modal.client.CloudProviderH\x01\x88\x01\x01\x12\x16\n\x0ewarm_pool_size\x18\x1b \x01(\r\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12.\n\x0cweb_url_info\x18\x1d \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12\x0f\n\x07runtime\x18\x1e \x01(\t\x12\x11\n\tstub_name\x18\x1f \x01(\t\x12\x30\n\rvolume_mounts\x18! \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\x1f\n\x17\x61llow_concurrent_inputs\x18\" \x01(\r\x12:\n\x12\x63ustom_domain_info\x18# \x03(\x0b\x32\x1e.modal.client.CustomDomainInfo\x12\x11\n\tworker_id\x18$ \x01(\t\x12\x15\n\rruntime_debug\x18% \x01(\x08\x12\x1b\n\x13is_builder_function\x18  \x01(\x08\x12\x18\n\x10is_auto_snapshot\x18& \x01(\x08\x12\x11\n\tis_method\x18\' \x01(\x08\x12!\n\x19is_checkpointing_function\x18( \x01(\x08\x12\x1d\n\x15\x63heckpointing_enabled\x18) \x01(\x08\x12\x30\n\ncheckpoint\x18* \x01(\x0b\x32\x1c.modal.client.CheckpointInfo\x12;\n\x13object_dependencies\x18+ \x03(\x0b\x32\x1e.modal.client.ObjectDependency\x12\x15\n\rblock_network\x18, \x01(\x08\x12\x12\n\nmax_inputs\x18. \x01(\r\x12(\n\ts3_mounts\x18/ \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x33 \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x1b\n\x13_experimental_boost\x18\x30 \x01(\x08\x12\x1f\n\x17_experimental_scheduler\x18\x31 \x01(\x08\x12P\n!_experimental_scheduler_placement\x18\x32 \x01(\x0b\x32 .modal.client.SchedulerPlacementH\x02\x88\x01\x01\"k\n\x0e\x44\x65\x66initionType\x12\x1f\n\x1b\x44\x45\x46INITION_TYPE_UNSPECIFIED\x10\x00\x12\x1e\n\x1a\x44\x45\x46INITION_TYPE_SERIALIZED\x10\x01\x12\x18\n\x14\x44\x45\x46INITION_TYPE_FILE\x10\x02\"f\n\x0c\x46unctionType\x12\x1d\n\x19\x46UNCTION_TYPE_UNSPECIFIED\x10\x00\x12\x1b\n\x17\x46UNCTION_TYPE_GENERATOR\x10\x01\x12\x1a\n\x16\x46UNCTION_TYPE_FUNCTION\x10\x02\x42\x0b\n\t_proxy_idB\x11\n\x0f_cloud_providerB$\n\"X_experimental_scheduler_placement\"|\n\x12SchedulerPlacement\x12\x14\n\x07_region\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x12\n\x05_zone\x18\x02 \x01(\tH\x01\x88\x01\x01\x12\x17\n\n_lifecycle\x18\x03 \x01(\tH\x02\x88\x01\x01\x42\n\n\x08X_regionB\x08\n\x06X_zoneB\r\n\x0bX_lifecycle\"\x8f\x01\n\x16\x46unctionHandleMetadata\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12\x11\n\tis_method\x18\' \x01(\x08\"\x9f\x01\n\x15\x46unctionCreateRequest\x12(\n\x08\x66unction\x18\x01 \x01(\x0b\x32\x16.modal.client.Function\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12(\n\x08schedule\x18\x06 \x01(\x0b\x32\x16.modal.client.Schedule\x12\x1c\n\x14\x65xisting_function_id\x18\x07 \x01(\t\"\xc7\x04\n\x0f\x46unctionOptions\x12\x12\n\nsecret_ids\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12/\n\tresources\x18\x03 \x01(\x0b\x32\x17.modal.client.ResourcesH\x00\x88\x01\x01\x12<\n\x0cretry_policy\x18\x04 \x01(\x0b\x32!.modal.client.FunctionRetryPolicyH\x01\x88\x01\x01\x12\x1e\n\x11\x63oncurrency_limit\x18\x05 \x01(\rH\x02\x88\x01\x01\x12\x19\n\x0ctimeout_secs\x18\x06 \x01(\rH\x03\x88\x01\x01\x12#\n\x16task_idle_timeout_secs\x18\x07 \x01(\rH\x04\x88\x01\x01\x12\x1b\n\x0ewarm_pool_size\x18\x08 \x01(\rH\x05\x88\x01\x01\x12\x30\n\rvolume_mounts\x18\t \x03(\x0b\x32\x19.modal.client.VolumeMount\x12$\n\x17\x61llow_concurrent_inputs\x18\n \x01(\rH\x06\x88\x01\x01\x12\x1d\n\x15replace_volume_mounts\x18\x0b \x01(\x08\x12\x1a\n\x12replace_secret_ids\x18\x0c \x01(\x08\x42\x0c\n\n_resourcesB\x0f\n\r_retry_policyB\x14\n\x12_concurrency_limitB\x0f\n\r_timeout_secsB\x19\n\x17_task_idle_timeout_secsB\x11\n\x0f_warm_pool_sizeB\x1a\n\x18_allow_concurrent_inputs\"\xd6\x01\n\x18\x46unctionPrecreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x1b\n\rfunction_name\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x1c\n\x14\x65xisting_function_id\x18\x03 \x01(\t\x12:\n\rfunction_type\x18\x04 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x33\n\x0ewebhook_config\x18\x05 \x01(\x0b\x32\x1b.modal.client.WebhookConfig\"o\n\x19\x46unctionPrecreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x9e\x01\n\x19\x46unctionBindParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x19\n\x11serialized_params\x18\x02 \x01(\x0c\x12\x37\n\x10\x66unction_options\x18\x03 \x01(\x0b\x32\x1d.modal.client.FunctionOptions\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"v\n\x1a\x46unctionBindParamsResponse\x12\x19\n\x11\x62ound_function_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\xd7\x01\n\x16\x46unctionCreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12.\n\x0cweb_url_info\x18\x03 \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12(\n\x08\x66unction\x18\x04 \x01(\x0b\x32\x16.modal.client.Function\x12=\n\x0fhandle_metadata\x18\x05 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x8a\x01\n\x12\x46unctionGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"i\n\x13\x46unctionGetResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"]\n%FunctionUpdateSchedulingParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x1f\n\x17warm_pool_size_override\x18\x02 \x01(\r\"(\n&FunctionUpdateSchedulingParamsResponse\"\x8a\x01\n\x15\x46unctionGetInputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\x12\x13\n\x0bkill_switch\x18\x03 \x01(\x08\x12\x18\n\x10\x66unction_call_id\x18\x05 \x01(\tJ\x04\x08\x04\x10\x05\"y\n\x18\x46unctionGetInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x03 \x01(\x05\x12\x19\n\x11\x61verage_call_time\x18\x05 \x01(\x02\x12\x19\n\x11input_concurrency\x18\x06 \x01(\x05\"s\n\x19\x46unctionGetInputsResponse\x12\x33\n\x06inputs\x18\x03 \x03(\x0b\x32#.modal.client.FunctionGetInputsItem\x12!\n\x19rate_limit_sleep_duration\x18\x04 \x01(\x02\"\xa6\x01\n\x16\x46unctionGetOutputsItem\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x10\n\x08input_id\x18\x03 \x01(\t\x12\x11\n\tgen_index\x18\x04 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x05 \x01(\x0e\x32\x18.modal.client.DataFormat\"\x8b\x01\n\x19\x46unctionGetOutputsRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x02 \x01(\x05\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x06 \x01(\t\x12\x18\n\x10\x63lear_on_success\x18\x07 \x01(\x08\"x\n\x1a\x46unctionGetOutputsResponse\x12\x0c\n\x04idxs\x18\x03 \x03(\x05\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionGetOutputsItem\x12\x15\n\rlast_entry_id\x18\x05 \x01(\t\"3\n\x1c\x46unctionGetSerializedRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"V\n\x1d\x46unctionGetSerializedResponse\x12\x1b\n\x13\x66unction_serialized\x18\x01 \x01(\x0c\x12\x18\n\x10\x63lass_serialized\x18\x02 \x01(\x0c\"\x89\x01\n\rFunctionInput\x12\x0e\n\x04\x61rgs\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x61rgs_blob_id\x18\x07 \x01(\tH\x00\x12\x13\n\x0b\x66inal_input\x18\t \x01(\x08\x12-\n\x0b\x64\x61ta_format\x18\n \x01(\x0e\x32\x18.modal.client.DataFormatB\x0c\n\nargs_oneof\"\xd8\x01\n\x12\x46unctionMapRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x19\n\x11return_exceptions\x18\x03 \x01(\x08\x12:\n\x12\x66unction_call_type\x18\x04 \x01(\x0e\x32\x1e.modal.client.FunctionCallType\x12=\n\x10pipelined_inputs\x18\x05 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\"v\n\x13\x46unctionMapResponse\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x45\n\x10pipelined_inputs\x18\x02 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"P\n\x15\x46unctionPutInputsItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"~\n\x18\x46unctionPutInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x33\n\x06inputs\x18\x04 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\">\n\x1d\x46unctionPutInputsResponseItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12\x10\n\x08input_id\x18\x02 \x01(\t\"X\n\x19\x46unctionPutInputsResponse\x12;\n\x06inputs\x18\x01 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"\xce\x01\n\x16\x46unctionPutOutputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x18\n\x10input_started_at\x18\x03 \x01(\x01\x12\x19\n\x11output_created_at\x18\x04 \x01(\x01\x12\x11\n\tgen_index\x18\x06 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x07 \x01(\x0e\x32\x18.modal.client.DataFormat\"R\n\x19\x46unctionPutOutputsRequest\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionPutOutputsItem\"s\n\x13\x46unctionRetryPolicy\x12\x1b\n\x13\x62\x61\x63koff_coefficient\x18\x01 \x01(\x02\x12\x18\n\x10initial_delay_ms\x18\x02 \x01(\r\x12\x14\n\x0cmax_delay_ms\x18\x03 \x01(\r\x12\x0f\n\x07retries\x18\x12 \x01(\r\"7\n\x1b\x46unctionGetCallGraphRequest\x12\x18\n\x10\x66unction_call_id\x18\x02 \x01(\t\"\x8c\x01\n\x12InputCallGraphInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x39\n\x06status\x18\x02 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x0f\n\x07task_id\x18\x04 \x01(\t\"z\n\x19\x46unctionCallCallGraphInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x15\n\rfunction_name\x18\x03 \x01(\t\x12\x13\n\x0bmodule_name\x18\x04 \x01(\t\"\x91\x01\n\x1c\x46unctionGetCallGraphResponse\x12\x30\n\x06inputs\x18\x01 \x03(\x0b\x32 .modal.client.InputCallGraphInfo\x12?\n\x0e\x66unction_calls\x18\x02 \x03(\x0b\x32\'.modal.client.FunctionCallCallGraphInfo\"5\n\x19\x46unctionCallCancelRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\".\n\x17\x46unctionCallListRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"\xc3\x03\n\x10\x46unctionCallInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x12\n\ncreated_at\x18\x06 \x01(\x01\x12\x14\n\x0cscheduled_at\x18\x07 \x01(\x01\x12\x37\n\x0epending_inputs\x18\x0c \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x36\n\rfailed_inputs\x18\r \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10succeeded_inputs\x18\x0e \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x37\n\x0etimeout_inputs\x18\x0f \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10\x63\x61ncelled_inputs\x18\x10 \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x14\n\x0ctotal_inputs\x18\x11 \x01(\x05J\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0c\"R\n\x18\x46unctionCallListResponse\x12\x36\n\x0e\x66unction_calls\x18\x01 \x03(\x0b\x32\x1e.modal.client.FunctionCallInfo\"5\n\x1e\x46unctionGetCurrentStatsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"S\n\rFunctionStats\x12\x0f\n\x07\x62\x61\x63klog\x18\x01 \x01(\r\x12\x18\n\x10num_active_tasks\x18\x02 \x01(\r\x12\x17\n\x0fnum_total_tasks\x18\x03 \x01(\r\"$\n\rGeneratorDone\x12\x13\n\x0bitems_total\x18\x01 \x01(\x04\"\xfe\x04\n\rGenericResult\x12\x39\n\x06status\x18\x01 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x11\n\texception\x18\x02 \x01(\t\x12\x10\n\x08\x65xitcode\x18\x03 \x01(\x05\x12\x11\n\ttraceback\x18\x04 \x01(\t\x12\x15\n\rserialized_tb\x18\x0b \x01(\x0c\x12\x15\n\rtb_line_cache\x18\x0c \x01(\x0c\x12\x0e\n\x04\x64\x61ta\x18\x05 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\n \x01(\tH\x00\x12?\n\ngen_status\x18\x07 \x01(\x0e\x32+.modal.client.GenericResult.GeneratorStatus\x12\x1a\n\x12propagation_reason\x18\r \x01(\t\"\xc3\x01\n\rGenericStatus\x12\x1e\n\x1aGENERIC_STATUS_UNSPECIFIED\x10\x00\x12\x1a\n\x16GENERIC_STATUS_SUCCESS\x10\x01\x12\x1a\n\x16GENERIC_STATUS_FAILURE\x10\x02\x12\x1d\n\x19GENERIC_STATUS_TERMINATED\x10\x03\x12\x1a\n\x16GENERIC_STATUS_TIMEOUT\x10\x04\x12\x1f\n\x1bGENERIC_STATUS_INIT_FAILURE\x10\x05\"s\n\x0fGeneratorStatus\x12 \n\x1cGENERATOR_STATUS_UNSPECIFIED\x10\x00\x12\x1f\n\x1bGENERATOR_STATUS_INCOMPLETE\x10\x01\x12\x1d\n\x19GENERATOR_STATUS_COMPLETE\x10\x02\x42\x0c\n\ndata_oneof\"O\n\tGPUConfig\x12#\n\x04type\x18\x01 \x01(\x0e\x32\x15.modal.client.GPUType\x12\r\n\x05\x63ount\x18\x02 \x01(\r\x12\x0e\n\x06memory\x18\x03 \x01(\r\"`\n\rBuildFunction\x12\x12\n\ndefinition\x18\x01 \x01(\t\x12\x0f\n\x07globals\x18\x02 \x01(\x0c\x12*\n\x05input\x18\x03 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"\xdd\x03\n\x05Image\x12,\n\x0b\x62\x61se_images\x18\x05 \x03(\x0b\x32\x17.modal.client.BaseImage\x12\x1b\n\x13\x64ockerfile_commands\x18\x06 \x03(\t\x12\x35\n\rcontext_files\x18\x07 \x03(\x0b\x32\x1e.modal.client.ImageContextFile\x12\x0f\n\x07version\x18\x0b \x01(\t\x12\x12\n\nsecret_ids\x18\x0c \x03(\t\x12\x0b\n\x03gpu\x18\r \x01(\x08\x12\x18\n\x10\x63ontext_mount_id\x18\x0f \x01(\t\x12+\n\ngpu_config\x18\x10 \x01(\x0b\x32\x17.modal.client.GPUConfig\x12@\n\x15image_registry_config\x18\x11 \x01(\x0b\x32!.modal.client.ImageRegistryConfig\x12\x1a\n\x12\x62uild_function_def\x18\x0e \x01(\t\x12\x1e\n\x16\x62uild_function_globals\x18\x12 \x01(\x0c\x12\x0f\n\x07runtime\x18\x13 \x01(\t\x12\x15\n\rruntime_debug\x18\x14 \x01(\x08\x12\x33\n\x0e\x62uild_function\x18\x15 \x01(\x0b\x32\x1b.modal.client.BuildFunction\"2\n\x10ImageContextFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\xed\x01\n\x17ImageGetOrCreateRequest\x12\"\n\x05image\x18\x02 \x01(\x0b\x32\x13.modal.client.Image\x12\x14\n\x06\x61pp_id\x18\x04 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_image_id\x18\x05 \x01(\t\x12\x19\n\x11\x62uild_function_id\x18\x06 \x01(\t\x12\x13\n\x0b\x66orce_build\x18\x07 \x01(\x08\x12\x34\n\tnamespace\x18\x08 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x17\n\x0f\x62uilder_version\x18\t \x01(\t\",\n\x18ImageGetOrCreateResponse\x12\x10\n\x08image_id\x18\x01 \x01(\t\"U\n\x19ImageJoinStreamingRequest\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x03 \x01(\t\"\x93\x01\n\x1aImageJoinStreamingResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12)\n\ttask_logs\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x03 \x01(\t\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"d\n\x13ImageRegistryConfig\x12:\n\x12registry_auth_type\x18\x01 \x01(\x0e\x32\x1e.modal.client.RegistryAuthType\x12\x11\n\tsecret_id\x18\x02 \x01(\t\"\x99\x01\n\tInputInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x0f\n\x07task_id\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x05 \x01(\x01\x12\x19\n\x11task_startup_time\x18\x06 \x01(\x01\x12\x18\n\x10task_first_input\x18\x07 \x01(\x08\"K\n\x11InputCategoryInfo\x12\r\n\x05total\x18\x01 \x01(\x05\x12\'\n\x06latest\x18\x02 \x03(\x0b\x32\x17.modal.client.InputInfo\"l\n\x11MountBuildRequest\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_mount_id\x18\x03 \x01(\t\x12&\n\x05\x66iles\x18\x04 \x03(\x0b\x32\x17.modal.client.MountFile\"b\n\x12MountBuildResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\"\xfa\x01\n\x17MountGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12&\n\x05\x66iles\x18\x05 \x03(\x0b\x32\x17.modal.client.MountFile\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\"h\n\x18MountGetOrCreateResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\":\n\x13MountHandleMetadata\x12#\n\x1b\x63ontent_checksum_sha256_hex\x18\x01 \x01(\t\"i\n\tMountFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x11\n\x04size\x18\x04 \x01(\x04H\x00\x88\x01\x01\x12\x11\n\x04mode\x18\x05 \x01(\rH\x01\x88\x01\x01\x42\x07\n\x05_sizeB\x07\n\x05_mode\"_\n\x13MountPutFileRequest\x12\x12\n\nsha256_hex\x18\x02 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x03 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"&\n\x14MountPutFileResponse\x12\x0e\n\x06\x65xists\x18\x02 \x01(\x08\"S\n\x0fMultiPartUpload\x12\x13\n\x0bpart_length\x18\x01 \x01(\x03\x12\x13\n\x0bupload_urls\x18\x02 \x03(\t\x12\x16\n\x0e\x63ompletion_url\x18\x03 \x01(\t\"\xce\x02\n\x06Object\x12\x11\n\tobject_id\x18\x01 \x01(\t\x12H\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadataH\x00\x12\x42\n\x15mount_handle_metadata\x18\x04 \x01(\x0b\x32!.modal.client.MountHandleMetadataH\x00\x12\x42\n\x15\x63lass_handle_metadata\x18\x05 \x01(\x0b\x32!.modal.client.ClassHandleMetadataH\x00\x12\x46\n\x17sandbox_handle_metadata\x18\x06 \x01(\x0b\x32#.modal.client.SandboxHandleMetadataH\x00\x42\x17\n\x15handle_metadata_oneof\"%\n\x10ObjectDependency\x12\x11\n\tobject_id\x18\x01 \x01(\t\"\xc2\x01\n\x17ProxyGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18ProxyGetOrCreateResponse\x12\x10\n\x08proxy_id\x18\x01 \x01(\t\"\\\n\tProxyInfo\x12\x12\n\nelastic_ip\x18\x01 \x01(\t\x12\x11\n\tproxy_key\x18\x02 \x01(\t\x12\x13\n\x0bremote_addr\x18\x03 \x01(\t\x12\x13\n\x0bremote_port\x18\x04 \x01(\x05\"\x86\x02\n\x07PTYInfo\x12\x0f\n\x07\x65nabled\x18\x01 \x01(\x08\x12\x12\n\nwinsz_rows\x18\x02 \x01(\r\x12\x12\n\nwinsz_cols\x18\x03 \x01(\r\x12\x10\n\x08\x65nv_term\x18\x04 \x01(\t\x12\x15\n\renv_colorterm\x18\x05 \x01(\t\x12\x18\n\x10\x65nv_term_program\x18\x06 \x01(\t\x12/\n\x08pty_type\x18\x07 \x01(\x0e\x32\x1d.modal.client.PTYInfo.PTYType\"N\n\x07PTYType\x12\x18\n\x14PTY_TYPE_UNSPECIFIED\x10\x00\x12\x15\n\x11PTY_TYPE_FUNCTION\x10\x01\x12\x12\n\x0ePTY_TYPE_SHELL\x10\x02\"?\n\x12QueueCreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11\x65xisting_queue_id\x18\x02 \x01(\t\"\'\n\x13QueueCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"\xc2\x01\n\x17QueueGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18QueueGetOrCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"]\n\x0fQueueGetRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x10\n\x08n_values\x18\x04 \x01(\x05\x12\x15\n\rpartition_key\x18\x05 \x01(\x0c\"\"\n\x10QueueGetResponse\x12\x0e\n\x06values\x18\x02 \x03(\x0c\")\n\x15QueueHeartbeatRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"i\n\x0fQueuePutRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0e\n\x06values\x18\x04 \x03(\x0c\x12\x15\n\rpartition_key\x18\x05 \x01(\x0c\x12\x1d\n\x15partition_ttl_seconds\x18\x06 \x01(\x05\":\n\x0fQueueLenRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x15\n\rpartition_key\x18\x02 \x01(\x0c\"\x1f\n\x10QueueLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\"r\n\x15QueueNextItemsRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x15\n\rpartition_key\x18\x02 \x01(\x0c\x12\x15\n\rlast_entry_id\x18\x03 \x01(\t\x12\x19\n\x11item_poll_timeout\x18\x04 \x01(\x02\",\n\tQueueItem\x12\r\n\x05value\x18\x01 \x01(\x0c\x12\x10\n\x08\x65ntry_id\x18\x02 \x01(\t\"@\n\x16QueueNextItemsResponse\x12&\n\x05items\x18\x01 \x03(\x0b\x32\x17.modal.client.QueueItem\"M\n\tRateLimit\x12\r\n\x05limit\x18\x01 \x01(\x05\x12\x31\n\x08interval\x18\x02 \x01(\x0e\x32\x1f.modal.client.RateLimitInterval\"u\n\tResources\x12\x11\n\tmemory_mb\x18\x02 \x01(\r\x12\x11\n\tmilli_cpu\x18\x03 \x01(\r\x12+\n\ngpu_config\x18\x04 \x01(\x0b\x32\x17.modal.client.GPUConfig\x12\x15\n\rmemory_mb_max\x18\x05 \x01(\r\"d\n\x07S3Mount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\"\x99\x04\n\x07Sandbox\x12\x17\n\x0f\x65ntrypoint_args\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12\x10\n\x08image_id\x18\x03 \x01(\t\x12\x12\n\nsecret_ids\x18\x04 \x03(\t\x12*\n\tresources\x18\x05 \x01(\x0b\x32\x17.modal.client.Resources\x12\x33\n\x0e\x63loud_provider\x18\x06 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x14\n\x0ctimeout_secs\x18\x07 \x01(\r\x12\x14\n\x07workdir\x18\x08 \x01(\tH\x00\x88\x01\x01\x12\x33\n\nnfs_mounts\x18\t \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\rruntime_debug\x18\n \x01(\x08\x12\x15\n\rblock_network\x18\x0b \x01(\x08\x12(\n\ts3_mounts\x18\x0c \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x0e \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x30\n\rvolume_mounts\x18\r \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\'\n\x08pty_info\x18\x0f \x01(\x0b\x32\x15.modal.client.PTYInfoB\n\n\x08_workdir\"W\n\x14SandboxCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\"+\n\x15SandboxCreateResponse\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"-\n\x17SandboxGetTaskIdRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"+\n\x18SandboxGetTaskIdResponse\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"\x8a\x01\n\x15SandboxGetLogsRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x02 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\"Y\n\x18SandboxStdinWriteRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\r\n\x05input\x18\x02 \x01(\x0c\x12\r\n\x05index\x18\x03 \x01(\r\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"\x1b\n\x19SandboxStdinWriteResponse\"D\n\x15SandboxHandleMetadata\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x83\x01\n\x0bSandboxInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12)\n\ttask_info\x18\x04 \x01(\x0b\x32\x16.modal.client.TaskInfo\">\n\x12SandboxListRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x18\n\x10\x62\x65\x66ore_timestamp\x18\x02 \x01(\x01\"C\n\x13SandboxListResponse\x12,\n\tsandboxes\x18\x01 \x03(\x0b\x32\x19.modal.client.SandboxInfo\"-\n\x17SandboxTerminateRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"P\n\x18SandboxTerminateResponse\x12\x34\n\x0f\x65xisting_result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"9\n\x12SandboxWaitRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\"B\n\x13SandboxWaitResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x8e\x02\n\x08Schedule\x12+\n\x04\x63ron\x18\x01 \x01(\x0b\x32\x1b.modal.client.Schedule.CronH\x00\x12/\n\x06period\x18\x02 \x01(\x0b\x32\x1d.modal.client.Schedule.PeriodH\x00\x1a\x1b\n\x04\x43ron\x12\x13\n\x0b\x63ron_string\x18\x01 \x01(\t\x1au\n\x06Period\x12\r\n\x05years\x18\x01 \x01(\x05\x12\x0e\n\x06months\x18\x02 \x01(\x05\x12\r\n\x05weeks\x18\x03 \x01(\x05\x12\x0c\n\x04\x64\x61ys\x18\x04 \x01(\x05\x12\r\n\x05hours\x18\x05 \x01(\x05\x12\x0f\n\x07minutes\x18\x06 \x01(\x05\x12\x0f\n\x07seconds\x18\x07 \x01(\x02\x42\x10\n\x0eschedule_oneof\"\xd0\x01\n\x13SecretCreateRequest\x12@\n\x08\x65nv_dict\x18\x01 \x03(\x0b\x32..modal.client.SecretCreateRequest.EnvDictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x15\n\rtemplate_type\x18\x03 \x01(\t\x12\x1a\n\x12\x65xisting_secret_id\x18\x04 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\")\n\x14SecretCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"\xca\x02\n\x18SecretGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x45\n\x08\x65nv_dict\x18\x05 \x03(\x0b\x32\x33.modal.client.SecretGetOrCreateRequest.EnvDictEntry\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\".\n\x19SecretGetOrCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"c\n\x0eSecretListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x12\n\ncreated_at\x18\x02 \x01(\x01\x12\x14\n\x0clast_used_at\x18\x03 \x01(\x01\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"-\n\x11SecretListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12SecretListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.SecretListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"\xd9\x01\n\x1eSharedVolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\";\n\x1fSharedVolumeGetOrCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"8\n\x1cSharedVolumeHeartbeatRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"f\n\x19SharedVolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x33\n\x0e\x63loud_provider\x18\x02 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"6\n\x1aSharedVolumeCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"\x88\x01\n\x14SharedVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12\x33\n\x0e\x63loud_provider\x18\x04 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"3\n\x17SharedVolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"g\n\x18SharedVolumeListResponse\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.SharedVolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"F\n\x1cSharedVolumeListFilesRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"\xa7\x01\n\x1aSharedVolumePutFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x04 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x12\x11\n\tresumable\x18\x06 \x01(\x08\x42\x0c\n\ndata_oneof\"-\n\x1bSharedVolumePutFileResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"D\n\x1aSharedVolumeGetFileRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"S\n\x1bSharedVolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"`\n\x1dSharedVolumeRemoveFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x11\n\trecursive\x18\x03 \x01(\x08\"I\n\x1dSharedVolumeListFilesResponse\x12(\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\x17.modal.client.FileEntry\"\x92\x01\n\x11SharedVolumeMount\x12\x12\n\nmount_path\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x33\n\x0e\x63loud_provider\x18\x03 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x1a\n\x12\x61llow_cross_region\x18\x04 \x01(\x08\".\n\x19TaskCurrentInputsResponse\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"l\n\x08TaskInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nstarted_at\x18\x02 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x03 \x01(\x01\x12+\n\x06result\x18\x04 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\xee\x01\n\x08TaskLogs\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12+\n\ntask_state\x18\x06 \x01(\x0e\x32\x17.modal.client.TaskState\x12\x11\n\ttimestamp\x18\x07 \x01(\x01\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x31\n\rtask_progress\x18\t \x01(\x0b\x32\x1a.modal.client.TaskProgress\x12\x18\n\x10\x66unction_call_id\x18\n \x01(\t\x12\x10\n\x08input_id\x18\x0b \x01(\t\"\x11\n\x0fTaskListRequest\":\n\x10TaskListResponse\x12&\n\x05tasks\x18\x01 \x03(\x0b\x32\x17.modal.client.TaskStats\"\xc6\x01\n\rTaskLogsBatch\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12%\n\x05items\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x05 \x01(\t\x12\x10\n\x08\x61pp_done\x18\n \x01(\x08\x12\x13\n\x0b\x66unction_id\x18\x0b \x01(\t\x12\x10\n\x08input_id\x18\x0c \x01(\t\x12\x10\n\x08image_id\x18\r \x01(\t\x12\x0b\n\x03\x65of\x18\x0e \x01(\x08\x12\x13\n\x0bpty_exec_id\x18\x0f \x01(\t\"p\n\x0cTaskProgress\x12\x0b\n\x03len\x18\x01 \x01(\x04\x12\x0b\n\x03pos\x18\x02 \x01(\x04\x12\x31\n\rprogress_type\x18\x03 \x01(\x0e\x32\x1a.modal.client.ProgressType\x12\x13\n\x0b\x64\x65scription\x18\x04 \x01(\t\"@\n\x11TaskResultRequest\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\"Y\n\tTaskStats\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x02 \x01(\t\x12\x17\n\x0f\x61pp_description\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\"V\n\x16TokenFlowCreateRequest\x12\x12\n\nutm_source\x18\x03 \x01(\t\x12\x16\n\x0elocalhost_port\x18\x04 \x01(\x05\x12\x10\n\x08next_url\x18\x05 \x01(\t\"d\n\x17TokenFlowCreateResponse\x12\x15\n\rtoken_flow_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12\x0c\n\x04\x63ode\x18\x03 \x01(\t\x12\x13\n\x0bwait_secret\x18\x04 \x01(\t\"S\n\x14TokenFlowWaitRequest\x12\x0f\n\x07timeout\x18\x01 \x01(\x02\x12\x15\n\rtoken_flow_id\x18\x02 \x01(\t\x12\x13\n\x0bwait_secret\x18\x03 \x01(\t\"l\n\x15TokenFlowWaitResponse\x12\x10\n\x08token_id\x18\x01 \x01(\t\x12\x14\n\x0ctoken_secret\x18\x02 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x08\x12\x1a\n\x12workspace_username\x18\x04 \x01(\t\"7\n\x12TunnelStartRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\x12\x13\n\x0bunencrypted\x18\x02 \x01(\x08\"\x99\x01\n\x13TunnelStartResponse\x12\x0c\n\x04host\x18\x01 \x01(\t\x12\x0c\n\x04port\x18\x02 \x01(\r\x12\x1d\n\x10unencrypted_host\x18\x03 \x01(\tH\x00\x88\x01\x01\x12\x1d\n\x10unencrypted_port\x18\x04 \x01(\rH\x01\x88\x01\x01\x42\x13\n\x11_unencrypted_hostB\x13\n\x11_unencrypted_port\"!\n\x11TunnelStopRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\"$\n\x12TunnelStopResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"\xd3\x01\n\x18VolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\".\n\x19VolumeGetOrCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x16VolumeHeartbeatRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x13VolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\")\n\x14VolumeCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\".\n\x13VolumeCommitRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"+\n\x14VolumeCommitResponse\x12\x13\n\x0bskip_reload\x18\x01 \x01(\x08\"F\n\x13VolumeDeleteRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x1c\n\x10\x65nvironment_name\x18\x02 \x01(\tB\x02\x18\x01\"S\n\x14VolumeGetFileRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\r\n\x05start\x18\x03 \x01(\x04\x12\x0b\n\x03len\x18\x04 \x01(\x04\"w\n\x15VolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x12\x0c\n\x04size\x18\x03 \x01(\x04\x12\r\n\x05start\x18\x04 \x01(\x04\x12\x0b\n\x03len\x18\x05 \x01(\x04\x42\x0c\n\ndata_oneof\"c\n\x16VolumeListFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x18\n\x0bmax_entries\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0e\n\x0c_max_entries\"C\n\x17VolumeListFilesResponse\x12(\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\x17.modal.client.FileEntry\"F\n\x0eVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x11\n\tvolume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\"-\n\x11VolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12VolumeListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.VolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"(\n\x13VolumeReloadRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"}\n\x15VolumePutFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12&\n\x05\x66iles\x18\x02 \x03(\x0b\x32\x17.modal.client.MountFile\x12)\n!disallow_overwrite_existing_files\x18\x03 \x01(\x08\"S\n\x17VolumeRemoveFileRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\x11\n\trecursive\x18\x03 \x01(\x08\"c\n\x16VolumeCopyFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x11\n\tsrc_paths\x18\x02 \x03(\x0c\x12\x10\n\x08\x64st_path\x18\x03 \x01(\x0c\x12\x11\n\trecursive\x18\x04 \x01(\x08\"V\n\x0bVolumeMount\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12 \n\x18\x61llow_background_commits\x18\x03 \x01(\x08\"\x8d\x02\n\rWebhookConfig\x12\'\n\x04type\x18\x01 \x01(\x0e\x32\x19.modal.client.WebhookType\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x18\n\x10requested_suffix\x18\x04 \x01(\t\x12\x32\n\nasync_mode\x18\x05 \x01(\x0e\x32\x1e.modal.client.WebhookAsyncMode\x12\x38\n\x0e\x63ustom_domains\x18\x06 \x03(\x0b\x32 .modal.client.CustomDomainConfig\x12\x17\n\x0fweb_server_port\x18\x07 \x01(\r\x12\"\n\x1aweb_server_startup_timeout\x18\x08 \x01(\x02\"N\n\nWebUrlInfo\x12\x11\n\ttruncated\x18\x01 \x01(\x08\x12\x17\n\x0fhas_unique_hash\x18\x02 \x01(\x08\x12\x14\n\x0clabel_stolen\x18\x03 \x01(\x08\"K\n\x1bWorkspaceNameLookupResponse\x12\x1a\n\x0eworkspace_name\x18\x01 \x01(\tB\x02\x18\x01\x12\x10\n\x08username\x18\x02 \x01(\t\"^\n\x14RuntimeOutputMessage\x12\x35\n\x0f\x66ile_descriptor\x18\x01 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07message\x18\x02 \x01(\t\"\x82\x01\n\x12RuntimeOutputBatch\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.RuntimeOutputMessage\x12\x13\n\x0b\x62\x61tch_index\x18\x02 \x01(\x04\x12\x16\n\texit_code\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0c\n\n_exit_code\"=\n\x13RuntimeInputMessage\x12\x0f\n\x07message\x18\x01 \x01(\x0c\x12\x15\n\rmessage_index\x18\x02 \x01(\x04*\x83\x01\n\x13\x41ppDeployVisibility\x12%\n!APP_DEPLOY_VISIBILITY_UNSPECIFIED\x10\x00\x12#\n\x1f\x41PP_DEPLOY_VISIBILITY_WORKSPACE\x10\x01\x12 \n\x1c\x41PP_DEPLOY_VISIBILITY_PUBLIC\x10\x02*\xf5\x01\n\x13\x41ppDisconnectReason\x12%\n!APP_DISCONNECT_REASON_UNSPECIFIED\x10\x00\x12)\n%APP_DISCONNECT_REASON_LOCAL_EXCEPTION\x10\x01\x12,\n(APP_DISCONNECT_REASON_KEYBOARD_INTERRUPT\x10\x02\x12.\n*APP_DISCONNECT_REASON_ENTRYPOINT_COMPLETED\x10\x03\x12.\n*APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION\x10\x04*\x8d\x02\n\x08\x41ppState\x12\x19\n\x15\x41PP_STATE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STATE_EPHEMERAL\x10\x01\x12\x16\n\x12\x41PP_STATE_DETACHED\x10\x02\x12\x16\n\x12\x41PP_STATE_DEPLOYED\x10\x03\x12\x16\n\x12\x41PP_STATE_STOPPING\x10\x04\x12\x15\n\x11\x41PP_STATE_STOPPED\x10\x05\x12\x1a\n\x16\x41PP_STATE_INITIALIZING\x10\x06\x12\x16\n\x12\x41PP_STATE_DISABLED\x10\x07\x12#\n\x1f\x41PP_STATE_DETACHED_DISCONNECTED\x10\x08\x12\x15\n\x11\x41PP_STATE_DERIVED\x10\t*\x85\x01\n\rAppStopSource\x12\x1f\n\x1b\x41PP_STOP_SOURCE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STOP_SOURCE_CLI\x10\x01\x12!\n\x1d\x41PP_STOP_SOURCE_PYTHON_CLIENT\x10\x02\x12\x17\n\x13\x41PP_STOP_SOURCE_WEB\x10\x03*\x91\x01\n\x11\x43\x65rtificateStatus\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_PENDING\x10\x00\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_ISSUED\x10\x01\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_FAILED\x10\x02\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_REVOKED\x10\x03*\xb1\x01\n\x10\x43heckpointStatus\x12!\n\x1d\x43HECKPOINT_STATUS_UNSPECIFIED\x10\x00\x12\x1d\n\x19\x43HECKPOINT_STATUS_PENDING\x10\x01\x12 \n\x1c\x43HECKPOINT_STATUS_PROCESSING\x10\x02\x12\x1b\n\x17\x43HECKPOINT_STATUS_READY\x10\x03\x12\x1c\n\x18\x43HECKPOINT_STATUS_FAILED\x10\x04*\xb0\x01\n\nClientType\x12\x1b\n\x17\x43LIENT_TYPE_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LIENT_TYPE_CLIENT\x10\x01\x12\x1a\n\x12\x43LIENT_TYPE_WORKER\x10\x02\x1a\x02\x08\x01\x12\x19\n\x15\x43LIENT_TYPE_CONTAINER\x10\x03\x12\x1a\n\x12\x43LIENT_TYPE_SERVER\x10\x04\x1a\x02\x08\x01\x12\x1a\n\x16\x43LIENT_TYPE_WEB_SERVER\x10\x05*\xb0\x01\n\rCloudProvider\x12\x1e\n\x1a\x43LOUD_PROVIDER_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LOUD_PROVIDER_AWS\x10\x01\x12\x16\n\x12\x43LOUD_PROVIDER_GCP\x10\x02\x12\x17\n\x13\x43LOUD_PROVIDER_AUTO\x10\x03\x12\x16\n\x12\x43LOUD_PROVIDER_OCI\x10\x04\x12\x1e\n\x1a\x43LOUD_PROVIDER_LAMBDA_LABS\x10\x05*w\n\nDataFormat\x12\x1b\n\x17\x44\x41TA_FORMAT_UNSPECIFIED\x10\x00\x12\x16\n\x12\x44\x41TA_FORMAT_PICKLE\x10\x01\x12\x14\n\x10\x44\x41TA_FORMAT_ASGI\x10\x02\x12\x1e\n\x1a\x44\x41TA_FORMAT_GENERATOR_DONE\x10\x03*\x80\x01\n\x13\x44\x65ploymentNamespace\x12$\n DEPLOYMENT_NAMESPACE_UNSPECIFIED\x10\x00\x12\"\n\x1e\x44\x45PLOYMENT_NAMESPACE_WORKSPACE\x10\x01\x12\x1f\n\x1b\x44\x45PLOYMENT_NAMESPACE_GLOBAL\x10\x03*Z\n\rDNSRecordType\x12\x15\n\x11\x44NS_RECORD_TYPE_A\x10\x00\x12\x17\n\x13\x44NS_RECORD_TYPE_TXT\x10\x01\x12\x19\n\x15\x44NS_RECORD_TYPE_CNAME\x10\x02*\x83\x01\n\x0e\x46ileDescriptor\x12\x1f\n\x1b\x46ILE_DESCRIPTOR_UNSPECIFIED\x10\x00\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDOUT\x10\x01\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDERR\x10\x02\x12\x18\n\x14\x46ILE_DESCRIPTOR_INFO\x10\x03*p\n\x10\x46unctionCallType\x12\"\n\x1e\x46UNCTION_CALL_TYPE_UNSPECIFIED\x10\x00\x12\x1c\n\x18\x46UNCTION_CALL_TYPE_UNARY\x10\x01\x12\x1a\n\x16\x46UNCTION_CALL_TYPE_MAP\x10\x02*\x82\x02\n\x07GPUType\x12\x18\n\x14GPU_TYPE_UNSPECIFIED\x10\x00\x12\x0f\n\x0bGPU_TYPE_T4\x10\x01\x12\x11\n\rGPU_TYPE_A100\x10\x02\x12\x11\n\rGPU_TYPE_A10G\x10\x03\x12\x10\n\x0cGPU_TYPE_ANY\x10\x04\x12\x19\n\x11GPU_TYPE_A100_20G\x10\x05\x1a\x02\x08\x01\x12\x1f\n\x17GPU_TYPE_A100_40GB_MANY\x10\x06\x1a\x02\x08\x01\x12\x1c\n\x14GPU_TYPE_INFERENTIA2\x10\x07\x1a\x02\x08\x01\x12\x16\n\x12GPU_TYPE_A100_80GB\x10\x08\x12\x0f\n\x0bGPU_TYPE_L4\x10\t\x12\x11\n\rGPU_TYPE_H100\x10\n*\xa0\x02\n\x12ObjectCreationType\x12$\n OBJECT_CREATION_TYPE_UNSPECIFIED\x10\x00\x12*\n&OBJECT_CREATION_TYPE_CREATE_IF_MISSING\x10\x01\x12.\n*OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS\x10\x02\x12\x33\n/OBJECT_CREATION_TYPE_CREATE_OVERWRITE_IF_EXISTS\x10\x03\x12/\n+OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP\x10\x04\x12\"\n\x1eOBJECT_CREATION_TYPE_EPHEMERAL\x10\x05*>\n\x0cProgressType\x12\x19\n\x15IMAGE_SNAPSHOT_UPLOAD\x10\x00\x12\x13\n\x0f\x46UNCTION_QUEUED\x10\x01*x\n\x11RateLimitInterval\x12#\n\x1fRATE_LIMIT_INTERVAL_UNSPECIFIED\x10\x00\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_SECOND\x10\x01\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_MINUTE\x10\x02*\xb2\x01\n\x10RegistryAuthType\x12\"\n\x1eREGISTRY_AUTH_TYPE_UNSPECIFIED\x10\x00\x12\x1a\n\x16REGISTRY_AUTH_TYPE_AWS\x10\x01\x12\x1a\n\x16REGISTRY_AUTH_TYPE_GCP\x10\x02\x12\x1d\n\x19REGISTRY_AUTH_TYPE_PUBLIC\x10\x03\x12#\n\x1fREGISTRY_AUTH_TYPE_STATIC_CREDS\x10\x04*\xdc\x02\n\tTaskState\x12\x1a\n\x16TASK_STATE_UNSPECIFIED\x10\x00\x12\x16\n\x12TASK_STATE_CREATED\x10\x06\x12\x15\n\x11TASK_STATE_QUEUED\x10\x01\x12\x1e\n\x1aTASK_STATE_WORKER_ASSIGNED\x10\x02\x12\x1c\n\x18TASK_STATE_LOADING_IMAGE\x10\x03\x12\x15\n\x11TASK_STATE_ACTIVE\x10\x04\x12\x18\n\x14TASK_STATE_COMPLETED\x10\x05\x12!\n\x1dTASK_STATE_CREATING_CONTAINER\x10\x07\x12\x13\n\x0fTASK_STATE_IDLE\x10\x08\x12\x1a\n\x16TASK_STATE_PREEMPTIBLE\x10\t\x12\x18\n\x14TASK_STATE_PREEMPTED\x10\n\x12\'\n#TASK_STATE_LOADING_CHECKPOINT_IMAGE\x10\x0b*\x99\x01\n\x0bWebhookType\x12\x1c\n\x18WEBHOOK_TYPE_UNSPECIFIED\x10\x00\x12\x19\n\x15WEBHOOK_TYPE_ASGI_APP\x10\x01\x12\x19\n\x15WEBHOOK_TYPE_FUNCTION\x10\x02\x12\x19\n\x15WEBHOOK_TYPE_WSGI_APP\x10\x03\x12\x1b\n\x17WEBHOOK_TYPE_WEB_SERVER\x10\x04*\x9a\x01\n\x10WebhookAsyncMode\x12\"\n\x1eWEBHOOK_ASYNC_MODE_UNSPECIFIED\x10\x00\x12\x1f\n\x1bWEBHOOK_ASYNC_MODE_DISABLED\x10\x02\x12\x1e\n\x1aWEBHOOK_ASYNC_MODE_TRIGGER\x10\x03\x12\x1b\n\x17WEBHOOK_ASYNC_MODE_AUTO\x10\x04\"\x04\x08\x01\x10\x01\x32\xe8N\n\x0bModalClient\x12L\n\tAppCreate\x12\x1e.modal.client.AppCreateRequest\x1a\x1f.modal.client.AppCreateResponse\x12W\n\x13\x41ppClientDisconnect\x12(.modal.client.AppClientDisconnectRequest\x1a\x16.google.protobuf.Empty\x12L\n\nAppGetLogs\x12\x1f.modal.client.AppGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12K\n\rAppSetObjects\x12\".modal.client.AppSetObjectsRequest\x1a\x16.google.protobuf.Empty\x12X\n\rAppGetObjects\x12\".modal.client.AppGetObjectsRequest\x1a#.modal.client.AppGetObjectsResponse\x12\x46\n\x07\x41ppList\x12\x1c.modal.client.AppListRequest\x1a\x1d.modal.client.AppListResponse\x12^\n\x0f\x41ppLookupObject\x12$.modal.client.AppLookupObjectRequest\x1a%.modal.client.AppLookupObjectResponse\x12L\n\tAppDeploy\x12\x1e.modal.client.AppDeployRequest\x1a\x1f.modal.client.AppDeployResponse\x12p\n\x15\x41ppDeploySingleObject\x12*.modal.client.AppDeploySingleObjectRequest\x1a+.modal.client.AppDeploySingleObjectResponse\x12s\n\x16\x41ppGetByDeploymentName\x12+.modal.client.AppGetByDeploymentNameRequest\x1a,.modal.client.AppGetByDeploymentNameResponse\x12?\n\x07\x41ppStop\x12\x1c.modal.client.AppStopRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0c\x41ppHeartbeat\x12!.modal.client.AppHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12O\n\nBlobCreate\x12\x1f.modal.client.BlobCreateRequest\x1a .modal.client.BlobCreateResponse\x12\x46\n\x07\x42lobGet\x12\x1c.modal.client.BlobGetRequest\x1a\x1d.modal.client.BlobGetResponse\x12R\n\x0b\x43lassCreate\x12 .modal.client.ClassCreateRequest\x1a!.modal.client.ClassCreateResponse\x12I\n\x08\x43lassGet\x12\x1d.modal.client.ClassGetRequest\x1a\x1e.modal.client.ClassGetResponse\x12U\n\x0c\x43lientCreate\x12!.modal.client.ClientCreateRequest\x1a\".modal.client.ClientCreateResponse\x12H\n\x0b\x43lientHello\x12\x16.google.protobuf.Empty\x1a!.modal.client.ClientHelloResponse\x12O\n\x0f\x43lientHeartbeat\x12$.modal.client.ClientHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12g\n\x12\x43ontainerHeartbeat\x12\'.modal.client.ContainerHeartbeatRequest\x1a(.modal.client.ContainerHeartbeatResponse\x12X\n\rContainerExec\x12\".modal.client.ContainerExecRequest\x1a#.modal.client.ContainerExecResponse\x12i\n\x16\x43ontainerExecGetOutput\x12+.modal.client.ContainerExecGetOutputRequest\x1a .modal.client.RuntimeOutputBatch0\x01\x12[\n\x15\x43ontainerExecPutInput\x12*.modal.client.ContainerExecPutInputRequest\x1a\x16.google.protobuf.Empty\x12W\n\x13\x43ontainerCheckpoint\x12(.modal.client.ContainerCheckpointRequest\x1a\x16.google.protobuf.Empty\x12\x43\n\tDictClear\x12\x1e.modal.client.DictClearRequest\x1a\x16.google.protobuf.Empty\x12O\n\nDictCreate\x12\x1f.modal.client.DictCreateRequest\x1a .modal.client.DictCreateResponse\x12^\n\x0f\x44ictGetOrCreate\x12$.modal.client.DictGetOrCreateRequest\x1a%.modal.client.DictGetOrCreateResponse\x12K\n\rDictHeartbeat\x12\".modal.client.DictHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12L\n\x0c\x44ictContents\x12!.modal.client.DictContentsRequest\x1a\x17.modal.client.DictEntry0\x01\x12O\n\nDictUpdate\x12\x1f.modal.client.DictUpdateRequest\x1a .modal.client.DictUpdateResponse\x12\x46\n\x07\x44ictGet\x12\x1c.modal.client.DictGetRequest\x1a\x1d.modal.client.DictGetResponse\x12\x46\n\x07\x44ictPop\x12\x1c.modal.client.DictPopRequest\x1a\x1d.modal.client.DictPopResponse\x12U\n\x0c\x44ictContains\x12!.modal.client.DictContainsRequest\x1a\".modal.client.DictContainsResponse\x12\x46\n\x07\x44ictLen\x12\x1c.modal.client.DictLenRequest\x1a\x1d.modal.client.DictLenResponse\x12U\n\x0c\x44omainCreate\x12!.modal.client.DomainCreateRequest\x1a\".modal.client.DomainCreateResponse\x12O\n\nDomainList\x12\x1f.modal.client.DomainListRequest\x1a .modal.client.DomainListResponse\x12v\n\x17\x44omainCertificateVerify\x12,.modal.client.DomainCertificateVerifyRequest\x1a-.modal.client.DomainCertificateVerifyResponse\x12S\n\x11\x45nvironmentCreate\x12&.modal.client.EnvironmentCreateRequest\x1a\x16.google.protobuf.Empty\x12P\n\x0f\x45nvironmentList\x12\x16.google.protobuf.Empty\x1a%.modal.client.EnvironmentListResponse\x12S\n\x11\x45nvironmentDelete\x12&.modal.client.EnvironmentDeleteRequest\x1a\x16.google.protobuf.Empty\x12^\n\x11\x45nvironmentUpdate\x12&.modal.client.EnvironmentUpdateRequest\x1a!.modal.client.EnvironmentListItem\x12g\n\x12\x46unctionBindParams\x12\'.modal.client.FunctionBindParamsRequest\x1a(.modal.client.FunctionBindParamsResponse\x12[\n\x0e\x46unctionCreate\x12#.modal.client.FunctionCreateRequest\x1a$.modal.client.FunctionCreateResponse\x12R\n\x0b\x46unctionGet\x12 .modal.client.FunctionGetRequest\x1a!.modal.client.FunctionGetResponse\x12m\n\x14\x46unctionGetCallGraph\x12).modal.client.FunctionGetCallGraphRequest\x1a*.modal.client.FunctionGetCallGraphResponse\x12\x64\n\x17\x46unctionGetCurrentStats\x12,.modal.client.FunctionGetCurrentStatsRequest\x1a\x1b.modal.client.FunctionStats\x12\x64\n\x11\x46unctionGetInputs\x12&.modal.client.FunctionGetInputsRequest\x1a\'.modal.client.FunctionGetInputsResponse\x12g\n\x12\x46unctionGetOutputs\x12\'.modal.client.FunctionGetOutputsRequest\x1a(.modal.client.FunctionGetOutputsResponse\x12p\n\x15\x46unctionGetSerialized\x12*.modal.client.FunctionGetSerializedRequest\x1a+.modal.client.FunctionGetSerializedResponse\x12R\n\x0b\x46unctionMap\x12 .modal.client.FunctionMapRequest\x1a!.modal.client.FunctionMapResponse\x12\x64\n\x11\x46unctionPrecreate\x12&.modal.client.FunctionPrecreateRequest\x1a\'.modal.client.FunctionPrecreateResponse\x12\x64\n\x11\x46unctionPutInputs\x12&.modal.client.FunctionPutInputsRequest\x1a\'.modal.client.FunctionPutInputsResponse\x12U\n\x12\x46unctionPutOutputs\x12\'.modal.client.FunctionPutOutputsRequest\x1a\x16.google.protobuf.Empty\x12\x8b\x01\n\x1e\x46unctionUpdateSchedulingParams\x12\x33.modal.client.FunctionUpdateSchedulingParamsRequest\x1a\x34.modal.client.FunctionUpdateSchedulingParamsResponse\x12U\n\x12\x46unctionCallCancel\x12\'.modal.client.FunctionCallCancelRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10\x46unctionCallList\x12%.modal.client.FunctionCallListRequest\x1a&.modal.client.FunctionCallListResponse\x12\\\n\x15\x46unctionCallGetDataIn\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12]\n\x16\x46unctionCallGetDataOut\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12Z\n\x16\x46unctionCallPutDataOut\x12(.modal.client.FunctionCallPutDataRequest\x1a\x16.google.protobuf.Empty\x12G\n\x15\x46unctionStartPtyShell\x12\x16.google.protobuf.Empty\x1a\x16.google.protobuf.Empty\x12\x61\n\x10ImageGetOrCreate\x12%.modal.client.ImageGetOrCreateRequest\x1a&.modal.client.ImageGetOrCreateResponse\x12i\n\x12ImageJoinStreaming\x12\'.modal.client.ImageJoinStreamingRequest\x1a(.modal.client.ImageJoinStreamingResponse0\x01\x12U\n\x0cMountPutFile\x12!.modal.client.MountPutFileRequest\x1a\".modal.client.MountPutFileResponse\x12O\n\nMountBuild\x12\x1f.modal.client.MountBuildRequest\x1a .modal.client.MountBuildResponse\x12\x61\n\x10MountGetOrCreate\x12%.modal.client.MountGetOrCreateRequest\x1a&.modal.client.MountGetOrCreateResponse\x12\x61\n\x10ProxyGetOrCreate\x12%.modal.client.ProxyGetOrCreateRequest\x1a&.modal.client.ProxyGetOrCreateResponse\x12R\n\x0bQueueCreate\x12 .modal.client.QueueCreateRequest\x1a!.modal.client.QueueCreateResponse\x12\x61\n\x10QueueGetOrCreate\x12%.modal.client.QueueGetOrCreateRequest\x1a&.modal.client.QueueGetOrCreateResponse\x12I\n\x08QueueGet\x12\x1d.modal.client.QueueGetRequest\x1a\x1e.modal.client.QueueGetResponse\x12M\n\x0eQueueHeartbeat\x12#.modal.client.QueueHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x41\n\x08QueuePut\x12\x1d.modal.client.QueuePutRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08QueueLen\x12\x1d.modal.client.QueueLenRequest\x1a\x1e.modal.client.QueueLenResponse\x12[\n\x0eQueueNextItems\x12#.modal.client.QueueNextItemsRequest\x1a$.modal.client.QueueNextItemsResponse\x12X\n\rSandboxCreate\x12\".modal.client.SandboxCreateRequest\x1a#.modal.client.SandboxCreateResponse\x12\x61\n\x10SandboxGetTaskId\x12%.modal.client.SandboxGetTaskIdRequest\x1a&.modal.client.SandboxGetTaskIdResponse\x12T\n\x0eSandboxGetLogs\x12#.modal.client.SandboxGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12R\n\x0bSandboxWait\x12 .modal.client.SandboxWaitRequest\x1a!.modal.client.SandboxWaitResponse\x12R\n\x0bSandboxList\x12 .modal.client.SandboxListRequest\x1a!.modal.client.SandboxListResponse\x12\x61\n\x10SandboxTerminate\x12%.modal.client.SandboxTerminateRequest\x1a&.modal.client.SandboxTerminateResponse\x12\x64\n\x11SandboxStdinWrite\x12&.modal.client.SandboxStdinWriteRequest\x1a\'.modal.client.SandboxStdinWriteResponse\x12U\n\x0cSecretCreate\x12!.modal.client.SecretCreateRequest\x1a\".modal.client.SecretCreateResponse\x12\x64\n\x11SecretGetOrCreate\x12&.modal.client.SecretGetOrCreateRequest\x1a\'.modal.client.SecretGetOrCreateResponse\x12O\n\nSecretList\x12\x1f.modal.client.SecretListRequest\x1a .modal.client.SecretListResponse\x12v\n\x17SharedVolumeGetOrCreate\x12,.modal.client.SharedVolumeGetOrCreateRequest\x1a-.modal.client.SharedVolumeGetOrCreateResponse\x12g\n\x12SharedVolumeCreate\x12\'.modal.client.SharedVolumeCreateRequest\x1a(.modal.client.SharedVolumeCreateResponse\x12[\n\x15SharedVolumeHeartbeat\x12*.modal.client.SharedVolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10SharedVolumeList\x12%.modal.client.SharedVolumeListRequest\x1a&.modal.client.SharedVolumeListResponse\x12p\n\x15SharedVolumeListFiles\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse\x12x\n\x1bSharedVolumeListFilesStream\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse0\x01\x12j\n\x13SharedVolumePutFile\x12(.modal.client.SharedVolumePutFileRequest\x1a).modal.client.SharedVolumePutFileResponse\x12j\n\x13SharedVolumeGetFile\x12(.modal.client.SharedVolumeGetFileRequest\x1a).modal.client.SharedVolumeGetFileResponse\x12]\n\x16SharedVolumeRemoveFile\x12+.modal.client.SharedVolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12\x45\n\nTaskResult\x12\x1f.modal.client.TaskResultRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08TaskList\x12\x1d.modal.client.TaskListRequest\x1a\x1e.modal.client.TaskListResponse\x12T\n\x11TaskCurrentInputs\x12\x16.google.protobuf.Empty\x1a\'.modal.client.TaskCurrentInputsResponse\x12^\n\x0fTokenFlowCreate\x12$.modal.client.TokenFlowCreateRequest\x1a%.modal.client.TokenFlowCreateResponse\x12X\n\rTokenFlowWait\x12\".modal.client.TokenFlowWaitRequest\x1a#.modal.client.TokenFlowWaitResponse\x12R\n\x0bTunnelStart\x12 .modal.client.TunnelStartRequest\x1a!.modal.client.TunnelStartResponse\x12O\n\nTunnelStop\x12\x1f.modal.client.TunnelStopRequest\x1a .modal.client.TunnelStopResponse\x12\x64\n\x11VolumeGetOrCreate\x12&.modal.client.VolumeGetOrCreateRequest\x1a\'.modal.client.VolumeGetOrCreateResponse\x12U\n\x0cVolumeCreate\x12!.modal.client.VolumeCreateRequest\x1a\".modal.client.VolumeCreateResponse\x12O\n\x0fVolumeHeartbeat\x12$.modal.client.VolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12U\n\x0cVolumeCommit\x12!.modal.client.VolumeCommitRequest\x1a\".modal.client.VolumeCommitResponse\x12I\n\x0cVolumeDelete\x12!.modal.client.VolumeDeleteRequest\x1a\x16.google.protobuf.Empty\x12X\n\rVolumeGetFile\x12\".modal.client.VolumeGetFileRequest\x1a#.modal.client.VolumeGetFileResponse\x12O\n\nVolumeList\x12\x1f.modal.client.VolumeListRequest\x1a .modal.client.VolumeListResponse\x12`\n\x0fVolumeListFiles\x12$.modal.client.VolumeListFilesRequest\x1a%.modal.client.VolumeListFilesResponse0\x01\x12M\n\x0eVolumePutFiles\x12#.modal.client.VolumePutFilesRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0cVolumeReload\x12!.modal.client.VolumeReloadRequest\x1a\x16.google.protobuf.Empty\x12Q\n\x10VolumeRemoveFile\x12%.modal.client.VolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12O\n\x0fVolumeCopyFiles\x12$.modal.client.VolumeCopyFilesRequest\x1a\x16.google.protobuf.Empty\x12X\n\x13WorkspaceNameLookup\x12\x16.google.protobuf.Empty\x1a).modal.client.WorkspaceNameLookupResponseb\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15modal_proto/api.proto\x12\x0cmodal.client\x1a\x19modal_proto/options.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1egoogle/protobuf/wrappers.proto\"\xec\x01\n\x10\x43loudBucketMount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\x12\x16\n\x0erequester_pays\x18\x06 \x01(\x08\x12>\n\x0b\x62ucket_type\x18\x05 \x01(\x0e\x32).modal.client.CloudBucketMount.BucketType\"%\n\nBucketType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x06\n\x02S3\x10\x01\"r\n\x1a\x41ppClientDisconnectRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x31\n\x06reason\x18\x02 \x01(\x0e\x32!.modal.client.AppDisconnectReason\x12\x11\n\texception\x18\x03 \x01(\t\"\xb3\x01\n\x10\x41ppCreateRequest\x12\x17\n\tclient_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x13\n\x0b\x64\x65scription\x18\x02 \x01(\t\x12\x12\n\x06\x64\x65tach\x18\x03 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x0cinitializing\x18\x04 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x10\x65nvironment_name\x18\x05 \x01(\t\x12)\n\tapp_state\x18\x06 \x01(\x0e\x32\x16.modal.client.AppState\"9\n\x11\x41ppCreateResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x14\n\x0c\x61pp_logs_url\x18\x02 \x01(\t\"S\n\x0e\x41ppStopRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12+\n\x06source\x18\x02 \x01(\x0e\x32\x1b.modal.client.AppStopSource\"\xba\x01\n\x10\x41ppDeployRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x15\n\robject_entity\x18\x04 \x01(\t\x12\x35\n\nvisibility\x18\x05 \x01(\x0e\x32!.modal.client.AppDeployVisibility\" \n\x11\x41ppDeployResponse\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x8f\x01\n\x1c\x41ppDeploySingleObjectRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12\x11\n\tobject_id\x18\x04 \x01(\t\"/\n\x1d\x41ppDeploySingleObjectResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"}\n\x1d\x41ppGetByDeploymentNameRequest\x12\x34\n\tnamespace\x18\x01 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"0\n\x1e\x41ppGetByDeploymentNameResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"\xba\x01\n\x11\x41ppGetLogsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x05 \x01(\t\x12\x10\n\x08input_id\x18\x06 \x01(\t\x12\x0f\n\x07task_id\x18\x07 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\"A\n\x14\x41ppGetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11include_unindexed\x18\x02 \x01(\x08\"F\n\x11\x41ppGetObjectsItem\x12\x0b\n\x03tag\x18\x01 \x01(\t\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\"G\n\x15\x41ppGetObjectsResponse\x12.\n\x05items\x18\x02 \x03(\x0b\x32\x1f.modal.client.AppGetObjectsItem\"%\n\x13\x41ppHeartbeatRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"*\n\x0e\x41ppListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"7\n\x0f\x41ppListResponse\x12$\n\x04\x61pps\x18\x01 \x03(\x0b\x32\x16.modal.client.AppStats\"\xb8\x01\n\x16\x41ppLookupObjectRequest\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x10\n\x08\x61pp_name\x18\x03 \x01(\t\x12\x12\n\nobject_tag\x18\x04 \x01(\t\x12\x11\n\tobject_id\x18\x05 \x01(\t\x12\x15\n\robject_entity\x18\x06 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x07 \x01(\t\"O\n\x17\x41ppLookupObjectResponse\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\x12\x0e\n\x06\x61pp_id\x18\x07 \x01(\t\"\xaf\x02\n\x14\x41ppSetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12T\n\x12indexed_object_ids\x18\x02 \x03(\x0b\x32\x38.modal.client.AppSetObjectsRequest.IndexedObjectIdsEntry\x12\x11\n\tclient_id\x18\x03 \x01(\t\x12\x1c\n\x14unindexed_object_ids\x18\x04 \x03(\t\x12-\n\rnew_app_state\x18\x05 \x01(\x0e\x32\x16.modal.client.AppState\x12\x18\n\x10single_object_id\x18\x06 \x01(\t\x1a\x37\n\x15IndexedObjectIdsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\xd1\x01\n\x08\x41ppStats\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64\x65scription\x18\x03 \x01(\t\x12%\n\x05state\x18\x04 \x01(\x0e\x32\x16.modal.client.AppState\x12\x12\n\ncreated_at\x18\x05 \x01(\x01\x12\x12\n\nstopped_at\x18\x06 \x01(\x01\x12\x17\n\x0fn_running_tasks\x18\x08 \x01(\x05\x12\x15\n\robject_entity\x18\t \x01(\t\x12\x0c\n\x04name\x18\n \x01(\t\x12\x13\n\x0b\x64\x65ployed_at\x18\x0b \x01(\x01\"\xa3\x0e\n\x04\x41sgi\x12\'\n\x04http\x18\x01 \x01(\x0b\x32\x17.modal.client.Asgi.HttpH\x00\x12\x36\n\x0chttp_request\x18\x02 \x01(\x0b\x32\x1e.modal.client.Asgi.HttpRequestH\x00\x12\x43\n\x13http_response_start\x18\x03 \x01(\x0b\x32$.modal.client.Asgi.HttpResponseStartH\x00\x12\x41\n\x12http_response_body\x18\x04 \x01(\x0b\x32#.modal.client.Asgi.HttpResponseBodyH\x00\x12I\n\x16http_response_trailers\x18\x05 \x01(\x0b\x32\'.modal.client.Asgi.HttpResponseTrailersH\x00\x12<\n\x0fhttp_disconnect\x18\x06 \x01(\x0b\x32!.modal.client.Asgi.HttpDisconnectH\x00\x12\x31\n\twebsocket\x18\x07 \x01(\x0b\x32\x1c.modal.client.Asgi.WebsocketH\x00\x12@\n\x11websocket_connect\x18\x08 \x01(\x0b\x32#.modal.client.Asgi.WebsocketConnectH\x00\x12>\n\x10websocket_accept\x18\t \x01(\x0b\x32\".modal.client.Asgi.WebsocketAcceptH\x00\x12@\n\x11websocket_receive\x18\n \x01(\x0b\x32#.modal.client.Asgi.WebsocketReceiveH\x00\x12:\n\x0ewebsocket_send\x18\x0b \x01(\x0b\x32 .modal.client.Asgi.WebsocketSendH\x00\x12\x46\n\x14websocket_disconnect\x18\x0c \x01(\x0b\x32&.modal.client.Asgi.WebsocketDisconnectH\x00\x12<\n\x0fwebsocket_close\x18\r \x01(\x0b\x32!.modal.client.Asgi.WebsocketCloseH\x00\x1a\xc5\x01\n\x04Http\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x0e\n\x06scheme\x18\x03 \x01(\t\x12\x0c\n\x04path\x18\x04 \x01(\t\x12\x14\n\x0cquery_string\x18\x05 \x01(\x0c\x12\x0f\n\x07headers\x18\x06 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x07 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x08 \x01(\rH\x01\x88\x01\x01\x42\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a.\n\x0bHttpRequest\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a\x46\n\x11HttpResponseStart\x12\x0e\n\x06status\x18\x01 \x01(\r\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x12\x10\n\x08trailers\x18\x03 \x01(\x08\x1a\x33\n\x10HttpResponseBody\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a>\n\x14HttpResponseTrailers\x12\x0f\n\x07headers\x18\x01 \x03(\x0c\x12\x15\n\rmore_trailers\x18\x02 \x01(\x08\x1a\x10\n\x0eHttpDisconnect\x1a\xd0\x01\n\tWebsocket\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06scheme\x18\x02 \x01(\t\x12\x0c\n\x04path\x18\x03 \x01(\t\x12\x14\n\x0cquery_string\x18\x04 \x01(\x0c\x12\x0f\n\x07headers\x18\x05 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x06 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x07 \x01(\rH\x01\x88\x01\x01\x12\x14\n\x0csubprotocols\x18\x08 \x03(\tB\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a\x12\n\x10WebsocketConnect\x1aL\n\x0fWebsocketAccept\x12\x18\n\x0bsubprotocol\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x42\x0e\n\x0c_subprotocol\x1a>\n\x10WebsocketReceive\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a;\n\rWebsocketSend\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a\x31\n\x13WebsocketDisconnect\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x42\x07\n\x05_code\x1a<\n\x0eWebsocketClose\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x12\x0e\n\x06reason\x18\x02 \x01(\tB\x07\n\x05_codeB\x06\n\x04type\"7\n\tBaseImage\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x12\n\ndocker_tag\x18\x02 \x01(\tJ\x04\x08\x04\x10\x05\"_\n\x11\x42lobCreateRequest\x12\x13\n\x0b\x63ontent_md5\x18\x01 \x01(\t\x12\x1d\n\x15\x63ontent_sha256_base64\x18\x02 \x01(\t\x12\x16\n\x0e\x63ontent_length\x18\x03 \x01(\x03\"\x84\x01\n\x12\x42lobCreateResponse\x12\x0f\n\x07\x62lob_id\x18\x02 \x01(\t\x12\x14\n\nupload_url\x18\x01 \x01(\tH\x00\x12\x32\n\tmultipart\x18\x03 \x01(\x0b\x32\x1d.modal.client.MultiPartUploadH\x00\x42\x13\n\x11upload_type_oneof\"!\n\x0e\x42lobGetRequest\x12\x0f\n\x07\x62lob_id\x18\x01 \x01(\t\"\'\n\x0f\x42lobGetResponse\x12\x14\n\x0c\x64ownload_url\x18\x01 \x01(\t\"i\n\x0e\x43heckpointInfo\x12\x10\n\x08\x63hecksum\x18\x01 \x01(\t\x12.\n\x06status\x18\x02 \x01(\x0e\x32\x1e.modal.client.CheckpointStatus\x12\x15\n\rcheckpoint_id\x18\x03 \x01(\t\"q\n\x12\x43lassCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_class_id\x18\x02 \x01(\t\x12*\n\x07methods\x18\x03 \x03(\x0b\x32\x19.modal.client.ClassMethod\"c\n\x13\x43lassCreateResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"\xb9\x01\n\x0f\x43lassGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\x12\x18\n\x10lookup_published\x18\x08 \x01(\x08\x12\x16\n\x0eworkspace_name\x18\t \x01(\t\"`\n\x10\x43lassGetResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"A\n\x13\x43lassHandleMetadata\x12*\n\x07methods\x18\x01 \x03(\x0b\x32\x19.modal.client.ClassMethod\"\x81\x01\n\x0b\x43lassMethod\x12\x15\n\rfunction_name\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x46\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"[\n\x13\x43lientCreateRequest\x12\x33\n\x0b\x63lient_type\x18\x01 \x01(\x0e\x32\x18.modal.client.ClientTypeB\x04\x80\xb5\x18\x01\x12\x0f\n\x07version\x18\x02 \x01(\t\"U\n\x14\x43lientCreateResponse\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12\x1b\n\x13\x64\x65precation_warning\x18\x03 \x01(\t\"E\n\x13\x43lientHelloResponse\x12\x0f\n\x07warning\x18\x01 \x01(\t\x12\x1d\n\x15image_builder_version\x18\x02 \x01(\t\"g\n\x16\x43lientHeartbeatRequest\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\x18\n\x10\x63urrent_input_id\x18\x03 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x04 \x01(\x01\"\x9f\x03\n\x12\x43ontainerArguments\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x04 \x01(\t\x12,\n\x0c\x66unction_def\x18\x07 \x01(\x0b\x32\x16.modal.client.Function\x12+\n\nproxy_info\x18\x08 \x01(\x0b\x32\x17.modal.client.ProxyInfo\x12M\n\x0ftracing_context\x18\t \x03(\x0b\x32\x34.modal.client.ContainerArguments.TracingContextEntry\x12\x19\n\x11serialized_params\x18\n \x01(\x0c\x12\x0f\n\x07runtime\x18\x0b \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\r \x01(\t\x12\x1a\n\rcheckpoint_id\x18\x0e \x01(\tH\x00\x88\x01\x01\x1a\x35\n\x13TracingContextEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x42\x10\n\x0e_checkpoint_id\"%\n\x10\x43\x61ncelInputEvent\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"t\n\x1a\x43ontainerHeartbeatResponse\x12?\n\x12\x63\x61ncel_input_event\x18\x01 \x01(\x0b\x32\x1e.modal.client.CancelInputEventH\x00\x88\x01\x01\x42\x15\n\x13_cancel_input_event\"\x85\x01\n\x19\x43ontainerHeartbeatRequest\x12\x18\n\x10\x63urrent_input_id\x18\x01 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x02 \x01(\x01\x12,\n$supports_graceful_input_cancellation\x18\x03 \x01(\x08\"3\n\x1a\x43ontainerCheckpointRequest\x12\x15\n\rcheckpoint_id\x18\x01 \x01(\t\"\x9d\x01\n\x14\x43ontainerExecRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0f\n\x07\x63ommand\x18\x02 \x03(\t\x12\'\n\x08pty_info\x18\x03 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12#\n\x1bterminate_container_on_exit\x18\x04 \x01(\x08\x12\x15\n\rruntime_debug\x18\x05 \x01(\x08\"[\n\x1d\x43ontainerExecGetOutputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x18\n\x10last_batch_index\x18\x03 \x01(\x04\"a\n\x1c\x43ontainerExecPutInputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x30\n\x05input\x18\x02 \x01(\x0b\x32!.modal.client.RuntimeInputMessage\"(\n\x15\x43ontainerExecResponse\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\"\"\n\x12\x43ustomDomainConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\"\x1f\n\x10\x43ustomDomainInfo\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x7f\n\tDataChunk\x12-\n\x0b\x64\x61ta_format\x18\x01 \x01(\x0e\x32\x18.modal.client.DataFormat\x12\x0e\n\x04\x64\x61ta\x18\x02 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x03 \x01(\tH\x00\x12\r\n\x05index\x18\x04 \x01(\x04\x42\x0c\n\ndata_oneof\"#\n\x10\x44ictClearRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"3\n\x13\x44ictContainsRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\"%\n\x14\x44ictContainsResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\"j\n\x11\x44ictCreateRequest\x12%\n\x04\x64\x61ta\x18\x01 \x03(\x0b\x32\x17.modal.client.DictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x18\n\x10\x65xisting_dict_id\x18\x03 \x01(\t\"%\n\x12\x44ictCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\xe8\x01\n\x16\x44ictGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12%\n\x04\x64\x61ta\x18\x05 \x03(\x0b\x32\x17.modal.client.DictEntry\"*\n\x17\x44ictGetOrCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\'\n\tDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\x0c\x12\r\n\x05value\x18\x02 \x01(\x0c\".\n\x0e\x44ictGetRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictGetResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"\'\n\x14\x44ictHeartbeatRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"!\n\x0e\x44ictLenRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\x1e\n\x0f\x44ictLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\".\n\x0e\x44ictPopRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictPopResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"N\n\x11\x44ictUpdateRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12(\n\x07updates\x18\x02 \x03(\x0b\x32\x17.modal.client.DictEntry\"\x14\n\x12\x44ictUpdateResponse\"S\n\tDNSRecord\x12)\n\x04type\x18\x01 \x01(\x0e\x32\x1b.modal.client.DNSRecordType\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\r\n\x05value\x18\x03 \x01(\t\"0\n\x13\x44omainCreateRequest\x12\x19\n\x0b\x64omain_name\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"W\n\x14\x44omainCreateResponse\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12,\n\x0b\x64ns_records\x18\x02 \x03(\x0b\x32\x17.modal.client.DNSRecord\"\x13\n\x11\x44omainListRequest\"\xaf\x01\n\x06\x44omain\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64omain_name\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12;\n\x12\x63\x65rtificate_status\x18\x04 \x01(\x0e\x32\x1f.modal.client.CertificateStatus\x12,\n\x0b\x64ns_records\x18\x05 \x03(\x0b\x32\x17.modal.client.DNSRecord\";\n\x12\x44omainListResponse\x12%\n\x07\x64omains\x18\x01 \x03(\x0b\x32\x14.modal.client.Domain\"3\n\x1e\x44omainCertificateVerifyRequest\x12\x11\n\tdomain_id\x18\x01 \x01(\t\"G\n\x1f\x44omainCertificateVerifyResponse\x12$\n\x06\x64omain\x18\x01 \x01(\x0b\x32\x14.modal.client.Domain\"(\n\x18\x45nvironmentCreateRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"(\n\x18\x45nvironmentDeleteRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\";\n\x13\x45nvironmentListItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x16\n\x0ewebhook_suffix\x18\x02 \x01(\t\"K\n\x17\x45nvironmentListResponse\x12\x30\n\x05items\x18\x02 \x03(\x0b\x32!.modal.client.EnvironmentListItem\"\x8e\x01\n\x18\x45nvironmentUpdateRequest\x12\x14\n\x0c\x63urrent_name\x18\x01 \x01(\t\x12*\n\x04name\x18\x02 \x01(\x0b\x32\x1c.google.protobuf.StringValue\x12\x30\n\nweb_suffix\x18\x03 \x01(\x0b\x32\x1c.google.protobuf.StringValue\"d\n\x1a\x46unctionCallPutDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12,\n\x0b\x64\x61ta_chunks\x18\x02 \x03(\x0b\x32\x17.modal.client.DataChunk\"J\n\x1a\x46unctionCallGetDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nlast_index\x18\x02 \x01(\x04\"\xf4\x0e\n\x08\x46unction\x12\x13\n\x0bmodule_name\x18\x01 \x01(\t\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12\x11\n\tmount_ids\x18\x03 \x03(\t\x12\x10\n\x08image_id\x18\x04 \x01(\t\x12\x1b\n\x13\x66unction_serialized\x18\x06 \x01(\x0c\x12>\n\x0f\x64\x65\x66inition_type\x18\x07 \x01(\x0e\x32%.modal.client.Function.DefinitionType\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12*\n\tresources\x18\t \x01(\x0b\x32\x17.modal.client.Resources\x12\x12\n\nsecret_ids\x18\n \x03(\t\x12+\n\nrate_limit\x18\x0b \x01(\x0b\x32\x17.modal.client.RateLimit\x12\x33\n\x0ewebhook_config\x18\x0f \x01(\x0b\x32\x1b.modal.client.WebhookConfig\x12=\n\x14shared_volume_mounts\x18\x10 \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\x08proxy_id\x18\x11 \x01(\tH\x00\x88\x01\x01\x12\x37\n\x0cretry_policy\x18\x12 \x01(\x0b\x32!.modal.client.FunctionRetryPolicy\x12\x19\n\x11\x63oncurrency_limit\x18\x13 \x01(\r\x12\x11\n\tkeep_warm\x18\x14 \x01(\x08\x12\x14\n\x0ctimeout_secs\x18\x15 \x01(\r\x12\'\n\x08pty_info\x18\x16 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12\x18\n\x10\x63lass_serialized\x18\x17 \x01(\x0c\x12\x1e\n\x16task_idle_timeout_secs\x18\x19 \x01(\r\x12\x38\n\x0e\x63loud_provider\x18\x1a \x01(\x0e\x32\x1b.modal.client.CloudProviderH\x01\x88\x01\x01\x12\x16\n\x0ewarm_pool_size\x18\x1b \x01(\r\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12.\n\x0cweb_url_info\x18\x1d \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12\x0f\n\x07runtime\x18\x1e \x01(\t\x12\x11\n\tstub_name\x18\x1f \x01(\t\x12\x30\n\rvolume_mounts\x18! \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\x1f\n\x17\x61llow_concurrent_inputs\x18\" \x01(\r\x12:\n\x12\x63ustom_domain_info\x18# \x03(\x0b\x32\x1e.modal.client.CustomDomainInfo\x12\x11\n\tworker_id\x18$ \x01(\t\x12\x15\n\rruntime_debug\x18% \x01(\x08\x12\x1b\n\x13is_builder_function\x18  \x01(\x08\x12\x18\n\x10is_auto_snapshot\x18& \x01(\x08\x12\x11\n\tis_method\x18\' \x01(\x08\x12!\n\x19is_checkpointing_function\x18( \x01(\x08\x12\x1d\n\x15\x63heckpointing_enabled\x18) \x01(\x08\x12\x30\n\ncheckpoint\x18* \x01(\x0b\x32\x1c.modal.client.CheckpointInfo\x12;\n\x13object_dependencies\x18+ \x03(\x0b\x32\x1e.modal.client.ObjectDependency\x12\x15\n\rblock_network\x18, \x01(\x08\x12\x12\n\nmax_inputs\x18. \x01(\r\x12(\n\ts3_mounts\x18/ \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x33 \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x1b\n\x13_experimental_boost\x18\x30 \x01(\x08\x12\x1f\n\x17_experimental_scheduler\x18\x31 \x01(\x08\x12P\n!_experimental_scheduler_placement\x18\x32 \x01(\x0b\x32 .modal.client.SchedulerPlacementH\x02\x88\x01\x01\"k\n\x0e\x44\x65\x66initionType\x12\x1f\n\x1b\x44\x45\x46INITION_TYPE_UNSPECIFIED\x10\x00\x12\x1e\n\x1a\x44\x45\x46INITION_TYPE_SERIALIZED\x10\x01\x12\x18\n\x14\x44\x45\x46INITION_TYPE_FILE\x10\x02\"f\n\x0c\x46unctionType\x12\x1d\n\x19\x46UNCTION_TYPE_UNSPECIFIED\x10\x00\x12\x1b\n\x17\x46UNCTION_TYPE_GENERATOR\x10\x01\x12\x1a\n\x16\x46UNCTION_TYPE_FUNCTION\x10\x02\x42\x0b\n\t_proxy_idB\x11\n\x0f_cloud_providerB$\n\"X_experimental_scheduler_placement\"|\n\x12SchedulerPlacement\x12\x14\n\x07_region\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x12\n\x05_zone\x18\x02 \x01(\tH\x01\x88\x01\x01\x12\x17\n\n_lifecycle\x18\x03 \x01(\tH\x02\x88\x01\x01\x42\n\n\x08X_regionB\x08\n\x06X_zoneB\r\n\x0bX_lifecycle\"\x8f\x01\n\x16\x46unctionHandleMetadata\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12\x11\n\tis_method\x18\' \x01(\x08\"\x9f\x01\n\x15\x46unctionCreateRequest\x12(\n\x08\x66unction\x18\x01 \x01(\x0b\x32\x16.modal.client.Function\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12(\n\x08schedule\x18\x06 \x01(\x0b\x32\x16.modal.client.Schedule\x12\x1c\n\x14\x65xisting_function_id\x18\x07 \x01(\t\"\xc7\x04\n\x0f\x46unctionOptions\x12\x12\n\nsecret_ids\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12/\n\tresources\x18\x03 \x01(\x0b\x32\x17.modal.client.ResourcesH\x00\x88\x01\x01\x12<\n\x0cretry_policy\x18\x04 \x01(\x0b\x32!.modal.client.FunctionRetryPolicyH\x01\x88\x01\x01\x12\x1e\n\x11\x63oncurrency_limit\x18\x05 \x01(\rH\x02\x88\x01\x01\x12\x19\n\x0ctimeout_secs\x18\x06 \x01(\rH\x03\x88\x01\x01\x12#\n\x16task_idle_timeout_secs\x18\x07 \x01(\rH\x04\x88\x01\x01\x12\x1b\n\x0ewarm_pool_size\x18\x08 \x01(\rH\x05\x88\x01\x01\x12\x30\n\rvolume_mounts\x18\t \x03(\x0b\x32\x19.modal.client.VolumeMount\x12$\n\x17\x61llow_concurrent_inputs\x18\n \x01(\rH\x06\x88\x01\x01\x12\x1d\n\x15replace_volume_mounts\x18\x0b \x01(\x08\x12\x1a\n\x12replace_secret_ids\x18\x0c \x01(\x08\x42\x0c\n\n_resourcesB\x0f\n\r_retry_policyB\x14\n\x12_concurrency_limitB\x0f\n\r_timeout_secsB\x19\n\x17_task_idle_timeout_secsB\x11\n\x0f_warm_pool_sizeB\x1a\n\x18_allow_concurrent_inputs\"\xd6\x01\n\x18\x46unctionPrecreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x1b\n\rfunction_name\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x1c\n\x14\x65xisting_function_id\x18\x03 \x01(\t\x12:\n\rfunction_type\x18\x04 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x33\n\x0ewebhook_config\x18\x05 \x01(\x0b\x32\x1b.modal.client.WebhookConfig\"o\n\x19\x46unctionPrecreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x9e\x01\n\x19\x46unctionBindParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x19\n\x11serialized_params\x18\x02 \x01(\x0c\x12\x37\n\x10\x66unction_options\x18\x03 \x01(\x0b\x32\x1d.modal.client.FunctionOptions\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"v\n\x1a\x46unctionBindParamsResponse\x12\x19\n\x11\x62ound_function_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\xd7\x01\n\x16\x46unctionCreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12.\n\x0cweb_url_info\x18\x03 \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12(\n\x08\x66unction\x18\x04 \x01(\x0b\x32\x16.modal.client.Function\x12=\n\x0fhandle_metadata\x18\x05 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x8a\x01\n\x12\x46unctionGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"i\n\x13\x46unctionGetResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"]\n%FunctionUpdateSchedulingParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x1f\n\x17warm_pool_size_override\x18\x02 \x01(\r\"(\n&FunctionUpdateSchedulingParamsResponse\"\x8a\x01\n\x15\x46unctionGetInputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\x12\x13\n\x0bkill_switch\x18\x03 \x01(\x08\x12\x18\n\x10\x66unction_call_id\x18\x05 \x01(\tJ\x04\x08\x04\x10\x05\"y\n\x18\x46unctionGetInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x03 \x01(\x05\x12\x19\n\x11\x61verage_call_time\x18\x05 \x01(\x02\x12\x19\n\x11input_concurrency\x18\x06 \x01(\x05\"s\n\x19\x46unctionGetInputsResponse\x12\x33\n\x06inputs\x18\x03 \x03(\x0b\x32#.modal.client.FunctionGetInputsItem\x12!\n\x19rate_limit_sleep_duration\x18\x04 \x01(\x02\"\xa6\x01\n\x16\x46unctionGetOutputsItem\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x10\n\x08input_id\x18\x03 \x01(\t\x12\x11\n\tgen_index\x18\x04 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x05 \x01(\x0e\x32\x18.modal.client.DataFormat\"\x8b\x01\n\x19\x46unctionGetOutputsRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x02 \x01(\x05\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x06 \x01(\t\x12\x18\n\x10\x63lear_on_success\x18\x07 \x01(\x08\"x\n\x1a\x46unctionGetOutputsResponse\x12\x0c\n\x04idxs\x18\x03 \x03(\x05\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionGetOutputsItem\x12\x15\n\rlast_entry_id\x18\x05 \x01(\t\"3\n\x1c\x46unctionGetSerializedRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"V\n\x1d\x46unctionGetSerializedResponse\x12\x1b\n\x13\x66unction_serialized\x18\x01 \x01(\x0c\x12\x18\n\x10\x63lass_serialized\x18\x02 \x01(\x0c\"\x89\x01\n\rFunctionInput\x12\x0e\n\x04\x61rgs\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x61rgs_blob_id\x18\x07 \x01(\tH\x00\x12\x13\n\x0b\x66inal_input\x18\t \x01(\x08\x12-\n\x0b\x64\x61ta_format\x18\n \x01(\x0e\x32\x18.modal.client.DataFormatB\x0c\n\nargs_oneof\"\xd8\x01\n\x12\x46unctionMapRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x19\n\x11return_exceptions\x18\x03 \x01(\x08\x12:\n\x12\x66unction_call_type\x18\x04 \x01(\x0e\x32\x1e.modal.client.FunctionCallType\x12=\n\x10pipelined_inputs\x18\x05 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\"v\n\x13\x46unctionMapResponse\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x45\n\x10pipelined_inputs\x18\x02 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"P\n\x15\x46unctionPutInputsItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"~\n\x18\x46unctionPutInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x33\n\x06inputs\x18\x04 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\">\n\x1d\x46unctionPutInputsResponseItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12\x10\n\x08input_id\x18\x02 \x01(\t\"X\n\x19\x46unctionPutInputsResponse\x12;\n\x06inputs\x18\x01 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"\xce\x01\n\x16\x46unctionPutOutputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x18\n\x10input_started_at\x18\x03 \x01(\x01\x12\x19\n\x11output_created_at\x18\x04 \x01(\x01\x12\x11\n\tgen_index\x18\x06 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x07 \x01(\x0e\x32\x18.modal.client.DataFormat\"R\n\x19\x46unctionPutOutputsRequest\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionPutOutputsItem\"s\n\x13\x46unctionRetryPolicy\x12\x1b\n\x13\x62\x61\x63koff_coefficient\x18\x01 \x01(\x02\x12\x18\n\x10initial_delay_ms\x18\x02 \x01(\r\x12\x14\n\x0cmax_delay_ms\x18\x03 \x01(\r\x12\x0f\n\x07retries\x18\x12 \x01(\r\"7\n\x1b\x46unctionGetCallGraphRequest\x12\x18\n\x10\x66unction_call_id\x18\x02 \x01(\t\"\x8c\x01\n\x12InputCallGraphInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x39\n\x06status\x18\x02 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x0f\n\x07task_id\x18\x04 \x01(\t\"z\n\x19\x46unctionCallCallGraphInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x15\n\rfunction_name\x18\x03 \x01(\t\x12\x13\n\x0bmodule_name\x18\x04 \x01(\t\"\x91\x01\n\x1c\x46unctionGetCallGraphResponse\x12\x30\n\x06inputs\x18\x01 \x03(\x0b\x32 .modal.client.InputCallGraphInfo\x12?\n\x0e\x66unction_calls\x18\x02 \x03(\x0b\x32\'.modal.client.FunctionCallCallGraphInfo\"5\n\x19\x46unctionCallCancelRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\".\n\x17\x46unctionCallListRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"\xc3\x03\n\x10\x46unctionCallInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x12\n\ncreated_at\x18\x06 \x01(\x01\x12\x14\n\x0cscheduled_at\x18\x07 \x01(\x01\x12\x37\n\x0epending_inputs\x18\x0c \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x36\n\rfailed_inputs\x18\r \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10succeeded_inputs\x18\x0e \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x37\n\x0etimeout_inputs\x18\x0f \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10\x63\x61ncelled_inputs\x18\x10 \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x14\n\x0ctotal_inputs\x18\x11 \x01(\x05J\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0c\"R\n\x18\x46unctionCallListResponse\x12\x36\n\x0e\x66unction_calls\x18\x01 \x03(\x0b\x32\x1e.modal.client.FunctionCallInfo\"5\n\x1e\x46unctionGetCurrentStatsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"S\n\rFunctionStats\x12\x0f\n\x07\x62\x61\x63klog\x18\x01 \x01(\r\x12\x18\n\x10num_active_tasks\x18\x02 \x01(\r\x12\x17\n\x0fnum_total_tasks\x18\x03 \x01(\r\"$\n\rGeneratorDone\x12\x13\n\x0bitems_total\x18\x01 \x01(\x04\"\xfe\x04\n\rGenericResult\x12\x39\n\x06status\x18\x01 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x11\n\texception\x18\x02 \x01(\t\x12\x10\n\x08\x65xitcode\x18\x03 \x01(\x05\x12\x11\n\ttraceback\x18\x04 \x01(\t\x12\x15\n\rserialized_tb\x18\x0b \x01(\x0c\x12\x15\n\rtb_line_cache\x18\x0c \x01(\x0c\x12\x0e\n\x04\x64\x61ta\x18\x05 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\n \x01(\tH\x00\x12?\n\ngen_status\x18\x07 \x01(\x0e\x32+.modal.client.GenericResult.GeneratorStatus\x12\x1a\n\x12propagation_reason\x18\r \x01(\t\"\xc3\x01\n\rGenericStatus\x12\x1e\n\x1aGENERIC_STATUS_UNSPECIFIED\x10\x00\x12\x1a\n\x16GENERIC_STATUS_SUCCESS\x10\x01\x12\x1a\n\x16GENERIC_STATUS_FAILURE\x10\x02\x12\x1d\n\x19GENERIC_STATUS_TERMINATED\x10\x03\x12\x1a\n\x16GENERIC_STATUS_TIMEOUT\x10\x04\x12\x1f\n\x1bGENERIC_STATUS_INIT_FAILURE\x10\x05\"s\n\x0fGeneratorStatus\x12 \n\x1cGENERATOR_STATUS_UNSPECIFIED\x10\x00\x12\x1f\n\x1bGENERATOR_STATUS_INCOMPLETE\x10\x01\x12\x1d\n\x19GENERATOR_STATUS_COMPLETE\x10\x02\x42\x0c\n\ndata_oneof\"O\n\tGPUConfig\x12#\n\x04type\x18\x01 \x01(\x0e\x32\x15.modal.client.GPUType\x12\r\n\x05\x63ount\x18\x02 \x01(\r\x12\x0e\n\x06memory\x18\x03 \x01(\r\"`\n\rBuildFunction\x12\x12\n\ndefinition\x18\x01 \x01(\t\x12\x0f\n\x07globals\x18\x02 \x01(\x0c\x12*\n\x05input\x18\x03 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"\xdd\x03\n\x05Image\x12,\n\x0b\x62\x61se_images\x18\x05 \x03(\x0b\x32\x17.modal.client.BaseImage\x12\x1b\n\x13\x64ockerfile_commands\x18\x06 \x03(\t\x12\x35\n\rcontext_files\x18\x07 \x03(\x0b\x32\x1e.modal.client.ImageContextFile\x12\x0f\n\x07version\x18\x0b \x01(\t\x12\x12\n\nsecret_ids\x18\x0c \x03(\t\x12\x0b\n\x03gpu\x18\r \x01(\x08\x12\x18\n\x10\x63ontext_mount_id\x18\x0f \x01(\t\x12+\n\ngpu_config\x18\x10 \x01(\x0b\x32\x17.modal.client.GPUConfig\x12@\n\x15image_registry_config\x18\x11 \x01(\x0b\x32!.modal.client.ImageRegistryConfig\x12\x1a\n\x12\x62uild_function_def\x18\x0e \x01(\t\x12\x1e\n\x16\x62uild_function_globals\x18\x12 \x01(\x0c\x12\x0f\n\x07runtime\x18\x13 \x01(\t\x12\x15\n\rruntime_debug\x18\x14 \x01(\x08\x12\x33\n\x0e\x62uild_function\x18\x15 \x01(\x0b\x32\x1b.modal.client.BuildFunction\"4\n!ImageBuilderVersionLookupResponse\x12\x0f\n\x07version\x18\x01 \x01(\t\"2\n\x10ImageContextFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\xed\x01\n\x17ImageGetOrCreateRequest\x12\"\n\x05image\x18\x02 \x01(\x0b\x32\x13.modal.client.Image\x12\x14\n\x06\x61pp_id\x18\x04 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_image_id\x18\x05 \x01(\t\x12\x19\n\x11\x62uild_function_id\x18\x06 \x01(\t\x12\x13\n\x0b\x66orce_build\x18\x07 \x01(\x08\x12\x34\n\tnamespace\x18\x08 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x17\n\x0f\x62uilder_version\x18\t \x01(\t\",\n\x18ImageGetOrCreateResponse\x12\x10\n\x08image_id\x18\x01 \x01(\t\"U\n\x19ImageJoinStreamingRequest\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x03 \x01(\t\"\x93\x01\n\x1aImageJoinStreamingResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12)\n\ttask_logs\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x03 \x01(\t\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"d\n\x13ImageRegistryConfig\x12:\n\x12registry_auth_type\x18\x01 \x01(\x0e\x32\x1e.modal.client.RegistryAuthType\x12\x11\n\tsecret_id\x18\x02 \x01(\t\"\x99\x01\n\tInputInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x0f\n\x07task_id\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x05 \x01(\x01\x12\x19\n\x11task_startup_time\x18\x06 \x01(\x01\x12\x18\n\x10task_first_input\x18\x07 \x01(\x08\"K\n\x11InputCategoryInfo\x12\r\n\x05total\x18\x01 \x01(\x05\x12\'\n\x06latest\x18\x02 \x03(\x0b\x32\x17.modal.client.InputInfo\"l\n\x11MountBuildRequest\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_mount_id\x18\x03 \x01(\t\x12&\n\x05\x66iles\x18\x04 \x03(\x0b\x32\x17.modal.client.MountFile\"b\n\x12MountBuildResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\"\xfa\x01\n\x17MountGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12&\n\x05\x66iles\x18\x05 \x03(\x0b\x32\x17.modal.client.MountFile\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\"h\n\x18MountGetOrCreateResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\":\n\x13MountHandleMetadata\x12#\n\x1b\x63ontent_checksum_sha256_hex\x18\x01 \x01(\t\"i\n\tMountFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x11\n\x04size\x18\x04 \x01(\x04H\x00\x88\x01\x01\x12\x11\n\x04mode\x18\x05 \x01(\rH\x01\x88\x01\x01\x42\x07\n\x05_sizeB\x07\n\x05_mode\"_\n\x13MountPutFileRequest\x12\x12\n\nsha256_hex\x18\x02 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x03 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"&\n\x14MountPutFileResponse\x12\x0e\n\x06\x65xists\x18\x02 \x01(\x08\"S\n\x0fMultiPartUpload\x12\x13\n\x0bpart_length\x18\x01 \x01(\x03\x12\x13\n\x0bupload_urls\x18\x02 \x03(\t\x12\x16\n\x0e\x63ompletion_url\x18\x03 \x01(\t\"\xce\x02\n\x06Object\x12\x11\n\tobject_id\x18\x01 \x01(\t\x12H\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadataH\x00\x12\x42\n\x15mount_handle_metadata\x18\x04 \x01(\x0b\x32!.modal.client.MountHandleMetadataH\x00\x12\x42\n\x15\x63lass_handle_metadata\x18\x05 \x01(\x0b\x32!.modal.client.ClassHandleMetadataH\x00\x12\x46\n\x17sandbox_handle_metadata\x18\x06 \x01(\x0b\x32#.modal.client.SandboxHandleMetadataH\x00\x42\x17\n\x15handle_metadata_oneof\"%\n\x10ObjectDependency\x12\x11\n\tobject_id\x18\x01 \x01(\t\"\xc2\x01\n\x17ProxyGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18ProxyGetOrCreateResponse\x12\x10\n\x08proxy_id\x18\x01 \x01(\t\"\\\n\tProxyInfo\x12\x12\n\nelastic_ip\x18\x01 \x01(\t\x12\x11\n\tproxy_key\x18\x02 \x01(\t\x12\x13\n\x0bremote_addr\x18\x03 \x01(\t\x12\x13\n\x0bremote_port\x18\x04 \x01(\x05\"\x86\x02\n\x07PTYInfo\x12\x0f\n\x07\x65nabled\x18\x01 \x01(\x08\x12\x12\n\nwinsz_rows\x18\x02 \x01(\r\x12\x12\n\nwinsz_cols\x18\x03 \x01(\r\x12\x10\n\x08\x65nv_term\x18\x04 \x01(\t\x12\x15\n\renv_colorterm\x18\x05 \x01(\t\x12\x18\n\x10\x65nv_term_program\x18\x06 \x01(\t\x12/\n\x08pty_type\x18\x07 \x01(\x0e\x32\x1d.modal.client.PTYInfo.PTYType\"N\n\x07PTYType\x12\x18\n\x14PTY_TYPE_UNSPECIFIED\x10\x00\x12\x15\n\x11PTY_TYPE_FUNCTION\x10\x01\x12\x12\n\x0ePTY_TYPE_SHELL\x10\x02\"?\n\x12QueueCreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11\x65xisting_queue_id\x18\x02 \x01(\t\"\'\n\x13QueueCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"\xc2\x01\n\x17QueueGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18QueueGetOrCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"F\n\x0fQueueGetRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x10\n\x08n_values\x18\x04 \x01(\x05\"\"\n\x10QueueGetResponse\x12\x0e\n\x06values\x18\x02 \x03(\x0c\")\n\x15QueueHeartbeatRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"3\n\x0fQueuePutRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0e\n\x06values\x18\x04 \x03(\x0c\"#\n\x0fQueueLenRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"\x1f\n\x10QueueLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\"M\n\tRateLimit\x12\r\n\x05limit\x18\x01 \x01(\x05\x12\x31\n\x08interval\x18\x02 \x01(\x0e\x32\x1f.modal.client.RateLimitInterval\"^\n\tResources\x12\x11\n\tmemory_mb\x18\x02 \x01(\r\x12\x11\n\tmilli_cpu\x18\x03 \x01(\r\x12+\n\ngpu_config\x18\x04 \x01(\x0b\x32\x17.modal.client.GPUConfig\"d\n\x07S3Mount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\"\x99\x04\n\x07Sandbox\x12\x17\n\x0f\x65ntrypoint_args\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12\x10\n\x08image_id\x18\x03 \x01(\t\x12\x12\n\nsecret_ids\x18\x04 \x03(\t\x12*\n\tresources\x18\x05 \x01(\x0b\x32\x17.modal.client.Resources\x12\x33\n\x0e\x63loud_provider\x18\x06 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x14\n\x0ctimeout_secs\x18\x07 \x01(\r\x12\x14\n\x07workdir\x18\x08 \x01(\tH\x00\x88\x01\x01\x12\x33\n\nnfs_mounts\x18\t \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\rruntime_debug\x18\n \x01(\x08\x12\x15\n\rblock_network\x18\x0b \x01(\x08\x12(\n\ts3_mounts\x18\x0c \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x0e \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x30\n\rvolume_mounts\x18\r \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\'\n\x08pty_info\x18\x0f \x01(\x0b\x32\x15.modal.client.PTYInfoB\n\n\x08_workdir\"W\n\x14SandboxCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\"+\n\x15SandboxCreateResponse\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"-\n\x17SandboxGetTaskIdRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"+\n\x18SandboxGetTaskIdResponse\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"\x8a\x01\n\x15SandboxGetLogsRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x02 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\"Y\n\x18SandboxStdinWriteRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\r\n\x05input\x18\x02 \x01(\x0c\x12\r\n\x05index\x18\x03 \x01(\r\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"\x1b\n\x19SandboxStdinWriteResponse\"D\n\x15SandboxHandleMetadata\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x83\x01\n\x0bSandboxInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12)\n\ttask_info\x18\x04 \x01(\x0b\x32\x16.modal.client.TaskInfo\">\n\x12SandboxListRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x18\n\x10\x62\x65\x66ore_timestamp\x18\x02 \x01(\x01\"C\n\x13SandboxListResponse\x12,\n\tsandboxes\x18\x01 \x03(\x0b\x32\x19.modal.client.SandboxInfo\"-\n\x17SandboxTerminateRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"P\n\x18SandboxTerminateResponse\x12\x34\n\x0f\x65xisting_result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"9\n\x12SandboxWaitRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\"B\n\x13SandboxWaitResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x8e\x02\n\x08Schedule\x12+\n\x04\x63ron\x18\x01 \x01(\x0b\x32\x1b.modal.client.Schedule.CronH\x00\x12/\n\x06period\x18\x02 \x01(\x0b\x32\x1d.modal.client.Schedule.PeriodH\x00\x1a\x1b\n\x04\x43ron\x12\x13\n\x0b\x63ron_string\x18\x01 \x01(\t\x1au\n\x06Period\x12\r\n\x05years\x18\x01 \x01(\x05\x12\x0e\n\x06months\x18\x02 \x01(\x05\x12\r\n\x05weeks\x18\x03 \x01(\x05\x12\x0c\n\x04\x64\x61ys\x18\x04 \x01(\x05\x12\r\n\x05hours\x18\x05 \x01(\x05\x12\x0f\n\x07minutes\x18\x06 \x01(\x05\x12\x0f\n\x07seconds\x18\x07 \x01(\x02\x42\x10\n\x0eschedule_oneof\"\xd0\x01\n\x13SecretCreateRequest\x12@\n\x08\x65nv_dict\x18\x01 \x03(\x0b\x32..modal.client.SecretCreateRequest.EnvDictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x15\n\rtemplate_type\x18\x03 \x01(\t\x12\x1a\n\x12\x65xisting_secret_id\x18\x04 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\")\n\x14SecretCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"\xca\x02\n\x18SecretGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x45\n\x08\x65nv_dict\x18\x05 \x03(\x0b\x32\x33.modal.client.SecretGetOrCreateRequest.EnvDictEntry\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\".\n\x19SecretGetOrCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"c\n\x0eSecretListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x12\n\ncreated_at\x18\x02 \x01(\x01\x12\x14\n\x0clast_used_at\x18\x03 \x01(\x01\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"-\n\x11SecretListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12SecretListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.SecretListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"\xd9\x01\n\x1eSharedVolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\";\n\x1fSharedVolumeGetOrCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"8\n\x1cSharedVolumeHeartbeatRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"f\n\x19SharedVolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x33\n\x0e\x63loud_provider\x18\x02 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"6\n\x1aSharedVolumeCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"\x88\x01\n\x14SharedVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12\x33\n\x0e\x63loud_provider\x18\x04 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"3\n\x17SharedVolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"g\n\x18SharedVolumeListResponse\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.SharedVolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"F\n\x1cSharedVolumeListFilesRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"\xa7\x01\n\x1aSharedVolumePutFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x04 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x12\x11\n\tresumable\x18\x06 \x01(\x08\x42\x0c\n\ndata_oneof\"-\n\x1bSharedVolumePutFileResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"D\n\x1aSharedVolumeGetFileRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"S\n\x1bSharedVolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"`\n\x1dSharedVolumeRemoveFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x11\n\trecursive\x18\x03 \x01(\x08\"\xa1\x01\n\x1aSharedVolumeListFilesEntry\x12\x0c\n\x04path\x18\x01 \x01(\t\x12?\n\x04type\x18\x02 \x01(\x0e\x32\x31.modal.client.SharedVolumeListFilesEntry.FileType\"4\n\x08\x46ileType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x08\n\x04\x46ILE\x10\x01\x12\r\n\tDIRECTORY\x10\x02\"Z\n\x1dSharedVolumeListFilesResponse\x12\x39\n\x07\x65ntries\x18\x01 \x03(\x0b\x32(.modal.client.SharedVolumeListFilesEntry\"\x92\x01\n\x11SharedVolumeMount\x12\x12\n\nmount_path\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x33\n\x0e\x63loud_provider\x18\x03 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x1a\n\x12\x61llow_cross_region\x18\x04 \x01(\x08\".\n\x19TaskCurrentInputsResponse\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"l\n\x08TaskInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nstarted_at\x18\x02 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x03 \x01(\x01\x12+\n\x06result\x18\x04 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\xee\x01\n\x08TaskLogs\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12+\n\ntask_state\x18\x06 \x01(\x0e\x32\x17.modal.client.TaskState\x12\x11\n\ttimestamp\x18\x07 \x01(\x01\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x31\n\rtask_progress\x18\t \x01(\x0b\x32\x1a.modal.client.TaskProgress\x12\x18\n\x10\x66unction_call_id\x18\n \x01(\t\x12\x10\n\x08input_id\x18\x0b \x01(\t\"\x11\n\x0fTaskListRequest\":\n\x10TaskListResponse\x12&\n\x05tasks\x18\x01 \x03(\x0b\x32\x17.modal.client.TaskStats\"\xc6\x01\n\rTaskLogsBatch\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12%\n\x05items\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x05 \x01(\t\x12\x10\n\x08\x61pp_done\x18\n \x01(\x08\x12\x13\n\x0b\x66unction_id\x18\x0b \x01(\t\x12\x10\n\x08input_id\x18\x0c \x01(\t\x12\x10\n\x08image_id\x18\r \x01(\t\x12\x0b\n\x03\x65of\x18\x0e \x01(\x08\x12\x13\n\x0bpty_exec_id\x18\x0f \x01(\t\"p\n\x0cTaskProgress\x12\x0b\n\x03len\x18\x01 \x01(\x04\x12\x0b\n\x03pos\x18\x02 \x01(\x04\x12\x31\n\rprogress_type\x18\x03 \x01(\x0e\x32\x1a.modal.client.ProgressType\x12\x13\n\x0b\x64\x65scription\x18\x04 \x01(\t\"@\n\x11TaskResultRequest\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\"Y\n\tTaskStats\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x02 \x01(\t\x12\x17\n\x0f\x61pp_description\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\"V\n\x16TokenFlowCreateRequest\x12\x12\n\nutm_source\x18\x03 \x01(\t\x12\x16\n\x0elocalhost_port\x18\x04 \x01(\x05\x12\x10\n\x08next_url\x18\x05 \x01(\t\"d\n\x17TokenFlowCreateResponse\x12\x15\n\rtoken_flow_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12\x0c\n\x04\x63ode\x18\x03 \x01(\t\x12\x13\n\x0bwait_secret\x18\x04 \x01(\t\"S\n\x14TokenFlowWaitRequest\x12\x0f\n\x07timeout\x18\x01 \x01(\x02\x12\x15\n\rtoken_flow_id\x18\x02 \x01(\t\x12\x13\n\x0bwait_secret\x18\x03 \x01(\t\"l\n\x15TokenFlowWaitResponse\x12\x10\n\x08token_id\x18\x01 \x01(\t\x12\x14\n\x0ctoken_secret\x18\x02 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x08\x12\x1a\n\x12workspace_username\x18\x04 \x01(\t\"7\n\x12TunnelStartRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\x12\x13\n\x0bunencrypted\x18\x02 \x01(\x08\"\x99\x01\n\x13TunnelStartResponse\x12\x0c\n\x04host\x18\x01 \x01(\t\x12\x0c\n\x04port\x18\x02 \x01(\r\x12\x1d\n\x10unencrypted_host\x18\x03 \x01(\tH\x00\x88\x01\x01\x12\x1d\n\x10unencrypted_port\x18\x04 \x01(\rH\x01\x88\x01\x01\x42\x13\n\x11_unencrypted_hostB\x13\n\x11_unencrypted_port\"!\n\x11TunnelStopRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\"$\n\x12TunnelStopResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"\xd3\x01\n\x18VolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\".\n\x19VolumeGetOrCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x16VolumeHeartbeatRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x13VolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\")\n\x14VolumeCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\".\n\x13VolumeCommitRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"+\n\x14VolumeCommitResponse\x12\x13\n\x0bskip_reload\x18\x01 \x01(\x08\"F\n\x13VolumeDeleteRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x1c\n\x10\x65nvironment_name\x18\x02 \x01(\tB\x02\x18\x01\"S\n\x14VolumeGetFileRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\r\n\x05start\x18\x03 \x01(\x04\x12\x0b\n\x03len\x18\x04 \x01(\x04\"w\n\x15VolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x12\x0c\n\x04size\x18\x03 \x01(\x04\x12\r\n\x05start\x18\x04 \x01(\x04\x12\x0b\n\x03len\x18\x05 \x01(\x04\x42\x0c\n\ndata_oneof\"\xbf\x01\n\x14VolumeListFilesEntry\x12\x0c\n\x04path\x18\x01 \x01(\t\x12\x39\n\x04type\x18\x02 \x01(\x0e\x32+.modal.client.VolumeListFilesEntry.FileType\x12\r\n\x05mtime\x18\x03 \x01(\x04\x12\x0c\n\x04size\x18\x04 \x01(\x04\"A\n\x08\x46ileType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x08\n\x04\x46ILE\x10\x01\x12\r\n\tDIRECTORY\x10\x02\x12\x0b\n\x07SYMLINK\x10\x03\"c\n\x16VolumeListFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x18\n\x0bmax_entries\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0e\n\x0c_max_entries\"N\n\x17VolumeListFilesResponse\x12\x33\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\".modal.client.VolumeListFilesEntry\"F\n\x0eVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x11\n\tvolume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\"-\n\x11VolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12VolumeListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.VolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"(\n\x13VolumeReloadRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"}\n\x15VolumePutFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12&\n\x05\x66iles\x18\x02 \x03(\x0b\x32\x17.modal.client.MountFile\x12)\n!disallow_overwrite_existing_files\x18\x03 \x01(\x08\"S\n\x17VolumeRemoveFileRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\x11\n\trecursive\x18\x03 \x01(\x08\"c\n\x16VolumeCopyFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x11\n\tsrc_paths\x18\x02 \x03(\x0c\x12\x10\n\x08\x64st_path\x18\x03 \x01(\x0c\x12\x11\n\trecursive\x18\x04 \x01(\x08\"V\n\x0bVolumeMount\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12 \n\x18\x61llow_background_commits\x18\x03 \x01(\x08\"\x8d\x02\n\rWebhookConfig\x12\'\n\x04type\x18\x01 \x01(\x0e\x32\x19.modal.client.WebhookType\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x18\n\x10requested_suffix\x18\x04 \x01(\t\x12\x32\n\nasync_mode\x18\x05 \x01(\x0e\x32\x1e.modal.client.WebhookAsyncMode\x12\x38\n\x0e\x63ustom_domains\x18\x06 \x03(\x0b\x32 .modal.client.CustomDomainConfig\x12\x17\n\x0fweb_server_port\x18\x07 \x01(\r\x12\"\n\x1aweb_server_startup_timeout\x18\x08 \x01(\x02\"N\n\nWebUrlInfo\x12\x11\n\ttruncated\x18\x01 \x01(\x08\x12\x17\n\x0fhas_unique_hash\x18\x02 \x01(\x08\x12\x14\n\x0clabel_stolen\x18\x03 \x01(\x08\"G\n\x1bWorkspaceNameLookupResponse\x12\x16\n\x0eworkspace_name\x18\x01 \x01(\t\x12\x10\n\x08username\x18\x02 \x01(\t\"^\n\x14RuntimeOutputMessage\x12\x35\n\x0f\x66ile_descriptor\x18\x01 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07message\x18\x02 \x01(\t\"\x82\x01\n\x12RuntimeOutputBatch\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.RuntimeOutputMessage\x12\x13\n\x0b\x62\x61tch_index\x18\x02 \x01(\x04\x12\x16\n\texit_code\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0c\n\n_exit_code\"=\n\x13RuntimeInputMessage\x12\x0f\n\x07message\x18\x01 \x01(\x0c\x12\x15\n\rmessage_index\x18\x02 \x01(\x04*\x83\x01\n\x13\x41ppDeployVisibility\x12%\n!APP_DEPLOY_VISIBILITY_UNSPECIFIED\x10\x00\x12#\n\x1f\x41PP_DEPLOY_VISIBILITY_WORKSPACE\x10\x01\x12 \n\x1c\x41PP_DEPLOY_VISIBILITY_PUBLIC\x10\x02*\xf5\x01\n\x13\x41ppDisconnectReason\x12%\n!APP_DISCONNECT_REASON_UNSPECIFIED\x10\x00\x12)\n%APP_DISCONNECT_REASON_LOCAL_EXCEPTION\x10\x01\x12,\n(APP_DISCONNECT_REASON_KEYBOARD_INTERRUPT\x10\x02\x12.\n*APP_DISCONNECT_REASON_ENTRYPOINT_COMPLETED\x10\x03\x12.\n*APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION\x10\x04*\x8d\x02\n\x08\x41ppState\x12\x19\n\x15\x41PP_STATE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STATE_EPHEMERAL\x10\x01\x12\x16\n\x12\x41PP_STATE_DETACHED\x10\x02\x12\x16\n\x12\x41PP_STATE_DEPLOYED\x10\x03\x12\x16\n\x12\x41PP_STATE_STOPPING\x10\x04\x12\x15\n\x11\x41PP_STATE_STOPPED\x10\x05\x12\x1a\n\x16\x41PP_STATE_INITIALIZING\x10\x06\x12\x16\n\x12\x41PP_STATE_DISABLED\x10\x07\x12#\n\x1f\x41PP_STATE_DETACHED_DISCONNECTED\x10\x08\x12\x15\n\x11\x41PP_STATE_DERIVED\x10\t*\x85\x01\n\rAppStopSource\x12\x1f\n\x1b\x41PP_STOP_SOURCE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STOP_SOURCE_CLI\x10\x01\x12!\n\x1d\x41PP_STOP_SOURCE_PYTHON_CLIENT\x10\x02\x12\x17\n\x13\x41PP_STOP_SOURCE_WEB\x10\x03*\x91\x01\n\x11\x43\x65rtificateStatus\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_PENDING\x10\x00\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_ISSUED\x10\x01\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_FAILED\x10\x02\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_REVOKED\x10\x03*\xb1\x01\n\x10\x43heckpointStatus\x12!\n\x1d\x43HECKPOINT_STATUS_UNSPECIFIED\x10\x00\x12\x1d\n\x19\x43HECKPOINT_STATUS_PENDING\x10\x01\x12 \n\x1c\x43HECKPOINT_STATUS_PROCESSING\x10\x02\x12\x1b\n\x17\x43HECKPOINT_STATUS_READY\x10\x03\x12\x1c\n\x18\x43HECKPOINT_STATUS_FAILED\x10\x04*\xb0\x01\n\nClientType\x12\x1b\n\x17\x43LIENT_TYPE_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LIENT_TYPE_CLIENT\x10\x01\x12\x1a\n\x12\x43LIENT_TYPE_WORKER\x10\x02\x1a\x02\x08\x01\x12\x19\n\x15\x43LIENT_TYPE_CONTAINER\x10\x03\x12\x1a\n\x12\x43LIENT_TYPE_SERVER\x10\x04\x1a\x02\x08\x01\x12\x1a\n\x16\x43LIENT_TYPE_WEB_SERVER\x10\x05*\x90\x01\n\rCloudProvider\x12\x1e\n\x1a\x43LOUD_PROVIDER_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LOUD_PROVIDER_AWS\x10\x01\x12\x16\n\x12\x43LOUD_PROVIDER_GCP\x10\x02\x12\x17\n\x13\x43LOUD_PROVIDER_AUTO\x10\x03\x12\x16\n\x12\x43LOUD_PROVIDER_OCI\x10\x04*w\n\nDataFormat\x12\x1b\n\x17\x44\x41TA_FORMAT_UNSPECIFIED\x10\x00\x12\x16\n\x12\x44\x41TA_FORMAT_PICKLE\x10\x01\x12\x14\n\x10\x44\x41TA_FORMAT_ASGI\x10\x02\x12\x1e\n\x1a\x44\x41TA_FORMAT_GENERATOR_DONE\x10\x03*\x80\x01\n\x13\x44\x65ploymentNamespace\x12$\n DEPLOYMENT_NAMESPACE_UNSPECIFIED\x10\x00\x12\"\n\x1e\x44\x45PLOYMENT_NAMESPACE_WORKSPACE\x10\x01\x12\x1f\n\x1b\x44\x45PLOYMENT_NAMESPACE_GLOBAL\x10\x03*Z\n\rDNSRecordType\x12\x15\n\x11\x44NS_RECORD_TYPE_A\x10\x00\x12\x17\n\x13\x44NS_RECORD_TYPE_TXT\x10\x01\x12\x19\n\x15\x44NS_RECORD_TYPE_CNAME\x10\x02*\x83\x01\n\x0e\x46ileDescriptor\x12\x1f\n\x1b\x46ILE_DESCRIPTOR_UNSPECIFIED\x10\x00\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDOUT\x10\x01\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDERR\x10\x02\x12\x18\n\x14\x46ILE_DESCRIPTOR_INFO\x10\x03*p\n\x10\x46unctionCallType\x12\"\n\x1e\x46UNCTION_CALL_TYPE_UNSPECIFIED\x10\x00\x12\x1c\n\x18\x46UNCTION_CALL_TYPE_UNARY\x10\x01\x12\x1a\n\x16\x46UNCTION_CALL_TYPE_MAP\x10\x02*\x82\x02\n\x07GPUType\x12\x18\n\x14GPU_TYPE_UNSPECIFIED\x10\x00\x12\x0f\n\x0bGPU_TYPE_T4\x10\x01\x12\x11\n\rGPU_TYPE_A100\x10\x02\x12\x11\n\rGPU_TYPE_A10G\x10\x03\x12\x10\n\x0cGPU_TYPE_ANY\x10\x04\x12\x19\n\x11GPU_TYPE_A100_20G\x10\x05\x1a\x02\x08\x01\x12\x1f\n\x17GPU_TYPE_A100_40GB_MANY\x10\x06\x1a\x02\x08\x01\x12\x1c\n\x14GPU_TYPE_INFERENTIA2\x10\x07\x1a\x02\x08\x01\x12\x16\n\x12GPU_TYPE_A100_80GB\x10\x08\x12\x0f\n\x0bGPU_TYPE_L4\x10\t\x12\x11\n\rGPU_TYPE_H100\x10\n*\xa0\x02\n\x12ObjectCreationType\x12$\n OBJECT_CREATION_TYPE_UNSPECIFIED\x10\x00\x12*\n&OBJECT_CREATION_TYPE_CREATE_IF_MISSING\x10\x01\x12.\n*OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS\x10\x02\x12\x33\n/OBJECT_CREATION_TYPE_CREATE_OVERWRITE_IF_EXISTS\x10\x03\x12/\n+OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP\x10\x04\x12\"\n\x1eOBJECT_CREATION_TYPE_EPHEMERAL\x10\x05*>\n\x0cProgressType\x12\x19\n\x15IMAGE_SNAPSHOT_UPLOAD\x10\x00\x12\x13\n\x0f\x46UNCTION_QUEUED\x10\x01*x\n\x11RateLimitInterval\x12#\n\x1fRATE_LIMIT_INTERVAL_UNSPECIFIED\x10\x00\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_SECOND\x10\x01\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_MINUTE\x10\x02*\xb2\x01\n\x10RegistryAuthType\x12\"\n\x1eREGISTRY_AUTH_TYPE_UNSPECIFIED\x10\x00\x12\x1a\n\x16REGISTRY_AUTH_TYPE_AWS\x10\x01\x12\x1a\n\x16REGISTRY_AUTH_TYPE_GCP\x10\x02\x12\x1d\n\x19REGISTRY_AUTH_TYPE_PUBLIC\x10\x03\x12#\n\x1fREGISTRY_AUTH_TYPE_STATIC_CREDS\x10\x04*\xdc\x02\n\tTaskState\x12\x1a\n\x16TASK_STATE_UNSPECIFIED\x10\x00\x12\x16\n\x12TASK_STATE_CREATED\x10\x06\x12\x15\n\x11TASK_STATE_QUEUED\x10\x01\x12\x1e\n\x1aTASK_STATE_WORKER_ASSIGNED\x10\x02\x12\x1c\n\x18TASK_STATE_LOADING_IMAGE\x10\x03\x12\x15\n\x11TASK_STATE_ACTIVE\x10\x04\x12\x18\n\x14TASK_STATE_COMPLETED\x10\x05\x12!\n\x1dTASK_STATE_CREATING_CONTAINER\x10\x07\x12\x13\n\x0fTASK_STATE_IDLE\x10\x08\x12\x1a\n\x16TASK_STATE_PREEMPTIBLE\x10\t\x12\x18\n\x14TASK_STATE_PREEMPTED\x10\n\x12\'\n#TASK_STATE_LOADING_CHECKPOINT_IMAGE\x10\x0b*\x99\x01\n\x0bWebhookType\x12\x1c\n\x18WEBHOOK_TYPE_UNSPECIFIED\x10\x00\x12\x19\n\x15WEBHOOK_TYPE_ASGI_APP\x10\x01\x12\x19\n\x15WEBHOOK_TYPE_FUNCTION\x10\x02\x12\x19\n\x15WEBHOOK_TYPE_WSGI_APP\x10\x03\x12\x1b\n\x17WEBHOOK_TYPE_WEB_SERVER\x10\x04*\x9a\x01\n\x10WebhookAsyncMode\x12\"\n\x1eWEBHOOK_ASYNC_MODE_UNSPECIFIED\x10\x00\x12\x1f\n\x1bWEBHOOK_ASYNC_MODE_DISABLED\x10\x02\x12\x1e\n\x1aWEBHOOK_ASYNC_MODE_TRIGGER\x10\x03\x12\x1b\n\x17WEBHOOK_ASYNC_MODE_AUTO\x10\x04\"\x04\x08\x01\x10\x01\x32\xa3N\n\x0bModalClient\x12L\n\tAppCreate\x12\x1e.modal.client.AppCreateRequest\x1a\x1f.modal.client.AppCreateResponse\x12W\n\x13\x41ppClientDisconnect\x12(.modal.client.AppClientDisconnectRequest\x1a\x16.google.protobuf.Empty\x12L\n\nAppGetLogs\x12\x1f.modal.client.AppGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12K\n\rAppSetObjects\x12\".modal.client.AppSetObjectsRequest\x1a\x16.google.protobuf.Empty\x12X\n\rAppGetObjects\x12\".modal.client.AppGetObjectsRequest\x1a#.modal.client.AppGetObjectsResponse\x12\x46\n\x07\x41ppList\x12\x1c.modal.client.AppListRequest\x1a\x1d.modal.client.AppListResponse\x12^\n\x0f\x41ppLookupObject\x12$.modal.client.AppLookupObjectRequest\x1a%.modal.client.AppLookupObjectResponse\x12L\n\tAppDeploy\x12\x1e.modal.client.AppDeployRequest\x1a\x1f.modal.client.AppDeployResponse\x12p\n\x15\x41ppDeploySingleObject\x12*.modal.client.AppDeploySingleObjectRequest\x1a+.modal.client.AppDeploySingleObjectResponse\x12s\n\x16\x41ppGetByDeploymentName\x12+.modal.client.AppGetByDeploymentNameRequest\x1a,.modal.client.AppGetByDeploymentNameResponse\x12?\n\x07\x41ppStop\x12\x1c.modal.client.AppStopRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0c\x41ppHeartbeat\x12!.modal.client.AppHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12O\n\nBlobCreate\x12\x1f.modal.client.BlobCreateRequest\x1a .modal.client.BlobCreateResponse\x12\x46\n\x07\x42lobGet\x12\x1c.modal.client.BlobGetRequest\x1a\x1d.modal.client.BlobGetResponse\x12R\n\x0b\x43lassCreate\x12 .modal.client.ClassCreateRequest\x1a!.modal.client.ClassCreateResponse\x12I\n\x08\x43lassGet\x12\x1d.modal.client.ClassGetRequest\x1a\x1e.modal.client.ClassGetResponse\x12U\n\x0c\x43lientCreate\x12!.modal.client.ClientCreateRequest\x1a\".modal.client.ClientCreateResponse\x12H\n\x0b\x43lientHello\x12\x16.google.protobuf.Empty\x1a!.modal.client.ClientHelloResponse\x12O\n\x0f\x43lientHeartbeat\x12$.modal.client.ClientHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12g\n\x12\x43ontainerHeartbeat\x12\'.modal.client.ContainerHeartbeatRequest\x1a(.modal.client.ContainerHeartbeatResponse\x12X\n\rContainerExec\x12\".modal.client.ContainerExecRequest\x1a#.modal.client.ContainerExecResponse\x12i\n\x16\x43ontainerExecGetOutput\x12+.modal.client.ContainerExecGetOutputRequest\x1a .modal.client.RuntimeOutputBatch0\x01\x12[\n\x15\x43ontainerExecPutInput\x12*.modal.client.ContainerExecPutInputRequest\x1a\x16.google.protobuf.Empty\x12W\n\x13\x43ontainerCheckpoint\x12(.modal.client.ContainerCheckpointRequest\x1a\x16.google.protobuf.Empty\x12\x43\n\tDictClear\x12\x1e.modal.client.DictClearRequest\x1a\x16.google.protobuf.Empty\x12O\n\nDictCreate\x12\x1f.modal.client.DictCreateRequest\x1a .modal.client.DictCreateResponse\x12^\n\x0f\x44ictGetOrCreate\x12$.modal.client.DictGetOrCreateRequest\x1a%.modal.client.DictGetOrCreateResponse\x12K\n\rDictHeartbeat\x12\".modal.client.DictHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12O\n\nDictUpdate\x12\x1f.modal.client.DictUpdateRequest\x1a .modal.client.DictUpdateResponse\x12\x46\n\x07\x44ictGet\x12\x1c.modal.client.DictGetRequest\x1a\x1d.modal.client.DictGetResponse\x12\x46\n\x07\x44ictPop\x12\x1c.modal.client.DictPopRequest\x1a\x1d.modal.client.DictPopResponse\x12U\n\x0c\x44ictContains\x12!.modal.client.DictContainsRequest\x1a\".modal.client.DictContainsResponse\x12\x46\n\x07\x44ictLen\x12\x1c.modal.client.DictLenRequest\x1a\x1d.modal.client.DictLenResponse\x12U\n\x0c\x44omainCreate\x12!.modal.client.DomainCreateRequest\x1a\".modal.client.DomainCreateResponse\x12O\n\nDomainList\x12\x1f.modal.client.DomainListRequest\x1a .modal.client.DomainListResponse\x12v\n\x17\x44omainCertificateVerify\x12,.modal.client.DomainCertificateVerifyRequest\x1a-.modal.client.DomainCertificateVerifyResponse\x12S\n\x11\x45nvironmentCreate\x12&.modal.client.EnvironmentCreateRequest\x1a\x16.google.protobuf.Empty\x12P\n\x0f\x45nvironmentList\x12\x16.google.protobuf.Empty\x1a%.modal.client.EnvironmentListResponse\x12S\n\x11\x45nvironmentDelete\x12&.modal.client.EnvironmentDeleteRequest\x1a\x16.google.protobuf.Empty\x12^\n\x11\x45nvironmentUpdate\x12&.modal.client.EnvironmentUpdateRequest\x1a!.modal.client.EnvironmentListItem\x12g\n\x12\x46unctionBindParams\x12\'.modal.client.FunctionBindParamsRequest\x1a(.modal.client.FunctionBindParamsResponse\x12[\n\x0e\x46unctionCreate\x12#.modal.client.FunctionCreateRequest\x1a$.modal.client.FunctionCreateResponse\x12R\n\x0b\x46unctionGet\x12 .modal.client.FunctionGetRequest\x1a!.modal.client.FunctionGetResponse\x12m\n\x14\x46unctionGetCallGraph\x12).modal.client.FunctionGetCallGraphRequest\x1a*.modal.client.FunctionGetCallGraphResponse\x12\x64\n\x17\x46unctionGetCurrentStats\x12,.modal.client.FunctionGetCurrentStatsRequest\x1a\x1b.modal.client.FunctionStats\x12\x64\n\x11\x46unctionGetInputs\x12&.modal.client.FunctionGetInputsRequest\x1a\'.modal.client.FunctionGetInputsResponse\x12g\n\x12\x46unctionGetOutputs\x12\'.modal.client.FunctionGetOutputsRequest\x1a(.modal.client.FunctionGetOutputsResponse\x12p\n\x15\x46unctionGetSerialized\x12*.modal.client.FunctionGetSerializedRequest\x1a+.modal.client.FunctionGetSerializedResponse\x12R\n\x0b\x46unctionMap\x12 .modal.client.FunctionMapRequest\x1a!.modal.client.FunctionMapResponse\x12\x64\n\x11\x46unctionPrecreate\x12&.modal.client.FunctionPrecreateRequest\x1a\'.modal.client.FunctionPrecreateResponse\x12\x64\n\x11\x46unctionPutInputs\x12&.modal.client.FunctionPutInputsRequest\x1a\'.modal.client.FunctionPutInputsResponse\x12U\n\x12\x46unctionPutOutputs\x12\'.modal.client.FunctionPutOutputsRequest\x1a\x16.google.protobuf.Empty\x12\x8b\x01\n\x1e\x46unctionUpdateSchedulingParams\x12\x33.modal.client.FunctionUpdateSchedulingParamsRequest\x1a\x34.modal.client.FunctionUpdateSchedulingParamsResponse\x12U\n\x12\x46unctionCallCancel\x12\'.modal.client.FunctionCallCancelRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10\x46unctionCallList\x12%.modal.client.FunctionCallListRequest\x1a&.modal.client.FunctionCallListResponse\x12\\\n\x15\x46unctionCallGetDataIn\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12]\n\x16\x46unctionCallGetDataOut\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12Z\n\x16\x46unctionCallPutDataOut\x12(.modal.client.FunctionCallPutDataRequest\x1a\x16.google.protobuf.Empty\x12G\n\x15\x46unctionStartPtyShell\x12\x16.google.protobuf.Empty\x1a\x16.google.protobuf.Empty\x12\x61\n\x10ImageGetOrCreate\x12%.modal.client.ImageGetOrCreateRequest\x1a&.modal.client.ImageGetOrCreateResponse\x12i\n\x12ImageJoinStreaming\x12\'.modal.client.ImageJoinStreamingRequest\x1a(.modal.client.ImageJoinStreamingResponse0\x01\x12\x64\n\x19ImageBuilderVersionLookup\x12\x16.google.protobuf.Empty\x1a/.modal.client.ImageBuilderVersionLookupResponse\x12U\n\x0cMountPutFile\x12!.modal.client.MountPutFileRequest\x1a\".modal.client.MountPutFileResponse\x12O\n\nMountBuild\x12\x1f.modal.client.MountBuildRequest\x1a .modal.client.MountBuildResponse\x12\x61\n\x10MountGetOrCreate\x12%.modal.client.MountGetOrCreateRequest\x1a&.modal.client.MountGetOrCreateResponse\x12\x61\n\x10ProxyGetOrCreate\x12%.modal.client.ProxyGetOrCreateRequest\x1a&.modal.client.ProxyGetOrCreateResponse\x12R\n\x0bQueueCreate\x12 .modal.client.QueueCreateRequest\x1a!.modal.client.QueueCreateResponse\x12\x61\n\x10QueueGetOrCreate\x12%.modal.client.QueueGetOrCreateRequest\x1a&.modal.client.QueueGetOrCreateResponse\x12I\n\x08QueueGet\x12\x1d.modal.client.QueueGetRequest\x1a\x1e.modal.client.QueueGetResponse\x12M\n\x0eQueueHeartbeat\x12#.modal.client.QueueHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x41\n\x08QueuePut\x12\x1d.modal.client.QueuePutRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08QueueLen\x12\x1d.modal.client.QueueLenRequest\x1a\x1e.modal.client.QueueLenResponse\x12X\n\rSandboxCreate\x12\".modal.client.SandboxCreateRequest\x1a#.modal.client.SandboxCreateResponse\x12\x61\n\x10SandboxGetTaskId\x12%.modal.client.SandboxGetTaskIdRequest\x1a&.modal.client.SandboxGetTaskIdResponse\x12T\n\x0eSandboxGetLogs\x12#.modal.client.SandboxGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12R\n\x0bSandboxWait\x12 .modal.client.SandboxWaitRequest\x1a!.modal.client.SandboxWaitResponse\x12R\n\x0bSandboxList\x12 .modal.client.SandboxListRequest\x1a!.modal.client.SandboxListResponse\x12\x61\n\x10SandboxTerminate\x12%.modal.client.SandboxTerminateRequest\x1a&.modal.client.SandboxTerminateResponse\x12\x64\n\x11SandboxStdinWrite\x12&.modal.client.SandboxStdinWriteRequest\x1a\'.modal.client.SandboxStdinWriteResponse\x12U\n\x0cSecretCreate\x12!.modal.client.SecretCreateRequest\x1a\".modal.client.SecretCreateResponse\x12\x64\n\x11SecretGetOrCreate\x12&.modal.client.SecretGetOrCreateRequest\x1a\'.modal.client.SecretGetOrCreateResponse\x12O\n\nSecretList\x12\x1f.modal.client.SecretListRequest\x1a .modal.client.SecretListResponse\x12v\n\x17SharedVolumeGetOrCreate\x12,.modal.client.SharedVolumeGetOrCreateRequest\x1a-.modal.client.SharedVolumeGetOrCreateResponse\x12g\n\x12SharedVolumeCreate\x12\'.modal.client.SharedVolumeCreateRequest\x1a(.modal.client.SharedVolumeCreateResponse\x12[\n\x15SharedVolumeHeartbeat\x12*.modal.client.SharedVolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10SharedVolumeList\x12%.modal.client.SharedVolumeListRequest\x1a&.modal.client.SharedVolumeListResponse\x12p\n\x15SharedVolumeListFiles\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse\x12x\n\x1bSharedVolumeListFilesStream\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse0\x01\x12j\n\x13SharedVolumePutFile\x12(.modal.client.SharedVolumePutFileRequest\x1a).modal.client.SharedVolumePutFileResponse\x12j\n\x13SharedVolumeGetFile\x12(.modal.client.SharedVolumeGetFileRequest\x1a).modal.client.SharedVolumeGetFileResponse\x12]\n\x16SharedVolumeRemoveFile\x12+.modal.client.SharedVolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12\x45\n\nTaskResult\x12\x1f.modal.client.TaskResultRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08TaskList\x12\x1d.modal.client.TaskListRequest\x1a\x1e.modal.client.TaskListResponse\x12T\n\x11TaskCurrentInputs\x12\x16.google.protobuf.Empty\x1a\'.modal.client.TaskCurrentInputsResponse\x12^\n\x0fTokenFlowCreate\x12$.modal.client.TokenFlowCreateRequest\x1a%.modal.client.TokenFlowCreateResponse\x12X\n\rTokenFlowWait\x12\".modal.client.TokenFlowWaitRequest\x1a#.modal.client.TokenFlowWaitResponse\x12R\n\x0bTunnelStart\x12 .modal.client.TunnelStartRequest\x1a!.modal.client.TunnelStartResponse\x12O\n\nTunnelStop\x12\x1f.modal.client.TunnelStopRequest\x1a .modal.client.TunnelStopResponse\x12\x64\n\x11VolumeGetOrCreate\x12&.modal.client.VolumeGetOrCreateRequest\x1a\'.modal.client.VolumeGetOrCreateResponse\x12U\n\x0cVolumeCreate\x12!.modal.client.VolumeCreateRequest\x1a\".modal.client.VolumeCreateResponse\x12O\n\x0fVolumeHeartbeat\x12$.modal.client.VolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12U\n\x0cVolumeCommit\x12!.modal.client.VolumeCommitRequest\x1a\".modal.client.VolumeCommitResponse\x12I\n\x0cVolumeDelete\x12!.modal.client.VolumeDeleteRequest\x1a\x16.google.protobuf.Empty\x12X\n\rVolumeGetFile\x12\".modal.client.VolumeGetFileRequest\x1a#.modal.client.VolumeGetFileResponse\x12O\n\nVolumeList\x12\x1f.modal.client.VolumeListRequest\x1a .modal.client.VolumeListResponse\x12`\n\x0fVolumeListFiles\x12$.modal.client.VolumeListFilesRequest\x1a%.modal.client.VolumeListFilesResponse0\x01\x12M\n\x0eVolumePutFiles\x12#.modal.client.VolumePutFilesRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0cVolumeReload\x12!.modal.client.VolumeReloadRequest\x1a\x16.google.protobuf.Empty\x12Q\n\x10VolumeRemoveFile\x12%.modal.client.VolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12O\n\x0fVolumeCopyFiles\x12$.modal.client.VolumeCopyFilesRequest\x1a\x16.google.protobuf.Empty\x12X\n\x13WorkspaceNameLookup\x12\x16.google.protobuf.Empty\x1a).modal.client.WorkspaceNameLookupResponseb\x06proto3')
 
 _APPDEPLOYVISIBILITY = DESCRIPTOR.enum_types_by_name['AppDeployVisibility']
 AppDeployVisibility = enum_type_wrapper.EnumTypeWrapper(_APPDEPLOYVISIBILITY)
 _APPDISCONNECTREASON = DESCRIPTOR.enum_types_by_name['AppDisconnectReason']
 AppDisconnectReason = enum_type_wrapper.EnumTypeWrapper(_APPDISCONNECTREASON)
 _APPSTATE = DESCRIPTOR.enum_types_by_name['AppState']
 AppState = enum_type_wrapper.EnumTypeWrapper(_APPSTATE)
@@ -100,15 +100,14 @@
 CLIENT_TYPE_SERVER = 4
 CLIENT_TYPE_WEB_SERVER = 5
 CLOUD_PROVIDER_UNSPECIFIED = 0
 CLOUD_PROVIDER_AWS = 1
 CLOUD_PROVIDER_GCP = 2
 CLOUD_PROVIDER_AUTO = 3
 CLOUD_PROVIDER_OCI = 4
-CLOUD_PROVIDER_LAMBDA_LABS = 5
 DATA_FORMAT_UNSPECIFIED = 0
 DATA_FORMAT_PICKLE = 1
 DATA_FORMAT_ASGI = 2
 DATA_FORMAT_GENERATOR_DONE = 3
 DEPLOYMENT_NAMESPACE_UNSPECIFIED = 0
 DEPLOYMENT_NAMESPACE_WORKSPACE = 1
 DEPLOYMENT_NAMESPACE_GLOBAL = 3
@@ -169,15 +168,14 @@
 WEBHOOK_ASYNC_MODE_UNSPECIFIED = 0
 WEBHOOK_ASYNC_MODE_DISABLED = 2
 WEBHOOK_ASYNC_MODE_TRIGGER = 3
 WEBHOOK_ASYNC_MODE_AUTO = 4
 
 
 _CLOUDBUCKETMOUNT = DESCRIPTOR.message_types_by_name['CloudBucketMount']
-_FILEENTRY = DESCRIPTOR.message_types_by_name['FileEntry']
 _APPCLIENTDISCONNECTREQUEST = DESCRIPTOR.message_types_by_name['AppClientDisconnectRequest']
 _APPCREATEREQUEST = DESCRIPTOR.message_types_by_name['AppCreateRequest']
 _APPCREATERESPONSE = DESCRIPTOR.message_types_by_name['AppCreateResponse']
 _APPSTOPREQUEST = DESCRIPTOR.message_types_by_name['AppStopRequest']
 _APPDEPLOYREQUEST = DESCRIPTOR.message_types_by_name['AppDeployRequest']
 _APPDEPLOYRESPONSE = DESCRIPTOR.message_types_by_name['AppDeployResponse']
 _APPDEPLOYSINGLEOBJECTREQUEST = DESCRIPTOR.message_types_by_name['AppDeploySingleObjectRequest']
@@ -246,15 +244,14 @@
 _DICTCREATERESPONSE = DESCRIPTOR.message_types_by_name['DictCreateResponse']
 _DICTGETORCREATEREQUEST = DESCRIPTOR.message_types_by_name['DictGetOrCreateRequest']
 _DICTGETORCREATERESPONSE = DESCRIPTOR.message_types_by_name['DictGetOrCreateResponse']
 _DICTENTRY = DESCRIPTOR.message_types_by_name['DictEntry']
 _DICTGETREQUEST = DESCRIPTOR.message_types_by_name['DictGetRequest']
 _DICTGETRESPONSE = DESCRIPTOR.message_types_by_name['DictGetResponse']
 _DICTHEARTBEATREQUEST = DESCRIPTOR.message_types_by_name['DictHeartbeatRequest']
-_DICTCONTENTSREQUEST = DESCRIPTOR.message_types_by_name['DictContentsRequest']
 _DICTLENREQUEST = DESCRIPTOR.message_types_by_name['DictLenRequest']
 _DICTLENRESPONSE = DESCRIPTOR.message_types_by_name['DictLenResponse']
 _DICTPOPREQUEST = DESCRIPTOR.message_types_by_name['DictPopRequest']
 _DICTPOPRESPONSE = DESCRIPTOR.message_types_by_name['DictPopResponse']
 _DICTUPDATEREQUEST = DESCRIPTOR.message_types_by_name['DictUpdateRequest']
 _DICTUPDATERESPONSE = DESCRIPTOR.message_types_by_name['DictUpdateResponse']
 _DNSRECORD = DESCRIPTOR.message_types_by_name['DNSRecord']
@@ -315,14 +312,15 @@
 _FUNCTIONGETCURRENTSTATSREQUEST = DESCRIPTOR.message_types_by_name['FunctionGetCurrentStatsRequest']
 _FUNCTIONSTATS = DESCRIPTOR.message_types_by_name['FunctionStats']
 _GENERATORDONE = DESCRIPTOR.message_types_by_name['GeneratorDone']
 _GENERICRESULT = DESCRIPTOR.message_types_by_name['GenericResult']
 _GPUCONFIG = DESCRIPTOR.message_types_by_name['GPUConfig']
 _BUILDFUNCTION = DESCRIPTOR.message_types_by_name['BuildFunction']
 _IMAGE = DESCRIPTOR.message_types_by_name['Image']
+_IMAGEBUILDERVERSIONLOOKUPRESPONSE = DESCRIPTOR.message_types_by_name['ImageBuilderVersionLookupResponse']
 _IMAGECONTEXTFILE = DESCRIPTOR.message_types_by_name['ImageContextFile']
 _IMAGEGETORCREATEREQUEST = DESCRIPTOR.message_types_by_name['ImageGetOrCreateRequest']
 _IMAGEGETORCREATERESPONSE = DESCRIPTOR.message_types_by_name['ImageGetOrCreateResponse']
 _IMAGEJOINSTREAMINGREQUEST = DESCRIPTOR.message_types_by_name['ImageJoinStreamingRequest']
 _IMAGEJOINSTREAMINGRESPONSE = DESCRIPTOR.message_types_by_name['ImageJoinStreamingResponse']
 _IMAGEREGISTRYCONFIG = DESCRIPTOR.message_types_by_name['ImageRegistryConfig']
 _INPUTINFO = DESCRIPTOR.message_types_by_name['InputInfo']
@@ -348,17 +346,14 @@
 _QUEUEGETORCREATERESPONSE = DESCRIPTOR.message_types_by_name['QueueGetOrCreateResponse']
 _QUEUEGETREQUEST = DESCRIPTOR.message_types_by_name['QueueGetRequest']
 _QUEUEGETRESPONSE = DESCRIPTOR.message_types_by_name['QueueGetResponse']
 _QUEUEHEARTBEATREQUEST = DESCRIPTOR.message_types_by_name['QueueHeartbeatRequest']
 _QUEUEPUTREQUEST = DESCRIPTOR.message_types_by_name['QueuePutRequest']
 _QUEUELENREQUEST = DESCRIPTOR.message_types_by_name['QueueLenRequest']
 _QUEUELENRESPONSE = DESCRIPTOR.message_types_by_name['QueueLenResponse']
-_QUEUENEXTITEMSREQUEST = DESCRIPTOR.message_types_by_name['QueueNextItemsRequest']
-_QUEUEITEM = DESCRIPTOR.message_types_by_name['QueueItem']
-_QUEUENEXTITEMSRESPONSE = DESCRIPTOR.message_types_by_name['QueueNextItemsResponse']
 _RATELIMIT = DESCRIPTOR.message_types_by_name['RateLimit']
 _RESOURCES = DESCRIPTOR.message_types_by_name['Resources']
 _S3MOUNT = DESCRIPTOR.message_types_by_name['S3Mount']
 _SANDBOX = DESCRIPTOR.message_types_by_name['Sandbox']
 _SANDBOXCREATEREQUEST = DESCRIPTOR.message_types_by_name['SandboxCreateRequest']
 _SANDBOXCREATERESPONSE = DESCRIPTOR.message_types_by_name['SandboxCreateResponse']
 _SANDBOXGETTASKIDREQUEST = DESCRIPTOR.message_types_by_name['SandboxGetTaskIdRequest']
@@ -396,14 +391,15 @@
 _SHAREDVOLUMELISTRESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumeListResponse']
 _SHAREDVOLUMELISTFILESREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumeListFilesRequest']
 _SHAREDVOLUMEPUTFILEREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumePutFileRequest']
 _SHAREDVOLUMEPUTFILERESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumePutFileResponse']
 _SHAREDVOLUMEGETFILEREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumeGetFileRequest']
 _SHAREDVOLUMEGETFILERESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumeGetFileResponse']
 _SHAREDVOLUMEREMOVEFILEREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumeRemoveFileRequest']
+_SHAREDVOLUMELISTFILESENTRY = DESCRIPTOR.message_types_by_name['SharedVolumeListFilesEntry']
 _SHAREDVOLUMELISTFILESRESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumeListFilesResponse']
 _SHAREDVOLUMEMOUNT = DESCRIPTOR.message_types_by_name['SharedVolumeMount']
 _TASKCURRENTINPUTSRESPONSE = DESCRIPTOR.message_types_by_name['TaskCurrentInputsResponse']
 _TASKINFO = DESCRIPTOR.message_types_by_name['TaskInfo']
 _TASKLOGS = DESCRIPTOR.message_types_by_name['TaskLogs']
 _TASKLISTREQUEST = DESCRIPTOR.message_types_by_name['TaskListRequest']
 _TASKLISTRESPONSE = DESCRIPTOR.message_types_by_name['TaskListResponse']
@@ -425,14 +421,15 @@
 _VOLUMECREATEREQUEST = DESCRIPTOR.message_types_by_name['VolumeCreateRequest']
 _VOLUMECREATERESPONSE = DESCRIPTOR.message_types_by_name['VolumeCreateResponse']
 _VOLUMECOMMITREQUEST = DESCRIPTOR.message_types_by_name['VolumeCommitRequest']
 _VOLUMECOMMITRESPONSE = DESCRIPTOR.message_types_by_name['VolumeCommitResponse']
 _VOLUMEDELETEREQUEST = DESCRIPTOR.message_types_by_name['VolumeDeleteRequest']
 _VOLUMEGETFILEREQUEST = DESCRIPTOR.message_types_by_name['VolumeGetFileRequest']
 _VOLUMEGETFILERESPONSE = DESCRIPTOR.message_types_by_name['VolumeGetFileResponse']
+_VOLUMELISTFILESENTRY = DESCRIPTOR.message_types_by_name['VolumeListFilesEntry']
 _VOLUMELISTFILESREQUEST = DESCRIPTOR.message_types_by_name['VolumeListFilesRequest']
 _VOLUMELISTFILESRESPONSE = DESCRIPTOR.message_types_by_name['VolumeListFilesResponse']
 _VOLUMELISTITEM = DESCRIPTOR.message_types_by_name['VolumeListItem']
 _VOLUMELISTREQUEST = DESCRIPTOR.message_types_by_name['VolumeListRequest']
 _VOLUMELISTRESPONSE = DESCRIPTOR.message_types_by_name['VolumeListResponse']
 _VOLUMERELOADREQUEST = DESCRIPTOR.message_types_by_name['VolumeReloadRequest']
 _VOLUMEPUTFILESREQUEST = DESCRIPTOR.message_types_by_name['VolumePutFilesRequest']
@@ -442,34 +439,28 @@
 _WEBHOOKCONFIG = DESCRIPTOR.message_types_by_name['WebhookConfig']
 _WEBURLINFO = DESCRIPTOR.message_types_by_name['WebUrlInfo']
 _WORKSPACENAMELOOKUPRESPONSE = DESCRIPTOR.message_types_by_name['WorkspaceNameLookupResponse']
 _RUNTIMEOUTPUTMESSAGE = DESCRIPTOR.message_types_by_name['RuntimeOutputMessage']
 _RUNTIMEOUTPUTBATCH = DESCRIPTOR.message_types_by_name['RuntimeOutputBatch']
 _RUNTIMEINPUTMESSAGE = DESCRIPTOR.message_types_by_name['RuntimeInputMessage']
 _CLOUDBUCKETMOUNT_BUCKETTYPE = _CLOUDBUCKETMOUNT.enum_types_by_name['BucketType']
-_FILEENTRY_FILETYPE = _FILEENTRY.enum_types_by_name['FileType']
 _FUNCTION_DEFINITIONTYPE = _FUNCTION.enum_types_by_name['DefinitionType']
 _FUNCTION_FUNCTIONTYPE = _FUNCTION.enum_types_by_name['FunctionType']
 _GENERICRESULT_GENERICSTATUS = _GENERICRESULT.enum_types_by_name['GenericStatus']
 _GENERICRESULT_GENERATORSTATUS = _GENERICRESULT.enum_types_by_name['GeneratorStatus']
 _PTYINFO_PTYTYPE = _PTYINFO.enum_types_by_name['PTYType']
+_SHAREDVOLUMELISTFILESENTRY_FILETYPE = _SHAREDVOLUMELISTFILESENTRY.enum_types_by_name['FileType']
+_VOLUMELISTFILESENTRY_FILETYPE = _VOLUMELISTFILESENTRY.enum_types_by_name['FileType']
 CloudBucketMount = _reflection.GeneratedProtocolMessageType('CloudBucketMount', (_message.Message,), {
   'DESCRIPTOR' : _CLOUDBUCKETMOUNT,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.CloudBucketMount)
   })
 _sym_db.RegisterMessage(CloudBucketMount)
 
-FileEntry = _reflection.GeneratedProtocolMessageType('FileEntry', (_message.Message,), {
-  'DESCRIPTOR' : _FILEENTRY,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.FileEntry)
-  })
-_sym_db.RegisterMessage(FileEntry)
-
 AppClientDisconnectRequest = _reflection.GeneratedProtocolMessageType('AppClientDisconnectRequest', (_message.Message,), {
   'DESCRIPTOR' : _APPCLIENTDISCONNECTREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.AppClientDisconnectRequest)
   })
 _sym_db.RegisterMessage(AppClientDisconnectRequest)
 
@@ -1009,21 +1000,14 @@
 DictHeartbeatRequest = _reflection.GeneratedProtocolMessageType('DictHeartbeatRequest', (_message.Message,), {
   'DESCRIPTOR' : _DICTHEARTBEATREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.DictHeartbeatRequest)
   })
 _sym_db.RegisterMessage(DictHeartbeatRequest)
 
-DictContentsRequest = _reflection.GeneratedProtocolMessageType('DictContentsRequest', (_message.Message,), {
-  'DESCRIPTOR' : _DICTCONTENTSREQUEST,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.DictContentsRequest)
-  })
-_sym_db.RegisterMessage(DictContentsRequest)
-
 DictLenRequest = _reflection.GeneratedProtocolMessageType('DictLenRequest', (_message.Message,), {
   'DESCRIPTOR' : _DICTLENREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.DictLenRequest)
   })
 _sym_db.RegisterMessage(DictLenRequest)
 
@@ -1492,14 +1476,21 @@
 Image = _reflection.GeneratedProtocolMessageType('Image', (_message.Message,), {
   'DESCRIPTOR' : _IMAGE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.Image)
   })
 _sym_db.RegisterMessage(Image)
 
+ImageBuilderVersionLookupResponse = _reflection.GeneratedProtocolMessageType('ImageBuilderVersionLookupResponse', (_message.Message,), {
+  'DESCRIPTOR' : _IMAGEBUILDERVERSIONLOOKUPRESPONSE,
+  '__module__' : 'modal_proto.api_pb2'
+  # @@protoc_insertion_point(class_scope:modal.client.ImageBuilderVersionLookupResponse)
+  })
+_sym_db.RegisterMessage(ImageBuilderVersionLookupResponse)
+
 ImageContextFile = _reflection.GeneratedProtocolMessageType('ImageContextFile', (_message.Message,), {
   'DESCRIPTOR' : _IMAGECONTEXTFILE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.ImageContextFile)
   })
 _sym_db.RegisterMessage(ImageContextFile)
 
@@ -1723,35 +1714,14 @@
 QueueLenResponse = _reflection.GeneratedProtocolMessageType('QueueLenResponse', (_message.Message,), {
   'DESCRIPTOR' : _QUEUELENRESPONSE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.QueueLenResponse)
   })
 _sym_db.RegisterMessage(QueueLenResponse)
 
-QueueNextItemsRequest = _reflection.GeneratedProtocolMessageType('QueueNextItemsRequest', (_message.Message,), {
-  'DESCRIPTOR' : _QUEUENEXTITEMSREQUEST,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.QueueNextItemsRequest)
-  })
-_sym_db.RegisterMessage(QueueNextItemsRequest)
-
-QueueItem = _reflection.GeneratedProtocolMessageType('QueueItem', (_message.Message,), {
-  'DESCRIPTOR' : _QUEUEITEM,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.QueueItem)
-  })
-_sym_db.RegisterMessage(QueueItem)
-
-QueueNextItemsResponse = _reflection.GeneratedProtocolMessageType('QueueNextItemsResponse', (_message.Message,), {
-  'DESCRIPTOR' : _QUEUENEXTITEMSRESPONSE,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.QueueNextItemsResponse)
-  })
-_sym_db.RegisterMessage(QueueNextItemsResponse)
-
 RateLimit = _reflection.GeneratedProtocolMessageType('RateLimit', (_message.Message,), {
   'DESCRIPTOR' : _RATELIMIT,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.RateLimit)
   })
 _sym_db.RegisterMessage(RateLimit)
 
@@ -2063,14 +2033,21 @@
 SharedVolumeRemoveFileRequest = _reflection.GeneratedProtocolMessageType('SharedVolumeRemoveFileRequest', (_message.Message,), {
   'DESCRIPTOR' : _SHAREDVOLUMEREMOVEFILEREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.SharedVolumeRemoveFileRequest)
   })
 _sym_db.RegisterMessage(SharedVolumeRemoveFileRequest)
 
+SharedVolumeListFilesEntry = _reflection.GeneratedProtocolMessageType('SharedVolumeListFilesEntry', (_message.Message,), {
+  'DESCRIPTOR' : _SHAREDVOLUMELISTFILESENTRY,
+  '__module__' : 'modal_proto.api_pb2'
+  # @@protoc_insertion_point(class_scope:modal.client.SharedVolumeListFilesEntry)
+  })
+_sym_db.RegisterMessage(SharedVolumeListFilesEntry)
+
 SharedVolumeListFilesResponse = _reflection.GeneratedProtocolMessageType('SharedVolumeListFilesResponse', (_message.Message,), {
   'DESCRIPTOR' : _SHAREDVOLUMELISTFILESRESPONSE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.SharedVolumeListFilesResponse)
   })
 _sym_db.RegisterMessage(SharedVolumeListFilesResponse)
 
@@ -2266,14 +2243,21 @@
 VolumeGetFileResponse = _reflection.GeneratedProtocolMessageType('VolumeGetFileResponse', (_message.Message,), {
   'DESCRIPTOR' : _VOLUMEGETFILERESPONSE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.VolumeGetFileResponse)
   })
 _sym_db.RegisterMessage(VolumeGetFileResponse)
 
+VolumeListFilesEntry = _reflection.GeneratedProtocolMessageType('VolumeListFilesEntry', (_message.Message,), {
+  'DESCRIPTOR' : _VOLUMELISTFILESENTRY,
+  '__module__' : 'modal_proto.api_pb2'
+  # @@protoc_insertion_point(class_scope:modal.client.VolumeListFilesEntry)
+  })
+_sym_db.RegisterMessage(VolumeListFilesEntry)
+
 VolumeListFilesRequest = _reflection.GeneratedProtocolMessageType('VolumeListFilesRequest', (_message.Message,), {
   'DESCRIPTOR' : _VOLUMELISTFILESREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.VolumeListFilesRequest)
   })
 _sym_db.RegisterMessage(VolumeListFilesRequest)
 
@@ -2444,614 +2428,610 @@
   _VOLUMECREATEREQUEST.fields_by_name['app_id']._serialized_options = b'\200\265\030\001'
   _VOLUMECOMMITREQUEST.fields_by_name['volume_id']._options = None
   _VOLUMECOMMITREQUEST.fields_by_name['volume_id']._serialized_options = b'\200\265\030\001'
   _VOLUMEDELETEREQUEST.fields_by_name['environment_name']._options = None
   _VOLUMEDELETEREQUEST.fields_by_name['environment_name']._serialized_options = b'\030\001'
   _VOLUMEREMOVEFILEREQUEST.fields_by_name['volume_id']._options = None
   _VOLUMEREMOVEFILEREQUEST.fields_by_name['volume_id']._serialized_options = b'\200\265\030\001'
-  _WORKSPACENAMELOOKUPRESPONSE.fields_by_name['workspace_name']._options = None
-  _WORKSPACENAMELOOKUPRESPONSE.fields_by_name['workspace_name']._serialized_options = b'\030\001'
-  _APPDEPLOYVISIBILITY._serialized_start=31253
-  _APPDEPLOYVISIBILITY._serialized_end=31384
-  _APPDISCONNECTREASON._serialized_start=31387
-  _APPDISCONNECTREASON._serialized_end=31632
-  _APPSTATE._serialized_start=31635
-  _APPSTATE._serialized_end=31904
-  _APPSTOPSOURCE._serialized_start=31907
-  _APPSTOPSOURCE._serialized_end=32040
-  _CERTIFICATESTATUS._serialized_start=32043
-  _CERTIFICATESTATUS._serialized_end=32188
-  _CHECKPOINTSTATUS._serialized_start=32191
-  _CHECKPOINTSTATUS._serialized_end=32368
-  _CLIENTTYPE._serialized_start=32371
-  _CLIENTTYPE._serialized_end=32547
-  _CLOUDPROVIDER._serialized_start=32550
-  _CLOUDPROVIDER._serialized_end=32726
-  _DATAFORMAT._serialized_start=32728
-  _DATAFORMAT._serialized_end=32847
-  _DEPLOYMENTNAMESPACE._serialized_start=32850
-  _DEPLOYMENTNAMESPACE._serialized_end=32978
-  _DNSRECORDTYPE._serialized_start=32980
-  _DNSRECORDTYPE._serialized_end=33070
-  _FILEDESCRIPTOR._serialized_start=33073
-  _FILEDESCRIPTOR._serialized_end=33204
-  _FUNCTIONCALLTYPE._serialized_start=33206
-  _FUNCTIONCALLTYPE._serialized_end=33318
-  _GPUTYPE._serialized_start=33321
-  _GPUTYPE._serialized_end=33579
-  _OBJECTCREATIONTYPE._serialized_start=33582
-  _OBJECTCREATIONTYPE._serialized_end=33870
-  _PROGRESSTYPE._serialized_start=33872
-  _PROGRESSTYPE._serialized_end=33934
-  _RATELIMITINTERVAL._serialized_start=33936
-  _RATELIMITINTERVAL._serialized_end=34056
-  _REGISTRYAUTHTYPE._serialized_start=34059
-  _REGISTRYAUTHTYPE._serialized_end=34237
-  _TASKSTATE._serialized_start=34240
-  _TASKSTATE._serialized_end=34588
-  _WEBHOOKTYPE._serialized_start=34591
-  _WEBHOOKTYPE._serialized_end=34744
-  _WEBHOOKASYNCMODE._serialized_start=34747
-  _WEBHOOKASYNCMODE._serialized_end=34901
+  _APPDEPLOYVISIBILITY._serialized_start=31021
+  _APPDEPLOYVISIBILITY._serialized_end=31152
+  _APPDISCONNECTREASON._serialized_start=31155
+  _APPDISCONNECTREASON._serialized_end=31400
+  _APPSTATE._serialized_start=31403
+  _APPSTATE._serialized_end=31672
+  _APPSTOPSOURCE._serialized_start=31675
+  _APPSTOPSOURCE._serialized_end=31808
+  _CERTIFICATESTATUS._serialized_start=31811
+  _CERTIFICATESTATUS._serialized_end=31956
+  _CHECKPOINTSTATUS._serialized_start=31959
+  _CHECKPOINTSTATUS._serialized_end=32136
+  _CLIENTTYPE._serialized_start=32139
+  _CLIENTTYPE._serialized_end=32315
+  _CLOUDPROVIDER._serialized_start=32318
+  _CLOUDPROVIDER._serialized_end=32462
+  _DATAFORMAT._serialized_start=32464
+  _DATAFORMAT._serialized_end=32583
+  _DEPLOYMENTNAMESPACE._serialized_start=32586
+  _DEPLOYMENTNAMESPACE._serialized_end=32714
+  _DNSRECORDTYPE._serialized_start=32716
+  _DNSRECORDTYPE._serialized_end=32806
+  _FILEDESCRIPTOR._serialized_start=32809
+  _FILEDESCRIPTOR._serialized_end=32940
+  _FUNCTIONCALLTYPE._serialized_start=32942
+  _FUNCTIONCALLTYPE._serialized_end=33054
+  _GPUTYPE._serialized_start=33057
+  _GPUTYPE._serialized_end=33315
+  _OBJECTCREATIONTYPE._serialized_start=33318
+  _OBJECTCREATIONTYPE._serialized_end=33606
+  _PROGRESSTYPE._serialized_start=33608
+  _PROGRESSTYPE._serialized_end=33670
+  _RATELIMITINTERVAL._serialized_start=33672
+  _RATELIMITINTERVAL._serialized_end=33792
+  _REGISTRYAUTHTYPE._serialized_start=33795
+  _REGISTRYAUTHTYPE._serialized_end=33973
+  _TASKSTATE._serialized_start=33976
+  _TASKSTATE._serialized_end=34324
+  _WEBHOOKTYPE._serialized_start=34327
+  _WEBHOOKTYPE._serialized_end=34480
+  _WEBHOOKASYNCMODE._serialized_start=34483
+  _WEBHOOKASYNCMODE._serialized_end=34637
   _CLOUDBUCKETMOUNT._serialized_start=128
-  _CLOUDBUCKETMOUNT._serialized_end=439
-  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_start=361
-  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_end=415
-  _FILEENTRY._serialized_start=442
-  _FILEENTRY._serialized_end=611
-  _FILEENTRY_FILETYPE._serialized_start=546
-  _FILEENTRY_FILETYPE._serialized_end=611
-  _APPCLIENTDISCONNECTREQUEST._serialized_start=613
-  _APPCLIENTDISCONNECTREQUEST._serialized_end=727
-  _APPCREATEREQUEST._serialized_start=730
-  _APPCREATEREQUEST._serialized_end=909
-  _APPCREATERESPONSE._serialized_start=911
-  _APPCREATERESPONSE._serialized_end=968
-  _APPSTOPREQUEST._serialized_start=970
-  _APPSTOPREQUEST._serialized_end=1053
-  _APPDEPLOYREQUEST._serialized_start=1056
-  _APPDEPLOYREQUEST._serialized_end=1242
-  _APPDEPLOYRESPONSE._serialized_start=1244
-  _APPDEPLOYRESPONSE._serialized_end=1276
-  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_start=1279
-  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_end=1422
-  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_start=1424
-  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_end=1471
-  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_start=1473
-  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_end=1598
-  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_start=1600
-  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_end=1648
-  _APPGETLOGSREQUEST._serialized_start=1651
-  _APPGETLOGSREQUEST._serialized_end=1837
-  _APPGETOBJECTSREQUEST._serialized_start=1839
-  _APPGETOBJECTSREQUEST._serialized_end=1904
-  _APPGETOBJECTSITEM._serialized_start=1906
-  _APPGETOBJECTSITEM._serialized_end=1976
-  _APPGETOBJECTSRESPONSE._serialized_start=1978
-  _APPGETOBJECTSRESPONSE._serialized_end=2049
-  _APPHEARTBEATREQUEST._serialized_start=2051
-  _APPHEARTBEATREQUEST._serialized_end=2088
-  _APPLISTREQUEST._serialized_start=2090
-  _APPLISTREQUEST._serialized_end=2132
-  _APPLISTRESPONSE._serialized_start=2134
-  _APPLISTRESPONSE._serialized_end=2189
-  _APPLOOKUPOBJECTREQUEST._serialized_start=2192
-  _APPLOOKUPOBJECTREQUEST._serialized_end=2376
-  _APPLOOKUPOBJECTRESPONSE._serialized_start=2378
-  _APPLOOKUPOBJECTRESPONSE._serialized_end=2457
-  _APPSETOBJECTSREQUEST._serialized_start=2460
-  _APPSETOBJECTSREQUEST._serialized_end=2763
-  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_start=2708
-  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_end=2763
-  _APPSTATS._serialized_start=2766
-  _APPSTATS._serialized_end=2975
-  _ASGI._serialized_start=2978
-  _ASGI._serialized_end=4805
-  _ASGI_HTTP._serialized_start=3798
-  _ASGI_HTTP._serialized_end=3995
-  _ASGI_HTTPREQUEST._serialized_start=3997
-  _ASGI_HTTPREQUEST._serialized_end=4043
-  _ASGI_HTTPRESPONSESTART._serialized_start=4045
-  _ASGI_HTTPRESPONSESTART._serialized_end=4115
-  _ASGI_HTTPRESPONSEBODY._serialized_start=4117
-  _ASGI_HTTPRESPONSEBODY._serialized_end=4168
-  _ASGI_HTTPRESPONSETRAILERS._serialized_start=4170
-  _ASGI_HTTPRESPONSETRAILERS._serialized_end=4232
-  _ASGI_HTTPDISCONNECT._serialized_start=4234
-  _ASGI_HTTPDISCONNECT._serialized_end=4250
-  _ASGI_WEBSOCKET._serialized_start=4253
-  _ASGI_WEBSOCKET._serialized_end=4461
-  _ASGI_WEBSOCKETCONNECT._serialized_start=4463
-  _ASGI_WEBSOCKETCONNECT._serialized_end=4481
-  _ASGI_WEBSOCKETACCEPT._serialized_start=4483
-  _ASGI_WEBSOCKETACCEPT._serialized_end=4559
-  _ASGI_WEBSOCKETRECEIVE._serialized_start=4561
-  _ASGI_WEBSOCKETRECEIVE._serialized_end=4623
-  _ASGI_WEBSOCKETSEND._serialized_start=4625
-  _ASGI_WEBSOCKETSEND._serialized_end=4684
-  _ASGI_WEBSOCKETDISCONNECT._serialized_start=4686
-  _ASGI_WEBSOCKETDISCONNECT._serialized_end=4735
-  _ASGI_WEBSOCKETCLOSE._serialized_start=4737
-  _ASGI_WEBSOCKETCLOSE._serialized_end=4797
-  _BASEIMAGE._serialized_start=4807
-  _BASEIMAGE._serialized_end=4862
-  _BLOBCREATEREQUEST._serialized_start=4864
-  _BLOBCREATEREQUEST._serialized_end=4959
-  _BLOBCREATERESPONSE._serialized_start=4962
-  _BLOBCREATERESPONSE._serialized_end=5094
-  _BLOBGETREQUEST._serialized_start=5096
-  _BLOBGETREQUEST._serialized_end=5129
-  _BLOBGETRESPONSE._serialized_start=5131
-  _BLOBGETRESPONSE._serialized_end=5170
-  _CHECKPOINTINFO._serialized_start=5172
-  _CHECKPOINTINFO._serialized_end=5277
-  _CLASSCREATEREQUEST._serialized_start=5279
-  _CLASSCREATEREQUEST._serialized_end=5392
-  _CLASSCREATERESPONSE._serialized_start=5394
-  _CLASSCREATERESPONSE._serialized_end=5493
-  _CLASSGETREQUEST._serialized_start=5496
-  _CLASSGETREQUEST._serialized_end=5681
-  _CLASSGETRESPONSE._serialized_start=5683
-  _CLASSGETRESPONSE._serialized_end=5779
-  _CLASSHANDLEMETADATA._serialized_start=5781
-  _CLASSHANDLEMETADATA._serialized_end=5846
-  _CLASSMETHOD._serialized_start=5849
-  _CLASSMETHOD._serialized_end=5978
-  _CLIENTCREATEREQUEST._serialized_start=5980
-  _CLIENTCREATEREQUEST._serialized_end=6071
-  _CLIENTCREATERESPONSE._serialized_start=6073
-  _CLIENTCREATERESPONSE._serialized_end=6158
-  _CLIENTHELLORESPONSE._serialized_start=6160
-  _CLIENTHELLORESPONSE._serialized_end=6229
-  _CLIENTHEARTBEATREQUEST._serialized_start=6231
-  _CLIENTHEARTBEATREQUEST._serialized_end=6334
-  _CONTAINERARGUMENTS._serialized_start=6337
-  _CONTAINERARGUMENTS._serialized_end=6752
-  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_start=6681
-  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_end=6734
-  _CANCELINPUTEVENT._serialized_start=6754
-  _CANCELINPUTEVENT._serialized_end=6791
-  _CONTAINERHEARTBEATRESPONSE._serialized_start=6793
-  _CONTAINERHEARTBEATRESPONSE._serialized_end=6909
-  _CONTAINERHEARTBEATREQUEST._serialized_start=6912
-  _CONTAINERHEARTBEATREQUEST._serialized_end=7045
-  _CONTAINERCHECKPOINTREQUEST._serialized_start=7047
-  _CONTAINERCHECKPOINTREQUEST._serialized_end=7098
-  _CONTAINEREXECREQUEST._serialized_start=7101
-  _CONTAINEREXECREQUEST._serialized_end=7258
-  _CONTAINEREXECGETOUTPUTREQUEST._serialized_start=7260
-  _CONTAINEREXECGETOUTPUTREQUEST._serialized_end=7351
-  _CONTAINEREXECPUTINPUTREQUEST._serialized_start=7353
-  _CONTAINEREXECPUTINPUTREQUEST._serialized_end=7450
-  _CONTAINEREXECRESPONSE._serialized_start=7452
-  _CONTAINEREXECRESPONSE._serialized_end=7492
-  _CUSTOMDOMAINCONFIG._serialized_start=7494
-  _CUSTOMDOMAINCONFIG._serialized_end=7528
-  _CUSTOMDOMAININFO._serialized_start=7530
-  _CUSTOMDOMAININFO._serialized_end=7561
-  _DATACHUNK._serialized_start=7563
-  _DATACHUNK._serialized_end=7690
-  _DICTCLEARREQUEST._serialized_start=7692
-  _DICTCLEARREQUEST._serialized_end=7727
-  _DICTCONTAINSREQUEST._serialized_start=7729
-  _DICTCONTAINSREQUEST._serialized_end=7780
-  _DICTCONTAINSRESPONSE._serialized_start=7782
-  _DICTCONTAINSRESPONSE._serialized_end=7819
-  _DICTCREATEREQUEST._serialized_start=7821
-  _DICTCREATEREQUEST._serialized_end=7927
-  _DICTCREATERESPONSE._serialized_start=7929
-  _DICTCREATERESPONSE._serialized_end=7966
-  _DICTGETORCREATEREQUEST._serialized_start=7969
-  _DICTGETORCREATEREQUEST._serialized_end=8201
-  _DICTGETORCREATERESPONSE._serialized_start=8203
-  _DICTGETORCREATERESPONSE._serialized_end=8245
-  _DICTENTRY._serialized_start=8247
-  _DICTENTRY._serialized_end=8286
-  _DICTGETREQUEST._serialized_start=8288
-  _DICTGETREQUEST._serialized_end=8334
-  _DICTGETRESPONSE._serialized_start=8336
-  _DICTGETRESPONSE._serialized_end=8398
-  _DICTHEARTBEATREQUEST._serialized_start=8400
-  _DICTHEARTBEATREQUEST._serialized_end=8439
-  _DICTCONTENTSREQUEST._serialized_start=8441
-  _DICTCONTENTSREQUEST._serialized_end=8509
-  _DICTLENREQUEST._serialized_start=8511
-  _DICTLENREQUEST._serialized_end=8544
-  _DICTLENRESPONSE._serialized_start=8546
-  _DICTLENRESPONSE._serialized_end=8576
-  _DICTPOPREQUEST._serialized_start=8578
-  _DICTPOPREQUEST._serialized_end=8624
-  _DICTPOPRESPONSE._serialized_start=8626
-  _DICTPOPRESPONSE._serialized_end=8688
-  _DICTUPDATEREQUEST._serialized_start=8690
-  _DICTUPDATEREQUEST._serialized_end=8768
-  _DICTUPDATERESPONSE._serialized_start=8770
-  _DICTUPDATERESPONSE._serialized_end=8790
-  _DNSRECORD._serialized_start=8792
-  _DNSRECORD._serialized_end=8875
-  _DOMAINCREATEREQUEST._serialized_start=8877
-  _DOMAINCREATEREQUEST._serialized_end=8925
-  _DOMAINCREATERESPONSE._serialized_start=8927
-  _DOMAINCREATERESPONSE._serialized_end=9014
-  _DOMAINLISTREQUEST._serialized_start=9016
-  _DOMAINLISTREQUEST._serialized_end=9035
-  _DOMAIN._serialized_start=9038
-  _DOMAIN._serialized_end=9213
-  _DOMAINLISTRESPONSE._serialized_start=9215
-  _DOMAINLISTRESPONSE._serialized_end=9274
-  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_start=9276
-  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_end=9327
-  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_start=9329
-  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_end=9400
-  _ENVIRONMENTCREATEREQUEST._serialized_start=9402
-  _ENVIRONMENTCREATEREQUEST._serialized_end=9442
-  _ENVIRONMENTDELETEREQUEST._serialized_start=9444
-  _ENVIRONMENTDELETEREQUEST._serialized_end=9484
-  _ENVIRONMENTLISTITEM._serialized_start=9486
-  _ENVIRONMENTLISTITEM._serialized_end=9545
-  _ENVIRONMENTLISTRESPONSE._serialized_start=9547
-  _ENVIRONMENTLISTRESPONSE._serialized_end=9622
-  _ENVIRONMENTUPDATEREQUEST._serialized_start=9625
-  _ENVIRONMENTUPDATEREQUEST._serialized_end=9767
-  _FUNCTIONCALLPUTDATAREQUEST._serialized_start=9769
-  _FUNCTIONCALLPUTDATAREQUEST._serialized_end=9869
-  _FUNCTIONCALLGETDATAREQUEST._serialized_start=9871
-  _FUNCTIONCALLGETDATAREQUEST._serialized_end=9945
-  _FUNCTION._serialized_start=9948
-  _FUNCTION._serialized_end=11856
-  _FUNCTION_DEFINITIONTYPE._serialized_start=11575
-  _FUNCTION_DEFINITIONTYPE._serialized_end=11682
-  _FUNCTION_FUNCTIONTYPE._serialized_start=11684
-  _FUNCTION_FUNCTIONTYPE._serialized_end=11786
-  _SCHEDULERPLACEMENT._serialized_start=11858
-  _SCHEDULERPLACEMENT._serialized_end=11982
-  _FUNCTIONHANDLEMETADATA._serialized_start=11985
-  _FUNCTIONHANDLEMETADATA._serialized_end=12128
-  _FUNCTIONCREATEREQUEST._serialized_start=12131
-  _FUNCTIONCREATEREQUEST._serialized_end=12290
-  _FUNCTIONOPTIONS._serialized_start=12293
-  _FUNCTIONOPTIONS._serialized_end=12876
-  _FUNCTIONPRECREATEREQUEST._serialized_start=12879
-  _FUNCTIONPRECREATEREQUEST._serialized_end=13093
-  _FUNCTIONPRECREATERESPONSE._serialized_start=13095
-  _FUNCTIONPRECREATERESPONSE._serialized_end=13206
-  _FUNCTIONBINDPARAMSREQUEST._serialized_start=13209
-  _FUNCTIONBINDPARAMSREQUEST._serialized_end=13367
-  _FUNCTIONBINDPARAMSRESPONSE._serialized_start=13369
-  _FUNCTIONBINDPARAMSRESPONSE._serialized_end=13487
-  _FUNCTIONCREATERESPONSE._serialized_start=13490
-  _FUNCTIONCREATERESPONSE._serialized_end=13705
-  _FUNCTIONGETREQUEST._serialized_start=13708
-  _FUNCTIONGETREQUEST._serialized_end=13846
-  _FUNCTIONGETRESPONSE._serialized_start=13848
-  _FUNCTIONGETRESPONSE._serialized_end=13953
-  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_start=13955
-  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_end=14048
-  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_start=14050
-  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_end=14090
-  _FUNCTIONGETINPUTSITEM._serialized_start=14093
-  _FUNCTIONGETINPUTSITEM._serialized_end=14231
-  _FUNCTIONGETINPUTSREQUEST._serialized_start=14233
-  _FUNCTIONGETINPUTSREQUEST._serialized_end=14354
-  _FUNCTIONGETINPUTSRESPONSE._serialized_start=14356
-  _FUNCTIONGETINPUTSRESPONSE._serialized_end=14471
-  _FUNCTIONGETOUTPUTSITEM._serialized_start=14474
-  _FUNCTIONGETOUTPUTSITEM._serialized_end=14640
-  _FUNCTIONGETOUTPUTSREQUEST._serialized_start=14643
-  _FUNCTIONGETOUTPUTSREQUEST._serialized_end=14782
-  _FUNCTIONGETOUTPUTSRESPONSE._serialized_start=14784
-  _FUNCTIONGETOUTPUTSRESPONSE._serialized_end=14904
-  _FUNCTIONGETSERIALIZEDREQUEST._serialized_start=14906
-  _FUNCTIONGETSERIALIZEDREQUEST._serialized_end=14957
-  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_start=14959
-  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_end=15045
-  _FUNCTIONINPUT._serialized_start=15048
-  _FUNCTIONINPUT._serialized_end=15185
-  _FUNCTIONMAPREQUEST._serialized_start=15188
-  _FUNCTIONMAPREQUEST._serialized_end=15404
-  _FUNCTIONMAPRESPONSE._serialized_start=15406
-  _FUNCTIONMAPRESPONSE._serialized_end=15524
-  _FUNCTIONPUTINPUTSITEM._serialized_start=15526
-  _FUNCTIONPUTINPUTSITEM._serialized_end=15606
-  _FUNCTIONPUTINPUTSREQUEST._serialized_start=15608
-  _FUNCTIONPUTINPUTSREQUEST._serialized_end=15734
-  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_start=15736
-  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_end=15798
-  _FUNCTIONPUTINPUTSRESPONSE._serialized_start=15800
-  _FUNCTIONPUTINPUTSRESPONSE._serialized_end=15888
-  _FUNCTIONPUTOUTPUTSITEM._serialized_start=15891
-  _FUNCTIONPUTOUTPUTSITEM._serialized_end=16097
-  _FUNCTIONPUTOUTPUTSREQUEST._serialized_start=16099
-  _FUNCTIONPUTOUTPUTSREQUEST._serialized_end=16181
-  _FUNCTIONRETRYPOLICY._serialized_start=16183
-  _FUNCTIONRETRYPOLICY._serialized_end=16298
-  _FUNCTIONGETCALLGRAPHREQUEST._serialized_start=16300
-  _FUNCTIONGETCALLGRAPHREQUEST._serialized_end=16355
-  _INPUTCALLGRAPHINFO._serialized_start=16358
-  _INPUTCALLGRAPHINFO._serialized_end=16498
-  _FUNCTIONCALLCALLGRAPHINFO._serialized_start=16500
-  _FUNCTIONCALLCALLGRAPHINFO._serialized_end=16622
-  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_start=16625
-  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_end=16770
-  _FUNCTIONCALLCANCELREQUEST._serialized_start=16772
-  _FUNCTIONCALLCANCELREQUEST._serialized_end=16825
-  _FUNCTIONCALLLISTREQUEST._serialized_start=16827
-  _FUNCTIONCALLLISTREQUEST._serialized_end=16873
-  _FUNCTIONCALLINFO._serialized_start=16876
-  _FUNCTIONCALLINFO._serialized_end=17327
-  _FUNCTIONCALLLISTRESPONSE._serialized_start=17329
-  _FUNCTIONCALLLISTRESPONSE._serialized_end=17411
-  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_start=17413
-  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_end=17466
-  _FUNCTIONSTATS._serialized_start=17468
-  _FUNCTIONSTATS._serialized_end=17551
-  _GENERATORDONE._serialized_start=17553
-  _GENERATORDONE._serialized_end=17589
-  _GENERICRESULT._serialized_start=17592
-  _GENERICRESULT._serialized_end=18230
-  _GENERICRESULT_GENERICSTATUS._serialized_start=17904
-  _GENERICRESULT_GENERICSTATUS._serialized_end=18099
-  _GENERICRESULT_GENERATORSTATUS._serialized_start=18101
-  _GENERICRESULT_GENERATORSTATUS._serialized_end=18216
-  _GPUCONFIG._serialized_start=18232
-  _GPUCONFIG._serialized_end=18311
-  _BUILDFUNCTION._serialized_start=18313
-  _BUILDFUNCTION._serialized_end=18409
-  _IMAGE._serialized_start=18412
-  _IMAGE._serialized_end=18889
-  _IMAGECONTEXTFILE._serialized_start=18891
-  _IMAGECONTEXTFILE._serialized_end=18941
-  _IMAGEGETORCREATEREQUEST._serialized_start=18944
-  _IMAGEGETORCREATEREQUEST._serialized_end=19181
-  _IMAGEGETORCREATERESPONSE._serialized_start=19183
-  _IMAGEGETORCREATERESPONSE._serialized_end=19227
-  _IMAGEJOINSTREAMINGREQUEST._serialized_start=19229
-  _IMAGEJOINSTREAMINGREQUEST._serialized_end=19314
-  _IMAGEJOINSTREAMINGRESPONSE._serialized_start=19317
-  _IMAGEJOINSTREAMINGRESPONSE._serialized_end=19464
-  _IMAGEREGISTRYCONFIG._serialized_start=19466
-  _IMAGEREGISTRYCONFIG._serialized_end=19566
-  _INPUTINFO._serialized_start=19569
-  _INPUTINFO._serialized_end=19722
-  _INPUTCATEGORYINFO._serialized_start=19724
-  _INPUTCATEGORYINFO._serialized_end=19799
-  _MOUNTBUILDREQUEST._serialized_start=19801
-  _MOUNTBUILDREQUEST._serialized_end=19909
-  _MOUNTBUILDRESPONSE._serialized_start=19911
-  _MOUNTBUILDRESPONSE._serialized_end=20009
-  _MOUNTGETORCREATEREQUEST._serialized_start=20012
-  _MOUNTGETORCREATEREQUEST._serialized_end=20262
-  _MOUNTGETORCREATERESPONSE._serialized_start=20264
-  _MOUNTGETORCREATERESPONSE._serialized_end=20368
-  _MOUNTHANDLEMETADATA._serialized_start=20370
-  _MOUNTHANDLEMETADATA._serialized_end=20428
-  _MOUNTFILE._serialized_start=20430
-  _MOUNTFILE._serialized_end=20535
-  _MOUNTPUTFILEREQUEST._serialized_start=20537
-  _MOUNTPUTFILEREQUEST._serialized_end=20632
-  _MOUNTPUTFILERESPONSE._serialized_start=20634
-  _MOUNTPUTFILERESPONSE._serialized_end=20672
-  _MULTIPARTUPLOAD._serialized_start=20674
-  _MULTIPARTUPLOAD._serialized_end=20757
-  _OBJECT._serialized_start=20760
-  _OBJECT._serialized_end=21094
-  _OBJECTDEPENDENCY._serialized_start=21096
-  _OBJECTDEPENDENCY._serialized_end=21133
-  _PROXYGETORCREATEREQUEST._serialized_start=21136
-  _PROXYGETORCREATEREQUEST._serialized_end=21330
-  _PROXYGETORCREATERESPONSE._serialized_start=21332
-  _PROXYGETORCREATERESPONSE._serialized_end=21376
-  _PROXYINFO._serialized_start=21378
-  _PROXYINFO._serialized_end=21470
-  _PTYINFO._serialized_start=21473
-  _PTYINFO._serialized_end=21735
-  _PTYINFO_PTYTYPE._serialized_start=21657
-  _PTYINFO_PTYTYPE._serialized_end=21735
-  _QUEUECREATEREQUEST._serialized_start=21737
-  _QUEUECREATEREQUEST._serialized_end=21800
-  _QUEUECREATERESPONSE._serialized_start=21802
-  _QUEUECREATERESPONSE._serialized_end=21841
-  _QUEUEGETORCREATEREQUEST._serialized_start=21844
-  _QUEUEGETORCREATEREQUEST._serialized_end=22038
-  _QUEUEGETORCREATERESPONSE._serialized_start=22040
-  _QUEUEGETORCREATERESPONSE._serialized_end=22084
-  _QUEUEGETREQUEST._serialized_start=22086
-  _QUEUEGETREQUEST._serialized_end=22179
-  _QUEUEGETRESPONSE._serialized_start=22181
-  _QUEUEGETRESPONSE._serialized_end=22215
-  _QUEUEHEARTBEATREQUEST._serialized_start=22217
-  _QUEUEHEARTBEATREQUEST._serialized_end=22258
-  _QUEUEPUTREQUEST._serialized_start=22260
-  _QUEUEPUTREQUEST._serialized_end=22365
-  _QUEUELENREQUEST._serialized_start=22367
-  _QUEUELENREQUEST._serialized_end=22425
-  _QUEUELENRESPONSE._serialized_start=22427
-  _QUEUELENRESPONSE._serialized_end=22458
-  _QUEUENEXTITEMSREQUEST._serialized_start=22460
-  _QUEUENEXTITEMSREQUEST._serialized_end=22574
-  _QUEUEITEM._serialized_start=22576
-  _QUEUEITEM._serialized_end=22620
-  _QUEUENEXTITEMSRESPONSE._serialized_start=22622
-  _QUEUENEXTITEMSRESPONSE._serialized_end=22686
-  _RATELIMIT._serialized_start=22688
-  _RATELIMIT._serialized_end=22765
-  _RESOURCES._serialized_start=22767
-  _RESOURCES._serialized_end=22884
-  _S3MOUNT._serialized_start=22886
-  _S3MOUNT._serialized_end=22986
-  _SANDBOX._serialized_start=22989
-  _SANDBOX._serialized_end=23526
-  _SANDBOXCREATEREQUEST._serialized_start=23528
-  _SANDBOXCREATEREQUEST._serialized_end=23615
-  _SANDBOXCREATERESPONSE._serialized_start=23617
-  _SANDBOXCREATERESPONSE._serialized_end=23660
-  _SANDBOXGETTASKIDREQUEST._serialized_start=23662
-  _SANDBOXGETTASKIDREQUEST._serialized_end=23707
-  _SANDBOXGETTASKIDRESPONSE._serialized_start=23709
-  _SANDBOXGETTASKIDRESPONSE._serialized_end=23752
-  _SANDBOXGETLOGSREQUEST._serialized_start=23755
-  _SANDBOXGETLOGSREQUEST._serialized_end=23893
-  _SANDBOXSTDINWRITEREQUEST._serialized_start=23895
-  _SANDBOXSTDINWRITEREQUEST._serialized_end=23984
-  _SANDBOXSTDINWRITERESPONSE._serialized_start=23986
-  _SANDBOXSTDINWRITERESPONSE._serialized_end=24013
-  _SANDBOXHANDLEMETADATA._serialized_start=24015
-  _SANDBOXHANDLEMETADATA._serialized_end=24083
-  _SANDBOXINFO._serialized_start=24086
-  _SANDBOXINFO._serialized_end=24217
-  _SANDBOXLISTREQUEST._serialized_start=24219
-  _SANDBOXLISTREQUEST._serialized_end=24281
-  _SANDBOXLISTRESPONSE._serialized_start=24283
-  _SANDBOXLISTRESPONSE._serialized_end=24350
-  _SANDBOXTERMINATEREQUEST._serialized_start=24352
-  _SANDBOXTERMINATEREQUEST._serialized_end=24397
-  _SANDBOXTERMINATERESPONSE._serialized_start=24399
-  _SANDBOXTERMINATERESPONSE._serialized_end=24479
-  _SANDBOXWAITREQUEST._serialized_start=24481
-  _SANDBOXWAITREQUEST._serialized_end=24538
-  _SANDBOXWAITRESPONSE._serialized_start=24540
-  _SANDBOXWAITRESPONSE._serialized_end=24606
-  _SCHEDULE._serialized_start=24609
-  _SCHEDULE._serialized_end=24879
-  _SCHEDULE_CRON._serialized_start=24715
-  _SCHEDULE_CRON._serialized_end=24742
-  _SCHEDULE_PERIOD._serialized_start=24744
-  _SCHEDULE_PERIOD._serialized_end=24861
-  _SECRETCREATEREQUEST._serialized_start=24882
-  _SECRETCREATEREQUEST._serialized_end=25090
-  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_start=25044
-  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_end=25090
-  _SECRETCREATERESPONSE._serialized_start=25092
-  _SECRETCREATERESPONSE._serialized_end=25133
-  _SECRETGETORCREATEREQUEST._serialized_start=25136
-  _SECRETGETORCREATEREQUEST._serialized_end=25466
-  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_start=25044
-  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_end=25090
-  _SECRETGETORCREATERESPONSE._serialized_start=25468
-  _SECRETGETORCREATERESPONSE._serialized_end=25514
-  _SECRETLISTITEM._serialized_start=25516
-  _SECRETLISTITEM._serialized_end=25615
-  _SECRETLISTREQUEST._serialized_start=25617
-  _SECRETLISTREQUEST._serialized_end=25662
-  _SECRETLISTRESPONSE._serialized_start=25664
-  _SECRETLISTRESPONSE._serialized_end=25755
-  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_start=25758
-  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_end=25975
-  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_start=25977
-  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_end=26036
-  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_start=26038
-  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_end=26094
-  _SHAREDVOLUMECREATEREQUEST._serialized_start=26096
-  _SHAREDVOLUMECREATEREQUEST._serialized_end=26198
-  _SHAREDVOLUMECREATERESPONSE._serialized_start=26200
-  _SHAREDVOLUMECREATERESPONSE._serialized_end=26254
-  _SHAREDVOLUMELISTITEM._serialized_start=26257
-  _SHAREDVOLUMELISTITEM._serialized_end=26393
-  _SHAREDVOLUMELISTREQUEST._serialized_start=26395
-  _SHAREDVOLUMELISTREQUEST._serialized_end=26446
-  _SHAREDVOLUMELISTRESPONSE._serialized_start=26448
-  _SHAREDVOLUMELISTRESPONSE._serialized_end=26551
-  _SHAREDVOLUMELISTFILESREQUEST._serialized_start=26553
-  _SHAREDVOLUMELISTFILESREQUEST._serialized_end=26623
-  _SHAREDVOLUMEPUTFILEREQUEST._serialized_start=26626
-  _SHAREDVOLUMEPUTFILEREQUEST._serialized_end=26793
-  _SHAREDVOLUMEPUTFILERESPONSE._serialized_start=26795
-  _SHAREDVOLUMEPUTFILERESPONSE._serialized_end=26840
-  _SHAREDVOLUMEGETFILEREQUEST._serialized_start=26842
-  _SHAREDVOLUMEGETFILEREQUEST._serialized_end=26910
-  _SHAREDVOLUMEGETFILERESPONSE._serialized_start=26912
-  _SHAREDVOLUMEGETFILERESPONSE._serialized_end=26995
-  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_start=26997
-  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_end=27093
-  _SHAREDVOLUMELISTFILESRESPONSE._serialized_start=27095
-  _SHAREDVOLUMELISTFILESRESPONSE._serialized_end=27168
-  _SHAREDVOLUMEMOUNT._serialized_start=27171
-  _SHAREDVOLUMEMOUNT._serialized_end=27317
-  _TASKCURRENTINPUTSRESPONSE._serialized_start=27319
-  _TASKCURRENTINPUTSRESPONSE._serialized_end=27365
-  _TASKINFO._serialized_start=27367
-  _TASKINFO._serialized_end=27475
-  _TASKLOGS._serialized_start=27478
-  _TASKLOGS._serialized_end=27716
-  _TASKLISTREQUEST._serialized_start=27718
-  _TASKLISTREQUEST._serialized_end=27735
-  _TASKLISTRESPONSE._serialized_start=27737
-  _TASKLISTRESPONSE._serialized_end=27795
-  _TASKLOGSBATCH._serialized_start=27798
-  _TASKLOGSBATCH._serialized_end=27996
-  _TASKPROGRESS._serialized_start=27998
-  _TASKPROGRESS._serialized_end=28110
-  _TASKRESULTREQUEST._serialized_start=28112
-  _TASKRESULTREQUEST._serialized_end=28176
-  _TASKSTATS._serialized_start=28178
-  _TASKSTATS._serialized_end=28267
-  _TOKENFLOWCREATEREQUEST._serialized_start=28269
-  _TOKENFLOWCREATEREQUEST._serialized_end=28355
-  _TOKENFLOWCREATERESPONSE._serialized_start=28357
-  _TOKENFLOWCREATERESPONSE._serialized_end=28457
-  _TOKENFLOWWAITREQUEST._serialized_start=28459
-  _TOKENFLOWWAITREQUEST._serialized_end=28542
-  _TOKENFLOWWAITRESPONSE._serialized_start=28544
-  _TOKENFLOWWAITRESPONSE._serialized_end=28652
-  _TUNNELSTARTREQUEST._serialized_start=28654
-  _TUNNELSTARTREQUEST._serialized_end=28709
-  _TUNNELSTARTRESPONSE._serialized_start=28712
-  _TUNNELSTARTRESPONSE._serialized_end=28865
-  _TUNNELSTOPREQUEST._serialized_start=28867
-  _TUNNELSTOPREQUEST._serialized_end=28900
-  _TUNNELSTOPRESPONSE._serialized_start=28902
-  _TUNNELSTOPRESPONSE._serialized_end=28938
-  _VOLUMEGETORCREATEREQUEST._serialized_start=28941
-  _VOLUMEGETORCREATEREQUEST._serialized_end=29152
-  _VOLUMEGETORCREATERESPONSE._serialized_start=29154
-  _VOLUMEGETORCREATERESPONSE._serialized_end=29200
-  _VOLUMEHEARTBEATREQUEST._serialized_start=29202
-  _VOLUMEHEARTBEATREQUEST._serialized_end=29245
-  _VOLUMECREATEREQUEST._serialized_start=29247
-  _VOLUMECREATEREQUEST._serialized_end=29290
-  _VOLUMECREATERESPONSE._serialized_start=29292
-  _VOLUMECREATERESPONSE._serialized_end=29333
-  _VOLUMECOMMITREQUEST._serialized_start=29335
-  _VOLUMECOMMITREQUEST._serialized_end=29381
-  _VOLUMECOMMITRESPONSE._serialized_start=29383
-  _VOLUMECOMMITRESPONSE._serialized_end=29426
-  _VOLUMEDELETEREQUEST._serialized_start=29428
-  _VOLUMEDELETEREQUEST._serialized_end=29498
-  _VOLUMEGETFILEREQUEST._serialized_start=29500
-  _VOLUMEGETFILEREQUEST._serialized_end=29583
-  _VOLUMEGETFILERESPONSE._serialized_start=29585
-  _VOLUMEGETFILERESPONSE._serialized_end=29704
-  _VOLUMELISTFILESREQUEST._serialized_start=29706
-  _VOLUMELISTFILESREQUEST._serialized_end=29805
-  _VOLUMELISTFILESRESPONSE._serialized_start=29807
-  _VOLUMELISTFILESRESPONSE._serialized_end=29874
-  _VOLUMELISTITEM._serialized_start=29876
-  _VOLUMELISTITEM._serialized_end=29946
-  _VOLUMELISTREQUEST._serialized_start=29948
-  _VOLUMELISTREQUEST._serialized_end=29993
-  _VOLUMELISTRESPONSE._serialized_start=29995
-  _VOLUMELISTRESPONSE._serialized_end=30086
-  _VOLUMERELOADREQUEST._serialized_start=30088
-  _VOLUMERELOADREQUEST._serialized_end=30128
-  _VOLUMEPUTFILESREQUEST._serialized_start=30130
-  _VOLUMEPUTFILESREQUEST._serialized_end=30255
-  _VOLUMEREMOVEFILEREQUEST._serialized_start=30257
-  _VOLUMEREMOVEFILEREQUEST._serialized_end=30340
-  _VOLUMECOPYFILESREQUEST._serialized_start=30342
-  _VOLUMECOPYFILESREQUEST._serialized_end=30441
-  _VOLUMEMOUNT._serialized_start=30443
-  _VOLUMEMOUNT._serialized_end=30529
-  _WEBHOOKCONFIG._serialized_start=30532
-  _WEBHOOKCONFIG._serialized_end=30801
-  _WEBURLINFO._serialized_start=30803
-  _WEBURLINFO._serialized_end=30881
-  _WORKSPACENAMELOOKUPRESPONSE._serialized_start=30883
-  _WORKSPACENAMELOOKUPRESPONSE._serialized_end=30958
-  _RUNTIMEOUTPUTMESSAGE._serialized_start=30960
-  _RUNTIMEOUTPUTMESSAGE._serialized_end=31054
-  _RUNTIMEOUTPUTBATCH._serialized_start=31057
-  _RUNTIMEOUTPUTBATCH._serialized_end=31187
-  _RUNTIMEINPUTMESSAGE._serialized_start=31189
-  _RUNTIMEINPUTMESSAGE._serialized_end=31250
-  _MODALCLIENT._serialized_start=34904
-  _MODALCLIENT._serialized_end=44992
+  _CLOUDBUCKETMOUNT._serialized_end=364
+  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_start=327
+  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_end=364
+  _APPCLIENTDISCONNECTREQUEST._serialized_start=366
+  _APPCLIENTDISCONNECTREQUEST._serialized_end=480
+  _APPCREATEREQUEST._serialized_start=483
+  _APPCREATEREQUEST._serialized_end=662
+  _APPCREATERESPONSE._serialized_start=664
+  _APPCREATERESPONSE._serialized_end=721
+  _APPSTOPREQUEST._serialized_start=723
+  _APPSTOPREQUEST._serialized_end=806
+  _APPDEPLOYREQUEST._serialized_start=809
+  _APPDEPLOYREQUEST._serialized_end=995
+  _APPDEPLOYRESPONSE._serialized_start=997
+  _APPDEPLOYRESPONSE._serialized_end=1029
+  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_start=1032
+  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_end=1175
+  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_start=1177
+  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_end=1224
+  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_start=1226
+  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_end=1351
+  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_start=1353
+  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_end=1401
+  _APPGETLOGSREQUEST._serialized_start=1404
+  _APPGETLOGSREQUEST._serialized_end=1590
+  _APPGETOBJECTSREQUEST._serialized_start=1592
+  _APPGETOBJECTSREQUEST._serialized_end=1657
+  _APPGETOBJECTSITEM._serialized_start=1659
+  _APPGETOBJECTSITEM._serialized_end=1729
+  _APPGETOBJECTSRESPONSE._serialized_start=1731
+  _APPGETOBJECTSRESPONSE._serialized_end=1802
+  _APPHEARTBEATREQUEST._serialized_start=1804
+  _APPHEARTBEATREQUEST._serialized_end=1841
+  _APPLISTREQUEST._serialized_start=1843
+  _APPLISTREQUEST._serialized_end=1885
+  _APPLISTRESPONSE._serialized_start=1887
+  _APPLISTRESPONSE._serialized_end=1942
+  _APPLOOKUPOBJECTREQUEST._serialized_start=1945
+  _APPLOOKUPOBJECTREQUEST._serialized_end=2129
+  _APPLOOKUPOBJECTRESPONSE._serialized_start=2131
+  _APPLOOKUPOBJECTRESPONSE._serialized_end=2210
+  _APPSETOBJECTSREQUEST._serialized_start=2213
+  _APPSETOBJECTSREQUEST._serialized_end=2516
+  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_start=2461
+  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_end=2516
+  _APPSTATS._serialized_start=2519
+  _APPSTATS._serialized_end=2728
+  _ASGI._serialized_start=2731
+  _ASGI._serialized_end=4558
+  _ASGI_HTTP._serialized_start=3551
+  _ASGI_HTTP._serialized_end=3748
+  _ASGI_HTTPREQUEST._serialized_start=3750
+  _ASGI_HTTPREQUEST._serialized_end=3796
+  _ASGI_HTTPRESPONSESTART._serialized_start=3798
+  _ASGI_HTTPRESPONSESTART._serialized_end=3868
+  _ASGI_HTTPRESPONSEBODY._serialized_start=3870
+  _ASGI_HTTPRESPONSEBODY._serialized_end=3921
+  _ASGI_HTTPRESPONSETRAILERS._serialized_start=3923
+  _ASGI_HTTPRESPONSETRAILERS._serialized_end=3985
+  _ASGI_HTTPDISCONNECT._serialized_start=3987
+  _ASGI_HTTPDISCONNECT._serialized_end=4003
+  _ASGI_WEBSOCKET._serialized_start=4006
+  _ASGI_WEBSOCKET._serialized_end=4214
+  _ASGI_WEBSOCKETCONNECT._serialized_start=4216
+  _ASGI_WEBSOCKETCONNECT._serialized_end=4234
+  _ASGI_WEBSOCKETACCEPT._serialized_start=4236
+  _ASGI_WEBSOCKETACCEPT._serialized_end=4312
+  _ASGI_WEBSOCKETRECEIVE._serialized_start=4314
+  _ASGI_WEBSOCKETRECEIVE._serialized_end=4376
+  _ASGI_WEBSOCKETSEND._serialized_start=4378
+  _ASGI_WEBSOCKETSEND._serialized_end=4437
+  _ASGI_WEBSOCKETDISCONNECT._serialized_start=4439
+  _ASGI_WEBSOCKETDISCONNECT._serialized_end=4488
+  _ASGI_WEBSOCKETCLOSE._serialized_start=4490
+  _ASGI_WEBSOCKETCLOSE._serialized_end=4550
+  _BASEIMAGE._serialized_start=4560
+  _BASEIMAGE._serialized_end=4615
+  _BLOBCREATEREQUEST._serialized_start=4617
+  _BLOBCREATEREQUEST._serialized_end=4712
+  _BLOBCREATERESPONSE._serialized_start=4715
+  _BLOBCREATERESPONSE._serialized_end=4847
+  _BLOBGETREQUEST._serialized_start=4849
+  _BLOBGETREQUEST._serialized_end=4882
+  _BLOBGETRESPONSE._serialized_start=4884
+  _BLOBGETRESPONSE._serialized_end=4923
+  _CHECKPOINTINFO._serialized_start=4925
+  _CHECKPOINTINFO._serialized_end=5030
+  _CLASSCREATEREQUEST._serialized_start=5032
+  _CLASSCREATEREQUEST._serialized_end=5145
+  _CLASSCREATERESPONSE._serialized_start=5147
+  _CLASSCREATERESPONSE._serialized_end=5246
+  _CLASSGETREQUEST._serialized_start=5249
+  _CLASSGETREQUEST._serialized_end=5434
+  _CLASSGETRESPONSE._serialized_start=5436
+  _CLASSGETRESPONSE._serialized_end=5532
+  _CLASSHANDLEMETADATA._serialized_start=5534
+  _CLASSHANDLEMETADATA._serialized_end=5599
+  _CLASSMETHOD._serialized_start=5602
+  _CLASSMETHOD._serialized_end=5731
+  _CLIENTCREATEREQUEST._serialized_start=5733
+  _CLIENTCREATEREQUEST._serialized_end=5824
+  _CLIENTCREATERESPONSE._serialized_start=5826
+  _CLIENTCREATERESPONSE._serialized_end=5911
+  _CLIENTHELLORESPONSE._serialized_start=5913
+  _CLIENTHELLORESPONSE._serialized_end=5982
+  _CLIENTHEARTBEATREQUEST._serialized_start=5984
+  _CLIENTHEARTBEATREQUEST._serialized_end=6087
+  _CONTAINERARGUMENTS._serialized_start=6090
+  _CONTAINERARGUMENTS._serialized_end=6505
+  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_start=6434
+  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_end=6487
+  _CANCELINPUTEVENT._serialized_start=6507
+  _CANCELINPUTEVENT._serialized_end=6544
+  _CONTAINERHEARTBEATRESPONSE._serialized_start=6546
+  _CONTAINERHEARTBEATRESPONSE._serialized_end=6662
+  _CONTAINERHEARTBEATREQUEST._serialized_start=6665
+  _CONTAINERHEARTBEATREQUEST._serialized_end=6798
+  _CONTAINERCHECKPOINTREQUEST._serialized_start=6800
+  _CONTAINERCHECKPOINTREQUEST._serialized_end=6851
+  _CONTAINEREXECREQUEST._serialized_start=6854
+  _CONTAINEREXECREQUEST._serialized_end=7011
+  _CONTAINEREXECGETOUTPUTREQUEST._serialized_start=7013
+  _CONTAINEREXECGETOUTPUTREQUEST._serialized_end=7104
+  _CONTAINEREXECPUTINPUTREQUEST._serialized_start=7106
+  _CONTAINEREXECPUTINPUTREQUEST._serialized_end=7203
+  _CONTAINEREXECRESPONSE._serialized_start=7205
+  _CONTAINEREXECRESPONSE._serialized_end=7245
+  _CUSTOMDOMAINCONFIG._serialized_start=7247
+  _CUSTOMDOMAINCONFIG._serialized_end=7281
+  _CUSTOMDOMAININFO._serialized_start=7283
+  _CUSTOMDOMAININFO._serialized_end=7314
+  _DATACHUNK._serialized_start=7316
+  _DATACHUNK._serialized_end=7443
+  _DICTCLEARREQUEST._serialized_start=7445
+  _DICTCLEARREQUEST._serialized_end=7480
+  _DICTCONTAINSREQUEST._serialized_start=7482
+  _DICTCONTAINSREQUEST._serialized_end=7533
+  _DICTCONTAINSRESPONSE._serialized_start=7535
+  _DICTCONTAINSRESPONSE._serialized_end=7572
+  _DICTCREATEREQUEST._serialized_start=7574
+  _DICTCREATEREQUEST._serialized_end=7680
+  _DICTCREATERESPONSE._serialized_start=7682
+  _DICTCREATERESPONSE._serialized_end=7719
+  _DICTGETORCREATEREQUEST._serialized_start=7722
+  _DICTGETORCREATEREQUEST._serialized_end=7954
+  _DICTGETORCREATERESPONSE._serialized_start=7956
+  _DICTGETORCREATERESPONSE._serialized_end=7998
+  _DICTENTRY._serialized_start=8000
+  _DICTENTRY._serialized_end=8039
+  _DICTGETREQUEST._serialized_start=8041
+  _DICTGETREQUEST._serialized_end=8087
+  _DICTGETRESPONSE._serialized_start=8089
+  _DICTGETRESPONSE._serialized_end=8151
+  _DICTHEARTBEATREQUEST._serialized_start=8153
+  _DICTHEARTBEATREQUEST._serialized_end=8192
+  _DICTLENREQUEST._serialized_start=8194
+  _DICTLENREQUEST._serialized_end=8227
+  _DICTLENRESPONSE._serialized_start=8229
+  _DICTLENRESPONSE._serialized_end=8259
+  _DICTPOPREQUEST._serialized_start=8261
+  _DICTPOPREQUEST._serialized_end=8307
+  _DICTPOPRESPONSE._serialized_start=8309
+  _DICTPOPRESPONSE._serialized_end=8371
+  _DICTUPDATEREQUEST._serialized_start=8373
+  _DICTUPDATEREQUEST._serialized_end=8451
+  _DICTUPDATERESPONSE._serialized_start=8453
+  _DICTUPDATERESPONSE._serialized_end=8473
+  _DNSRECORD._serialized_start=8475
+  _DNSRECORD._serialized_end=8558
+  _DOMAINCREATEREQUEST._serialized_start=8560
+  _DOMAINCREATEREQUEST._serialized_end=8608
+  _DOMAINCREATERESPONSE._serialized_start=8610
+  _DOMAINCREATERESPONSE._serialized_end=8697
+  _DOMAINLISTREQUEST._serialized_start=8699
+  _DOMAINLISTREQUEST._serialized_end=8718
+  _DOMAIN._serialized_start=8721
+  _DOMAIN._serialized_end=8896
+  _DOMAINLISTRESPONSE._serialized_start=8898
+  _DOMAINLISTRESPONSE._serialized_end=8957
+  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_start=8959
+  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_end=9010
+  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_start=9012
+  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_end=9083
+  _ENVIRONMENTCREATEREQUEST._serialized_start=9085
+  _ENVIRONMENTCREATEREQUEST._serialized_end=9125
+  _ENVIRONMENTDELETEREQUEST._serialized_start=9127
+  _ENVIRONMENTDELETEREQUEST._serialized_end=9167
+  _ENVIRONMENTLISTITEM._serialized_start=9169
+  _ENVIRONMENTLISTITEM._serialized_end=9228
+  _ENVIRONMENTLISTRESPONSE._serialized_start=9230
+  _ENVIRONMENTLISTRESPONSE._serialized_end=9305
+  _ENVIRONMENTUPDATEREQUEST._serialized_start=9308
+  _ENVIRONMENTUPDATEREQUEST._serialized_end=9450
+  _FUNCTIONCALLPUTDATAREQUEST._serialized_start=9452
+  _FUNCTIONCALLPUTDATAREQUEST._serialized_end=9552
+  _FUNCTIONCALLGETDATAREQUEST._serialized_start=9554
+  _FUNCTIONCALLGETDATAREQUEST._serialized_end=9628
+  _FUNCTION._serialized_start=9631
+  _FUNCTION._serialized_end=11539
+  _FUNCTION_DEFINITIONTYPE._serialized_start=11258
+  _FUNCTION_DEFINITIONTYPE._serialized_end=11365
+  _FUNCTION_FUNCTIONTYPE._serialized_start=11367
+  _FUNCTION_FUNCTIONTYPE._serialized_end=11469
+  _SCHEDULERPLACEMENT._serialized_start=11541
+  _SCHEDULERPLACEMENT._serialized_end=11665
+  _FUNCTIONHANDLEMETADATA._serialized_start=11668
+  _FUNCTIONHANDLEMETADATA._serialized_end=11811
+  _FUNCTIONCREATEREQUEST._serialized_start=11814
+  _FUNCTIONCREATEREQUEST._serialized_end=11973
+  _FUNCTIONOPTIONS._serialized_start=11976
+  _FUNCTIONOPTIONS._serialized_end=12559
+  _FUNCTIONPRECREATEREQUEST._serialized_start=12562
+  _FUNCTIONPRECREATEREQUEST._serialized_end=12776
+  _FUNCTIONPRECREATERESPONSE._serialized_start=12778
+  _FUNCTIONPRECREATERESPONSE._serialized_end=12889
+  _FUNCTIONBINDPARAMSREQUEST._serialized_start=12892
+  _FUNCTIONBINDPARAMSREQUEST._serialized_end=13050
+  _FUNCTIONBINDPARAMSRESPONSE._serialized_start=13052
+  _FUNCTIONBINDPARAMSRESPONSE._serialized_end=13170
+  _FUNCTIONCREATERESPONSE._serialized_start=13173
+  _FUNCTIONCREATERESPONSE._serialized_end=13388
+  _FUNCTIONGETREQUEST._serialized_start=13391
+  _FUNCTIONGETREQUEST._serialized_end=13529
+  _FUNCTIONGETRESPONSE._serialized_start=13531
+  _FUNCTIONGETRESPONSE._serialized_end=13636
+  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_start=13638
+  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_end=13731
+  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_start=13733
+  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_end=13773
+  _FUNCTIONGETINPUTSITEM._serialized_start=13776
+  _FUNCTIONGETINPUTSITEM._serialized_end=13914
+  _FUNCTIONGETINPUTSREQUEST._serialized_start=13916
+  _FUNCTIONGETINPUTSREQUEST._serialized_end=14037
+  _FUNCTIONGETINPUTSRESPONSE._serialized_start=14039
+  _FUNCTIONGETINPUTSRESPONSE._serialized_end=14154
+  _FUNCTIONGETOUTPUTSITEM._serialized_start=14157
+  _FUNCTIONGETOUTPUTSITEM._serialized_end=14323
+  _FUNCTIONGETOUTPUTSREQUEST._serialized_start=14326
+  _FUNCTIONGETOUTPUTSREQUEST._serialized_end=14465
+  _FUNCTIONGETOUTPUTSRESPONSE._serialized_start=14467
+  _FUNCTIONGETOUTPUTSRESPONSE._serialized_end=14587
+  _FUNCTIONGETSERIALIZEDREQUEST._serialized_start=14589
+  _FUNCTIONGETSERIALIZEDREQUEST._serialized_end=14640
+  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_start=14642
+  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_end=14728
+  _FUNCTIONINPUT._serialized_start=14731
+  _FUNCTIONINPUT._serialized_end=14868
+  _FUNCTIONMAPREQUEST._serialized_start=14871
+  _FUNCTIONMAPREQUEST._serialized_end=15087
+  _FUNCTIONMAPRESPONSE._serialized_start=15089
+  _FUNCTIONMAPRESPONSE._serialized_end=15207
+  _FUNCTIONPUTINPUTSITEM._serialized_start=15209
+  _FUNCTIONPUTINPUTSITEM._serialized_end=15289
+  _FUNCTIONPUTINPUTSREQUEST._serialized_start=15291
+  _FUNCTIONPUTINPUTSREQUEST._serialized_end=15417
+  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_start=15419
+  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_end=15481
+  _FUNCTIONPUTINPUTSRESPONSE._serialized_start=15483
+  _FUNCTIONPUTINPUTSRESPONSE._serialized_end=15571
+  _FUNCTIONPUTOUTPUTSITEM._serialized_start=15574
+  _FUNCTIONPUTOUTPUTSITEM._serialized_end=15780
+  _FUNCTIONPUTOUTPUTSREQUEST._serialized_start=15782
+  _FUNCTIONPUTOUTPUTSREQUEST._serialized_end=15864
+  _FUNCTIONRETRYPOLICY._serialized_start=15866
+  _FUNCTIONRETRYPOLICY._serialized_end=15981
+  _FUNCTIONGETCALLGRAPHREQUEST._serialized_start=15983
+  _FUNCTIONGETCALLGRAPHREQUEST._serialized_end=16038
+  _INPUTCALLGRAPHINFO._serialized_start=16041
+  _INPUTCALLGRAPHINFO._serialized_end=16181
+  _FUNCTIONCALLCALLGRAPHINFO._serialized_start=16183
+  _FUNCTIONCALLCALLGRAPHINFO._serialized_end=16305
+  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_start=16308
+  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_end=16453
+  _FUNCTIONCALLCANCELREQUEST._serialized_start=16455
+  _FUNCTIONCALLCANCELREQUEST._serialized_end=16508
+  _FUNCTIONCALLLISTREQUEST._serialized_start=16510
+  _FUNCTIONCALLLISTREQUEST._serialized_end=16556
+  _FUNCTIONCALLINFO._serialized_start=16559
+  _FUNCTIONCALLINFO._serialized_end=17010
+  _FUNCTIONCALLLISTRESPONSE._serialized_start=17012
+  _FUNCTIONCALLLISTRESPONSE._serialized_end=17094
+  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_start=17096
+  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_end=17149
+  _FUNCTIONSTATS._serialized_start=17151
+  _FUNCTIONSTATS._serialized_end=17234
+  _GENERATORDONE._serialized_start=17236
+  _GENERATORDONE._serialized_end=17272
+  _GENERICRESULT._serialized_start=17275
+  _GENERICRESULT._serialized_end=17913
+  _GENERICRESULT_GENERICSTATUS._serialized_start=17587
+  _GENERICRESULT_GENERICSTATUS._serialized_end=17782
+  _GENERICRESULT_GENERATORSTATUS._serialized_start=17784
+  _GENERICRESULT_GENERATORSTATUS._serialized_end=17899
+  _GPUCONFIG._serialized_start=17915
+  _GPUCONFIG._serialized_end=17994
+  _BUILDFUNCTION._serialized_start=17996
+  _BUILDFUNCTION._serialized_end=18092
+  _IMAGE._serialized_start=18095
+  _IMAGE._serialized_end=18572
+  _IMAGEBUILDERVERSIONLOOKUPRESPONSE._serialized_start=18574
+  _IMAGEBUILDERVERSIONLOOKUPRESPONSE._serialized_end=18626
+  _IMAGECONTEXTFILE._serialized_start=18628
+  _IMAGECONTEXTFILE._serialized_end=18678
+  _IMAGEGETORCREATEREQUEST._serialized_start=18681
+  _IMAGEGETORCREATEREQUEST._serialized_end=18918
+  _IMAGEGETORCREATERESPONSE._serialized_start=18920
+  _IMAGEGETORCREATERESPONSE._serialized_end=18964
+  _IMAGEJOINSTREAMINGREQUEST._serialized_start=18966
+  _IMAGEJOINSTREAMINGREQUEST._serialized_end=19051
+  _IMAGEJOINSTREAMINGRESPONSE._serialized_start=19054
+  _IMAGEJOINSTREAMINGRESPONSE._serialized_end=19201
+  _IMAGEREGISTRYCONFIG._serialized_start=19203
+  _IMAGEREGISTRYCONFIG._serialized_end=19303
+  _INPUTINFO._serialized_start=19306
+  _INPUTINFO._serialized_end=19459
+  _INPUTCATEGORYINFO._serialized_start=19461
+  _INPUTCATEGORYINFO._serialized_end=19536
+  _MOUNTBUILDREQUEST._serialized_start=19538
+  _MOUNTBUILDREQUEST._serialized_end=19646
+  _MOUNTBUILDRESPONSE._serialized_start=19648
+  _MOUNTBUILDRESPONSE._serialized_end=19746
+  _MOUNTGETORCREATEREQUEST._serialized_start=19749
+  _MOUNTGETORCREATEREQUEST._serialized_end=19999
+  _MOUNTGETORCREATERESPONSE._serialized_start=20001
+  _MOUNTGETORCREATERESPONSE._serialized_end=20105
+  _MOUNTHANDLEMETADATA._serialized_start=20107
+  _MOUNTHANDLEMETADATA._serialized_end=20165
+  _MOUNTFILE._serialized_start=20167
+  _MOUNTFILE._serialized_end=20272
+  _MOUNTPUTFILEREQUEST._serialized_start=20274
+  _MOUNTPUTFILEREQUEST._serialized_end=20369
+  _MOUNTPUTFILERESPONSE._serialized_start=20371
+  _MOUNTPUTFILERESPONSE._serialized_end=20409
+  _MULTIPARTUPLOAD._serialized_start=20411
+  _MULTIPARTUPLOAD._serialized_end=20494
+  _OBJECT._serialized_start=20497
+  _OBJECT._serialized_end=20831
+  _OBJECTDEPENDENCY._serialized_start=20833
+  _OBJECTDEPENDENCY._serialized_end=20870
+  _PROXYGETORCREATEREQUEST._serialized_start=20873
+  _PROXYGETORCREATEREQUEST._serialized_end=21067
+  _PROXYGETORCREATERESPONSE._serialized_start=21069
+  _PROXYGETORCREATERESPONSE._serialized_end=21113
+  _PROXYINFO._serialized_start=21115
+  _PROXYINFO._serialized_end=21207
+  _PTYINFO._serialized_start=21210
+  _PTYINFO._serialized_end=21472
+  _PTYINFO_PTYTYPE._serialized_start=21394
+  _PTYINFO_PTYTYPE._serialized_end=21472
+  _QUEUECREATEREQUEST._serialized_start=21474
+  _QUEUECREATEREQUEST._serialized_end=21537
+  _QUEUECREATERESPONSE._serialized_start=21539
+  _QUEUECREATERESPONSE._serialized_end=21578
+  _QUEUEGETORCREATEREQUEST._serialized_start=21581
+  _QUEUEGETORCREATEREQUEST._serialized_end=21775
+  _QUEUEGETORCREATERESPONSE._serialized_start=21777
+  _QUEUEGETORCREATERESPONSE._serialized_end=21821
+  _QUEUEGETREQUEST._serialized_start=21823
+  _QUEUEGETREQUEST._serialized_end=21893
+  _QUEUEGETRESPONSE._serialized_start=21895
+  _QUEUEGETRESPONSE._serialized_end=21929
+  _QUEUEHEARTBEATREQUEST._serialized_start=21931
+  _QUEUEHEARTBEATREQUEST._serialized_end=21972
+  _QUEUEPUTREQUEST._serialized_start=21974
+  _QUEUEPUTREQUEST._serialized_end=22025
+  _QUEUELENREQUEST._serialized_start=22027
+  _QUEUELENREQUEST._serialized_end=22062
+  _QUEUELENRESPONSE._serialized_start=22064
+  _QUEUELENRESPONSE._serialized_end=22095
+  _RATELIMIT._serialized_start=22097
+  _RATELIMIT._serialized_end=22174
+  _RESOURCES._serialized_start=22176
+  _RESOURCES._serialized_end=22270
+  _S3MOUNT._serialized_start=22272
+  _S3MOUNT._serialized_end=22372
+  _SANDBOX._serialized_start=22375
+  _SANDBOX._serialized_end=22912
+  _SANDBOXCREATEREQUEST._serialized_start=22914
+  _SANDBOXCREATEREQUEST._serialized_end=23001
+  _SANDBOXCREATERESPONSE._serialized_start=23003
+  _SANDBOXCREATERESPONSE._serialized_end=23046
+  _SANDBOXGETTASKIDREQUEST._serialized_start=23048
+  _SANDBOXGETTASKIDREQUEST._serialized_end=23093
+  _SANDBOXGETTASKIDRESPONSE._serialized_start=23095
+  _SANDBOXGETTASKIDRESPONSE._serialized_end=23138
+  _SANDBOXGETLOGSREQUEST._serialized_start=23141
+  _SANDBOXGETLOGSREQUEST._serialized_end=23279
+  _SANDBOXSTDINWRITEREQUEST._serialized_start=23281
+  _SANDBOXSTDINWRITEREQUEST._serialized_end=23370
+  _SANDBOXSTDINWRITERESPONSE._serialized_start=23372
+  _SANDBOXSTDINWRITERESPONSE._serialized_end=23399
+  _SANDBOXHANDLEMETADATA._serialized_start=23401
+  _SANDBOXHANDLEMETADATA._serialized_end=23469
+  _SANDBOXINFO._serialized_start=23472
+  _SANDBOXINFO._serialized_end=23603
+  _SANDBOXLISTREQUEST._serialized_start=23605
+  _SANDBOXLISTREQUEST._serialized_end=23667
+  _SANDBOXLISTRESPONSE._serialized_start=23669
+  _SANDBOXLISTRESPONSE._serialized_end=23736
+  _SANDBOXTERMINATEREQUEST._serialized_start=23738
+  _SANDBOXTERMINATEREQUEST._serialized_end=23783
+  _SANDBOXTERMINATERESPONSE._serialized_start=23785
+  _SANDBOXTERMINATERESPONSE._serialized_end=23865
+  _SANDBOXWAITREQUEST._serialized_start=23867
+  _SANDBOXWAITREQUEST._serialized_end=23924
+  _SANDBOXWAITRESPONSE._serialized_start=23926
+  _SANDBOXWAITRESPONSE._serialized_end=23992
+  _SCHEDULE._serialized_start=23995
+  _SCHEDULE._serialized_end=24265
+  _SCHEDULE_CRON._serialized_start=24101
+  _SCHEDULE_CRON._serialized_end=24128
+  _SCHEDULE_PERIOD._serialized_start=24130
+  _SCHEDULE_PERIOD._serialized_end=24247
+  _SECRETCREATEREQUEST._serialized_start=24268
+  _SECRETCREATEREQUEST._serialized_end=24476
+  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_start=24430
+  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_end=24476
+  _SECRETCREATERESPONSE._serialized_start=24478
+  _SECRETCREATERESPONSE._serialized_end=24519
+  _SECRETGETORCREATEREQUEST._serialized_start=24522
+  _SECRETGETORCREATEREQUEST._serialized_end=24852
+  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_start=24430
+  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_end=24476
+  _SECRETGETORCREATERESPONSE._serialized_start=24854
+  _SECRETGETORCREATERESPONSE._serialized_end=24900
+  _SECRETLISTITEM._serialized_start=24902
+  _SECRETLISTITEM._serialized_end=25001
+  _SECRETLISTREQUEST._serialized_start=25003
+  _SECRETLISTREQUEST._serialized_end=25048
+  _SECRETLISTRESPONSE._serialized_start=25050
+  _SECRETLISTRESPONSE._serialized_end=25141
+  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_start=25144
+  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_end=25361
+  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_start=25363
+  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_end=25422
+  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_start=25424
+  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_end=25480
+  _SHAREDVOLUMECREATEREQUEST._serialized_start=25482
+  _SHAREDVOLUMECREATEREQUEST._serialized_end=25584
+  _SHAREDVOLUMECREATERESPONSE._serialized_start=25586
+  _SHAREDVOLUMECREATERESPONSE._serialized_end=25640
+  _SHAREDVOLUMELISTITEM._serialized_start=25643
+  _SHAREDVOLUMELISTITEM._serialized_end=25779
+  _SHAREDVOLUMELISTREQUEST._serialized_start=25781
+  _SHAREDVOLUMELISTREQUEST._serialized_end=25832
+  _SHAREDVOLUMELISTRESPONSE._serialized_start=25834
+  _SHAREDVOLUMELISTRESPONSE._serialized_end=25937
+  _SHAREDVOLUMELISTFILESREQUEST._serialized_start=25939
+  _SHAREDVOLUMELISTFILESREQUEST._serialized_end=26009
+  _SHAREDVOLUMEPUTFILEREQUEST._serialized_start=26012
+  _SHAREDVOLUMEPUTFILEREQUEST._serialized_end=26179
+  _SHAREDVOLUMEPUTFILERESPONSE._serialized_start=26181
+  _SHAREDVOLUMEPUTFILERESPONSE._serialized_end=26226
+  _SHAREDVOLUMEGETFILEREQUEST._serialized_start=26228
+  _SHAREDVOLUMEGETFILEREQUEST._serialized_end=26296
+  _SHAREDVOLUMEGETFILERESPONSE._serialized_start=26298
+  _SHAREDVOLUMEGETFILERESPONSE._serialized_end=26381
+  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_start=26383
+  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_end=26479
+  _SHAREDVOLUMELISTFILESENTRY._serialized_start=26482
+  _SHAREDVOLUMELISTFILESENTRY._serialized_end=26643
+  _SHAREDVOLUMELISTFILESENTRY_FILETYPE._serialized_start=26591
+  _SHAREDVOLUMELISTFILESENTRY_FILETYPE._serialized_end=26643
+  _SHAREDVOLUMELISTFILESRESPONSE._serialized_start=26645
+  _SHAREDVOLUMELISTFILESRESPONSE._serialized_end=26735
+  _SHAREDVOLUMEMOUNT._serialized_start=26738
+  _SHAREDVOLUMEMOUNT._serialized_end=26884
+  _TASKCURRENTINPUTSRESPONSE._serialized_start=26886
+  _TASKCURRENTINPUTSRESPONSE._serialized_end=26932
+  _TASKINFO._serialized_start=26934
+  _TASKINFO._serialized_end=27042
+  _TASKLOGS._serialized_start=27045
+  _TASKLOGS._serialized_end=27283
+  _TASKLISTREQUEST._serialized_start=27285
+  _TASKLISTREQUEST._serialized_end=27302
+  _TASKLISTRESPONSE._serialized_start=27304
+  _TASKLISTRESPONSE._serialized_end=27362
+  _TASKLOGSBATCH._serialized_start=27365
+  _TASKLOGSBATCH._serialized_end=27563
+  _TASKPROGRESS._serialized_start=27565
+  _TASKPROGRESS._serialized_end=27677
+  _TASKRESULTREQUEST._serialized_start=27679
+  _TASKRESULTREQUEST._serialized_end=27743
+  _TASKSTATS._serialized_start=27745
+  _TASKSTATS._serialized_end=27834
+  _TOKENFLOWCREATEREQUEST._serialized_start=27836
+  _TOKENFLOWCREATEREQUEST._serialized_end=27922
+  _TOKENFLOWCREATERESPONSE._serialized_start=27924
+  _TOKENFLOWCREATERESPONSE._serialized_end=28024
+  _TOKENFLOWWAITREQUEST._serialized_start=28026
+  _TOKENFLOWWAITREQUEST._serialized_end=28109
+  _TOKENFLOWWAITRESPONSE._serialized_start=28111
+  _TOKENFLOWWAITRESPONSE._serialized_end=28219
+  _TUNNELSTARTREQUEST._serialized_start=28221
+  _TUNNELSTARTREQUEST._serialized_end=28276
+  _TUNNELSTARTRESPONSE._serialized_start=28279
+  _TUNNELSTARTRESPONSE._serialized_end=28432
+  _TUNNELSTOPREQUEST._serialized_start=28434
+  _TUNNELSTOPREQUEST._serialized_end=28467
+  _TUNNELSTOPRESPONSE._serialized_start=28469
+  _TUNNELSTOPRESPONSE._serialized_end=28505
+  _VOLUMEGETORCREATEREQUEST._serialized_start=28508
+  _VOLUMEGETORCREATEREQUEST._serialized_end=28719
+  _VOLUMEGETORCREATERESPONSE._serialized_start=28721
+  _VOLUMEGETORCREATERESPONSE._serialized_end=28767
+  _VOLUMEHEARTBEATREQUEST._serialized_start=28769
+  _VOLUMEHEARTBEATREQUEST._serialized_end=28812
+  _VOLUMECREATEREQUEST._serialized_start=28814
+  _VOLUMECREATEREQUEST._serialized_end=28857
+  _VOLUMECREATERESPONSE._serialized_start=28859
+  _VOLUMECREATERESPONSE._serialized_end=28900
+  _VOLUMECOMMITREQUEST._serialized_start=28902
+  _VOLUMECOMMITREQUEST._serialized_end=28948
+  _VOLUMECOMMITRESPONSE._serialized_start=28950
+  _VOLUMECOMMITRESPONSE._serialized_end=28993
+  _VOLUMEDELETEREQUEST._serialized_start=28995
+  _VOLUMEDELETEREQUEST._serialized_end=29065
+  _VOLUMEGETFILEREQUEST._serialized_start=29067
+  _VOLUMEGETFILEREQUEST._serialized_end=29150
+  _VOLUMEGETFILERESPONSE._serialized_start=29152
+  _VOLUMEGETFILERESPONSE._serialized_end=29271
+  _VOLUMELISTFILESENTRY._serialized_start=29274
+  _VOLUMELISTFILESENTRY._serialized_end=29465
+  _VOLUMELISTFILESENTRY_FILETYPE._serialized_start=29400
+  _VOLUMELISTFILESENTRY_FILETYPE._serialized_end=29465
+  _VOLUMELISTFILESREQUEST._serialized_start=29467
+  _VOLUMELISTFILESREQUEST._serialized_end=29566
+  _VOLUMELISTFILESRESPONSE._serialized_start=29568
+  _VOLUMELISTFILESRESPONSE._serialized_end=29646
+  _VOLUMELISTITEM._serialized_start=29648
+  _VOLUMELISTITEM._serialized_end=29718
+  _VOLUMELISTREQUEST._serialized_start=29720
+  _VOLUMELISTREQUEST._serialized_end=29765
+  _VOLUMELISTRESPONSE._serialized_start=29767
+  _VOLUMELISTRESPONSE._serialized_end=29858
+  _VOLUMERELOADREQUEST._serialized_start=29860
+  _VOLUMERELOADREQUEST._serialized_end=29900
+  _VOLUMEPUTFILESREQUEST._serialized_start=29902
+  _VOLUMEPUTFILESREQUEST._serialized_end=30027
+  _VOLUMEREMOVEFILEREQUEST._serialized_start=30029
+  _VOLUMEREMOVEFILEREQUEST._serialized_end=30112
+  _VOLUMECOPYFILESREQUEST._serialized_start=30114
+  _VOLUMECOPYFILESREQUEST._serialized_end=30213
+  _VOLUMEMOUNT._serialized_start=30215
+  _VOLUMEMOUNT._serialized_end=30301
+  _WEBHOOKCONFIG._serialized_start=30304
+  _WEBHOOKCONFIG._serialized_end=30573
+  _WEBURLINFO._serialized_start=30575
+  _WEBURLINFO._serialized_end=30653
+  _WORKSPACENAMELOOKUPRESPONSE._serialized_start=30655
+  _WORKSPACENAMELOOKUPRESPONSE._serialized_end=30726
+  _RUNTIMEOUTPUTMESSAGE._serialized_start=30728
+  _RUNTIMEOUTPUTMESSAGE._serialized_end=30822
+  _RUNTIMEOUTPUTBATCH._serialized_start=30825
+  _RUNTIMEOUTPUTBATCH._serialized_end=30955
+  _RUNTIMEINPUTMESSAGE._serialized_start=30957
+  _RUNTIMEINPUTMESSAGE._serialized_end=31018
+  _MODALCLIENT._serialized_start=34640
+  _MODALCLIENT._serialized_end=44659
 # @@protoc_insertion_point(module_scope)
```

## modal_proto/api_pb2_grpc.py

```diff
@@ -151,19 +151,14 @@
                 response_deserializer=modal__proto_dot_api__pb2.DictGetOrCreateResponse.FromString,
                 )
         self.DictHeartbeat = channel.unary_unary(
                 '/modal.client.ModalClient/DictHeartbeat',
                 request_serializer=modal__proto_dot_api__pb2.DictHeartbeatRequest.SerializeToString,
                 response_deserializer=google_dot_protobuf_dot_empty__pb2.Empty.FromString,
                 )
-        self.DictContents = channel.unary_stream(
-                '/modal.client.ModalClient/DictContents',
-                request_serializer=modal__proto_dot_api__pb2.DictContentsRequest.SerializeToString,
-                response_deserializer=modal__proto_dot_api__pb2.DictEntry.FromString,
-                )
         self.DictUpdate = channel.unary_unary(
                 '/modal.client.ModalClient/DictUpdate',
                 request_serializer=modal__proto_dot_api__pb2.DictUpdateRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.DictUpdateResponse.FromString,
                 )
         self.DictGet = channel.unary_unary(
                 '/modal.client.ModalClient/DictGet',
@@ -321,14 +316,19 @@
                 response_deserializer=modal__proto_dot_api__pb2.ImageGetOrCreateResponse.FromString,
                 )
         self.ImageJoinStreaming = channel.unary_stream(
                 '/modal.client.ModalClient/ImageJoinStreaming',
                 request_serializer=modal__proto_dot_api__pb2.ImageJoinStreamingRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.ImageJoinStreamingResponse.FromString,
                 )
+        self.ImageBuilderVersionLookup = channel.unary_unary(
+                '/modal.client.ModalClient/ImageBuilderVersionLookup',
+                request_serializer=google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
+                response_deserializer=modal__proto_dot_api__pb2.ImageBuilderVersionLookupResponse.FromString,
+                )
         self.MountPutFile = channel.unary_unary(
                 '/modal.client.ModalClient/MountPutFile',
                 request_serializer=modal__proto_dot_api__pb2.MountPutFileRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.MountPutFileResponse.FromString,
                 )
         self.MountBuild = channel.unary_unary(
                 '/modal.client.ModalClient/MountBuild',
@@ -371,19 +371,14 @@
                 response_deserializer=google_dot_protobuf_dot_empty__pb2.Empty.FromString,
                 )
         self.QueueLen = channel.unary_unary(
                 '/modal.client.ModalClient/QueueLen',
                 request_serializer=modal__proto_dot_api__pb2.QueueLenRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.QueueLenResponse.FromString,
                 )
-        self.QueueNextItems = channel.unary_unary(
-                '/modal.client.ModalClient/QueueNextItems',
-                request_serializer=modal__proto_dot_api__pb2.QueueNextItemsRequest.SerializeToString,
-                response_deserializer=modal__proto_dot_api__pb2.QueueNextItemsResponse.FromString,
-                )
         self.SandboxCreate = channel.unary_unary(
                 '/modal.client.ModalClient/SandboxCreate',
                 request_serializer=modal__proto_dot_api__pb2.SandboxCreateRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.SandboxCreateResponse.FromString,
                 )
         self.SandboxGetTaskId = channel.unary_unary(
                 '/modal.client.ModalClient/SandboxGetTaskId',
@@ -752,20 +747,14 @@
 
     def DictHeartbeat(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
-    def DictContents(self, request, context):
-        """Missing associated documentation comment in .proto file."""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details('Method not implemented!')
-        raise NotImplementedError('Method not implemented!')
-
     def DictUpdate(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
     def DictGet(self, request, context):
@@ -965,14 +954,20 @@
 
     def ImageJoinStreaming(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
+    def ImageBuilderVersionLookup(self, request, context):
+        """Missing associated documentation comment in .proto file."""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
     def MountPutFile(self, request, context):
         """Mounts
         """
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
@@ -1028,20 +1023,14 @@
 
     def QueueLen(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
-    def QueueNextItems(self, request, context):
-        """Missing associated documentation comment in .proto file."""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details('Method not implemented!')
-        raise NotImplementedError('Method not implemented!')
-
     def SandboxCreate(self, request, context):
         """Sandboxes
         """
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
@@ -1422,19 +1411,14 @@
                     response_serializer=modal__proto_dot_api__pb2.DictGetOrCreateResponse.SerializeToString,
             ),
             'DictHeartbeat': grpc.unary_unary_rpc_method_handler(
                     servicer.DictHeartbeat,
                     request_deserializer=modal__proto_dot_api__pb2.DictHeartbeatRequest.FromString,
                     response_serializer=google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
             ),
-            'DictContents': grpc.unary_stream_rpc_method_handler(
-                    servicer.DictContents,
-                    request_deserializer=modal__proto_dot_api__pb2.DictContentsRequest.FromString,
-                    response_serializer=modal__proto_dot_api__pb2.DictEntry.SerializeToString,
-            ),
             'DictUpdate': grpc.unary_unary_rpc_method_handler(
                     servicer.DictUpdate,
                     request_deserializer=modal__proto_dot_api__pb2.DictUpdateRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.DictUpdateResponse.SerializeToString,
             ),
             'DictGet': grpc.unary_unary_rpc_method_handler(
                     servicer.DictGet,
@@ -1592,14 +1576,19 @@
                     response_serializer=modal__proto_dot_api__pb2.ImageGetOrCreateResponse.SerializeToString,
             ),
             'ImageJoinStreaming': grpc.unary_stream_rpc_method_handler(
                     servicer.ImageJoinStreaming,
                     request_deserializer=modal__proto_dot_api__pb2.ImageJoinStreamingRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.ImageJoinStreamingResponse.SerializeToString,
             ),
+            'ImageBuilderVersionLookup': grpc.unary_unary_rpc_method_handler(
+                    servicer.ImageBuilderVersionLookup,
+                    request_deserializer=google_dot_protobuf_dot_empty__pb2.Empty.FromString,
+                    response_serializer=modal__proto_dot_api__pb2.ImageBuilderVersionLookupResponse.SerializeToString,
+            ),
             'MountPutFile': grpc.unary_unary_rpc_method_handler(
                     servicer.MountPutFile,
                     request_deserializer=modal__proto_dot_api__pb2.MountPutFileRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.MountPutFileResponse.SerializeToString,
             ),
             'MountBuild': grpc.unary_unary_rpc_method_handler(
                     servicer.MountBuild,
@@ -1642,19 +1631,14 @@
                     response_serializer=google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
             ),
             'QueueLen': grpc.unary_unary_rpc_method_handler(
                     servicer.QueueLen,
                     request_deserializer=modal__proto_dot_api__pb2.QueueLenRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.QueueLenResponse.SerializeToString,
             ),
-            'QueueNextItems': grpc.unary_unary_rpc_method_handler(
-                    servicer.QueueNextItems,
-                    request_deserializer=modal__proto_dot_api__pb2.QueueNextItemsRequest.FromString,
-                    response_serializer=modal__proto_dot_api__pb2.QueueNextItemsResponse.SerializeToString,
-            ),
             'SandboxCreate': grpc.unary_unary_rpc_method_handler(
                     servicer.SandboxCreate,
                     request_deserializer=modal__proto_dot_api__pb2.SandboxCreateRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.SandboxCreateResponse.SerializeToString,
             ),
             'SandboxGetTaskId': grpc.unary_unary_rpc_method_handler(
                     servicer.SandboxGetTaskId,
@@ -2329,31 +2313,14 @@
         return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/DictHeartbeat',
             modal__proto_dot_api__pb2.DictHeartbeatRequest.SerializeToString,
             google_dot_protobuf_dot_empty__pb2.Empty.FromString,
             options, channel_credentials,
             insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def DictContents(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_stream(request, target, '/modal.client.ModalClient/DictContents',
-            modal__proto_dot_api__pb2.DictContentsRequest.SerializeToString,
-            modal__proto_dot_api__pb2.DictEntry.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
-
-    @staticmethod
     def DictUpdate(request,
             target,
             options=(),
             channel_credentials=None,
             call_credentials=None,
             insecure=False,
             compression=None,
@@ -2907,14 +2874,31 @@
         return grpc.experimental.unary_stream(request, target, '/modal.client.ModalClient/ImageJoinStreaming',
             modal__proto_dot_api__pb2.ImageJoinStreamingRequest.SerializeToString,
             modal__proto_dot_api__pb2.ImageJoinStreamingResponse.FromString,
             options, channel_credentials,
             insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
+    def ImageBuilderVersionLookup(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/ImageBuilderVersionLookup',
+            google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
+            modal__proto_dot_api__pb2.ImageBuilderVersionLookupResponse.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
     def MountPutFile(request,
             target,
             options=(),
             channel_credentials=None,
             call_credentials=None,
             insecure=False,
             compression=None,
@@ -3077,31 +3061,14 @@
         return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/QueueLen',
             modal__proto_dot_api__pb2.QueueLenRequest.SerializeToString,
             modal__proto_dot_api__pb2.QueueLenResponse.FromString,
             options, channel_credentials,
             insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def QueueNextItems(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/QueueNextItems',
-            modal__proto_dot_api__pb2.QueueNextItemsRequest.SerializeToString,
-            modal__proto_dot_api__pb2.QueueNextItemsResponse.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
-
-    @staticmethod
     def SandboxCreate(request,
             target,
             options=(),
             channel_credentials=None,
             call_credentials=None,
             insecure=False,
             compression=None,
```

## modal_version/_version_generated.py

```diff
@@ -1,4 +1,4 @@
 # Copyright Modal Labs 2024
 
 # Note: Reset this value to -1 whenever you make a minor `0.X` release of the client.
-build_number = 87  # git: 1e0a2e7
+build_number = 9  # git: 828a4bc
```

## test/aio_test.py

```diff
@@ -1,12 +1,12 @@
 # Copyright Modal Labs 2023
 import pytest
 
 
 @pytest.mark.asyncio
 async def test_new(servicer, client):
-    from modal import App
+    from modal import Stub
 
-    app = App()
+    stub = Stub()
 
-    async with app.run(client=client):
+    async with stub.run(client=client):
         pass
```

## test/async_utils_test.py

```diff
@@ -153,15 +153,15 @@
         await asyncio.sleep(DEBOUNCE_TIME + 0.001)
 
         assert len(drained_items) == 3
 
 
 @pytest.mark.asyncio
 async def test_warn_if_generator_is_not_consumed(caplog):
-    @warn_if_generator_is_not_consumed()
+    @warn_if_generator_is_not_consumed
     async def my_generator():
         yield 42
 
     with caplog.at_level(logging.WARNING):
         g = my_generator()
         assert "my_generator" in repr(g)
         del g  # Force destructor
@@ -169,33 +169,16 @@
     assert len(caplog.records) == 1
     assert "my_generator" in caplog.text
     assert "for" in caplog.text
     assert "list" in caplog.text
 
 
 @pytest.mark.asyncio
-def test_warn_if_generator_is_not_consumed_sync(caplog):
-    @warn_if_generator_is_not_consumed()
-    def my_generator():
-        yield 42
-
-    with caplog.at_level(logging.WARNING):
-        g = my_generator()
-        assert "my_generator" in repr(g)
-        del g  # Force destructor
-
-    assert len(caplog.records) == 1
-    assert "my_generator" in caplog.text
-    assert "for" in caplog.text
-    assert "list" in caplog.text
-
-
-@pytest.mark.asyncio
 async def test_no_warn_if_generator_is_consumed(caplog):
-    @warn_if_generator_is_not_consumed()
+    @warn_if_generator_is_not_consumed
     async def my_generator():
         yield 42
 
     with caplog.at_level(logging.WARNING):
         g = my_generator()
         async for _ in g:
             pass
```

## test/cli_imports_test.py

```diff
@@ -1,18 +1,18 @@
 # Copyright Modal Labs 2023
 import pytest
 
 from modal._utils.async_utils import synchronizer
-from modal.app import _App, _LocalEntrypoint
 from modal.cli.import_refs import (
-    DEFAULT_APP_NAME,
+    DEFAULT_STUB_NAME,
     get_by_object_path,
     import_file_or_module,
     parse_import_ref,
 )
+from modal.stub import _LocalEntrypoint, _Stub
 
 # Some helper vars for import_stub tests:
 local_entrypoint_src = """
 import modal
 
 stub = modal.Stub()
 @stub.local_entrypoint()
@@ -81,34 +81,34 @@
 }
 
 
 @pytest.mark.parametrize(
     ["dir_structure", "ref", "expected_object_type"],
     [
         # # file syntax
-        (empty_dir_with_python_file, "mod.py", _App),
-        (empty_dir_with_python_file, "mod.py::stub", _App),
-        (empty_dir_with_python_file, "mod.py::other_stub", _App),
-        (dir_containing_python_package, "pack/file.py", _App),
-        (dir_containing_python_package, "pack/sub/subfile.py", _App),
-        (dir_containing_python_package, "dir/sub/subfile.py", _App),
+        (empty_dir_with_python_file, "mod.py", _Stub),
+        (empty_dir_with_python_file, "mod.py::stub", _Stub),
+        (empty_dir_with_python_file, "mod.py::other_stub", _Stub),
+        (dir_containing_python_package, "pack/file.py", _Stub),
+        (dir_containing_python_package, "pack/sub/subfile.py", _Stub),
+        (dir_containing_python_package, "dir/sub/subfile.py", _Stub),
         # # python module syntax
-        (empty_dir_with_python_file, "mod", _App),
-        (empty_dir_with_python_file, "mod::stub", _App),
-        (empty_dir_with_python_file, "mod::other_stub", _App),
-        (dir_containing_python_package, "pack.mod", _App),
-        (dir_containing_python_package, "pack.mod::other_stub", _App),
+        (empty_dir_with_python_file, "mod", _Stub),
+        (empty_dir_with_python_file, "mod::stub", _Stub),
+        (empty_dir_with_python_file, "mod::other_stub", _Stub),
+        (dir_containing_python_package, "pack.mod", _Stub),
+        (dir_containing_python_package, "pack.mod::other_stub", _Stub),
         (dir_containing_python_package, "pack/local.py::stub.main", _LocalEntrypoint),
     ],
 )
 def test_import_object(dir_structure, ref, expected_object_type, mock_dir):
     with mock_dir(dir_structure):
         import_ref = parse_import_ref(ref)
         module = import_file_or_module(import_ref.file_or_module)
-        imported_object = get_by_object_path(module, import_ref.object_path or DEFAULT_APP_NAME)
+        imported_object = get_by_object_path(module, import_ref.object_path or DEFAULT_STUB_NAME)
         _translated_obj = synchronizer._translate_in(imported_object)
         assert isinstance(_translated_obj, expected_object_type)
 
 
 def test_import_package_and_module_names(monkeypatch, supports_dir):
     # We try to reproduce the package/module naming standard that the `python` command line tool uses,
     # i.e. when loading using a module path (-m flag w/ python) you get a fully qualified package/module name
```

## test/cli_test.py

```diff
@@ -10,16 +10,18 @@
 import tempfile
 import traceback
 from typing import List, Optional
 from unittest import mock
 
 import click
 import click.testing
+import pytest_asyncio
 import toml
 
+from modal import Client
 from modal.cli.entry_point import entrypoint_cli
 from modal_proto import api_pb2
 
 from .supports.skip import skip_windows
 
 dummy_app_file = """
 import modal
@@ -33,14 +35,23 @@
 mod = sys.modules[__name__]
 assert mod.stub == stub
 """
 
 dummy_other_module_file = "x = 42"
 
 
+@pytest_asyncio.fixture
+async def set_env_client(client):
+    try:
+        Client.set_env_client(client)
+        yield
+    finally:
+        Client.set_env_client(None)
+
+
 def _run(args: List[str], expected_exit_code: int = 0, expected_stderr: Optional[str] = ""):
     runner = click.testing.CliRunner(mix_stderr=False)
     with mock.patch.object(sys, "argv", args):
         res = runner.invoke(entrypoint_cli, args)
     if res.exit_code != expected_exit_code:
         print("stdout:", repr(res.stdout))
         print("stderr:", repr(res.stderr))
@@ -113,28 +124,21 @@
         _run(["setup", "--profile", "_test"])
         assert "_test" in toml.load(config_file_path)
 
 
 def test_run(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "default_stub.py"
     _run(["run", stub_file.as_posix()])
-    _run(["run", stub_file.as_posix() + "::app"])
+    _run(["run", stub_file.as_posix() + "::stub"])
     _run(["run", stub_file.as_posix() + "::foo"])
     _run(["run", stub_file.as_posix() + "::bar"], expected_exit_code=1, expected_stderr=None)
     file_with_entrypoint = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
     _run(["run", file_with_entrypoint.as_posix()])
     _run(["run", file_with_entrypoint.as_posix() + "::main"])
-    _run(["run", file_with_entrypoint.as_posix() + "::app.main"])
-
-
-def test_run_app(servicer, set_env_client, test_dir):
-    stub_file = test_dir / "supports" / "app_run_tests" / "stub_is_now_app.py"
-    _run(["run", stub_file.as_posix()])
-    _run(["run", stub_file.as_posix() + "::app"])
-    _run(["run", stub_file.as_posix() + "::foo"])
+    _run(["run", file_with_entrypoint.as_posix() + "::stub.main"])
 
 
 def test_run_async(servicer, set_env_client, test_dir):
     sync_fn = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
     res = _run(["run", sync_fn.as_posix()])
     assert "called locally" in res.stdout
 
@@ -192,32 +196,32 @@
     stub_file = test_dir / "supports" / "app_run_tests" / "default_stub.py"
     _run(["deploy", "--name=deployment_name", stub_file.as_posix()])
     assert servicer.app_state_history["ap-1"] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
 
 def test_run_custom_stub(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "custom_stub.py"
-    res = _run(["run", stub_file.as_posix() + "::app"], expected_exit_code=1, expected_stderr=None)
+    res = _run(["run", stub_file.as_posix() + "::stub"], expected_exit_code=1, expected_stderr=None)
     assert "Could not find" in res.stderr
-    res = _run(["run", stub_file.as_posix() + "::app.foo"], expected_exit_code=1, expected_stderr=None)
+    res = _run(["run", stub_file.as_posix() + "::stub.foo"], expected_exit_code=1, expected_stderr=None)
     assert "Could not find" in res.stderr
 
     _run(["run", stub_file.as_posix() + "::foo"])
 
 
 def test_run_aiofunc(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "async_stub.py"
     _run(["run", stub_file.as_posix()])
     assert len(servicer.client_calls) == 1
 
 
 def test_run_local_entrypoint(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
 
-    res = _run(["run", stub_file.as_posix() + "::app.main"])  # explicit name
+    res = _run(["run", stub_file.as_posix() + "::stub.main"])  # explicit name
     assert "called locally" in res.stdout
     assert len(servicer.client_calls) == 2
 
     res = _run(["run", stub_file.as_posix()])  # only one entry-point, no name needed
     assert "called locally" in res.stdout
     assert len(servicer.client_calls) == 4
 
@@ -236,15 +240,15 @@
     res = _run(["run", stub_file.as_posix()], expected_exit_code=2, expected_stderr=None)
     assert "You need to specify a Modal function or local entrypoint to run" in res.stderr
 
     valid_call_args = [
         (
             [
                 "run",
-                f"{stub_file.as_posix()}::app.dt_arg",
+                f"{stub_file.as_posix()}::stub.dt_arg",
                 "--dt",
                 "2022-10-31",
             ],
             "the day is 31",
         ),
         (["run", f"{stub_file.as_posix()}::dt_arg", "--dt=2022-10-31"], "the day is 31"),
         (["run", f"{stub_file.as_posix()}::int_arg", "--i=200"], "200 <class 'int'>"),
@@ -649,18 +653,7 @@
                 os.environ["MODAL_TOKEN_ID"] = orig_env_token_id
             else:
                 del os.environ["MODAL_TOKEN_ID"]
             if orig_env_token_secret:
                 os.environ["MODAL_TOKEN_SECRET"] = orig_env_token_secret
             else:
                 del os.environ["MODAL_TOKEN_SECRET"]
-
-
-def test_list_apps(servicer, mock_dir, set_env_client):
-    res = _run(["app", "list"])
-    assert "my_app_foo" not in res.stdout
-
-    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
-        _run(["deploy", "myapp.py", "--name", "my_app_foo"])
-
-    res = _run(["app", "list"])
-    assert "my_app_foo" in res.stdout
```

## test/cls_test.py

```diff
@@ -1,139 +1,132 @@
 # Copyright Modal Labs 2022
 import pytest
 import threading
 from typing import TYPE_CHECKING, Callable, Dict
 
 from typing_extensions import assert_type
 
-from modal import App, Cls, Function, Image, Queue, build, enter, exit, method
+from modal import Cls, Function, Image, Queue, Stub, build, enter, exit, method
 from modal._serialization import deserialize
+from modal.app import ContainerApp
 from modal.exception import DeprecationError, ExecutionError, InvalidError
 from modal.partial_function import (
     _find_callables_for_obj,
     _find_partial_methods_for_cls,
     _PartialFunction,
     _PartialFunctionFlags,
 )
-from modal.runner import deploy_app
-from modal.running_app import RunningApp
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
 from .supports.base_class import BaseCls2
 
-app = App("app")
+stub = Stub("stub")
 
 
-@pytest.fixture(autouse=True)
-def auto_use_set_env_client(set_env_client):
-    # TODO(elias): remove set_env_client fixture here if/when possible - this is required only since
-    #  Client.from_env happens to inject an unused client when loading the
-    #  parameterized function
-    return
-
-
-@app.cls(cpu=42)
+@stub.cls(cpu=42)
 class Foo:
     @method()
     def bar(self, x: int) -> float:
         return x**3
 
 
 def test_run_class(client, servicer):
     assert servicer.n_functions == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         function_id = Foo.bar.object_id
         assert isinstance(Foo, Cls)
         class_id = Foo.object_id
-        app_id = app.app_id
+        app_id = stub.app_id
 
     objects = servicer.app_objects[app_id]
     assert len(objects) == 2  # classes and functions
     assert objects["Foo.bar"] == function_id
     assert objects["Foo"] == class_id
 
 
 def test_call_class_sync(client, servicer):
-    with app.run(client=client):
+    with stub.run(client=client):
         foo: Foo = Foo()
         ret: float = foo.bar.remote(42)
         assert ret == 1764
 
 
-# Reusing the app runs into an issue with stale function handles.
-# TODO (akshat): have all the client tests use separate apps, and throw
-# an exception if the user tries to reuse a app.
-app_remote = App()
+# Reusing the stub runs into an issue with stale function handles.
+# TODO (akshat): have all the client tests use separate stubs, and throw
+# an exception if the user tries to reuse a stub.
+stub_remote = Stub()
 
 
-@app_remote.cls(cpu=42)
+@stub_remote.cls(cpu=42)
 class FooRemote:
     def __init__(self, x: int, y: str) -> None:
         self.x = x
         self.y = y
 
     @method()
     def bar(self, z: int):
         return z**3
 
 
 def test_call_cls_remote_sync(client):
-    with app_remote.run(client=client):
+    with stub_remote.run(client=client):
         foo_remote: FooRemote = FooRemote(3, "hello")
         ret: float = foo_remote.bar.remote(8)
         assert ret == 64  # Mock servicer just squares the argument
 
 
 def test_call_cls_remote_invalid_type(client):
-    with app_remote.run(client=client):
+    with stub_remote.run(client=client):
 
         def my_function():
             print("Hello, world!")
 
         with pytest.raises(ValueError) as excinfo:
             FooRemote(42, my_function)  # type: ignore
 
         exc = excinfo.value
         assert "function" in str(exc)
 
 
 def test_call_cls_remote_modal_type(client):
-    with app_remote.run(client=client):
+    with stub_remote.run(client=client):
         with Queue.ephemeral(client) as q:
             FooRemote(42, q)  # type: ignore
 
 
-app_2 = App()
+
+stub_2 = Stub()
 
 
-@app_2.cls(cpu=42)
+@stub_2.cls(cpu=42)
 class Bar:
     @method()
     def baz(self, x):
         return x**3
 
 
 @pytest.mark.asyncio
 async def test_call_class_async(client, servicer):
-    async with app_2.run(client=client):
+    async with stub_2.run(client=client):
         bar = Bar()
         assert await bar.baz.remote.aio(42) == 1764
 
 
 def test_run_class_serialized(client, servicer):
-    app_ser = App()
+    stub_ser = Stub()
 
-    @app_ser.cls(cpu=42, serialized=True)
+    @stub_ser.cls(cpu=42, serialized=True)
     class FooSer:
         @method()
         def bar(self, x):
             return x**3
 
     assert servicer.n_functions == 0
-    with app_ser.run(client=client):
+    with stub_ser.run(client=client):
         pass
 
     assert servicer.n_functions == 1
     (function_id,) = servicer.app_functions.keys()
     function = servicer.app_functions[function_id]
     assert function.function_name.endswith("FooSer.bar")  # because it's defined in a local scope
     assert function.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED
@@ -144,93 +137,93 @@
     obj = cls()
     meth = fun.__get__(obj, cls)
 
     # Make sure it's callable
     assert meth(100) == 1000000
 
 
-app_remote_2 = App()
+stub_remote_2 = Stub()
 
 
-@app_remote_2.cls(cpu=42)
+@stub_remote_2.cls(cpu=42)
 class BarRemote:
     def __init__(self, x: int, y: str) -> None:
         self.x = x
         self.y = y
 
     @method()
     def baz(self, z: int):
         return z**3
 
 
 @pytest.mark.asyncio
 async def test_call_cls_remote_async(client):
-    async with app_remote_2.run(client=client):
+    async with stub_remote_2.run(client=client):
         bar_remote = BarRemote(3, "hello")
         assert await bar_remote.baz.remote.aio(8) == 64  # Mock servicer just squares the argument
 
 
-app_local = App()
+stub_local = Stub()
 
 
-@app_local.cls(cpu=42)
+@stub_local.cls(cpu=42)
 class FooLocal:
     @method()
     def bar(self, x):
         return x**3
 
     @method()
     def baz(self, y):
         return self.bar.local(y + 1)
 
 
 def test_can_call_locally(client):
     foo = FooLocal()
     assert foo.bar.local(4) == 64
     assert foo.baz.local(4) == 125
-    with app_local.run(client=client):
+    with stub_local.run(client=client):
         assert foo.baz.local(2) == 27
 
 
 def test_can_call_remotely_from_local(client):
-    with app_local.run(client=client):
+    with stub_local.run(client=client):
         foo = FooLocal()
         # remote calls use the mockservicer func impl
         # which just squares the arguments
         assert foo.bar.remote(8) == 64
         assert foo.baz.remote(9) == 81
 
 
-app_remote_3 = App()
+stub_remote_3 = Stub()
 
 
-@app_remote_3.cls(cpu=42)
+@stub_remote_3.cls(cpu=42)
 class NoArgRemote:
     def __init__(self) -> None:
         pass
 
     @method()
     def baz(self, z: int):
         return z**3
 
 
 def test_call_cls_remote_no_args(client):
-    with app_remote_3.run(client=client):
+    with stub_remote_3.run(client=client):
         foo_remote = NoArgRemote()
         assert foo_remote.baz.remote(8) == 64  # Mock servicer just squares the argument
 
 
 if TYPE_CHECKING:
     # Check that type annotations carry through to the decorated classes
     assert_type(Foo(), Foo)
     assert_type(Foo().bar, Function)
 
 
 def test_lookup(client, servicer):
-    deploy_app(app, "my-cls-app", client=client)
+    deploy_stub(stub, "my-cls-app", client=client)
 
     cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)
 
     assert cls.object_id.startswith("cs-")
     assert cls.bar.object_id.startswith("fu-")
 
     # Check that function properties are preserved
@@ -246,55 +239,55 @@
     # Make sure local calls fail
     with pytest.raises(ExecutionError):
         assert obj.bar.local(1, 2)
 
 
 def test_lookup_lazy_remote(client, servicer):
     # See #972 (PR) and #985 (revert PR): adding unit test to catch regression
-    deploy_app(app, "my-cls-app", client=client)
+    deploy_stub(stub, "my-cls-app", client=client)
     cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)
     obj = cls("foo", 234)
     assert obj.bar.remote(42, 77) == 7693
 
 
 def test_lookup_lazy_spawn(client, servicer):
     # See #1071
-    deploy_app(app, "my-cls-app", client=client)
+    deploy_stub(stub, "my-cls-app", client=client)
     cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)
     obj = cls("foo", 234)
     function_call = obj.bar.spawn(42, 77)
     assert function_call.get() == 7693
 
 
-baz_app = App()
+baz_stub = Stub()
 
 
-@baz_app.cls()
+@baz_stub.cls()
 class Baz:
     def __init__(self, x):
         self.x = x
 
     def not_modal_method(self, y: int) -> int:
         return self.x * y
 
 
 def test_call_not_modal_method():
     baz: Baz = Baz(5)
     assert baz.x == 5
     assert baz.not_modal_method(7) == 35
 
 
-cls_with_enter_app = App()
+cls_with_enter_stub = Stub()
 
 
 def get_thread_id():
     return threading.current_thread().name
 
 
-@cls_with_enter_app.cls()
+@cls_with_enter_stub.cls()
 class ClsWithEnter:
     def __init__(self, thread_id):
         self.inited = True
         self.entered = False
         self.thread_id = thread_id
         assert get_thread_id() == self.thread_id
 
@@ -329,15 +322,15 @@
 def test_enter_on_local_modal_call():
     obj = ClsWithEnter(get_thread_id())
     assert obj.modal_method.local(7) == 49
     assert obj.inited
     assert obj.entered
 
 
-@cls_with_enter_app.cls()
+@cls_with_enter_stub.cls()
 class ClsWithAsyncEnter:
     def __init__(self):
         self.inited = True
         self.entered = False
 
     @enter()
     async def enter(self):
@@ -352,105 +345,106 @@
 async def test_async_enter_on_local_modal_call():
     obj = ClsWithAsyncEnter()
     assert await obj.modal_method.local(7) == 49
     assert obj.inited
     assert obj.entered
 
 
-inheritance_app = App()
+inheritance_stub = Stub()
 
 
 class BaseCls:
     @enter()
     def enter(self):
         self.x = 2
 
     @method()
     def run(self, y):
         return self.x * y
 
 
-@inheritance_app.cls()
+@inheritance_stub.cls()
 class DerivedCls(BaseCls):
     pass
 
 
 def test_derived_cls(client, servicer):
-    with inheritance_app.run(client=client):
+    with inheritance_stub.run(client=client):
         # default servicer fn just squares the number
         assert DerivedCls().run.remote(3) == 9
 
 
-inheritance_app_2 = App()
+inheritance_stub_2 = Stub()
 
 
-@inheritance_app_2.cls()
+@inheritance_stub_2.cls()
 class DerivedCls2(BaseCls2):
     pass
 
 
 def test_derived_cls_external_file(client, servicer):
-    with inheritance_app_2.run(client=client):
+    with inheritance_stub_2.run(client=client):
         # default servicer fn just squares the number
         assert DerivedCls2().run.remote(3) == 9
 
 
-def test_rehydrate(client, servicer, reset_container_app):
+def test_rehydrate(client, servicer):
     # Issue introduced in #922 - brief description in #931
 
     # Sanity check that local calls work
     obj = Foo()
     assert obj.bar.local(7) == 343
 
-    # Deploy app to get an app id
-    app_id = deploy_app(app, "my-cls-app", client=client).app_id
+    # Deploy stub to get an app id
+    app_id = deploy_stub(stub, "my-cls-app", client=client).app_id
 
     # Initialize a container
-    container_app = RunningApp(app_id=app_id)
+    app = ContainerApp()
+    app.init(client, app_id, "stub")
 
-    # Associate app with app
-    app._init_container(client, container_app)
+    # Associate app with stub
+    app.associate_stub_container(stub)
 
     # Hydration shouldn't overwrite local function definition
     obj = Foo()
     assert obj.bar.local(7) == 343
 
 
-app_unhydrated = App()
+stub_unhydrated = Stub()
 
 
-@app_unhydrated.cls()
+@stub_unhydrated.cls()
 class FooUnhydrated:
     @method()
     def bar(self):
         ...
 
 
 def test_unhydrated():
     foo = FooUnhydrated()
     with pytest.raises(ExecutionError, match="hydrated"):
         foo.bar.remote(42)
 
 
-app_method_args = App()
+stub_method_args = Stub()
 
 
-@app_method_args.cls()
+@stub_method_args.cls()
 class XYZ:
     @method(keep_warm=3)
     def foo(self):
         ...
 
     @method(keep_warm=7)
     def bar(self):
         ...
 
 
 def test_method_args(servicer, client):
-    with app_method_args.run(client=client):
+    with stub_method_args.run(client=client):
         funcs = servicer.app_functions.values()
         assert [f.function_name for f in funcs] == ["XYZ.foo", "XYZ.bar"]
         assert [f.warm_pool_size for f in funcs] == [3, 7]
 
 
 class ClsWith1Method:
     @method()
@@ -465,21 +459,21 @@
 
     @method()
     def bar(self):
         ...
 
 
 def test_keep_warm_depr():
-    app = App()
+    stub = Stub()
 
     # This should be fine
-    app.cls(keep_warm=2)(ClsWith1Method)
+    stub.cls(keep_warm=2)(ClsWith1Method)
 
     with pytest.warns(DeprecationError, match="@method"):
-        app.cls(keep_warm=2)(ClsWith2Methods)
+        stub.cls(keep_warm=2)(ClsWith2Methods)
 
 
 class ClsWithHandlers:
     @build()
     def my_build(self):
         pass
 
@@ -513,33 +507,33 @@
     pfs = _find_partial_methods_for_cls(ClsWithHandlers, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
     assert list(pfs.keys()) == ["my_enter", "my_build_and_enter"]
 
     pfs = _find_partial_methods_for_cls(ClsWithHandlers, _PartialFunctionFlags.EXIT)
     assert list(pfs.keys()) == ["my_exit"]
 
 
-handler_app = App("handler-app")
+handler_stub = Stub("handler-stub")
 
 
 image = Image.debian_slim().pip_install("xyz")
 
 
-@handler_app.cls(image=image)
+@handler_stub.cls(image=image)
 class ClsWithBuild:
     @build()
     def build(self):
         pass
 
     @method()
     def method(self):
         pass
 
 
 def test_build_image(client, servicer):
-    with handler_app.run(client=client):
+    with handler_stub.run(client=client):
         f_def = servicer.app_functions[ClsWithBuild.method.object_id]
         # The function image should have added a new layer with original image as the parent
         f_image = servicer.images[f_def.image_id]
         assert f_image.base_images[0].image_id == image.object_id
 
 
 @pytest.mark.parametrize("decorator", [build, enter, exit])
@@ -578,17 +572,17 @@
         enter_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
     assert [meth() for meth in enter_methods.values()] == [42, 43]
 
     with pytest.warns(DeprecationError, match="Using `__exit__`.+`modal.exit` decorator"):
         exit_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.EXIT)
     assert [meth(None, None, None) for meth in exit_methods.values()] == [44, 45]
 
-    app = App("deprecated-sync-cls")
+    stub = Stub("deprecated-sync-cls")
     with pytest.warns(DeprecationError):
-        app.cls()(ClsWithDeprecatedSyncMethods)()
+        stub.cls()(ClsWithDeprecatedSyncMethods)()
 
 
 @pytest.mark.asyncio
 async def test_deprecated_async_methods():
     with pytest.warns(DeprecationError, match="Support for decorating parameterized methods with `@exit`"):
 
         class ClsWithDeprecatedAsyncMethods:
@@ -612,25 +606,25 @@
         enter_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
     assert [await meth() for meth in enter_methods.values()] == [42, 43]
 
     with pytest.warns(DeprecationError, match=r"Using `__aexit__`.+`modal.exit` decorator \(on an async method\)"):
         exit_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.EXIT)
     assert [await meth(None, None, None) for meth in exit_methods.values()] == [44, 45]
 
-    app = App("deprecated-async-cls")
+    stub = Stub("deprecated-async-cls")
     with pytest.warns(DeprecationError):
-        app.cls()(ClsWithDeprecatedAsyncMethods)()
+        stub.cls()(ClsWithDeprecatedAsyncMethods)()
 
 
 class HasSnapMethod:
     @enter(snap=True)
     def enter(self):
         pass
 
     @method()
     def f(self):
         pass
 
 
 def test_snap_method_without_snapshot_enabled():
     with pytest.raises(InvalidError, match="A class must have `enable_memory_snapshot=True`"):
-        app.cls(enable_memory_snapshot=False)(HasSnapMethod)
+        stub.cls(enable_memory_snapshot=False)(HasSnapMethod)
```

## test/config_test.py

```diff
@@ -4,15 +4,15 @@
 import pytest
 import subprocess
 import sys
 
 import toml
 
 import modal
-from modal.config import Config, _lookup_workspace, config
+from modal.config import _lookup_workspace, config
 
 
 def _cli(args, env={}):
     lib_dir = pathlib.Path(modal.__file__).parent.parent
     args = [sys.executable, "-m", "modal.cli.entry_point"] + args
     env = {
         **os.environ,
@@ -130,20 +130,8 @@
     config.override_locally(key, value)
     assert os.getenv(key) == value
 
 
 @pytest.mark.asyncio
 async def test_workspace_lookup(servicer, server_url_env):
     resp = await _lookup_workspace(servicer.remote_addr, "ak-abc", "as-xyz")
-    assert resp.username == "test-username"
-
-
-@pytest.mark.parametrize("automount", ["false", "'false'", "'False'", "'0'", 0, "''"])
-def test_config_boolean(modal_config, automount):
-    modal_toml = f"""
-    [prof-1]
-    token_id = 'ak-abc'
-    token_secret = 'as_xyz'
-    automount = {automount}
-    """
-    with modal_config(modal_toml):
-        assert not Config().get("automount", "prof-1")
+    assert resp.workspace_name == "test-workspace"
```

## test/conftest.py

```diff
@@ -12,35 +12,34 @@
 import sys
 import tempfile
 import textwrap
 import threading
 import traceback
 from collections import defaultdict
 from pathlib import Path
-from typing import Dict, Iterator, Optional, get_args
+from typing import Dict, Iterator, Optional
 
 import aiohttp.web
 import aiohttp.web_runner
 import grpclib.server
 import pkg_resources
 import pytest_asyncio
 from google.protobuf.empty_pb2 import Empty
 from grpclib import GRPCError, Status
 
 import modal._serialization
 from modal import __version__, config
-from modal._container_io_manager import _ContainerIOManager
 from modal._serialization import serialize_data_format
 from modal._utils.async_utils import asyncify, synchronize_api
 from modal._utils.grpc_testing import patch_mock_servicer
 from modal._utils.grpc_utils import find_free_port
 from modal._utils.http_utils import run_temporary_http_server
 from modal._vendor import cloudpickle
+from modal.app import _ContainerApp
 from modal.client import Client
-from modal.image import ImageBuilderVersion
 from modal.mount import client_mount_name
 from modal_proto import api_grpc, api_pb2
 
 
 @dataclasses.dataclass
 class VolumeFile:
     data: bytes
@@ -59,15 +58,14 @@
     # TODO(erikbern): add more annotations
     container_inputs: list[api_pb2.FunctionGetInputsResponse]
     container_outputs: list[api_pb2.FunctionPutOutputsRequest]
     fc_data_in: defaultdict[str, asyncio.Queue[api_pb2.DataChunk]]
     fc_data_out: defaultdict[str, asyncio.Queue[api_pb2.DataChunk]]
 
     def __init__(self, blob_host, blobs):
-        self.use_blob_outputs = False
         self.put_outputs_barrier = threading.Barrier(
             1, timeout=10
         )  # set to non-1 to get lock-step of output pushing within a test
         self.get_inputs_barrier = threading.Barrier(
             1, timeout=10
         )  # set to non-1 to get lock-step of input releases within a test
 
@@ -119,15 +117,14 @@
 
         self.task_result = None
 
         self.nfs_files: Dict[str, Dict[str, api_pb2.SharedVolumePutFileRequest]] = defaultdict(dict)
         self.volume_files: Dict[str, Dict[str, VolumeFile]] = defaultdict(dict)
         self.images = {}
         self.image_build_function_ids = {}
-        self.image_builder_versions = {}
         self.force_built_images = []
         self.fail_blob_create = []
         self.blob_create_metadata = None
         self.blob_multipart_threshold = 10_000_000
 
         self.precreated_functions = set()
         self.app_functions = {}
@@ -302,21 +299,14 @@
 
     async def AppHeartbeat(self, stream):
         request: api_pb2.AppHeartbeatRequest = await stream.recv_message()
         self.requests.append(request)
         self.app_heartbeats[request.app_id] += 1
         await stream.send_message(Empty())
 
-    async def AppList(self, stream):
-        await stream.recv_message()
-        apps = []
-        for app_name, app_id in self.deployed_apps.items():
-            apps.append(api_pb2.AppStats(name=app_name, description=app_name, app_id=app_id))
-        await stream.send_message(api_pb2.AppListResponse(apps=apps))
-
     ### Checkpoint
 
     async def ContainerCheckpoint(self, stream):
         request: api_pb2.ContainerCheckpointRequest = await stream.recv_message()
         self.requests.append(request)
         self.container_checkpoint_requests += 1
         await stream.send_message(Empty())
@@ -327,15 +317,16 @@
         req = await stream.recv_message()
         # This is used to test retry_transient_errors, see grpc_utils_test.py
         self.blob_create_metadata = stream.metadata
         if len(self.fail_blob_create) > 0:
             status_code = self.fail_blob_create.pop()
             raise GRPCError(status_code, "foobar")
         elif req.content_length > self.blob_multipart_threshold:
-            blob_id = await self.next_blob_id()
+            self.n_blobs += 1
+            blob_id = f"bl-{self.n_blobs}"
             num_parts = (req.content_length + self.blob_multipart_threshold - 1) // self.blob_multipart_threshold
             upload_urls = []
             for part_number in range(num_parts):
                 upload_url = f"{self.blob_host}/upload?blob_id={blob_id}&part_number={part_number}"
                 upload_urls.append(upload_url)
 
             await stream.send_message(
@@ -345,23 +336,19 @@
                         part_length=self.blob_multipart_threshold,
                         upload_urls=upload_urls,
                         completion_url=f"{self.blob_host}/complete_multipart?blob_id={blob_id}",
                     ),
                 )
             )
         else:
-            blob_id = await self.next_blob_id()
+            self.n_blobs += 1
+            blob_id = f"bl-{self.n_blobs}"
             upload_url = f"{self.blob_host}/upload?blob_id={blob_id}"
             await stream.send_message(api_pb2.BlobCreateResponse(blob_id=blob_id, upload_url=upload_url))
 
-    async def next_blob_id(self):
-        self.n_blobs += 1
-        blob_id = f"bl-{self.n_blobs}"
-        return blob_id
-
     async def BlobGet(self, stream):
         request: api_pb2.BlobGetRequest = await stream.recv_message()
         download_url = f"{self.blob_host}/download?blob_id={request.blob_id}"
         await stream.send_message(api_pb2.BlobGetResponse(download_url=download_url))
 
     ### Class
 
@@ -389,29 +376,28 @@
     ### Client
 
     async def ClientHello(self, stream):
         request: Empty = await stream.recv_message()
         self.requests.append(request)
         self.client_create_metadata = stream.metadata
         client_version = stream.metadata["x-modal-client-version"]
-        image_builder_version = max(get_args(ImageBuilderVersion))
-        warning = ""
         assert stream.user_agent.startswith(f"modal-client/{__version__} ")
         if stream.metadata.get("x-modal-token-id") == "bad":
             raise GRPCError(Status.UNAUTHENTICATED, "bad bad bad")
+        elif client_version == "timeout":
+            await asyncio.sleep(60)
+            await stream.send_message(api_pb2.ClientHelloResponse())
         elif client_version == "unauthenticated":
             raise GRPCError(Status.UNAUTHENTICATED, "failed authentication")
         elif client_version == "deprecated":
-            warning = "SUPER OLD"
-        elif client_version == "timeout":
-            await asyncio.sleep(60)
+            await stream.send_message(api_pb2.ClientHelloResponse(warning="SUPER OLD"))
         elif pkg_resources.parse_version(client_version) < pkg_resources.parse_version(__version__):
             raise GRPCError(Status.FAILED_PRECONDITION, "Old client")
-        resp = api_pb2.ClientHelloResponse(warning=warning, image_builder_version=image_builder_version)
-        await stream.send_message(resp)
+        else:
+            await stream.send_message(api_pb2.ClientHelloResponse())
 
     # Container
 
     async def ContainerHeartbeat(self, stream):
         request: api_pb2.ContainerHeartbeatRequest = await stream.recv_message()
         self.requests.append(request)
         # Return earlier than the usual 15-second heartbeat to avoid suspending tests.
@@ -606,19 +592,15 @@
         await stream.send_message(api_pb2.FunctionMapResponse(function_call_id=function_call_id))
 
     async def FunctionPutInputs(self, stream):
         request: api_pb2.FunctionPutInputsRequest = await stream.recv_message()
         response_items = []
         function_call_inputs = self.client_calls.setdefault(request.function_call_id, [])
         for item in request.inputs:
-            if item.input.WhichOneof("args_oneof") == "args":
-                args, kwargs = modal._serialization.deserialize(item.input.args, None)
-            else:
-                args, kwargs = modal._serialization.deserialize(self.blobs[item.input.args_blob_id], None)
-
+            args, kwargs = modal._serialization.deserialize(item.input.args, None) if item.input.args else ((), {})
             input_id = f"in-{self.n_inputs}"
             self.n_inputs += 1
             response_items.append(api_pb2.FunctionPutInputsResponseItem(input_id=input_id, idx=item.idx))
             function_call_inputs.append(((item.idx, input_id), (args, kwargs)))
         if self.slow_put_inputs:
             await asyncio.sleep(0.001)
         await stream.send_message(api_pb2.FunctionPutInputsResponse(inputs=response_items))
@@ -666,27 +648,21 @@
                 output_exc = api_pb2.FunctionGetOutputsItem(
                     input_id=input_id, idx=idx, result=result, gen_index=0, data_format=api_pb2.DATA_FORMAT_PICKLE
                 )
 
             if output_exc:
                 output = output_exc
             else:
-                serialized_data = serialize_data_format(result, result_data_format)
-                if self.use_blob_outputs:
-                    blob_id = await self.next_blob_id()
-                    self.blobs[blob_id] = serialized_data
-                    data_kwargs = {
-                        "data_blob_id": blob_id,
-                    }
-                else:
-                    data_kwargs = {"data": serialized_data}
                 output = api_pb2.FunctionGetOutputsItem(
                     input_id=input_id,
                     idx=idx,
-                    result=api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS, **data_kwargs),
+                    result=api_pb2.GenericResult(
+                        status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
+                        data=serialize_data_format(result, result_data_format),
+                    ),
                     data_format=result_data_format,
                 )
 
             await stream.send_message(api_pb2.FunctionGetOutputsResponse(outputs=[output]))
         else:
             await stream.send_message(api_pb2.FunctionGetOutputsResponse(outputs=[]))
 
@@ -726,15 +702,14 @@
     async def ImageGetOrCreate(self, stream):
         request: api_pb2.ImageGetOrCreateRequest = await stream.recv_message()
         idx = len(self.images) + 1
         image_id = f"im-{idx}"
 
         self.images[image_id] = request.image
         self.image_build_function_ids[image_id] = request.build_function_id
-        self.image_builder_versions[image_id] = request.builder_version
         if request.force_build:
             self.force_built_images.append(image_id)
         await stream.send_message(api_pb2.ImageGetOrCreateResponse(image_id=image_id))
 
     async def ImageJoinStreaming(self, stream):
         await stream.recv_message()
         task_log_1 = api_pb2.TaskLogs(data="hello, world\n", file_descriptor=api_pb2.FILE_DESCRIPTOR_INFO)
@@ -842,25 +817,14 @@
             values = []
         await stream.send_message(api_pb2.QueueGetResponse(values=values))
 
     async def QueueLen(self, stream):
         await stream.recv_message()
         await stream.send_message(api_pb2.QueueLenResponse(len=len(self.queue)))
 
-    async def QueueNextItems(self, stream):
-        request: api_pb2.QueueNextItemsRequest = await stream.recv_message()
-        next_item_idx = int(request.last_entry_id) + 1 if request.last_entry_id else 0
-        if next_item_idx < len(self.queue):
-            item = api_pb2.QueueItem(value=self.queue[next_item_idx], entry_id=f"{next_item_idx}")
-            await stream.send_message(api_pb2.QueueNextItemsResponse(items=[item]))
-        else:
-            if request.item_poll_timeout > 0:
-                await asyncio.sleep(0.1)
-            await stream.send_message(api_pb2.QueueNextItemsResponse(items=[]))
-
     ### Sandbox
 
     async def SandboxCreate(self, stream):
         request: api_pb2.SandboxCreateRequest = await stream.recv_message()
         if request.definition.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL:
             self.sandbox_is_interactive = True
 
@@ -1026,15 +990,15 @@
             await stream.send_message(api_pb2.SharedVolumeGetFileResponse(data_blob_id=put_req.data_blob_id))
         else:
             await stream.send_message(api_pb2.SharedVolumeGetFileResponse(data=put_req.data))
 
     async def SharedVolumeListFilesStream(self, stream):
         req: api_pb2.SharedVolumeListFilesRequest = await stream.recv_message()
         for path in self.nfs_files[req.shared_volume_id].keys():
-            entry = api_pb2.FileEntry(path=path)
+            entry = api_pb2.SharedVolumeListFilesEntry(path=path)
             response = api_pb2.SharedVolumeListFilesResponse(entries=[entry])
             await stream.send_message(response)
 
     ### Task
 
     async def TaskCurrentInputs(
         self, stream: "grpclib.server.Stream[Empty, api_pb2.TaskCurrentInputsResponse]"
@@ -1060,15 +1024,17 @@
             api_pb2.TokenFlowWaitResponse(
                 token_id="abc",
                 token_secret="xyz",
             )
         )
 
     async def WorkspaceNameLookup(self, stream):
-        await stream.send_message(api_pb2.WorkspaceNameLookupResponse(username="test-username"))
+        await stream.send_message(
+            api_pb2.WorkspaceNameLookupResponse(workspace_name="test-workspace", username="test-username")
+        )
 
     ### Tunnel
 
     async def TunnelStart(self, stream):
         request: api_pb2.TunnelStartRequest = await stream.recv_message()
         port = request.port
         await stream.send_message(api_pb2.TunnelStartResponse(host=f"{port}.modal.test", port=443))
@@ -1165,15 +1131,19 @@
         await stream.send_message(Empty())
 
     async def VolumeListFiles(self, stream):
         req = await stream.recv_message()
         if req.path != "**":
             raise NotImplementedError("Only '**' listing is supported.")
         for k, vol_file in self.volume_files[req.volume_id].items():
-            entries = [api_pb2.FileEntry(path=k, type=api_pb2.FileEntry.FileType.FILE, size=len(vol_file.data))]
+            entries = [
+                api_pb2.VolumeListFilesEntry(
+                    path=k, type=api_pb2.VolumeListFilesEntry.FileType.FILE, size=len(vol_file.data)
+                )
+            ]
             await stream.send_message(api_pb2.VolumeListFilesResponse(entries=entries))
 
     async def VolumePutFiles(self, stream):
         req = await stream.recv_message()
         for file in req.files:
             blob_data = self.files_sha2data[file.sha256_hex]
 
@@ -1388,15 +1358,15 @@
 
 
 @pytest.fixture(autouse=True)
 def reset_container_app():
     try:
         yield
     finally:
-        _ContainerIOManager._reset_singleton()
+        _ContainerApp._reset_container()
 
 
 @pytest.fixture
 def repo_root(request):
     return Path(request.config.rootdir)
 
 
@@ -1444,16 +1414,7 @@
 
     return mock_modal_toml
 
 
 @pytest.fixture
 def supports_dir(test_dir):
     return test_dir / Path("supports")
-
-
-@pytest_asyncio.fixture
-async def set_env_client(client):
-    try:
-        Client.set_env_client(client)
-        yield
-    finally:
-        Client.set_env_client(None)
```

## test/container_app_test.py

```diff
@@ -1,50 +1,32 @@
 # Copyright Modal Labs 2022
 import pytest
-from typing import Dict
 
-from google.protobuf.empty_pb2 import Empty
-from google.protobuf.message import Message
-
-from modal import App, interact
-from modal._container_io_manager import ContainerIOManager
-from modal.running_app import RunningApp
+from modal import Stub
+from modal.app import container_app
 from modal_proto import api_pb2
 
 from .supports.skip import skip_windows_unix_socket
 
 
 def my_f_1(x):
     pass
 
 
 @skip_windows_unix_socket
 @pytest.mark.asyncio
-async def test_container_function_lazily_imported(container_client):
-    tag_to_object_id: Dict[str, str] = {
+async def test_container_function_lazily_imported(unix_servicer, container_client):
+    unix_servicer.app_objects["ap-123"] = {
         "my_f_1": "fu-123",
         "my_d": "di-123",
     }
-    object_handle_metadata: Dict[str, Message] = {
-        "fu-123": api_pb2.FunctionHandleMetadata(),
-    }
-    container_app = RunningApp(
-        app_id="ap-123", tag_to_object_id=tag_to_object_id, object_handle_metadata=object_handle_metadata
-    )
-    app = App()
+    unix_servicer.app_functions["fu-123"] = api_pb2.Function()
+
+    await container_app.init.aio(container_client, "ap-123")
+    stub = Stub()
 
     # This is normally done in _container_entrypoint
-    app._init_container(container_client, container_app)
+    container_app.associate_stub_container(stub)
 
     # Now, let's create my_f after the app started running and make sure it works
-    my_f_container = app.function()(my_f_1)
+    my_f_container = stub.function()(my_f_1)
     assert await my_f_container.remote.aio(42) == 1764  # type: ignore
-
-
-@skip_windows_unix_socket
-def test_interact(container_client, unix_servicer):
-    # Initialize container singleton
-    ContainerIOManager(api_pb2.ContainerArguments(), container_client)
-
-    with unix_servicer.intercept() as ctx:
-        ctx.add_response("FunctionStartPtyShell", Empty())
-        interact()
```

## test/container_test.py

```diff
@@ -17,29 +17,29 @@
 from typing import Any, Dict, List, Optional, Tuple
 from unittest import mock
 from unittest.mock import MagicMock
 
 from grpclib import Status
 from grpclib.exceptions import GRPCError
 
-from modal import Client, is_local
+from modal import Client
 from modal._container_entrypoint import UserException, main
 from modal._serialization import (
     deserialize,
     deserialize_data_format,
     serialize,
     serialize_data_format,
 )
 from modal._utils import async_utils
-from modal.app import _App
 from modal.exception import InvalidError
 from modal.partial_function import enter
+from modal.stub import _Stub
 from modal_proto import api_pb2
 
-from .helpers import deploy_app_externally
+from .helpers import deploy_stub_externally
 from .supports.skip import skip_windows_signals, skip_windows_unix_socket
 
 EXTRA_TOLERANCE_DELAY = 2.0 if sys.platform == "linux" else 5.0
 FUNCTION_CALL_ID = "fc-123"
 SLEEP_DELAY = 0.1
 
 
@@ -195,16 +195,16 @@
             with pathlib.Path(tmp_file_name).open("w") as target:
                 json.dump({}, target)
             env["MODAL_RESTORE_STATE_PATH"] = tmp_file_name
 
             # Override server URL to reproduce restore behavior.
             env["MODAL_SERVER_URL"] = servicer.remote_addr
 
-        # reset _App tracking state between runs
-        _App._all_apps.clear()
+        # reset _Stub tracking state between runs
+        _Stub._all_stubs = {}
 
         try:
             with mock.patch.dict(os.environ, env):
                 main(container_args, client)
         except UserException:
             # Handle it gracefully
             pass
@@ -722,22 +722,22 @@
 
     assert stdout == ""
     assert stderr == ""
 
 
 @skip_windows_unix_socket
 def test_function_sibling_hydration(unix_servicer):
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(unix_servicer, "test.supports.functions", "check_sibling_hydration")
     assert _unwrap_scalar(ret) is None
 
 
 @skip_windows_unix_socket
 def test_multistub(unix_servicer, caplog):
-    deploy_app_externally(unix_servicer, "test.supports.multistub", "a")
+    deploy_stub_externally(unix_servicer, "test.supports.multistub", "a")
     ret = _run_container(unix_servicer, "test.supports.multistub", "a_func")
     assert _unwrap_scalar(ret) is None
     assert len(caplog.messages) == 0
     # Note that the stub can be inferred from the function, even though there are multiple
     # stubs present in the file
 
 
@@ -905,28 +905,27 @@
         inputs=_get_inputs(((3,), {})),
     )
     assert _unwrap_scalar(ret) == 6
 
 
 @skip_windows_unix_socket
 def test_call_function_that_calls_function(unix_servicer):
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(
         unix_servicer,
         "test.supports.functions",
         "cube",
         inputs=_get_inputs(((42,), {})),
     )
     assert _unwrap_scalar(ret) == 42**3
 
 
 @skip_windows_unix_socket
-def test_call_function_that_calls_method(unix_servicer, set_env_client):
-    # TODO (elias): Remove set_env_client fixture dependency - shouldn't need an env client here?
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+def test_call_function_that_calls_method(unix_servicer):
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(
         unix_servicer,
         "test.supports.functions",
         "function_calling_method",
         inputs=_get_inputs(((42, "abc", 123), {})),
     )
     assert _unwrap_scalar(ret) == 123**2  # servicer's implementation of function calling
@@ -1019,15 +1018,15 @@
     volume_commit_rpcs = [r for r in unix_servicer.requests if isinstance(r, api_pb2.VolumeCommitRequest)]
     assert len(volume_commit_rpcs) == 3
     assert _unwrap_scalar(ret) == 42**2
 
 
 @skip_windows_unix_socket
 def test_function_dep_hydration(unix_servicer):
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(
         unix_servicer,
         "test.supports.functions",
         "check_dep_hydration",
         deps=["im-1", "vo-0", "im-1", "im-2", "vo-0", "vo-1"],
     )
     assert _unwrap_scalar(ret) is None
@@ -1072,15 +1071,15 @@
     # set up spys to track synchronicity calls to _translate_scalar_in/out
     translate_in_spy = MagicMock(wraps=synchronizer._translate_scalar_in)
     monkeypatch.setattr(synchronizer, "_translate_scalar_in", translate_in_spy)
     translate_out_spy = MagicMock(wraps=synchronizer._translate_scalar_out)
     monkeypatch.setattr(synchronizer, "_translate_scalar_out", translate_out_spy)
 
     # don't do blobbing for this test
-    monkeypatch.setattr("modal._container_io_manager.MAX_OBJECT_SIZE_BYTES", 1e100)
+    monkeypatch.setattr("modal._container_entrypoint.MAX_OBJECT_SIZE_BYTES", 1e100)
 
     large_data_list = list(range(int(1e6)))  # large data set
 
     t0 = time.perf_counter()
     # pr = cProfile.Profile()
     # pr.enable()
     _run_container(
@@ -1245,15 +1244,15 @@
 
     assert len(ret.items) == 2
     assert ret.items[0].result.status == api_pb2.GenericResult.GENERIC_STATUS_SUCCESS
 
 
 @skip_windows_unix_socket
 def test_container_heartbeat_survives_grpc_deadlines(servicer, caplog, monkeypatch):
-    monkeypatch.setattr("modal._container_io_manager.HEARTBEAT_INTERVAL", 0.01)
+    monkeypatch.setattr("modal._container_entrypoint.HEARTBEAT_INTERVAL", 0.01)
     num_heartbeats = 0
 
     async def heartbeat_responder(servicer, stream):
         nonlocal num_heartbeats
         num_heartbeats += 1
         await stream.recv_message()
         raise GRPCError(Status.DEADLINE_EXCEEDED)
@@ -1281,17 +1280,17 @@
     numcalls = 0
 
     async def custom_heartbeater(self):
         nonlocal numcalls
         numcalls += 1
         raise Exception("oops")
 
-    monkeypatch.setattr("modal._container_io_manager.HEARTBEAT_INTERVAL", 0.01)
+    monkeypatch.setattr("modal._container_entrypoint.HEARTBEAT_INTERVAL", 0.01)
     monkeypatch.setattr(
-        "modal._container_io_manager._ContainerIOManager._heartbeat_handle_cancellations", custom_heartbeater
+        "modal._container_entrypoint._FunctionIOManager._heartbeat_handle_cancellations", custom_heartbeater
     )
 
     ret = _run_container(
         servicer,
         "test.supports.functions",
         "delay",
         inputs=_get_inputs(((0.5,), {})),
@@ -1384,21 +1383,7 @@
     stdout, stderr = container_process.communicate(timeout=5)
 
     assert len(servicer.container_outputs) == 1
     assert container_process.returncode == 0
     assert "[events:enter_sync,enter_async,delay,exit_sync,exit_async]" in stdout.decode()
     assert "Traceback" not in stderr.decode()
     assert servicer.task_result is None
-
-
-@skip_windows_unix_socket
-def test_sandbox(unix_servicer, event_loop):
-    ret = _run_container(unix_servicer, "test.supports.functions", "sandbox_f")
-    assert _unwrap_scalar(ret) == "sb-123"
-
-
-@skip_windows_unix_socket
-def test_is_local(unix_servicer, event_loop):
-    assert is_local() == True
-
-    ret = _run_container(unix_servicer, "test.supports.functions", "is_local_f")
-    assert _unwrap_scalar(ret) == False
```

## test/cpu_test.py

```diff
@@ -1,23 +1,23 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App
+from modal import Stub
 from modal.exception import InvalidError
 
 
 def dummy():
     pass
 
 
 def test_cpu_lower_bound(client, servicer):
-    app = App()
+    stub = Stub()
 
-    app.function(cpu=0.0)(dummy)
+    stub.function(cpu=0.0)(dummy)
 
     with pytest.raises(InvalidError):
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
-    app.function(cpu=42)(dummy)
-    with app.run(client=client):
+    stub.function(cpu=42)(dummy)
+    with stub.run(client=client):
         pass
```

## test/decorator_test.py

```diff
@@ -1,85 +1,85 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App, asgi_app, method, web_endpoint, wsgi_app
+from modal import Stub, asgi_app, method, web_endpoint, wsgi_app
 from modal.exception import InvalidError
 
 
 def test_local_entrypoint_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="local_entrypoint()"):
 
-        @app.local_entrypoint  # type: ignore
+        @stub.local_entrypoint  # type: ignore
         def f():
             pass
 
 
 def test_function_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="function()"):
 
-        @app.function  # type: ignore
+        @stub.function  # type: ignore
         def f():
             pass
 
 
 def test_cls_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="cls()"):
 
-        @app.cls  # type: ignore
+        @stub.cls  # type: ignore
         class XYZ:
             pass
 
 
 def test_method_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="method()"):
 
-        @app.cls()
+        @stub.cls()
         class XYZ:
             @method  # type: ignore
             def f(self):
                 pass
 
 
 def test_invalid_web_decorator_usage():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="web_endpoint()"):
 
-        @app.function()  # type: ignore
+        @stub.function()  # type: ignore
         @web_endpoint  # type: ignore
         def my_handle():
             pass
 
     with pytest.raises(InvalidError, match="asgi_app()"):
 
-        @app.function()  # type: ignore
+        @stub.function()  # type: ignore
         @asgi_app  # type: ignore
         def my_handle_asgi():
             pass
 
     with pytest.raises(InvalidError, match="wsgi_app()"):
 
-        @app.function()  # type: ignore
+        @stub.function()  # type: ignore
         @wsgi_app  # type: ignore
         def my_handle_wsgi():
             pass
 
 
 def test_web_endpoint_method():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="remove the `@method`"):
 
-        @app.cls()
+        @stub.cls()
         class Container:
             @method()  # type: ignore
             @web_endpoint()
             def generate(self):
                 pass
```

## test/dict_test.py

```diff
@@ -27,16 +27,7 @@
     with Dict.ephemeral({"bar": 123}, client=client, _heartbeat_sleep=1) as d:
         d["foo"] = 42
         assert d.len() == 2
         assert d["foo"] == 42
         assert d["bar"] == 123
         time.sleep(1.5)  # Make time for 2 heartbeats
     assert servicer.n_dict_heartbeats == 2
-
-
-def test_dict_lazy_hydrate_named(set_env_client, servicer):
-    with servicer.intercept() as ctx:
-        d = Dict.from_name("foo", create_if_missing=True)
-        assert len(ctx.get_requests("DictGetOrCreate")) == 0  # sanity check that the get request is lazy
-        d["foo"] = 42
-        assert d["foo"] == 42
-        assert len(ctx.get_requests("DictGetOrCreate")) == 1  # just sanity check that object is only hydrated once...
```

## test/function_serialization_test.py

```diff
@@ -1,25 +1,25 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App
+from modal import Stub
 from modal._serialization import deserialize
 
 
 @pytest.mark.asyncio
 async def test_serialize_deserialize_function(servicer, client):
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True, name="foo")
+    @stub.function(serialized=True, name="foo")
     def foo():
         2 * foo.remote()
 
     assert foo.object_id is None
 
-    with app.run(client=client):
+    with stub.run(client=client):
         object_id = foo.object_id
 
     assert object_id is not None
     assert {object_id} == servicer.precreated_functions
 
     foo_def = servicer.app_functions[object_id]
```

## test/function_test.py

```diff
@@ -1,234 +1,134 @@
 # Copyright Modal Labs 2022
 import asyncio
 import inspect
-import os
 import pytest
 import time
 import typing
-from contextlib import contextmanager
 
 from synchronicity.exceptions import UserCodeException
 
 import modal
-from modal import App, Image, Mount, NetworkFileSystem, Proxy, web_endpoint
-from modal._utils.async_utils import synchronize_api
+from modal import Image, Mount, NetworkFileSystem, Proxy, Stub, web_endpoint
 from modal._vendor import cloudpickle
 from modal.exception import ExecutionError, InvalidError
 from modal.functions import Function, FunctionCall, gather
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-if os.environ.get("GITHUB_ACTIONS") == "true":
-    TIME_TOLERANCE = 0.25
-else:
-    TIME_TOLERANCE = 0.05
-
-
-@app.function()
+@stub.function()
 def foo(p, q):
     return p + q + 11  # not actually used in test (servicer returns sum of square of all args)
 
 
-@app.function()
+@stub.function()
 async def async_foo(p, q):
     return p + q + 12
 
 
 def dummy():
     pass  # not actually used in test (servicer returns sum of square of all args)
 
 
 def test_run_function(client, servicer):
     assert len(servicer.cleared_function_calls) == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         assert foo.remote(2, 4) == 20
         assert len(servicer.cleared_function_calls) == 1
 
 
 @pytest.mark.asyncio
 async def test_call_function_locally(client, servicer):
     assert foo.local(22, 44) == 77  # call it locally
     assert await async_foo.local(22, 44) == 78
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert foo.remote(2, 4) == 20
         assert async_foo.remote(2, 4) == 20
         assert await async_foo.remote.aio(2, 4) == 20
 
 
 @pytest.mark.parametrize("slow_put_inputs", [False, True])
 @pytest.mark.timeout(120)
 def test_map(client, servicer, slow_put_inputs):
     servicer.slow_put_inputs = slow_put_inputs
 
-    app = App()
-    dummy_modal = app.function()(dummy)
+    stub = Stub()
+    dummy_modal = stub.function()(dummy)
 
     assert len(servicer.cleared_function_calls) == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         assert list(dummy_modal.map([5, 2], [4, 3])) == [41, 13]
         assert len(servicer.cleared_function_calls) == 1
         assert set(dummy_modal.map([5, 2], [4, 3], order_outputs=False)) == {13, 41}
         assert len(servicer.cleared_function_calls) == 2
 
 
-@pytest.mark.asyncio
-async def test_map_async_generator(client):
-    app = App()
-    dummy_modal = app.function()(dummy)
-
-    async def gen_num():
-        yield 2
-        yield 3
-
-    async with app.run(client=client):
-        res = [num async for num in dummy_modal.map.aio(gen_num())]
-        assert res == [4, 9]
-
-
-def _pow2(x: int):
-    return x**2
-
-
-@contextmanager
-def synchronicity_loop_delay_tracker():
-    done = False
-
-    async def _track_eventloop_blocking():
-        max_dur = 0.0
-        BLOCK_TIME = 0.01
-        while not done:
-            t0 = time.perf_counter()
-            await asyncio.sleep(BLOCK_TIME)
-            max_dur = max(max_dur, time.perf_counter() - t0)
-        return max_dur - BLOCK_TIME  # if it takes exactly BLOCK_TIME we would have zero delay
-
-    track_eventloop_blocking = synchronize_api(_track_eventloop_blocking)
-    yield track_eventloop_blocking(_future=True)
-    done = True
-
-
-def test_map_blocking_iterator_blocking_synchronicity_loop(client):
-    app = App()
-    SLEEP_DUR = 0.5
-
-    def blocking_iter():
-        yield 1
-        time.sleep(SLEEP_DUR)
-        yield 2
-
-    pow2 = app.function()(_pow2)
-
-    with app.run(client=client):
-        t0 = time.monotonic()
-        with synchronicity_loop_delay_tracker() as max_delay:
-            for _ in pow2.map(blocking_iter()):
-                pass
-        dur = time.monotonic() - t0
-    assert dur >= SLEEP_DUR
-    assert max_delay.result() < TIME_TOLERANCE  # should typically be much smaller than this
-
-
-@pytest.mark.asyncio
-async def test_map_blocking_iterator_blocking_synchronicity_loop_async(client):
-    app = App()
-    SLEEP_DUR = 0.5
-
-    def blocking_iter():
-        yield 1
-        time.sleep(SLEEP_DUR)
-        yield 2
-
-    pow2 = app.function()(_pow2)
-
-    async with app.run(client=client):
-        t0 = time.monotonic()
-        with synchronicity_loop_delay_tracker() as max_delay:
-            async for _ in pow2.map.aio(blocking_iter()):
-                pass
-        dur = time.monotonic() - t0
-    assert dur >= SLEEP_DUR
-    assert max_delay.result() < TIME_TOLERANCE  # should typically be much smaller than this
-
-
 _side_effect_count = 0
 
 
 def side_effect(_):
     global _side_effect_count
     _side_effect_count += 1
 
 
 def test_for_each(client, servicer):
-    app = App()
-    side_effect_modal = app.function()(servicer.function_body(side_effect))
+    stub = Stub()
+    side_effect_modal = stub.function()(servicer.function_body(side_effect))
     assert _side_effect_count == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         side_effect_modal.for_each(range(10))
 
     assert _side_effect_count == 10
 
 
 def custom_function(x):
     if x % 2 == 0:
         return x
 
 
 def test_map_none_values(client, servicer):
-    app = App()
+    stub = Stub()
 
-    custom_function_modal = app.function()(servicer.function_body(custom_function))
+    custom_function_modal = stub.function()(servicer.function_body(custom_function))
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert list(custom_function_modal.map(range(4))) == [0, None, 2, None]
 
 
 def test_starmap(client):
-    app = App()
+    stub = Stub()
 
-    dummy_modal = app.function()(dummy)
-    with app.run(client=client):
+    dummy_modal = stub.function()(dummy)
+    with stub.run(client=client):
         assert list(dummy_modal.starmap([[5, 2], [4, 3]])) == [29, 25]
 
 
 def test_function_memory_request(client):
-    app = App()
-    app.function(memory=2048)(dummy)
-
-
-def test_function_memory_limit(client):
-    app = App()
-    f = app.function(memory=(2048, 4096))(dummy)
-
-    with app.run(client=client):
-        f.remote()
-
-    g = app.function(memory=(2048, 2048 - 1))(custom_function)
-    with pytest.raises(InvalidError), app.run(client=client):
-        g.remote()
+    stub = Stub()
+    stub.function(memory=2048)(dummy)
 
 
 def test_function_cpu_request(client):
-    app = App()
-    app.function(cpu=2.0)(dummy)
+    stub = Stub()
+    stub.function(cpu=2.0)(dummy)
 
 
 def later():
     return "hello"
 
 
 def test_function_future(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_modal = app.function()(servicer.function_body(later))
-    with app.run(client=client):
+    later_modal = stub.function()(servicer.function_body(later))
+    with stub.run(client=client):
         future = later_modal.spawn()
         assert isinstance(future, FunctionCall)
 
         servicer.function_is_running = True
         assert future.object_id == "fc-1"
 
         with pytest.raises(TimeoutError):
@@ -247,19 +147,19 @@
         assert "fc-2" in servicer.cancelled_calls
 
         assert future.object_id not in servicer.cleared_function_calls
 
 
 @pytest.mark.asyncio
 async def test_function_future_async(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_modal = app.function()(servicer.function_body(later))
+    later_modal = stub.function()(servicer.function_body(later))
 
-    async with app.run(client=client):
+    async with stub.run(client=client):
         future = await later_modal.spawn.aio()
         servicer.function_is_running = True
 
         with pytest.raises(TimeoutError):
             await future.get.aio(0.01)
 
         servicer.function_is_running = False
@@ -273,90 +173,90 @@
 
 async def async_later_gen():
     yield "foo"
 
 
 @pytest.mark.asyncio
 async def test_generator(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_gen_modal = app.function()(later_gen)
+    later_gen_modal = stub.function()(later_gen)
 
     def dummy():
         yield "bar"
         yield "baz"
         yield "boo"
 
     servicer.function_body(dummy)
 
     assert len(servicer.cleared_function_calls) == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         assert later_gen_modal.is_generator
         res: typing.Generator = later_gen_modal.remote_gen()  # type: ignore
         # Generators fulfil the *iterator protocol*, which requires both these methods.
         # https://docs.python.org/3/library/stdtypes.html#typeiter
         assert hasattr(res, "__iter__")  # strangely inspect.isgenerator returns false
         assert hasattr(res, "__next__")
         assert next(res) == "bar"
         assert list(res) == ["baz", "boo"]
         assert len(servicer.cleared_function_calls) == 1
 
 
-def test_generator_map_invalid(client, servicer):
-    app = App()
+@pytest.mark.asyncio
+async def test_generator_map_invalid(client, servicer):
+    stub = Stub()
 
-    later_gen_modal = app.function()(later_gen)
+    later_gen_modal = stub.function()(later_gen)
 
     def dummy(x):
         yield x
 
     servicer.function_body(dummy)
 
-    with app.run(client=client):
-        with pytest.raises(InvalidError, match="A generator function cannot be called with"):
+    with stub.run(client=client):
+        with pytest.raises(InvalidError):
             # Support for .map() on generators was removed in version 0.57
             for _ in later_gen_modal.map([1, 2, 3]):
                 pass
-
-        with pytest.raises(InvalidError, match="A generator function cannot be called with"):
+        with pytest.raises(InvalidError):
             later_gen_modal.for_each([1, 2, 3])
 
 
 @pytest.mark.asyncio
 async def test_generator_async(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_gen_modal = app.function()(async_later_gen)
+    later_gen_modal = stub.function()(async_later_gen)
 
     async def async_dummy():
         yield "bar"
         yield "baz"
 
     servicer.function_body(async_dummy)
 
     assert len(servicer.cleared_function_calls) == 0
-    async with app.run(client=client):
+    async with stub.run(client=client):
         assert later_gen_modal.is_generator
         res = later_gen_modal.remote_gen.aio()
         # Async generators fulfil the *asynchronous iterator protocol*, which requires both these methods.
         # https://peps.python.org/pep-0525/#support-for-asynchronous-iteration-protocol
         assert hasattr(res, "__aiter__")
         assert hasattr(res, "__anext__")
         # TODO(Jonathon): This works outside of testing, but here gives:
         # `TypeError: cannot pickle 'async_generator' object`
         # await res.__anext__() == "bar"
         # assert len(servicer.cleared_function_calls) == 1
 
 
 @pytest.mark.asyncio
 async def test_generator_future(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_gen_modal = app.function()(later_gen)
-    with app.run(client=client):
+    later_gen_modal = stub.function()(later_gen)
+    with stub.run(client=client):
         assert later_gen_modal.spawn() is None  # until we have a nice interface for polling generator futures
 
 
 def gen_with_arg(i):
     yield "foo"
 
 
@@ -364,58 +264,58 @@
     # need to use async function body in client test to run stuff in parallel
     # but calling interface is still non-asyncio
     await asyncio.sleep(sleep_seconds)
     return sleep_seconds
 
 
 def test_sync_parallelism(client, servicer):
-    app = App()
+    stub = Stub()
 
-    slo1_modal = app.function()(servicer.function_body(slo1))
-    with app.run(client=client):
+    slo1_modal = stub.function()(servicer.function_body(slo1))
+    with stub.run(client=client):
         t0 = time.time()
         # NOTE tests breaks in macOS CI if the smaller time is smaller than ~300ms
         res = gather(slo1_modal.spawn(0.31), slo1_modal.spawn(0.3))
         t1 = time.time()
         assert res == [0.31, 0.3]  # results should be ordered as inputs, not by completion time
         assert t1 - t0 < 0.6  # less than the combined runtime, make sure they run in parallel
 
 
 def test_proxy(client, servicer):
-    app = App()
+    stub = Stub()
 
-    app.function(proxy=Proxy.from_name("my-proxy"))(dummy)
-    with app.run(client=client):
+    stub.function(proxy=Proxy.from_name("my-proxy"))(dummy)
+    with stub.run(client=client):
         pass
 
 
 class CustomException(Exception):
     pass
 
 
 def failure():
     raise CustomException("foo!")
 
 
 def test_function_exception(client, servicer):
-    app = App()
+    stub = Stub()
 
-    failure_modal = app.function()(servicer.function_body(failure))
-    with app.run(client=client):
+    failure_modal = stub.function()(servicer.function_body(failure))
+    with stub.run(client=client):
         with pytest.raises(CustomException) as excinfo:
             failure_modal.remote()
         assert "foo!" in str(excinfo.value)
 
 
 @pytest.mark.asyncio
 async def test_function_exception_async(client, servicer):
-    app = App()
+    stub = Stub()
 
-    failure_modal = app.function()(servicer.function_body(failure))
-    async with app.run(client=client):
+    failure_modal = stub.function()(servicer.function_body(failure))
+    async with stub.run(client=client):
         with pytest.raises(CustomException) as excinfo:
             coro = failure_modal.remote.aio()
             assert inspect.isawaitable(
                 coro
             )  # mostly for mypy, since output could technically be an async generator which isn't awaitable in the same sense
             await coro
         assert "foo!" in str(excinfo.value)
@@ -424,19 +324,19 @@
 def custom_exception_function(x):
     if x == 4:
         raise CustomException("bad")
     return x * x
 
 
 def test_map_exceptions(client, servicer):
-    app = App()
+    stub = Stub()
 
-    custom_function_modal = app.function()(servicer.function_body(custom_exception_function))
+    custom_function_modal = stub.function()(servicer.function_body(custom_exception_function))
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert list(custom_function_modal.map(range(4))) == [0, 1, 4, 9]
 
         with pytest.raises(CustomException) as excinfo:
             list(custom_function_modal.map(range(6)))
         assert "bad" in str(excinfo.value)
 
         res = list(custom_function_modal.map(range(6), return_exceptions=True))
@@ -445,56 +345,56 @@
 
 
 def import_failure():
     raise ImportError("attempted relative import with no known parent package")
 
 
 def test_function_relative_import_hint(client, servicer):
-    app = App()
+    stub = Stub()
 
-    import_failure_modal = app.function()(servicer.function_body(import_failure))
+    import_failure_modal = stub.function()(servicer.function_body(import_failure))
 
-    with app.run(client=client):
+    with stub.run(client=client):
         with pytest.raises(ImportError) as excinfo:
             import_failure_modal.remote()
         assert "HINT" in str(excinfo.value)
 
 
 def test_nonglobal_function():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError) as excinfo:
 
-        @app.function()
+        @stub.function()
         def f():
             pass
 
     assert "global scope" in str(excinfo.value)
 
 
 def test_non_global_serialized_function():
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     def f():
         pass
 
 
 def test_closure_valued_serialized_function(client, servicer):
-    app = App()
+    stub = Stub()
 
     def make_function(s):
-        @app.function(name=f"ret_{s}", serialized=True)
+        @stub.function(name=f"ret_{s}", serialized=True)
         def returner():
             return s
 
     for s in ["foo", "bar"]:
         make_function(s)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
     functions = {}
     for func in servicer.app_functions.values():
         functions[func.function_name] = cloudpickle.loads(func.function_serialized)
 
     assert len(functions) == 2
@@ -504,104 +404,104 @@
 
 def test_new_hydrated_internal(client, servicer):
     obj = FunctionCall._new_hydrated("fc-123", client, None)
     assert obj.object_id == "fc-123"
 
 
 def test_from_id(client, servicer):
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @web_endpoint()
     def foo():
         pass
 
-    deploy_app(app, "dummy", client=client)
+    deploy_stub(stub, "dummy", client=client)
 
     function_id = foo.object_id
     assert function_id
     assert foo.web_url
 
     function_call = foo.spawn()
     assert function_call.object_id
     # Used in a few examples to construct FunctionCall objects
     rehydrated_function_call = FunctionCall.from_id(function_call.object_id, client)
     assert rehydrated_function_call.object_id == function_call.object_id
 
 
-lc_app = App()
+lc_stub = Stub()
 
 
-@lc_app.function()
+@lc_stub.function()
 def f(x):
     return x**2
 
 
 def test_allow_cross_region_volumes(client, servicer):
-    app = App()
+    stub = Stub()
     vol1 = NetworkFileSystem.from_name("xyz-1", create_if_missing=True)
     vol2 = NetworkFileSystem.from_name("xyz-2", create_if_missing=True)
     # Should pass flag for all the function's NetworkFileSystemMounts
-    app.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(dummy)
+    stub.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert len(servicer.app_functions) == 1
         for func in servicer.app_functions.values():
             assert len(func.shared_volume_mounts) == 2
             for svm in func.shared_volume_mounts:
                 assert svm.allow_cross_region
 
 
 def test_allow_cross_region_volumes_webhook(client, servicer):
     # TODO(erikbern): this test seems a bit redundant
-    app = App()
+    stub = Stub()
     vol1 = NetworkFileSystem.from_name("xyz-1", create_if_missing=True)
     vol2 = NetworkFileSystem.from_name("xyz-2", create_if_missing=True)
     # Should pass flag for all the function's NetworkFileSystemMounts
-    app.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(
+    stub.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(
         web_endpoint()(dummy)
     )
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert len(servicer.app_functions) == 1
         for func in servicer.app_functions.values():
             assert len(func.shared_volume_mounts) == 2
             for svm in func.shared_volume_mounts:
                 assert svm.allow_cross_region
 
 
 def test_serialize_deserialize_function_handle(servicer, client):
     from modal._serialization import deserialize, serialize
 
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @web_endpoint()
     def my_handle():
         pass
 
     with pytest.raises(InvalidError, match="hasn't been created"):
         serialize(my_handle)  # handle is not "live" yet! should not be serializable yet
 
-    with app.run(client=client):
+    with stub.run(client=client):
         blob = serialize(my_handle)
 
         rehydrated_function_handle = deserialize(blob, client)
         assert rehydrated_function_handle.object_id == my_handle.object_id
         assert isinstance(rehydrated_function_handle, Function)
         assert rehydrated_function_handle.web_url == "http://xyz.internal"
 
 
 def test_default_cloud_provider(client, servicer, monkeypatch):
-    app = App()
+    stub = Stub()
 
     monkeypatch.setenv("MODAL_DEFAULT_CLOUD", "oci")
-    app.function()(dummy)
-    with app.run(client=client):
-        object_id: str = app.indexed_objects["dummy"].object_id
+    stub.function()(dummy)
+    with stub.run(client=client):
+        object_id: str = stub.indexed_objects["dummy"].object_id
         f = servicer.app_functions[object_id]
 
     assert f.cloud_provider == api_pb2.CLOUD_PROVIDER_OCI
 
 
 def test_not_hydrated():
     with pytest.raises(ExecutionError):
@@ -611,86 +511,86 @@
 def test_invalid_large_serialization(client):
     big_data = b"1" * 500000
 
     def f():
         return big_data
 
     with pytest.warns(UserWarning, match="larger than the recommended limit"):
-        app = App()
-        app.function(serialized=True)(f)
-        with app.run(client=client):
+        stub = Stub()
+        stub.function(serialized=True)(f)
+        with stub.run(client=client):
             pass
 
     bigger_data = b"1" * 50000000
 
     def g():
         return bigger_data
 
     with pytest.raises(InvalidError):
-        app = App()
-        app.function(serialized=True)(g)
-        with app.run(client=client):
+        stub = Stub()
+        stub.function(serialized=True)(g)
+        with stub.run(client=client):
             pass
 
 
 def test_call_unhydrated_function():
     with pytest.raises(ExecutionError, match="hydrated"):
         foo.remote(123)
 
 
 def test_deps_explicit(client, servicer):
-    app = App()
+    stub = Stub()
 
     image = Image.debian_slim()
     nfs_1 = NetworkFileSystem.from_name("nfs-1", create_if_missing=True)
     nfs_2 = NetworkFileSystem.from_name("nfs-2", create_if_missing=True)
 
-    app.function(image=image, network_file_systems={"/nfs_1": nfs_1, "/nfs_2": nfs_2})(dummy)
+    stub.function(image=image, network_file_systems={"/nfs_1": nfs_1, "/nfs_2": nfs_2})(dummy)
 
-    with app.run(client=client):
-        object_id: str = app.indexed_objects["dummy"].object_id
+    with stub.run(client=client):
+        object_id: str = stub.indexed_objects["dummy"].object_id
         f = servicer.app_functions[object_id]
 
     dep_object_ids = set(d.object_id for d in f.object_dependencies)
     assert dep_object_ids == set([image.object_id, nfs_1.object_id, nfs_2.object_id])
 
 
 nfs = NetworkFileSystem.from_name("my-persisted-nfs", create_if_missing=True)
 
 
 def dummy_closurevars():
     nfs.listdir("/")
 
 
 def test_deps_closurevars(client, servicer):
-    app = App()
+    stub = Stub()
 
     image = Image.debian_slim()
-    modal_f = app.function(image=image)(dummy_closurevars)
+    modal_f = stub.function(image=image)(dummy_closurevars)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         f = servicer.app_functions[modal_f.object_id]
 
     assert set(d.object_id for d in f.object_dependencies) == set([nfs.object_id, image.object_id])
 
 
 def assert_is_wrapped_dict(some_arg):
     assert type(some_arg) == modal.Dict  # this should not be a modal._Dict unwrapped instance!
     return some_arg
 
 
 def test_calls_should_not_unwrap_modal_objects(servicer, client):
     some_modal_object = modal.Dict.lookup("blah", create_if_missing=True, client=client)
 
-    app = App()
-    foo = app.function()(assert_is_wrapped_dict)
+    stub = Stub()
+    foo = stub.function()(assert_is_wrapped_dict)
     servicer.function_body(assert_is_wrapped_dict)
 
     # make sure the serialized object is an actual Dict and not a _Dict in all user code contexts
-    with app.run(client=client):
+    with stub.run(client=client):
         assert type(foo.remote(some_modal_object)) == modal.Dict
         fc = foo.spawn(some_modal_object)
         assert type(fc.get()) == modal.Dict
         for ret in foo.map([some_modal_object]):
             assert type(ret) == modal.Dict
         for ret in foo.starmap([[some_modal_object]]):
             assert type(ret) == modal.Dict
@@ -703,89 +603,51 @@
     assert type(some_arg) == modal.Dict  # this should not be a modal._Dict unwrapped instance!
     yield some_arg
 
 
 def test_calls_should_not_unwrap_modal_objects_gen(servicer, client):
     some_modal_object = modal.Dict.lookup("blah", create_if_missing=True, client=client)
 
-    app = App()
-    foo = app.function()(assert_is_wrapped_dict_gen)
+    stub = Stub()
+    foo = stub.function()(assert_is_wrapped_dict_gen)
     servicer.function_body(assert_is_wrapped_dict_gen)
 
     # make sure the serialized object is an actual Dict and not a _Dict in all user code contexts
-    with app.run(client=client):
+    with stub.run(client=client):
         assert type(next(foo.remote_gen(some_modal_object))) == modal.Dict
         foo.spawn(some_modal_object)  # spawn on generator returns None, but starts the generator
 
     assert len(servicer.client_calls) == 2
 
 
 def test_mount_deps_have_ids(client, servicer, monkeypatch, test_dir):
     # This test can possibly break if a function's deps diverge between
     # local and remote environments
     monkeypatch.syspath_prepend(test_dir / "supports")
-    app = App()
-    app.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
+    stub = Stub()
+    stub.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
 
     with servicer.intercept() as ctx:
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
     function_create = ctx.pop_request("FunctionCreate")
     for dep in function_create.function.object_dependencies:
         assert dep.object_id
 
 
 def test_no_state_reuse(client, servicer, supports_dir):
     # two separate instances of the same mount content - triggers deduplication logic
     mount_instance_1 = Mount.from_local_file(supports_dir / "pyproject.toml")
     mount_instance_2 = Mount.from_local_file(supports_dir / "pyproject.toml")
 
-    app = App("reuse-mount-app")
-    app.function(mounts=[mount_instance_1, mount_instance_2])(dummy)
+    stub = Stub("reuse-mount-stub")
+    stub.function(mounts=[mount_instance_1, mount_instance_2])(dummy)
 
-    deploy_app(app, client=client, show_progress=False)
+    deploy_stub(stub, client=client, show_progress=False)
     first_deploy = {mount_instance_1.object_id, mount_instance_2.object_id}
 
-    deploy_app(app, client=client, show_progress=False)
+    deploy_stub(stub, client=client, show_progress=False)
     second_deploy = {mount_instance_1.object_id, mount_instance_2.object_id}
 
     # mount ids should not overlap between first and second deploy
     assert not (first_deploy & second_deploy)
-
-
-@pytest.mark.asyncio
-async def test_map_large_inputs(client, servicer, monkeypatch, blob_server):
-    # TODO: tests making use of mock blob server currently have to be async, since the
-    #  blob server runs as an async pytest fixture which will have its event loop blocked
-    #  by the test itself otherwise... Should move to its own thread.
-    monkeypatch.setattr("modal.functions.MAX_OBJECT_SIZE_BYTES", 1)
-    servicer.use_blob_outputs = True
-    app = App()
-    dummy_modal = app.function()(dummy)
-
-    _, blobs = blob_server
-    async with app.run.aio(client=client):
-        assert len(blobs) == 0
-        assert [a async for a in dummy_modal.map.aio(range(100))] == [i**2 for i in range(100)]
-        assert len(servicer.cleared_function_calls) == 1
-
-    assert len(blobs) == 200  # inputs + outputs
-
-
-@pytest.mark.asyncio
-async def test_non_aio_map_in_async_caller_error(client):
-    dummy_function = app.function()(dummy)
-
-    with app.run(client=client):
-        with pytest.raises(InvalidError, match=".map.aio"):
-            for _ in dummy_function.map([1, 2, 3]):
-                pass
-
-        # using .aio should be ok:
-        res = [r async for r in dummy_function.map.aio([1, 2, 3])]
-        assert res == [1, 4, 9]
-
-        # we might want to deprecate this syntax (async for ... in map without .aio),
-        # but we support it for backwards compatibility for now:
-        res = [r async for r in dummy_function.map([1, 2, 4])]
-        assert res == [1, 4, 16]
```

## test/gpu_test.py

```diff
@@ -1,109 +1,109 @@
 # Copyright Modal Labs 2022
 import pytest
 
-from modal import App
+from modal import Stub
 from modal.exception import DeprecationError, InvalidError
 from modal_proto import api_pb2
 
 
 def dummy():
     pass  # not actually used in test (servicer returns sum of square of all args)
 
 
 def test_gpu_true_function(client, servicer):
-    app = App()
+    stub = Stub()
 
     with pytest.raises(DeprecationError):
-        app.function(gpu=True)(dummy)
+        stub.function(gpu=True)(dummy)
 
 
 def test_gpu_any_function(client, servicer):
-    app = App()
+    stub = Stub()
 
-    app.function(gpu="any")(dummy)
-    with app.run(client=client):
+    stub.function(gpu="any")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_ANY
 
 
 def test_gpu_string_config(client, servicer):
-    app = App()
+    stub = Stub()
 
     # Invalid enum value.
     with pytest.raises(InvalidError):
-        app.function(gpu="foo")(dummy)
+        stub.function(gpu="foo")(dummy)
 
-    app.function(gpu="A100")(dummy)
-    with app.run(client=client):
+    stub.function(gpu="A100")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
 
 
 def test_gpu_string_count_config(client, servicer):
-    app = App()
+    stub = Stub()
 
     # Invalid count values.
     with pytest.raises(InvalidError):
-        app.function(gpu="A10G:hello")(dummy)
+        stub.function(gpu="A10G:hello")(dummy)
     with pytest.raises(InvalidError):
-        app.function(gpu="Nonexistent:2")(dummy)
+        stub.function(gpu="Nonexistent:2")(dummy)
 
-    app.function(gpu="A10G:4")(dummy)
-    with app.run(client=client):
+    stub.function(gpu="A10G:4")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 4
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A10G
 
 
 def test_gpu_config_function(client, servicer):
     import modal
 
-    app = App()
+    stub = Stub()
 
-    app.function(gpu=modal.gpu.A100())(dummy)
-    with app.run(client=client):
+    stub.function(gpu=modal.gpu.A100())(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
 
 
 def test_cloud_provider_selection(client, servicer):
     import modal
 
-    app = App()
+    stub = Stub()
 
-    app.function(gpu=modal.gpu.A100(), cloud="gcp")(dummy)
-    with app.run(client=client):
+    stub.function(gpu=modal.gpu.A100(), cloud="gcp")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.cloud_provider == api_pb2.CLOUD_PROVIDER_GCP
 
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
 
     # Invalid enum value.
     with pytest.raises(InvalidError):
-        app.function(cloud="foo")(dummy)
+        stub.function(cloud="foo")(dummy)
 
 
 @pytest.mark.parametrize(
     "memory_arg,gpu_type,memory_gb",
     [
         (0, api_pb2.GPU_TYPE_A100, 40),
         (40, api_pb2.GPU_TYPE_A100, 40),
@@ -111,49 +111,49 @@
         ("40GB", api_pb2.GPU_TYPE_A100, 40),
         ("80GB", api_pb2.GPU_TYPE_A100_80GB, 80),
     ],
 )
 def test_memory_selection_gpu_variant(client, servicer, memory_arg, gpu_type, memory_gb):
     import modal
 
-    app = App()
+    stub = Stub()
     if isinstance(memory_arg, int):
-        app.function(gpu=modal.gpu.A100(memory=memory_arg))(dummy)
+        stub.function(gpu=modal.gpu.A100(memory=memory_arg))(dummy)
     elif isinstance(memory_arg, str):
-        app.function(gpu=modal.gpu.A100(size=memory_arg))(dummy)
+        stub.function(gpu=modal.gpu.A100(size=memory_arg))(dummy)
     else:
         raise RuntimeError(f"Unexpected test parameterization arg type {type(memory_arg)}")
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
     func_def = next(iter(servicer.app_functions.values()))
 
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == gpu_type
     assert func_def.resources.gpu_config.memory == memory_gb
 
 
 def test_a100_20gb_gpu_unsupported():
     import modal
 
-    app = App()
+    stub = Stub()
 
     with pytest.raises(ValueError, match="A100 20GB is unsupported, consider"):
-        app.function(gpu=modal.gpu.A100(memory=20))(dummy)
+        stub.function(gpu=modal.gpu.A100(memory=20))(dummy)
 
 
 @pytest.mark.parametrize("count", [1, 2, 3, 4])
 def test_gpu_type_selection_from_count(client, servicer, count):
     import modal
 
-    app = App()
+    stub = Stub()
 
     # Task type does not change when user asks more than 1 GPU on an A100.
-    app.function(gpu=modal.gpu.A100(count=count))(dummy)
-    with app.run(client=client):
+    stub.function(gpu=modal.gpu.A100(count=count))(dummy)
+    with stub.run(client=client):
         pass
 
     func_def = next(iter(servicer.app_functions.values()))
 
     assert func_def.resources.gpu_config.count == count
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
```

## test/helpers.py

```diff
@@ -2,45 +2,41 @@
 import os
 import pathlib
 import subprocess
 import sys
 from typing import Optional
 
 
-def deploy_app_externally(
+def deploy_stub_externally(
     servicer,
     file_or_module: str,
-    app_variable: Optional[str] = None,
+    stub_variable: Optional[str] = None,
     deployment_name="Deployment",
     cwd=None,
     env={},
     capture_output=True,
 ) -> Optional[str]:
-    # deploys a app from another interpreter to prevent leaking state from client into a container process (apart from what goes through the servicer)
+    # deploys a stub from another interpreter to prevent leaking state from client into a container process (apart from what goes through the servicer)
     # also has the advantage that no modules imported by the test files themselves will be added to sys.modules and included in mounts etc.
     windows_support: dict[str, str] = {}
 
     if sys.platform == "win32":
         windows_support = {
             **os.environ.copy(),
             **{"PYTHONUTF8": "1"},
         }  # windows apparently needs a bunch of env vars to start python...
 
     env = {**windows_support, "MODAL_SERVER_URL": servicer.remote_addr, **env}
     if cwd is None:
         cwd = pathlib.Path(__file__).parent.parent
 
-    app_ref = file_or_module if app_variable is None else f"{file_or_module}::{app_variable}"
+    stub_ref = file_or_module if stub_variable is None else f"{file_or_module}::{stub_variable}"
 
     p = subprocess.Popen(
-        [sys.executable, "-m", "modal.cli.entry_point", "deploy", app_ref, "--name", deployment_name],
+        [sys.executable, "-m", "modal.cli.entry_point", "deploy", stub_ref, "--name", deployment_name],
         cwd=cwd,
         env=env,
         stderr=subprocess.STDOUT,
         stdout=subprocess.PIPE if capture_output else None,
     )
-    stdout_b, stderr_b = p.communicate()
-    stdout_s, stderr_s = (b.decode() if b is not None else None for b in (stdout_b, stderr_b))
-    if p.returncode != 0:
-        print(f"Deploying app failed!\n### stdout ###\n{stdout_s}\n### stderr ###\n{stderr_s}")
-        raise Exception("Test helper failed to deploy app")
-    return stdout_s
+    stdout, _ = p.communicate()
+    return stdout.decode("utf8")
```

## test/image_test.py

```diff
@@ -1,38 +1,32 @@
 # Copyright Modal Labs 2022
 import os
 import pytest
-import re
 import sys
 import threading
 from hashlib import sha256
 from tempfile import NamedTemporaryFile
-from typing import List, Literal, get_args
+from typing import List
 from unittest import mock
 
-from modal import App, Image, Mount, Secret, build, gpu, method
+from modal import Image, Mount, Secret, Stub, build, gpu, method
 from modal._serialization import serialize
-from modal.client import Client
-from modal.exception import DeprecationError, InvalidError, VersionError
-from modal.image import (
-    SUPPORTED_PYTHON_SERIES,
-    ImageBuilderVersion,
-    _dockerhub_debian_codename,
-    _dockerhub_python_version,
-    _get_modal_requirements_path,
-    _validate_python_version,
-)
-from modal.mount import PYTHON_STANDALONE_VERSIONS
+from modal.exception import DeprecationError, InvalidError
+from modal.image import _dockerhub_python_version, _get_client_requirements_path
 from modal_proto import api_pb2
 
 from .supports.skip import skip_windows
 
 
-def test_supported_python_series():
-    assert SUPPORTED_PYTHON_SERIES == PYTHON_STANDALONE_VERSIONS.keys()
+def test_python_version():
+    assert _dockerhub_python_version("3.9.1") == "3.9.1"
+    assert _dockerhub_python_version("3.9") == "3.9.15"
+    v = _dockerhub_python_version().split(".")
+    assert len(v) == 3
+    assert (int(v[0]), int(v[1])) == sys.version_info[:2]
 
 
 def get_image_layers(image_id: str, servicer) -> List[api_pb2.Image]:
     """Follow pointers to the previous image recursively in the servicer's list of images,
     and return a list of image layers from top to bottom."""
 
     result = []
@@ -48,380 +42,273 @@
             break
 
         image_id = image.base_images[0].image_id
 
     return result
 
 
-def get_all_dockerfile_commands(image_id: str, servicer) -> str:
-    layers = get_image_layers(image_id, servicer)
-    return "\n".join([cmd for layer in layers for cmd in layer.dockerfile_commands])
-
-
-@pytest.fixture(params=get_args(ImageBuilderVersion))
-def builder_version(request, server_url_env, modal_config):
-    version = request.param
-    with modal_config():
-        with mock.patch("test.conftest.ImageBuilderVersion", Literal[version]):  # type: ignore
-            yield version
-
-
-def test_python_version_validation():
-    assert _validate_python_version(None) == "{0}.{1}".format(*sys.version_info)
-    assert _validate_python_version("3.12") == "3.12"
-    assert _validate_python_version("3.12.0") == "3.12.0"
-
-    with pytest.raises(InvalidError, match="Unsupported Python version"):
-        _validate_python_version("3.7")
-
-    with pytest.raises(InvalidError, match="Python version must be specified as a string"):
-        _validate_python_version(3.10)  # type: ignore
-
-    with pytest.raises(InvalidError, match="Invalid Python version"):
-        _validate_python_version("3.10.2.9")
-
-    with pytest.raises(InvalidError, match="Invalid Python version"):
-        _validate_python_version("3.10.x")
-
-    with pytest.raises(InvalidError, match="Python version must be specified as 'major.minor'"):
-        _validate_python_version("3.10.5", allow_micro_granularity=False)
-
-
-def test_dockerhub_python_version(builder_version):
-    assert _dockerhub_python_version(builder_version, "3.9.1") == "3.9.1"
-
-    expected_39_full = {"2023.12": "3.9.15", "2024.04": "3.9.19"}[builder_version]
-    assert _dockerhub_python_version(builder_version, "3.9") == expected_39_full
-
-    v = _dockerhub_python_version(builder_version, None).split(".")
-    assert len(v) == 3
-    assert (int(v[0]), int(v[1])) == sys.version_info[:2]
-
-
-def test_image_base(builder_version, servicer, client, test_dir):
-    app = App()
-    constructors = [
-        (Image.debian_slim, ()),
-        (Image.from_registry, ("ubuntu",)),
-        (Image.from_dockerfile, (test_dir / "supports" / "test-dockerfile",)),
-        (Image.conda, ()),
-        (Image.micromamba, ()),
-    ]
-    for meth, args in constructors:
-        app.image = meth(*args)  # type: ignore
-        with app.run(client=client):
-            commands = get_all_dockerfile_commands(app.image.object_id, servicer)
-            assert "COPY /modal_requirements.txt /modal_requirements.txt" in commands
-            if builder_version == "2023.12":
-                assert "pip install -r /modal_requirements.txt" in commands
-            else:
-                assert "pip install --no-cache --no-deps -r /modal_requirements.txt" in commands
-                assert "rm /modal_requirements.txt" in commands
-
-
-@pytest.mark.parametrize("python_version", [None, "3.10", "3.11.4"])
-def test_python_version(builder_version, servicer, client, python_version):
-    local_python = "{0}.{1}".format(*sys.version_info)
-    expected_python = local_python if python_version is None else python_version
-
-    app = App()
-    app.image = Image.debian_slim() if python_version is None else Image.debian_slim(python_version)
-    expected_dockerhub_python = _dockerhub_python_version(builder_version, expected_python)
-    expected_dockerhub_debian = _dockerhub_debian_codename(builder_version)
-    assert expected_dockerhub_python.startswith(expected_python)
-    with app.run(client):
-        commands = get_all_dockerfile_commands(app.image.object_id, servicer)
-        assert re.match(rf"FROM python:{expected_dockerhub_python}-slim-{expected_dockerhub_debian}", commands)
-
-    for constructor in [Image.conda, Image.micromamba]:
-        app.image = constructor() if python_version is None else constructor(python_version)
-        if python_version is None and builder_version == "2023.12":
-            expected_python = "3.9"
-        with app.run(client):
-            commands = get_all_dockerfile_commands(app.image.object_id, servicer)
-            assert re.search(rf"install.* python={expected_python}", commands)
-
-
-def test_image_python_packages(builder_version, servicer, client):
-    app = App()
-    app.image = (
+def test_image_python_packages(client, servicer):
+    stub = Stub()
+    stub.image = (
         Image.debian_slim()
         .pip_install("sklearn[xyz]")
         .pip_install("numpy", "scipy", extra_index_url="https://xyz", find_links="https://abc?q=123", pre=True)
     )
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert any("pip install 'sklearn[xyz]'" in cmd for cmd in layers[1].dockerfile_commands)
         assert any(
             "pip install numpy scipy --find-links 'https://abc?q=123' --extra-index-url https://xyz --pre" in cmd
             for cmd in layers[0].dockerfile_commands
         )
 
 
-def test_image_kwargs_validation(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_commands(
+def test_image_kwargs_validation(servicer, client):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_commands(
         "echo hi", secrets=[Secret.from_dict({"xyz": "123"}), Secret.from_name("foo")]
     )
     with pytest.raises(InvalidError):
-        app.image = Image.debian_slim().run_commands(
+        stub.image = Image.debian_slim().run_commands(
             "echo hi",
             secrets=[
                 Secret.from_dict({"xyz": "123"}),
                 Secret.from_name("foo"),
                 Mount.from_local_dir("/", remote_path="/"),  # type: ignore
             ],  # Mount is not a valid Secret
         )
 
-    app = App()
-    app.image = Image.debian_slim().copy_local_dir("/", remote_path="/dummy")
-    app.image = Image.debian_slim().copy_mount(Mount.from_name("foo"), remote_path="/dummy")
+    stub = Stub()
+    stub.image = Image.debian_slim().copy_local_dir("/", remote_path="/dummy")
+    stub.image = Image.debian_slim().copy_mount(Mount.from_name("foo"), remote_path="/dummy")
     with pytest.raises(InvalidError):
         # Secret is not a valid Mount
-        app.image = Image.debian_slim().copy_mount(Secret.from_dict({"xyz": "123"}), remote_path="/dummy")  # type: ignore
+        stub.image = Image.debian_slim().copy_mount(Secret.from_dict({"xyz": "123"}), remote_path="/dummy")  # type: ignore
 
 
-def test_wrong_type(builder_version, servicer, client):
+def test_wrong_type(servicer, client):
     image = Image.debian_slim()
     for m in [image.pip_install, image.apt_install, image.run_commands]:
         m(["xyz"])  # type: ignore
         m("xyz")  # type: ignore
         m("xyz", ["def", "foo"], "ghi")  # type: ignore
         with pytest.raises(InvalidError):
             m(3)  # type: ignore
         with pytest.raises(InvalidError):
             m([3])  # type: ignore
         with pytest.raises(InvalidError):
             m([["double-nested-package"]])  # type: ignore
 
 
-def test_image_requirements_txt(builder_version, servicer, client):
+def test_image_requirements_txt(servicer, client):
     requirements_txt = os.path.join(os.path.dirname(__file__), "supports/test-requirements.txt")
 
-    app = App()
-    app.image = Image.debian_slim().pip_install_from_requirements(requirements_txt)
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install_from_requirements(requirements_txt)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("COPY /.requirements.txt /.requirements.txt" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("pip install -r /.requirements.txt" in cmd for cmd in layers[0].dockerfile_commands)
         assert any(b"banana" in f.data for f in layers[0].context_files)
 
 
-def test_empty_install(builder_version, servicer, client):
+def test_empty_install(servicer, client):
     # Install functions with no packages should be ignored.
-    app = App(
+    stub = Stub(
         image=Image.debian_slim()
         .pip_install()
         .pip_install([], [], [], [])
         .apt_install([])
         .run_commands()
         .conda_install()
     )
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert len(layers) == 1
 
 
-def test_debian_slim_apt_install(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().pip_install("numpy").apt_install("git", "ssh").pip_install("scikit-learn"))
+def test_debian_slim_apt_install(servicer, client):
+    stub = Stub(image=Image.debian_slim().pip_install("numpy").apt_install("git", "ssh").pip_install("scikit-learn"))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("pip install scikit-learn" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("apt-get install -y git ssh" in cmd for cmd in layers[1].dockerfile_commands)
         assert any("pip install numpy" in cmd for cmd in layers[2].dockerfile_commands)
 
 
-def test_image_pip_install_pyproject(builder_version, servicer, client):
+def test_image_pip_install_pyproject(servicer, client):
     pyproject_toml = os.path.join(os.path.dirname(__file__), "supports/test-pyproject.toml")
 
-    app = App()
-    app.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml)
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         print(layers[0].dockerfile_commands)
         assert any("pip install 'banana >=1.2.0' 'potato >=0.1.0'" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-def test_image_pip_install_pyproject_with_optionals(builder_version, servicer, client):
+def test_image_pip_install_pyproject_with_optionals(servicer, client):
     pyproject_toml = os.path.join(os.path.dirname(__file__), "supports/test-pyproject.toml")
 
-    app = App()
-    app.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml, optional_dependencies=["dev", "test"])
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml, optional_dependencies=["dev", "test"])
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         print(layers[0].dockerfile_commands)
         assert any(
             "pip install 'banana >=1.2.0' 'linting-tool >=0.0.0' 'potato >=0.1.0' 'pytest >=1.2.0'" in cmd
             for cmd in layers[0].dockerfile_commands
         )
         assert not (any("'mkdocs >=1.4.2'" in cmd for cmd in layers[0].dockerfile_commands))
 
 
-def test_image_pip_install_private_repos(builder_version, servicer, client):
-    app = App()
+def test_image_pip_install_private_repos(servicer, client):
+    stub = Stub()
     with pytest.raises(InvalidError):
-        app.image = Image.debian_slim().pip_install_private_repos(
+        stub.image = Image.debian_slim().pip_install_private_repos(
             "github.com/ecorp/private-one@1.0.0",
             git_user="erikbern",
             secrets=[],  # Invalid: missing secret
         )
 
     bad_repo_refs = [
         "ecorp/private-one@1.0.0",
         "gitspace.com/corp/private-one@1.0.0",
     ]
     for invalid_ref in bad_repo_refs:
         with pytest.raises(InvalidError):
-            app.image = Image.debian_slim().pip_install_private_repos(
+            stub.image = Image.debian_slim().pip_install_private_repos(
                 invalid_ref,
                 git_user="erikbern",
                 secrets=[Secret.from_name("test-gh-read")],
             )
 
-    app.image = Image.debian_slim().pip_install_private_repos(
+    stub.image = Image.debian_slim().pip_install_private_repos(
         "github.com/corp/private-one@1.0.0",
         "gitlab.com/corp2/private-two@0.0.2",
         git_user="erikbern",
         secrets=[
             Secret.from_dict({"GITHUB_TOKEN": "not-a-secret"}),
             Secret.from_dict({"GITLAB_TOKEN": "not-a-secret"}),
         ],
     )
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert len(layers[0].secret_ids) == 2
         assert any(
             'pip install "git+https://erikbern:$GITHUB_TOKEN@github.com/corp/private-one@1.0.0"' in cmd
             for cmd in layers[0].dockerfile_commands
         )
         assert any(
             'pip install "git+https://erikbern:$GITLAB_TOKEN@gitlab.com/corp2/private-two@0.0.2"' in cmd
             for cmd in layers[0].dockerfile_commands
         )
 
 
-def test_conda_install(builder_version, servicer, client):
-    app = App(image=Image.conda().pip_install("numpy").conda_install("pymc3", "theano").pip_install("scikit-learn"))
+def test_conda_install(servicer, client):
+    stub = Stub(image=Image.conda().pip_install("numpy").conda_install("pymc3", "theano").pip_install("scikit-learn"))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("pip install scikit-learn" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("conda install pymc3 theano --yes" in cmd for cmd in layers[1].dockerfile_commands)
         assert any("pip install numpy" in cmd for cmd in layers[2].dockerfile_commands)
 
 
-def test_dockerfile_image(builder_version, servicer, client):
+def test_dockerfile_image(servicer, client):
     path = os.path.join(os.path.dirname(__file__), "supports/test-dockerfile")
 
-    app = App(image=Image.from_dockerfile(path))
+    stub = Stub(image=Image.from_dockerfile(path))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("RUN pip install numpy" in cmd for cmd in layers[1].dockerfile_commands)
 
 
-def test_conda_update_from_environment(builder_version, servicer, client):
+def test_conda_update_from_environment(servicer, client):
     path = os.path.join(os.path.dirname(__file__), "supports/test-conda-environment.yml")
 
-    app = App(image=Image.conda().conda_update_from_environment(path))
+    stub = Stub(image=Image.conda().conda_update_from_environment(path))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("RUN conda env update" in cmd for cmd in layers[0].dockerfile_commands)
         assert any(b"foo=1.0" in f.data for f in layers[0].context_files)
         assert any(b"bar=2.1" in f.data for f in layers[0].context_files)
 
 
-def test_run_commands(builder_version, servicer, client):
-    base = Image.debian_slim()
+def test_dockerhub_install(servicer, client):
+    stub = Stub(image=Image.from_registry("gisops/valhalla:latest", setup_dockerfile_commands=["RUN apt-get update"]))
 
-    command = "echo 'Hello Modal'"
-    app = App(image=base.run_commands(command))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
-        assert layers[0].dockerfile_commands[1] == f"RUN {command}"
-
-    commands = ["echo 'Hello world'", "touch agi.yaml"]
-    for image in [base.run_commands(commands), base.run_commands(*commands)]:
-        app = App(image=image)
-        with app.run(client=client):
-            layers = get_image_layers(app.image.object_id, servicer)
-            for i, cmd in enumerate(commands, 1):
-                assert layers[0].dockerfile_commands[i] == f"RUN {cmd}"
-
-
-def test_dockerhub_install(builder_version, servicer, client):
-    app = App(image=Image.from_registry("gisops/valhalla:latest", setup_dockerfile_commands=["RUN apt-get update"]))
-
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("FROM gisops/valhalla:latest" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("RUN apt-get update" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-def test_ecr_install(builder_version, servicer, client):
+def test_ecr_install(servicer, client):
     image_tag = "000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:latest"
-    app = App(
+    stub = Stub(
         image=Image.from_aws_ecr(
             image_tag,
             setup_dockerfile_commands=["RUN apt-get update"],
             secret=Secret.from_dict({"AWS_ACCESS_KEY_ID": "", "AWS_SECRET_ACCESS_KEY": ""}),
         )
     )
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any(f"FROM {image_tag}" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("RUN apt-get update" in cmd for cmd in layers[0].dockerfile_commands)
 
 
 def run_f():
     print("foo!")
 
 
-def test_image_run_function(builder_version, servicer, client):
-    app = App()
-    app.image = (
+def test_image_run_function(client, servicer):
+    stub = Stub()
+    stub.image = (
         Image.debian_slim().pip_install("pandas").run_function(run_f, secrets=[Secret.from_dict({"xyz": "123"})])
     )
 
-    with app.run(client=client):
-        image_id = app.image.object_id
+    with stub.run(client=client):
+        image_id = stub.image.object_id
         layers = get_image_layers(image_id, servicer)
         assert "foo!" in layers[0].build_function.definition
         assert "Secret.from_dict([xyz])" in layers[0].build_function.definition
         # globals is none when no globals are referenced
         assert layers[0].build_function.globals == b""
 
     function_id = servicer.image_build_function_ids[image_id]
     assert function_id
     assert servicer.app_functions[function_id].function_name == "run_f"
     assert len(servicer.app_functions[function_id].secret_ids) == 1
 
 
-def test_image_run_function_interactivity(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().pip_install("pandas").run_function(run_f)
+def test_image_run_function_interactivity(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install("pandas").run_function(run_f)
 
-    from modal.runner import run_app
+    from modal.runner import run_stub
 
-    with run_app(app, client=client, shell=True):
-        image_id = app.image.object_id
+    with run_stub(stub, client=client, shell=True):
+        image_id = stub.image.object_id
         layers = get_image_layers(image_id, servicer)
         assert "foo!" in layers[0].build_function.definition
 
     function_id = servicer.image_build_function_ids[image_id]
     assert function_id
     assert servicer.app_functions[function_id].function_name == "run_f"
     assert not servicer.app_functions[function_id].pty_info.enabled
@@ -431,70 +318,70 @@
 VARIABLE_2 = 3
 
 
 def run_f_globals():
     print("foo!", VARIABLE_1)
 
 
-def test_image_run_function_globals(builder_version, servicer, client):
+def test_image_run_function_globals(client, servicer):
     global VARIABLE_1, VARIABLE_2
 
-    app = App()
-    app.image = Image.debian_slim().run_function(run_f_globals)
+    stub = Stub()
+    stub.image = Image.debian_slim().run_function(run_f_globals)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         old_globals = layers[0].build_function.globals
         assert b"VARIABLE_1" in old_globals
         assert b"VARIABLE_2" not in old_globals
 
     VARIABLE_1 = 3
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].build_function.globals != old_globals
 
     VARIABLE_1 = 1
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].build_function.globals == old_globals
 
 
 VARIABLE_3 = threading.Lock()
 VARIABLE_4 = "bar"
 
 
 def run_f_unserializable_globals():
     print("foo!", VARIABLE_3, VARIABLE_4)
 
 
-def test_image_run_unserializable_function(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_function(run_f_unserializable_globals)
+def test_image_run_unserializable_function(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_function(run_f_unserializable_globals)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         old_globals = layers[0].build_function.globals
         assert b"VARIABLE_4" in old_globals
 
 
 def run_f_with_args(arg, *, kwarg):
     print("building!", arg, kwarg)
 
 
-def test_image_run_function_with_args(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_function(run_f_with_args, args=("foo",), kwargs={"kwarg": "bar"})
+def test_image_run_function_with_args(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_function(run_f_with_args, args=("foo",), kwargs={"kwarg": "bar"})
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         input = layers[0].build_function.input
         assert input.args == serialize((("foo",), {"kwarg": "bar"}))
 
 
-def test_poetry(builder_version, servicer, client):
+def test_poetry(client, servicer):
     path = os.path.join(os.path.dirname(__file__), "supports/pyproject.toml")
 
     # No lockfile provided and there's no lockfile found
     # TODO we deferred the exception until _load runs, not sure how to test that here
     # with pytest.raises(NotFoundError):
     #     Image.debian_slim().poetry_install_from_file(path)
 
@@ -502,126 +389,126 @@
     Image.debian_slim().poetry_install_from_file(path, ignore_lockfile=True)
 
     # Provide lockfile explicitly - this should also work
     lockfile_path = os.path.join(os.path.dirname(__file__), "supports/special_poetry.lock")
     image = Image.debian_slim().poetry_install_from_file(path, lockfile_path)
 
     # Build iamge
-    app = App()
-    app.image = image
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = image
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         context_files = {f.filename for layer in layers for f in layer.context_files}
         assert context_files == {"/.poetry.lock", "/.pyproject.toml", "/modal_requirements.txt"}
 
 
 @pytest.fixture
 def tmp_path_with_content(tmp_path):
     (tmp_path / "data.txt").write_text("hello")
     (tmp_path / "data").mkdir()
     (tmp_path / "data" / "sub").write_text("world")
     return tmp_path
 
 
-def test_image_copy_local_dir(builder_version, servicer, client, tmp_path_with_content):
-    app = App()
-    app.image = Image.debian_slim().copy_local_dir(tmp_path_with_content, remote_path="/dummy")
+def test_image_copy_local_dir(client, servicer, tmp_path_with_content):
+    stub = Stub()
+    stub.image = Image.debian_slim().copy_local_dir(tmp_path_with_content, remote_path="/dummy")
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert "COPY . /dummy" in layers[0].dockerfile_commands
         assert set(servicer.mount_contents["mo-1"].keys()) == {"/data.txt", "/data/sub"}
 
 
-def test_image_docker_command_copy(builder_version, servicer, client, tmp_path_with_content):
-    app = App()
+def test_image_docker_command_copy(client, servicer, tmp_path_with_content):
+    stub = Stub()
     data_mount = Mount.from_local_dir(tmp_path_with_content, remote_path="/")
-    app.image = Image.debian_slim().dockerfile_commands(["COPY . /dummy"], context_mount=data_mount)
+    stub.image = Image.debian_slim().dockerfile_commands(["COPY . /dummy"], context_mount=data_mount)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert "COPY . /dummy" in layers[0].dockerfile_commands
         files = {f.mount_filename: f.content for f in Mount._get_files(data_mount.entries)}
         assert files == {"/data.txt": b"hello", "/data/sub": b"world"}
 
 
-def test_image_dockerfile_copy(builder_version, servicer, client, tmp_path_with_content):
+def test_image_dockerfile_copy(client, servicer, tmp_path_with_content):
     dockerfile = NamedTemporaryFile("w", delete=False)
     dockerfile.write("COPY . /dummy\n")
     dockerfile.close()
 
-    app = App()
+    stub = Stub()
     data_mount = Mount.from_local_dir(tmp_path_with_content, remote_path="/")
-    app.image = Image.debian_slim().from_dockerfile(dockerfile.name, context_mount=data_mount)
+    stub.image = Image.debian_slim().from_dockerfile(dockerfile.name, context_mount=data_mount)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert "COPY . /dummy" in layers[1].dockerfile_commands
         files = {f.mount_filename: f.content for f in Mount._get_files(data_mount.entries)}
         assert files == {"/data.txt": b"hello", "/data/sub": b"world"}
 
 
-def test_image_env(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().env({"HELLO": "world!"}))
+def test_image_env(client, servicer):
+    stub = Stub(image=Image.debian_slim().env({"HELLO": "world!"}))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert any("ENV HELLO=" in cmd and "world!" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-def test_image_gpu(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().run_commands("echo 0"))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+def test_image_gpu(client, servicer):
+    stub = Stub(image=Image.debian_slim().run_commands("echo 0"))
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].gpu_config.type == api_pb2.GPU_TYPE_UNSPECIFIED
 
     with pytest.warns(DeprecationError):
-        app = App(image=Image.debian_slim().run_commands("echo 1", gpu=True))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+        stub = Stub(image=Image.debian_slim().run_commands("echo 1", gpu=True))
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].gpu_config.type == api_pb2.GPU_TYPE_ANY
 
-    app = App(image=Image.debian_slim().run_commands("echo 2", gpu=gpu.A10G()))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub(image=Image.debian_slim().run_commands("echo 2", gpu=gpu.A10G()))
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].gpu_config.type == api_pb2.GPU_TYPE_A10G
 
 
-def test_image_force_build(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_commands("echo 1").pip_install("foo", force_build=True).run_commands("echo 2")
-    with app.run(client=client):
+def test_image_force_build(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_commands("echo 1").pip_install("foo", force_build=True).run_commands("echo 2")
+    with stub.run(client=client):
         assert servicer.force_built_images == ["im-3", "im-4"]
 
-    app.image = (
+    stub.image = (
         Image.from_gcp_artifact_registry("foo", force_build=True)
         .run_commands("python_packagesecho 1")
         .pip_install("foo", force_build=True)
         .run_commands("echo 2")
     )
-    with app.run(client=client):
+    with stub.run(client=client):
         assert servicer.force_built_images == ["im-3", "im-4", "im-5", "im-6", "im-7", "im-8"]
 
 
-def test_workdir(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().workdir("/foo/bar"))
+def test_workdir(servicer, client):
+    stub = Stub(image=Image.debian_slim().workdir("/foo/bar"))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("WORKDIR /foo/bar" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-cls_app = App()
+cls_stub = Stub()
 
 VARIABLE_5 = 1
 VARIABLE_6 = 1
 
 
-@cls_app.cls(
+@cls_stub.cls(
     image=Image.debian_slim().pip_install("pandas"),
     secrets=[Secret.from_dict({"xyz": "123"})],
 )
 class Foo:
     @build()
     def build_func(self):
         global VARIABLE_5
@@ -632,15 +519,15 @@
     def f(self):
         global VARIABLE_6
 
         print("bar!", VARIABLE_6)
 
 
 def test_image_build_snapshot(client, servicer):
-    with cls_app.run(client=client):
+    with cls_stub.run(client=client):
         image_id = list(servicer.images.keys())[-1]
         layers = get_image_layers(image_id, servicer)
 
         assert "foo!" in layers[0].build_function.definition
         assert "Secret.from_dict([xyz])" in layers[0].build_function.definition
         assert any("pip install pandas" in cmd for cmd in layers[1].dockerfile_commands)
 
@@ -671,17 +558,18 @@
             raise ImportError("bar")
 
         # non-ImportErrors should trigger a warning
         with pytest.warns(match="ImportError"):
             with image_2.imports():
                 raise Exception("foo")
 
-        # Old one raises
-        with pytest.raises(DeprecationError, match="imports()"):
-            image_1.run_inside()
+        # Make sure run_inside works but is depreated
+        with pytest.warns(DeprecationError, match="imports()"):
+            with image_1.run_inside():
+                pass
 
         # Hydration of the image should raise the exception
         with pytest.raises(ImportError, match="foo"):
             image_1._hydrate("im-123", client, None)
 
         # Should not raise since it's a different image
         image_2._hydrate("im-456", client, None)
@@ -702,66 +590,33 @@
                 raise ImportError("baz")
 
         # We're not inside this image so this should be swallowed
         with image_2.imports():
             raise ImportError("bar")
 
 
-@pytest.mark.parametrize("python_version", ["3.11", "3.12", "3.12.1", "3.12.1-gnu"])
-def test_get_modal_requirements_path(builder_version, python_version):
-    path = _get_modal_requirements_path(builder_version, python_version)
-    if builder_version == "2023.12" and python_version.startswith("3.12"):
-        assert path.endswith("2023.12.312.txt")
-    else:
-        assert path.endswith(f"{builder_version}.txt")
-
-
-def test_image_builder_version(servicer, test_dir, modal_config):
-    app = App(image=Image.debian_slim())
-    # TODO use a single with statement and tuple of managers when we drop Py3.8
-    test_requirements = str(test_dir / "supports" / "test-requirements.txt")
-    with mock.patch("modal.image._get_modal_requirements_path", lambda *_, **__: test_requirements):
-        with mock.patch("modal.image._dockerhub_python_version", lambda *_, **__: "3.11.0"):
-            with mock.patch("modal.image._dockerhub_debian_codename", lambda *_, **__: "bullseye"):
-                with mock.patch("test.conftest.ImageBuilderVersion", Literal["2000.01"]):
-                    with mock.patch("modal.image.ImageBuilderVersion", Literal["2000.01"]):
-                        with Client(
-                            servicer.remote_addr, api_pb2.CLIENT_TYPE_CONTAINER, ("ak-123", "as-xyz")
-                        ) as client:
-                            with modal_config():
-                                with app.run(client=client):
-                                    assert servicer.image_builder_versions
-                                    for version in servicer.image_builder_versions.values():
-                                        assert version == "2000.01"
-
-
-def test_image_builder_supported_versions(servicer):
-    app = App(image=Image.debian_slim())
-    # TODO use a single with statement and tuple of managers when we drop Py3.8
-    with pytest.raises(VersionError, match=r"This version of the modal client supports.+{'2000.01'}"):
-        with mock.patch("modal.image.ImageBuilderVersion", Literal["2000.01"]):
-            with mock.patch("test.conftest.ImageBuilderVersion", Literal["2023.11"]):
-                with Client(servicer.remote_addr, api_pb2.CLIENT_TYPE_CONTAINER, ("ak-123", "as-xyz")) as client:
-                    with app.run(client=client):
-                        pass
-
-
-@pytest.fixture
-def force_2023_12(modal_config):
-    with mock.patch("test.conftest.ImageBuilderVersion", Literal["2023.12"]):
-        with modal_config():
-            yield
+@pytest.mark.parametrize(
+    "version,expected",
+    [
+        ("3.12", "requirements.312.txt"),
+        ("3.12.1", "requirements.312.txt"),
+        ("3.12.1-gnu", "requirements.312.txt"),
+    ],
+)
+def test_get_client_requirements_path(version, expected):
+    path = _get_client_requirements_path(version)
+    assert os.path.basename(path) == expected
 
 
 @skip_windows("Different hash values for context file paths")
-def test_image_stability_on_2023_12(force_2023_12, servicer, client, test_dir):
+def test_image_stability_on_2023_12(servicer, client, test_dir):
     def get_hash(img: Image) -> str:
-        app = App(image=img)
-        with app.run(client=client):
-            layers = get_image_layers(app.image.object_id, servicer)
+        stub = Stub(image=img)
+        with stub.run(client=client):
+            layers = get_image_layers(stub.image.object_id, servicer)
             commands = [layer.dockerfile_commands for layer in layers]
             context_files = [[(f.filename, f.data) for f in layer.context_files] for layer in layers]
         return sha256(repr(list(zip(commands, context_files))).encode()).hexdigest()
 
     if sys.version_info[:2] == (3, 11):
         # Matches my development environment — default is to match Python version from local system
         img = Image.debian_slim()
```

## test/live_reload_test.py

```diff
@@ -2,78 +2,78 @@
 import asyncio
 import pytest
 import threading
 import time
 from unittest import mock
 
 from modal import Function
-from modal.serving import serve_app
+from modal.serving import serve_stub
 
-from .supports.app_run_tests.webhook import app
+from .supports.app_run_tests.webhook import stub
 from .supports.skip import skip_windows
 
 
 @pytest.fixture
-def app_ref(test_dir):
+def stub_ref(test_dir):
     return str(test_dir / "supports" / "app_run_tests" / "webhook.py")
 
 
 @pytest.mark.asyncio
-async def test_live_reload(app_ref, server_url_env, servicer):
-    async with serve_app.aio(app, app_ref):
+async def test_live_reload(stub_ref, server_url_env, servicer):
+    async with serve_stub.aio(stub, stub_ref):
         await asyncio.sleep(3.0)
     assert servicer.app_set_objects_count == 1
     assert servicer.app_client_disconnect_count == 1
     assert servicer.app_get_logs_initial_count == 1
 
 
 @skip_windows("live-reload not supported on windows")
-def test_file_changes_trigger_reloads(app_ref, server_url_env, servicer):
+def test_file_changes_trigger_reloads(stub_ref, server_url_env, servicer):
     watcher_done = threading.Event()
 
     async def fake_watch():
         for i in range(3):
             yield {"/some/file"}
         watcher_done.set()
 
-    with serve_app(app, app_ref, _watcher=fake_watch()):
+    with serve_stub(stub, stub_ref, _watcher=fake_watch()):
         watcher_done.wait()  # wait until watcher loop is done
 
     # TODO ideally we would assert the specific expected number here, but this test
     # is consistently flaking in CI and I cannot reproduce locally to debug.
     # I'm relaxing the assertion for now to stop the test from blocking deployments.
     # assert servicer.app_set_objects_count == 4  # 1 + number of file changes
     assert servicer.app_set_objects_count > 1
     assert servicer.app_client_disconnect_count == 1
     assert servicer.app_get_logs_initial_count == 1
-    foo = app.indexed_objects["foo"]
+    foo = stub.indexed_objects["foo"]
     assert isinstance(foo, Function)
     assert foo.web_url.startswith("http://")
 
 
 @pytest.mark.asyncio
-async def test_no_change(app_ref, server_url_env, servicer):
+async def test_no_change(stub_ref, server_url_env, servicer):
     async def fake_watch():
         # Iterator that returns immediately, yielding nothing
         if False:
             yield
 
-    async with serve_app.aio(app, app_ref, _watcher=fake_watch()):
+    async with serve_stub.aio(stub, stub_ref, _watcher=fake_watch()):
         pass
 
     assert servicer.app_set_objects_count == 1  # Should create the initial app once
     assert servicer.app_client_disconnect_count == 1
     assert servicer.app_get_logs_initial_count == 1
 
 
 @pytest.mark.asyncio
-async def test_heartbeats(app_ref, server_url_env, servicer):
+async def test_heartbeats(stub_ref, server_url_env, servicer):
     with mock.patch("modal.runner.HEARTBEAT_INTERVAL", 1):
         t0 = time.time()
-        async with serve_app.aio(app, app_ref):
+        async with serve_stub.aio(stub, stub_ref):
             await asyncio.sleep(3.1)
         total_secs = int(time.time() - t0)
 
     apps = list(servicer.app_heartbeats.keys())
     assert len(apps) == 1
     # Typically [0s, 1s, 2s, 3s], but asyncio.sleep may lag.
     actual_heartbeats = servicer.app_heartbeats[apps[0]]
```

## test/lookup_test.py

```diff
@@ -1,13 +1,13 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App, Function, Volume, web_endpoint
+from modal import Function, Stub, Volume, web_endpoint
 from modal.exception import ExecutionError, NotFoundError
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 
 
 def test_persistent_object(servicer, client):
     volume_id = Volume.create_deployed("my-volume", client=client)
 
     v: Volume = Volume.lookup("my-volume", client=client)
     assert isinstance(v, Volume)
@@ -19,18 +19,18 @@
 
 def square(x):
     # This function isn't deployed anyway
     pass
 
 
 def test_lookup_function(servicer, client):
-    app = App()
+    stub = Stub()
 
-    app.function()(square)
-    deploy_app(app, "my-function", client=client)
+    stub.function()(square)
+    deploy_stub(stub, "my-function", client=client)
 
     f = Function.lookup("my-function", "square", client=client)
     assert f.object_id == "fu-1"
 
     # Call it using two arguments
     f = Function.lookup("my-function", "square", client=client)
     assert f.object_id == "fu-1"
@@ -43,17 +43,17 @@
 
     # Make sure the new-style local calls raise an error
     with pytest.raises(ExecutionError):
         assert f.local(2, 4) == 20
 
 
 def test_webhook_lookup(servicer, client):
-    app = App()
-    app.function()(web_endpoint(method="POST")(square))
-    deploy_app(app, "my-webhook", client=client)
+    stub = Stub()
+    stub.function()(web_endpoint(method="POST")(square))
+    deploy_stub(stub, "my-webhook", client=client)
 
     f = Function.lookup("my-webhook", "square", client=client)
     assert f.web_url
 
 
 def test_deploy_exists(servicer, client):
     with pytest.raises(NotFoundError):
```

## test/mount_test.py

```diff
@@ -2,15 +2,15 @@
 import hashlib
 import os
 import platform
 import pytest
 import sys
 from pathlib import Path
 
-from modal import App
+from modal import Stub
 from modal._utils.blob_utils import LARGE_FILE_LIMIT
 from modal.exception import ModuleNotMountable
 from modal.mount import Mount
 
 
 @pytest.mark.asyncio
 async def test_get_files(servicer, client, tmpdir):
@@ -85,62 +85,62 @@
 
 
 def dummy():
     pass
 
 
 def test_from_local_python_packages(servicer, client, test_dir):
-    app = App()
+    stub = Stub()
 
     sys.path.append((test_dir / "supports").as_posix())
 
-    app.function(mounts=[Mount.from_local_python_packages("pkg_a", "pkg_b", "standalone_file")])(dummy)
+    stub.function(mounts=[Mount.from_local_python_packages("pkg_a", "pkg_b", "standalone_file")])(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         files = set(servicer.files_name2sha.keys())
         expected_files = {
             "/root/pkg_a/a.py",
             "/root/pkg_a/b/c.py",
             "/root/pkg_b/f.py",
             "/root/pkg_b/g/h.py",
             "/root/standalone_file.py",
         }
         assert expected_files.issubset(files)
 
         assert "/root/pkg_c/i.py" not in files
         assert "/root/pkg_c/j/k.py" not in files
 
 
-def test_app_mounts(servicer, client, test_dir):
+def test_stub_mounts(servicer, client, test_dir):
     sys.path.append((test_dir / "supports").as_posix())
 
-    app = App(mounts=[Mount.from_local_python_packages("pkg_b")])
+    stub = Stub(mounts=[Mount.from_local_python_packages("pkg_b")])
 
-    app.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
+    stub.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         files = set(servicer.files_name2sha.keys())
         expected_files = {
             "/root/pkg_a/a.py",
             "/root/pkg_a/b/c.py",
             "/root/pkg_b/f.py",
             "/root/pkg_b/g/h.py",
         }
         assert expected_files.issubset(files)
 
         assert "/root/pkg_c/i.py" not in files
         assert "/root/pkg_c/j/k.py" not in files
 
 
 def test_from_local_python_packages_missing_module(servicer, client, test_dir, server_url_env):
-    app = App()
-    app.function(mounts=[Mount.from_local_python_packages("nonexistent_package")])(dummy)
+    stub = Stub()
+    stub.function(mounts=[Mount.from_local_python_packages("nonexistent_package")])(dummy)
 
     with pytest.raises(ModuleNotMountable):
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
 
 def test_chained_entries(test_dir):
     a_txt = str(test_dir / "a.txt")
     b_txt = str(test_dir / "b.txt")
     with open(a_txt, "w") as f:
```

## test/mounted_files_test.py

```diff
@@ -52,15 +52,15 @@
         async for file_info in mount._get_files(mount.entries):
             filenames.append(file_info.mount_filename)
 
     return filenames
 
 
 def test_mounted_files_script(servicer, supports_dir, env_mount_files, server_url_env):
-    helpers.deploy_app_externally(servicer, script_path, cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, script_path, cwd=supports_dir)
     files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
 
     # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
     assert files == {
         "/root/a.py",
         "/root/b/c.py",
         "/root/b/e.py",
@@ -71,15 +71,15 @@
     }
 
 
 serialized_fn_path = "pkg_a/serialized_fn.py"
 
 
 def test_mounted_files_serialized(servicer, supports_dir, env_mount_files, server_url_env):
-    helpers.deploy_app_externally(servicer, serialized_fn_path, cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, serialized_fn_path, cwd=supports_dir)
     files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
 
     # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
     assert (
         files
         == {
             "/root/serialized_fn.py",  # should serialized_fn be included? It's not needed to run the function, but it's loaded into sys.modules at definition time...
@@ -200,24 +200,24 @@
     files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
     assert files == {
         "/root/script.py",
     }
 
 
 def test_e2e_modal_run_py_file_mounts(servicer, supports_dir):
-    helpers.deploy_app_externally(servicer, "hello.py", cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, "hello.py", cwd=supports_dir)
     # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
     # assert len(servicer.files_name2sha) == 1
     # assert servicer.n_mounts == 1  # there should be a single mount
     # assert servicer.n_mount_files == 1
     assert "/root/hello.py" in servicer.files_name2sha
 
 
 def test_e2e_modal_run_py_module_mounts(servicer, supports_dir):
-    helpers.deploy_app_externally(servicer, "hello", cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, "hello", cwd=supports_dir)
     # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
     # assert len(servicer.files_name2sha) == 1
     # assert servicer.n_mounts == 1  # there should be a single mount
     # assert servicer.n_mount_files == 1
     assert "/root/hello.py" in servicer.files_name2sha
 
 
@@ -231,20 +231,20 @@
 
     def mock_get_files_to_upload(self):
         r = list(original(self))
         return_values.append(r)
         return r
 
     monkeypatch.setattr("modal.mount._MountDir.get_files_to_upload", mock_get_files_to_upload)
-    app = modal.App()
+    stub = modal.Stub()
     mount_with_many_files = Mount.from_local_dir(test_dir, remote_path="/test")
-    app.function(mounts=[mount_with_many_files])(foo)
+    stub.function(mounts=[mount_with_many_files])(foo)
     assert len(return_values) == 0  # ensure we don't look at the files yet
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
     assert return_values  # at this point we should have gotten all the mount files
     # flatten inspected files
     files = set()
     for r in return_values:
         for fn, _ in r:
@@ -254,15 +254,15 @@
 
 
 def test_mount_dedupe(servicer, test_dir, server_url_env):
     supports_dir = test_dir / "supports"
     normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
     normally_not_included_file.touch(exist_ok=True)
     print(
-        helpers.deploy_app_externally(
+        helpers.deploy_stub_externally(
             # no explicit mounts, rely on auto-mounting
             servicer,
             "mount_dedupe.py",
             cwd=test_dir / "supports",
             env={"USE_EXPLICIT": "0"},
         )
     )
@@ -275,15 +275,15 @@
 
 
 def test_mount_dedupe_explicit(servicer, test_dir, server_url_env):
     supports_dir = test_dir / "supports"
     normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
     normally_not_included_file.touch(exist_ok=True)
     print(
-        helpers.deploy_app_externally(
+        helpers.deploy_stub_externally(
             # two explicit mounts of the same package
             servicer,
             "mount_dedupe.py",
             cwd=supports_dir,
             env={"USE_EXPLICIT": "1"},
         )
     )
```

## test/network_file_system_test.py

```diff
@@ -1,47 +1,46 @@
 # Copyright Modal Labs 2022
 import pytest
 import time
-from io import BytesIO
 from unittest import mock
 
 import modal
 from modal.exception import DeprecationError, InvalidError, NotFoundError
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 
 
 def dummy():
     pass
 
 
 def test_network_file_system_files(client, test_dir, servicer):
-    app = modal.App()
+    stub = modal.Stub()
     nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)
 
-    dummy_modal = app.function(network_file_systems={"/root/foo": nfs})(dummy)
+    dummy_modal = stub.function(network_file_systems={"/root/foo": nfs})(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         dummy_modal.remote()
 
 
 def test_network_file_system_bad_paths():
-    app = modal.App()
+    stub = modal.Stub()
     nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)
 
     def _f():
         pass
 
     with pytest.raises(InvalidError):
-        app.function(network_file_systems={"/root/../../foo": nfs})(dummy)
+        stub.function(network_file_systems={"/root/../../foo": nfs})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(network_file_systems={"/": nfs})(dummy)
+        stub.function(network_file_systems={"/": nfs})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(network_file_systems={"/tmp/": nfs})(dummy)
+        stub.function(network_file_systems={"/tmp/": nfs})(dummy)
 
 
 def test_network_file_system_handle_single_file(client, tmp_path, servicer):
     local_file_path = tmp_path / "some_file"
     local_file_path.write_text("hello world")
 
     with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
@@ -94,46 +93,46 @@
         assert servicer.nfs_files[object_id]["/bigfile"].data_blob_id == "bl-1"
 
         _, blobs = blob_server
         assert blobs["bl-1"] == b"hello world, this is a lot of text"
 
 
 def test_old_syntax(client, servicer):
-    app = modal.App()
+    stub = modal.Stub()
     with pytest.raises(DeprecationError):
-        app.vol1 = modal.SharedVolume()  # type: ignore  # This is just a post-deprecation husk
+        stub.vol1 = modal.SharedVolume()  # type: ignore  # This is just a post-deprecation husk
     with pytest.raises(DeprecationError):
-        app.vol2 = modal.SharedVolume.new()
+        stub.vol2 = modal.SharedVolume.new()
 
 
 def test_redeploy(servicer, client):
-    app = modal.App()
+    stub = modal.Stub()
     with pytest.warns(DeprecationError):
         n1 = modal.NetworkFileSystem.new()
         n2 = modal.NetworkFileSystem.new()
         n3 = modal.NetworkFileSystem.new()
-        app.n1, app.n2, app.n3 = n1, n2, n3
+        stub.n1, stub.n2, stub.n3 = n1, n2, n3
 
     # Deploy app once
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app1_ids = [n1.object_id, n2.object_id, n3.object_id]
 
     # Deploy app again
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app2_ids = [n1.object_id, n2.object_id, n3.object_id]
 
     # Make sure ids are stable
     assert app1_ids == app2_ids
 
     # Make sure ids are unique
     assert len(set(app1_ids)) == 3
     assert len(set(app2_ids)) == 3
 
     # Deploy to a different app
-    deploy_app(app, "my-other-app", client=client)
+    deploy_stub(stub, "my-other-app", client=client)
     app3_ids = [n1.object_id, n2.object_id, n3.object_id]
 
     # Should be unique and different
     assert len(set(app3_ids)) == 3
     assert set(app1_ids) & set(app3_ids) == set()
 
 
@@ -176,13 +175,7 @@
         assert nfs.listdir("/") == []
         nfs.write_file("xyz.txt", open(local_file_path, "rb"))
         (entry,) = nfs.listdir("/")
         assert entry.path == "xyz.txt"
 
         time.sleep(1.5)  # Make time for 2 heartbeats
     assert servicer.n_nfs_heartbeats == 2
-
-
-def test_nfs_lazy_hydration_from_name(set_env_client):
-    nfs = modal.NetworkFileSystem.from_name("nfs", create_if_missing=True)
-    bio = BytesIO(b"content")
-    nfs.write_file("blah", bio)
```

## test/object_test.py

```diff
@@ -1,22 +1,22 @@
 # Copyright Modal Labs 2022
 import pytest
 
-from modal import App, Queue, Secret
+from modal import Queue, Secret, Stub
 from modal.exception import DeprecationError, InvalidError
 
 
 @pytest.mark.asyncio
 async def test_async_factory(client):
-    app = App()
+    stub = Stub()
     with pytest.warns(DeprecationError):
-        app.my_factory = Queue.new()
-        async with app.run(client=client):
-            assert isinstance(app.my_factory, Queue)
-            assert app.my_factory.object_id == "qu-1"
+        stub.my_factory = Queue.new()
+        async with stub.run(client=client):
+            assert isinstance(stub.my_factory, Queue)
+            assert stub.my_factory.object_id == "qu-1"
 
 
 def test_new_hydrated(client):
     from modal.dict import _Dict
     from modal.object import _Object
     from modal.queue import _Queue
```

## test/queue_test.py

```diff
@@ -15,21 +15,14 @@
     q.put(42)
     assert q.len() == 1
     assert q.get() == 42
     with pytest.raises(queue.Empty):
         q.get(timeout=0)
     assert q.len() == 0
 
-    # test iter
-    q.put_many([1, 2, 3])
-    t0 = time.time()
-    assert [v for v in q.iterate(item_poll_timeout=1.0)] == [1, 2, 3]
-    assert 1.0 < time.time() - t0 < 2.0
-    assert [v for v in q.iterate(item_poll_timeout=0.0)] == [1, 2, 3]
-
 
 def test_queue_ephemeral(servicer, client):
     with Queue.ephemeral(client=client, _heartbeat_sleep=1) as q:
         q.put("hello")
         assert q.len() == 1
         assert q.get() == "hello"
         time.sleep(1.5)  # enough to trigger two heartbeats
@@ -98,13 +91,7 @@
     assert str(servicer.queue_max_len) in str(excinfo.value)
     assert i == servicer.queue_max_len
 
 
 def test_queue_deploy(servicer, client):
     d = Queue.lookup("xyz", create_if_missing=True, client=client)
     d.put(123)
-
-
-def test_queue_lazy_hydrate_from_name(set_env_client):
-    q = Queue.from_name("foo", create_if_missing=True)
-    q.put(123)
-    assert q.get() == 123
```

## test/resolver_test.py

```diff
@@ -5,15 +5,14 @@
 from typing import Optional
 
 from modal._output import OutputManager
 from modal._resolver import Resolver
 from modal.object import _Object
 
 
-@pytest.mark.flaky(max_runs=2)
 @pytest.mark.asyncio
 async def test_multi_resolve_sequential_loads_once():
     output_manager = OutputManager(None, show_progress=False)
     resolver = Resolver(None, output_mgr=output_manager, environment_name="", app_id=None)
 
     load_count = 0
```

## test/retries_test.py

```diff
@@ -26,42 +26,42 @@
 
 
 def zero_retries():
     pass
 
 
 def test_retries(client):
-    app = modal.App()
+    stub = modal.Stub()
 
-    default_retries_from_int_modal = app.function(retries=5)(default_retries_from_int)
-    fixed_delay_retries_modal = app.function(retries=modal.Retries(max_retries=5, backoff_coefficient=1.0))(
+    default_retries_from_int_modal = stub.function(retries=5)(default_retries_from_int)
+    fixed_delay_retries_modal = stub.function(retries=modal.Retries(max_retries=5, backoff_coefficient=1.0))(
         fixed_delay_retries
     )
 
-    exponential_backoff_modal = app.function(
+    exponential_backoff_modal = stub.function(
         retries=modal.Retries(max_retries=2, initial_delay=2.0, backoff_coefficient=2.0)
     )(exponential_backoff)
 
-    exponential_with_max_delay_modal = app.function(
+    exponential_with_max_delay_modal = stub.function(
         retries=modal.Retries(max_retries=2, backoff_coefficient=2.0, max_delay=30.0)
     )(exponential_with_max_delay)
 
-    zero_retries_modal = app.function(retries=0)(zero_retries)
+    zero_retries_modal = stub.function(retries=0)(zero_retries)
 
     with pytest.raises(TypeError):
         # Reject no-args constructions, which is unreadable and harder to support long-term
-        app.function(retries=modal.Retries())(dummy)  # type: ignore
+        stub.function(retries=modal.Retries())(dummy)  # type: ignore
 
     # Reject weird inputs:
     # Don't need server to detect and reject nonsensical input. Can do client-side.
     with pytest.raises(InvalidError):
-        app.function(retries=modal.Retries(max_retries=-2))(dummy)
+        stub.function(retries=modal.Retries(max_retries=-2))(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(retries=modal.Retries(max_retries=2, backoff_coefficient=0.0))(dummy)
+        stub.function(retries=modal.Retries(max_retries=2, backoff_coefficient=0.0))(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         default_retries_from_int_modal.remote()
         fixed_delay_retries_modal.remote()
         exponential_backoff_modal.remote()
         exponential_with_max_delay_modal.remote()
         zero_retries_modal.remote()
```

## test/runner_test.py

```diff
@@ -1,79 +1,71 @@
 # Copyright Modal Labs 2023
 import pytest
 import typing
 
 import modal
-from modal.client import Client
-from modal.exception import ExecutionError
-from modal.runner import run_app
+from modal.runner import run_stub
 from modal_proto import api_pb2
 
 T = typing.TypeVar("T")
 
 
-def test_run_app(servicer, client):
-    dummy_app = modal.App()
+def test_run_stub(servicer, client):
+    dummy_stub = modal.Stub()
     with servicer.intercept() as ctx:
-        with run_app(dummy_app, client=client):
+        with run_stub(dummy_stub, client=client):
             pass
 
     ctx.pop_request("AppCreate")
     ctx.pop_request("AppSetObjects")
     ctx.pop_request("AppClientDisconnect")
 
 
-def test_run_app_unauthenticated(servicer):
-    dummy_app = modal.App()
-    with Client.anonymous(servicer.remote_addr) as client:
-        with pytest.raises(ExecutionError, match=".+unauthenticated client"):
-            with run_app(dummy_app, client=client):
-                pass
-
-
 def dummy():
     ...
 
 
-def test_run_app_profile_env_with_refs(servicer, client, monkeypatch):
+def test_run_stub_profile_env_with_refs(servicer, client, monkeypatch):
     monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
     with servicer.intercept() as ctx:
-        dummy_app = modal.App()
+        dummy_stub = modal.Stub()
         ref = modal.Secret.from_name("some_secret")
-        dummy_app.function(secrets=[ref])(dummy)
+        dummy_stub.function(secrets=[ref])(dummy)
 
     assert ctx.calls == []  # all calls should be deferred
 
     with servicer.intercept() as ctx:
         ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
-        with run_app(dummy_app, client=client):
+        with run_stub(dummy_stub, client=client):
             pass
 
     with pytest.raises(Exception):
         ctx.pop_request("SecretCreate")  # should not create a new secret...
 
     app_create = ctx.pop_request("AppCreate")
     assert app_create.environment_name == "profile_env"
 
     secret_get_or_create = ctx.pop_request("SecretGetOrCreate")
     assert secret_get_or_create.environment_name == "profile_env"
 
 
-def test_run_app_custom_env_with_refs(servicer, client, monkeypatch):
+def test_run_stub_custom_env_with_refs(servicer, client, monkeypatch):
     monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
-    dummy_app = modal.App()
+    dummy_stub = modal.Stub()
     own_env_secret = modal.Secret.from_name("own_env_secret")
-    other_env_secret = modal.Secret.from_name("other_env_secret", environment_name="third")  # explicit lookup
+    other_env_secret = modal.Secret.from_name(
+        "other_env_secret", environment_name="third"
+    )  # explicit lookup
 
-    dummy_app.function(secrets=[own_env_secret, other_env_secret])(dummy)
+    dummy_stub.function(secrets=[own_env_secret, other_env_secret])(dummy)
 
     with servicer.intercept() as ctx:
         ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
         ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-456"))
-        with run_app(dummy_app, client=client, environment_name="custom"):
+        with run_stub(dummy_stub, client=client, environment_name="custom"):
             pass
 
     with pytest.raises(Exception):
         ctx.pop_request("SecretCreate")
 
     app_create = ctx.pop_request("AppCreate")
     assert app_create.environment_name == "custom"
```

## test/sandbox_test.py

```diff
@@ -2,27 +2,27 @@
 
 import hashlib
 import platform
 import pytest
 import time
 from pathlib import Path
 
-from modal import App, Image, Mount, NetworkFileSystem, Sandbox, Secret
+from modal import Image, Mount, NetworkFileSystem, Sandbox, Secret, Stub
 from modal.exception import InvalidError
 
-app = App()
+stub = Stub()
 
 
 skip_non_linux = pytest.mark.skipif(platform.system() != "Linux", reason="sandbox mock uses subprocess")
 
 
 @skip_non_linux
 def test_spawn_sandbox(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo bye >&2 && sleep 1 && echo hi && exit 42", timeout=600)
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo bye >&2 && sleep 1 && echo hi && exit 42", timeout=600)
 
         assert sb.poll() is None
 
         t0 = time.time()
         sb.wait()
         # Test that we actually waited for the sandbox to finish.
         assert time.time() - t0 > 0.3
@@ -37,86 +37,86 @@
         assert sb.poll() == 42
 
 
 @skip_non_linux
 def test_sandbox_mount(client, servicer, tmpdir):
     tmpdir.join("a.py").write(b"foo")
 
-    with app.run(client=client):
-        sb = app.spawn_sandbox(
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox(
             "echo",
             "hi",
             mounts=[Mount.from_local_dir(Path(tmpdir), remote_path="/m")],
         )
         sb.wait()
 
     sha = hashlib.sha256(b"foo").hexdigest()
     assert servicer.files_sha2data[sha]["data"] == b"foo"
 
 
 @skip_non_linux
 def test_sandbox_image(client, servicer, tmpdir):
     tmpdir.join("a.py").write(b"foo")
 
-    with app.run(client=client):
-        sb = app.spawn_sandbox("echo", "hi", image=Image.debian_slim().pip_install("foo", "bar", "potato"))
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("echo", "hi", image=Image.debian_slim().pip_install("foo", "bar", "potato"))
         sb.wait()
 
     idx = max(servicer.images.keys())
     last_image = servicer.images[idx]
 
     assert all(c in last_image.dockerfile_commands[-1] for c in ["foo", "bar", "potato"])
 
 
 @skip_non_linux
 def test_sandbox_secret(client, servicer, tmpdir):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("echo", "$FOO", secrets=[Secret.from_dict({"FOO": "BAR"})])
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("echo", "$FOO", secrets=[Secret.from_dict({"FOO": "BAR"})])
         sb.wait()
 
     assert len(servicer.sandbox_defs[0].secret_ids) == 1
 
 
 @skip_non_linux
 def test_sandbox_nfs(client, servicer, tmpdir):
-    with app.run(client=client):
+    with stub.run(client=client):
         with NetworkFileSystem.ephemeral(client=client) as nfs:
             with pytest.raises(InvalidError):
-                app.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/": nfs})
+                stub.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/": nfs})
 
-            app.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/cache": nfs})
+            stub.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/cache": nfs})
 
     assert len(servicer.sandbox_defs[0].nfs_mounts) == 1
 
 
 @skip_non_linux
 def test_sandbox_from_id(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo foo && exit 42", timeout=600)
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo foo && exit 42", timeout=600)
         sb.wait()
 
     sb2 = Sandbox.from_id(sb.object_id, client=client)
     assert sb2.stdout.read() == "foo\n"
     assert sb2.returncode == 42
 
 
 @skip_non_linux
 def test_sandbox_terminate(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "sleep 10000")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "sleep 10000")
         sb.terminate()
 
         assert sb.returncode != 0
 
 
 @skip_non_linux
 @pytest.mark.asyncio
 async def test_sandbox_stdin_async(client, servicer):
-    async with app.run.aio(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
+    async with stub.run.aio(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
 
         sb.stdin.write(b"foo\n")
         sb.stdin.write(b"bar\n")
 
         sb.stdin.write_eof()
 
         await sb.stdin.drain.aio()
@@ -125,16 +125,16 @@
 
         assert sb.stdout.read() == "foo\nbar\n"
         assert sb.returncode == 13
 
 
 @skip_non_linux
 def test_sandbox_stdin(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
 
         sb.stdin.write(b"foo\n")
         sb.stdin.write(b"bar\n")
 
         sb.stdin.write_eof()
 
         sb.stdin.drain()
@@ -143,34 +143,34 @@
 
         assert sb.stdout.read() == "foo\nbar\n"
         assert sb.returncode == 13
 
 
 @skip_non_linux
 def test_sandbox_stdin_invalid_write(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo foo")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo foo")
         with pytest.raises(TypeError):
             sb.stdin.write("foo\n")  # type: ignore
 
 
 @skip_non_linux
 def test_sandbox_stdin_write_after_eof(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo foo")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo foo")
         sb.stdin.write_eof()
         with pytest.raises(EOFError):
             sb.stdin.write(b"foo")
 
 
 @skip_non_linux
 @pytest.mark.asyncio
 async def test_sandbox_async_for(client, servicer):
-    async with app.run.aio(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo hello && echo world && echo bye >&2")
+    async with stub.run.aio(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo hello && echo world && echo bye >&2")
 
         out = ""
 
         async for message in sb.stdout:
             out += message
         assert out == "hello\nworld\n"
```

## test/schedule_test.py

```diff
@@ -1,15 +1,15 @@
 # Copyright Modal Labs 2022
-from modal import App, Period
+from modal import Period, Stub
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-@app.function(schedule=Period(seconds=5))
+@stub.function(schedule=Period(seconds=5))
 def f():
     pass
 
 
 def test_schedule(servicer, client):
-    with app.run(client=client):
+    with stub.run(client=client):
         assert servicer.function2schedule == {"fu-1": api_pb2.Schedule(period=api_pb2.Schedule.Period(seconds=5.0))}
```

## test/scheduler_placement_test.py

```diff
@@ -1,28 +1,28 @@
 # Copyright Modal Labs 2024
-from modal import App, SchedulerPlacement
+from modal import SchedulerPlacement, Stub
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-@app.function(
+@stub.function(
     _experimental_scheduler=True,
     _experimental_scheduler_placement=SchedulerPlacement(
         region="us-east-1",
         zone="us-east-1a",
         spot=False,
     ),
 )
 def f():
     pass
 
 
 def test_scheduler_placement(servicer, client):
-    with app.run(client=client):
+    with stub.run(client=client):
         assert len(servicer.app_functions) == 1
         fn = servicer.app_functions["fu-1"]
         assert fn._experimental_scheduler
         assert fn._experimental_scheduler_placement == api_pb2.SchedulerPlacement(
             _region="us-east-1",
             _zone="us-east-1a",
             _lifecycle="on-demand",
```

## test/secret_test.py

```diff
@@ -1,78 +1,78 @@
 # Copyright Modal Labs 2022
 import os
 import pytest
 import tempfile
 from unittest import mock
 
-from modal import App, Secret
+from modal import Secret, Stub
 from modal.exception import InvalidError
 
 from .supports.skip import skip_old_py
 
 
 def dummy():
     ...
 
 
 def test_secret_from_dict(servicer, client):
-    app = App()
+    stub = Stub()
     secret = Secret.from_dict({"FOO": "hello, world"})
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert secret.object_id == "st-0"
         assert servicer.secrets["st-0"] == {"FOO": "hello, world"}
 
 
 @skip_old_py("python-dotenv requires python3.8 or higher", (3, 8))
 def test_secret_from_dotenv(servicer, client):
     with tempfile.TemporaryDirectory() as tmpdirname:
         with open(os.path.join(tmpdirname, ".env"), "w") as f:
             f.write("# My settings\nUSER=user\nPASSWORD=abc123\n")
-        app = App()
+        stub = Stub()
         secret = Secret.from_dotenv(tmpdirname)
-        app.function(secrets=[secret])(dummy)
-        with app.run(client=client):
+        stub.function(secrets=[secret])(dummy)
+        with stub.run(client=client):
             assert secret.object_id == "st-0"
             assert servicer.secrets["st-0"] == {"USER": "user", "PASSWORD": "abc123"}
 
 @mock.patch.dict(os.environ, {"FOO": "easy", "BAR": "1234"})
 def test_secret_from_local_environ(servicer, client):
-    app = App()
+    stub = Stub()
     secret = Secret.from_local_environ(["FOO", "BAR"])
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert secret.object_id == "st-0"
         assert servicer.secrets["st-0"] == {"FOO": "easy", "BAR": "1234"}
 
     with pytest.raises(InvalidError, match="NOTFOUND"):
         Secret.from_local_environ(["FOO", "NOTFOUND"])
 
 
 
 def test_init_types():
     with pytest.raises(InvalidError):
         Secret.from_dict({"foo": 1.0})  # type: ignore
 
 
 def test_secret_from_dict_none(servicer, client):
-    app = App()
+    stub = Stub()
     secret = Secret.from_dict({"FOO": os.getenv("xyz"), "BAR": os.environ.get("abc"), "BAZ": "baz"})
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert servicer.secrets["st-0"] == {"BAZ": "baz"}
 
 
 def test_secret_from_name(servicer, client):
     # Deploy secret
     secret_id = Secret.create_deployed("my-secret", {"FOO": "123"}, client=client)
 
     # Look up secret
     secret = Secret.lookup("my-secret", client=client)
     assert secret.object_id == secret_id
 
     # Look up secret through app
-    app = App()
+    stub = Stub()
     secret = Secret.from_name("my-secret")
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert secret.object_id == secret_id
```

## test/stub_composition_test.py

```diff
@@ -1,10 +1,10 @@
 # Copyright Modal Labs 2024
-from test.helpers import deploy_app_externally
+from test.helpers import deploy_stub_externally
 
 
-def test_app_composition_includes_all_functions(servicer, supports_dir, monkeypatch, client):
-    print(deploy_app_externally(servicer, "main.py", cwd=supports_dir / "multifile_project"))
+def test_stub_composition_includes_all_functions(servicer, supports_dir, monkeypatch, client):
+    print(deploy_stub_externally(servicer, "main.py", cwd=supports_dir / "multifile_project"))
     assert servicer.n_functions == 3
     assert {"/root/main.py", "/root/a.py", "/root/b.py", "/root/c.py"} == set(servicer.files_name2sha.keys())
     assert len(servicer.secrets) == 1  # secret from B should be included
     assert servicer.n_mounts == 4  # mounts should not be duplicated
```

## test/stub_test.py

```diff
@@ -2,176 +2,198 @@
 import asyncio
 import logging
 import pytest
 
 from google.protobuf.empty_pb2 import Empty
 from grpclib import GRPCError, Status
 
-from modal import App, Dict, Image, Mount, Queue, Secret, Volume, web_endpoint
-from modal.app import list_apps  # type: ignore
+import modal.app
+from modal import Dict, Image, Queue, Stub, web_endpoint
 from modal.config import config
 from modal.exception import DeprecationError, ExecutionError, InvalidError, NotFoundError
 from modal.partial_function import _parse_custom_domains
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
 from .supports import module_1, module_2
 
 
 @pytest.mark.asyncio
 async def test_kwargs(servicer, client):
-    with pytest.raises(DeprecationError):
-        App(
+    with pytest.warns(DeprecationError):
+        stub = Stub(
             d=Dict.new(),
             q=Queue.new(),
         )
+    async with stub.run(client=client):
+        with pytest.warns(DeprecationError):
+            await stub["d"].put.aio("foo", "bar")  # type: ignore
+            await stub["q"].put.aio("baz")  # type: ignore
+            assert await stub["d"].get.aio("foo") == "bar"  # type: ignore
+            assert await stub["q"].get.aio() == "baz"  # type: ignore
 
 
 @pytest.mark.asyncio
 async def test_attrs(servicer, client):
-    app = App()
+    stub = Stub()
     with pytest.warns(DeprecationError):
-        app.d = Dict.new()
-        app.q = Queue.new()
-    async with app.run(client=client):
+        stub.d = Dict.new()
+        stub.q = Queue.new()
+    async with stub.run(client=client):
         with pytest.warns(DeprecationError):
-            await app.d.put.aio("foo", "bar")  # type: ignore
-            await app.q.put.aio("baz")  # type: ignore
-            assert await app.d.get.aio("foo") == "bar"  # type: ignore
-            assert await app.q.get.aio() == "baz"  # type: ignore
+            await stub.d.put.aio("foo", "bar")  # type: ignore
+            await stub.q.put.aio("baz")  # type: ignore
+            assert await stub.d.get.aio("foo") == "bar"  # type: ignore
+            assert await stub.q.get.aio() == "baz"  # type: ignore
+
+
+@pytest.mark.asyncio
+async def test_stub_type_validation(servicer, client):
+    with pytest.raises(InvalidError):
+        with pytest.warns(DeprecationError):
+            stub = Stub(
+                foo=4242,  # type: ignore
+            )
+
+    stub = Stub()
+
+    with pytest.raises(InvalidError) as excinfo:
+        stub.bar = 4242  # type: ignore
+
+    assert "4242" in str(excinfo.value)
 
 
 def square(x):
     return x**2
 
 
 @pytest.mark.asyncio
 async def test_redeploy(servicer, client):
-    app = App(image=Image.debian_slim().pip_install("pandas"))
-    app.function()(square)
+    stub = Stub(image=Image.debian_slim().pip_install("pandas"))
+    stub.function()(square)
 
     # Deploy app
-    res = await deploy_app.aio(app, "my-app", client=client)
-    assert res.app_id == "ap-1"
+    app = await deploy_stub.aio(stub, "my-app", client=client)
+    assert app.app_id == "ap-1"
     assert servicer.app_objects["ap-1"]["square"] == "fu-1"
-    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
+    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
     # Redeploy, make sure all ids are the same
-    res = await deploy_app.aio(app, "my-app", client=client)
-    assert res.app_id == "ap-1"
+    app = await deploy_stub.aio(stub, "my-app", client=client)
+    assert app.app_id == "ap-1"
     assert servicer.app_objects["ap-1"]["square"] == "fu-1"
-    assert servicer.app_state_history[res.app_id] == [
+    assert servicer.app_state_history[app.app_id] == [
         api_pb2.APP_STATE_INITIALIZING,
         api_pb2.APP_STATE_DEPLOYED,
         api_pb2.APP_STATE_DEPLOYED,
     ]
 
     # Deploy to a different name, ids should change
-    res = await deploy_app.aio(app, "my-app-xyz", client=client)
-    assert res.app_id == "ap-2"
+    app = await deploy_stub.aio(stub, "my-app-xyz", client=client)
+    assert app.app_id == "ap-2"
     assert servicer.app_objects["ap-2"]["square"] == "fu-2"
-    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
+    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
 
 def dummy():
     pass
 
 
 # Should exit without waiting for the "logs_timeout" grace period.
 @pytest.mark.timeout(5)
 def test_create_object_exception(servicer, client):
     servicer.function_create_error = True
 
-    app = App()
-    app.function()(dummy)
+    stub = Stub()
+    stub.function()(dummy)
 
     with pytest.raises(GRPCError) as excinfo:
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
     assert excinfo.value.status == Status.INTERNAL
 
 
 def test_deploy_falls_back_to_app_name(servicer, client):
-    named_app = App(name="foo_app")
-    deploy_app(named_app, client=client)
+    named_stub = Stub(name="foo_app")
+    deploy_stub(named_stub, client=client)
     assert "foo_app" in servicer.deployed_apps
 
 
 def test_deploy_uses_deployment_name_if_specified(servicer, client):
-    named_app = App(name="foo_app")
-    deploy_app(named_app, "bar_app", client=client)
+    named_stub = Stub(name="foo_app")
+    deploy_stub(named_stub, "bar_app", client=client)
     assert "bar_app" in servicer.deployed_apps
     assert "foo_app" not in servicer.deployed_apps
 
 
 def test_run_function_without_app_error():
-    app = App()
-    dummy_modal = app.function()(dummy)
+    stub = Stub()
+    dummy_modal = stub.function()(dummy)
 
     with pytest.raises(ExecutionError) as excinfo:
         dummy_modal.remote()
 
     assert "hydrated" in str(excinfo.value)
 
 
 def test_is_inside_basic():
-    app = App()
+    stub = Stub()
     with pytest.raises(DeprecationError, match="imports()"):
-        app.is_inside()
+        stub.is_inside()
 
 
 def test_missing_attr():
-    """Trying to call a non-existent function on the App should produce
+    """Trying to call a non-existent function on the Stub should produce
     an understandable error message."""
 
-    app = App()
+    stub = Stub()
     with pytest.raises(AttributeError):
-        app.fun()  # type: ignore
+        stub.fun()  # type: ignore
 
 
 def test_same_function_name(caplog):
-    app = App()
+    stub = Stub()
 
     # Add first function
     with caplog.at_level(logging.WARNING):
-        app.function()(module_1.square)
+        stub.function()(module_1.square)
     assert len(caplog.records) == 0
 
     # Add second function: check warning
     with caplog.at_level(logging.WARNING):
-        app.function()(module_2.square)
+        stub.function()(module_2.square)
     assert len(caplog.records) == 1
     assert "module_1" in caplog.text
     assert "module_2" in caplog.text
     assert "square" in caplog.text
 
 
 def test_run_state(client, servicer):
-    app = App()
-    with app.run(client=client):
-        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_EPHEMERAL]
+    stub = Stub()
+    with stub.run(client=client):
+        assert servicer.app_state_history[stub.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_EPHEMERAL]
 
 
 def test_deploy_state(client, servicer):
-    app = App()
-    res = deploy_app(app, "foobar", client=client)
-    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
+    stub = Stub()
+    app = deploy_stub(stub, "foobar", client=client)
+    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
 
 def test_detach_state(client, servicer):
-    app = App()
-    with app.run(client=client, detach=True):
-        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]
+    stub = Stub()
+    with stub.run(client=client, detach=True):
+        assert servicer.app_state_history[stub.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]
 
 
 @pytest.mark.asyncio
 async def test_grpc_protocol(client, servicer):
-    app = App()
-    async with app.run(client=client):
+    stub = Stub()
+    async with stub.run(client=client):
         await asyncio.sleep(0.01)  # wait for heartbeat
     assert len(servicer.requests) == 4
     assert isinstance(servicer.requests[0], Empty)  # ClientHello
     assert isinstance(servicer.requests[1], api_pb2.AppCreateRequest)
     assert isinstance(servicer.requests[2], api_pb2.AppHeartbeatRequest)
     assert isinstance(servicer.requests[3], api_pb2.AppClientDisconnectRequest)
 
@@ -181,169 +203,158 @@
 
 
 async def web2(x):
     return {"cube": x**3}
 
 
 def test_registered_web_endpoints(client, servicer):
-    app = App()
-    app.function()(square)
-    app.function()(web_endpoint()(web1))
-    app.function()(web_endpoint()(web2))
+    stub = Stub()
+    stub.function()(square)
+    stub.function()(web_endpoint()(web1))
+    stub.function()(web_endpoint()(web2))
 
-    assert app.registered_web_endpoints == ["web1", "web2"]
+    assert stub.registered_web_endpoints == ["web1", "web2"]
 
 
 def test_init_types():
     with pytest.raises(InvalidError):
         # singular secret to plural argument
-        App(secrets=Secret.from_dict())  # type: ignore
+        Stub(secrets=modal.Secret.from_dict())  # type: ignore
     with pytest.raises(InvalidError):
         # not a Secret Object
-        App(secrets=[{"foo": "bar"}])  # type: ignore
+        Stub(secrets=[{"foo": "bar"}])  # type: ignore
+    with pytest.raises(InvalidError):
+        # blueprint needs to use _Providers
+        with pytest.warns(DeprecationError):
+            Stub(some_arg=5)  # type: ignore
     with pytest.raises(InvalidError):
         # should be an Image
-        App(image=Secret.from_dict())  # type: ignore
+        Stub(image=modal.Secret.from_dict())  # type: ignore
 
-    App(
-        image=Image.debian_slim().pip_install("pandas"),
-        secrets=[Secret.from_dict()],
-        mounts=[Mount.from_local_file(__file__)],
-    )
+    with pytest.warns(DeprecationError):
+        Stub(
+            image=modal.Image.debian_slim().pip_install("pandas"),
+            secrets=[modal.Secret.from_dict()],
+            mounts=[modal.Mount.from_local_file(__file__)],
+            some_dict=modal.Dict.new(),
+            some_queue=modal.Queue.new(),
+        )
 
 
-def test_set_image_on_app_as_attribute():
+def test_set_image_on_stub_as_attribute():
     # TODO: do we want to deprecate this syntax? It's kind of random for image to
     #     have a reserved name in the blueprint, and being the only of the construction
     #     arguments that can be set on the instance after construction
-    custom_img = Image.debian_slim().apt_install("emacs")
-    app = App(image=custom_img)
-    assert app._get_default_image() == custom_img
+    custom_img = modal.Image.debian_slim().apt_install("emacs")
+    stub = Stub(image=custom_img)
+    assert stub._get_default_image() == custom_img
 
 
 def test_redeploy_delete_objects(servicer, client):
     # Deploy an app with objects d1 and d2
-    app = App()
-    app.function(name="d1")(dummy)
-    app.function(name="d2")(dummy)
-    res = deploy_app(app, "xyz", client=client)
+    stub = Stub()
+    stub.function(name="d1")(dummy)
+    stub.function(name="d2")(dummy)
+    app = deploy_stub(stub, "xyz", client=client)
 
     # Check objects
-    assert set(servicer.app_objects[res.app_id].keys()) == set(["d1", "d2"])
+    assert set(servicer.app_objects[app.app_id].keys()) == set(["d1", "d2"])
 
     # Deploy an app with objects d2 and d3
-    app = App()
-    app.function(name="d2")(dummy)
-    app.function(name="d3")(dummy)
-    res = deploy_app(app, "xyz", client=client)
+    stub = Stub()
+    stub.function(name="d2")(dummy)
+    stub.function(name="d3")(dummy)
+    app = deploy_stub(stub, "xyz", client=client)
 
     # Make sure d1 is deleted
-    assert set(servicer.app_objects[res.app_id].keys()) == set(["d2", "d3"])
+    assert set(servicer.app_objects[app.app_id].keys()) == set(["d2", "d3"])
 
 
 @pytest.mark.asyncio
 async def test_unhydrate(servicer, client):
-    app = App()
+    stub = Stub()
 
-    f = app.function()(dummy)
+    f = stub.function()(dummy)
 
     assert not f.is_hydrated
-    async with app.run(client=client):
+    async with stub.run(client=client):
         assert f.is_hydrated
 
     # After app finishes, it should unhydrate
     assert not f.is_hydrated
 
 
 def test_keyboard_interrupt(servicer, client):
-    app = App()
-    app.function()(square)
-    with app.run(client=client):
+    stub = Stub()
+    stub.function()(square)
+    with stub.run(client=client):
         # The exit handler should catch this interrupt and exit gracefully
         raise KeyboardInterrupt()
 
 
 def test_function_image_positional():
-    app = App()
+    stub = Stub()
     image = Image.debian_slim()
 
     with pytest.raises(InvalidError) as excinfo:
 
-        @app.function(image)  # type: ignore
+        @stub.function(image)  # type: ignore
         def f():
             pass
 
     assert "function(image=image)" in str(excinfo.value)
 
 
 @pytest.mark.asyncio
 async def test_deploy_disconnect(servicer, client):
-    app = App()
-    app.function(secrets=[Secret.from_name("nonexistent-secret")])(square)
+    stub = Stub()
+    stub.function(secrets=[modal.Secret.from_name("nonexistent-secret")])(square)
 
     with pytest.raises(NotFoundError):
-        await deploy_app.aio(app, "my-app", client=client)
+        await deploy_stub.aio(stub, "my-app", client=client)
 
     assert servicer.app_state_history["ap-1"] == [
         api_pb2.APP_STATE_INITIALIZING,
         api_pb2.APP_STATE_STOPPED,
     ]
 
 
 def test_redeploy_from_name_change(servicer, client):
     # Deploy queue
-    Queue.lookup("foo-queue", create_if_missing=True, client=client)
+    modal.Queue.lookup("foo-queue", create_if_missing=True, client=client)
 
-    # Use it from app
-    app = App()
+    # Use it from stub
+    stub = Stub()
     with pytest.warns(DeprecationError):
-        app.q = Queue.from_name("foo-queue")
-    deploy_app(app, "my-app", client=client)
+        stub.q = modal.Queue.from_name("foo-queue")
+    deploy_stub(stub, "my-app", client=client)
 
     # Change the object id of foo-queue
     k = ("foo-queue", api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, config.get("environment"))
     assert servicer.deployed_queues[k]
     servicer.deployed_queues[k] = "qu-baz123"
 
     # Redeploy app
     # This should not fail because the object_id changed - it's a different app
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
 
 
 def test_parse_custom_domains():
     assert len(_parse_custom_domains(None)) == 0
     assert len(_parse_custom_domains(["foo.com", "bar.com"])) == 2
     with pytest.raises(AssertionError):
         assert _parse_custom_domains("foo.com")
 
 
 def test_hydrated_other_app_object_gets_referenced(servicer, client):
-    app = App("my-app")
+    stub = Stub("my-stub")
     with servicer.intercept() as ctx:
-        with Volume.ephemeral(client=client) as vol:
-            app.function(volumes={"/vol": vol})(dummy)  # implicitly load vol
-            deploy_app(app, client=client)
+        with modal.Volume.ephemeral(client=client) as vol:
+            stub.function(volumes={"/vol": vol})(dummy)  # implicitly load vol
+            deploy_stub(stub, client=client)
             app_set_objects_req = ctx.pop_request("AppSetObjects")
             assert vol.object_id in app_set_objects_req.unindexed_object_ids
 
 
 def test_hasattr():
-    app = App()
-    assert not hasattr(app, "xyz")
-
-
-def test_app(client):
-    app = App()
-    square_modal = app.function()(square)
-
-    with app.run(client=client):
-        square_modal.remote(42)
-
-
-def test_list_apps(client):
-    apps_0 = [app.name for app in list_apps(client=client)]
-    app = App()
-    deploy_app(app, "foobar", client=client)
-    apps_1 = [app.name for app in list_apps(client=client)]
-
-    assert len(apps_1) == len(apps_0) + 1
-    assert set(apps_1) - set(apps_0) == set(["foobar"])
+    stub = Stub()
+    assert not hasattr(stub, "xyz")
```

## test/volume_test.py

```diff
@@ -1,60 +1,54 @@
 # Copyright Modal Labs 2023
-import asyncio
 import io
-import os
-import platform
 import pytest
-import re
-import sys
 import time
 from pathlib import Path
 from unittest import mock
 
 import modal
 from modal.exception import DeprecationError, InvalidError, NotFoundError, VolumeUploadTimeoutError
-from modal.runner import deploy_app
-from modal.volume import _open_files_error_annotation
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
 
 def dummy():
     pass
 
 
 def test_volume_mount(client, servicer):
-    app = modal.App()
+    stub = modal.Stub()
     vol = modal.Volume.from_name("xyz", create_if_missing=True)
 
-    _ = app.function(volumes={"/root/foo": vol})(dummy)
+    _ = stub.function(volumes={"/root/foo": vol})(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
 
 def test_volume_bad_paths():
-    app = modal.App()
+    stub = modal.Stub()
     vol = modal.Volume.from_name("xyz")
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/root/../../foo": vol})(dummy)
+        stub.function(volumes={"/root/../../foo": vol})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/": vol})(dummy)
+        stub.function(volumes={"/": vol})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/tmp/": vol})(dummy)
+        stub.function(volumes={"/tmp/": vol})(dummy)
 
 
 def test_volume_duplicate_mount():
-    app = modal.App()
+    stub = modal.Stub()
     vol = modal.Volume.from_name("xyz")
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/foo": vol, "/bar": vol})(dummy)
+        stub.function(volumes={"/foo": vol, "/bar": vol})(dummy)
 
 
 @pytest.mark.parametrize("skip_reload", [False, True])
 def test_volume_commit(client, servicer, skip_reload):
     with servicer.intercept() as ctx:
         ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))
         ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))
@@ -105,39 +99,39 @@
         # Note that in practice this will not work unless run in a task.
         vol.reload()
 
         assert servicer.volume_reloads[vol.object_id] == 1
 
 
 def test_redeploy(servicer, client):
-    app = modal.App()
+    stub = modal.Stub()
 
     with pytest.warns(DeprecationError):
         v1 = modal.Volume.new()
         v2 = modal.Volume.new()
         v3 = modal.Volume.new()
-        app.v1, app.v2, app.v3 = v1, v2, v3
+        stub.v1, stub.v2, stub.v3 = v1, v2, v3
 
     # Deploy app once
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app1_ids = [v1.object_id, v2.object_id, v3.object_id]
 
     # Deploy app again
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app2_ids = [v1.object_id, v2.object_id, v3.object_id]
 
     # Make sure ids are stable
     assert app1_ids == app2_ids
 
     # Make sure ids are unique
     assert len(set(app1_ids)) == 3
     assert len(set(app2_ids)) == 3
 
     # Deploy to a different app
-    deploy_app(app, "my-other-app", client=client)
+    deploy_stub(stub, "my-other-app", client=client)
     app3_ids = [v1.object_id, v2.object_id, v3.object_id]
 
     # Should be unique and different
     assert len(set(app3_ids)) == 3
     assert set(app1_ids) & set(app3_ids) == set()
 
 
@@ -341,52 +335,7 @@
 def test_ephemeral(servicer, client):
     assert servicer.n_vol_heartbeats == 0
     with modal.Volume.ephemeral(client=client, _heartbeat_sleep=1) as vol:
         assert vol.listdir("**") == []
         # TODO(erikbern): perform some operations
         time.sleep(1.5)  # Make time for 2 heartbeats
     assert servicer.n_vol_heartbeats == 2
-
-
-def test_lazy_hydration_from_named(set_env_client):
-    vol = modal.Volume.from_name("my-vol", create_if_missing=True)
-    assert vol.listdir("**") == []
-
-
-@pytest.mark.skipif(platform.system() != "Linux", reason="needs /proc")
-@pytest.mark.asyncio
-async def test_open_files_error_annotation(tmp_path):
-    assert _open_files_error_annotation(tmp_path) is None
-
-    # Current process keeps file open
-    with (tmp_path / "foo.txt").open("w") as _f:
-        assert _open_files_error_annotation(tmp_path) == "path foo.txt is open"
-
-    # cwd of current process is inside volume
-    cwd = os.getcwd()
-    os.chdir(tmp_path)
-    assert _open_files_error_annotation(tmp_path) == "cwd is inside volume"
-    os.chdir(cwd)
-
-    # Subprocess keeps open file
-    open_path = tmp_path / "bar.txt"
-    open_path.write_text("")
-    proc = await asyncio.create_subprocess_exec("tail", "-f", open_path.as_posix())
-    await asyncio.sleep(0.01)  # Give process some time to start
-    assert _open_files_error_annotation(tmp_path) == f"path bar.txt is open from 'tail -f {open_path.as_posix()}'"
-    proc.kill()
-    await proc.wait()
-    assert _open_files_error_annotation(tmp_path) is None
-
-    # Subprocess cwd inside volume
-    proc = await asyncio.create_subprocess_exec(
-        sys.executable, "-c", f"import time; import os; os.chdir('{tmp_path}'); time.sleep(60)"
-    )
-    # Wait for process to chdir
-    for _ in range(100):
-        if os.readlink(f"/proc/{proc.pid}/cwd") == tmp_path.as_posix():
-            break
-        await asyncio.sleep(0.05)
-    assert re.match(f"^cwd of '{sys.executable} -c .*' is inside volume$", _open_files_error_annotation(tmp_path))
-    proc.kill()
-    await proc.wait()
-    assert _open_files_error_annotation(tmp_path) is None
```

## test/webhook_test.py

```diff
@@ -2,46 +2,47 @@
 import pathlib
 import pytest
 import subprocess
 import sys
 
 from fastapi.testclient import TestClient
 
-from modal import App, asgi_app, web_endpoint, wsgi_app
+from modal import Stub, asgi_app, web_endpoint, wsgi_app
 from modal._asgi import webhook_asgi_app
+from modal.app import ContainerApp
 from modal.exception import InvalidError
 from modal.functions import Function
-from modal.running_app import RunningApp
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-@app.function(cpu=42)
+@stub.function(cpu=42)
 @web_endpoint(method="PATCH")
 async def f(x):
     return {"square": x**2}
 
 
 @pytest.mark.asyncio
-async def test_webhook(servicer, client, reset_container_app):
-    async with app.run(client=client):
+async def test_webhook(servicer, client):
+    async with stub.run(client=client):
         assert f.web_url
 
         assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION
         assert servicer.app_functions["fu-1"].webhook_config.method == "PATCH"
 
         # Make sure we can call the webhooks
         # TODO: reinstate `.remote` check when direct webhook fn invocation is fixed.
         # assert await f.remote(10)
         assert await f.local(100) == {"square": 10000}
 
         # Make sure the container gets the app id as well
-        container_app = RunningApp(app_id=app.app_id)
-        app._init_container(client, container_app)
+        container_app = ContainerApp()
+        await ContainerApp.init.aio(client, stub.app_id)
+        container_app.associate_stub_container(stub)
         assert isinstance(f, Function)
         assert f.web_url
 
 
 def test_webhook_cors():
     def handler():
         return {"message": "Hello, World!"}
@@ -74,19 +75,19 @@
     app = webhook_asgi_app(handler, method="GET")
     client = TestClient(app)
     assert client.get("/docs").status_code == 404
     assert client.get("/redoc").status_code == 404
 
 
 def test_webhook_generator():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError) as excinfo:
 
-        @app.function(serialized=True)
+        @stub.function(serialized=True)
         @web_endpoint()
         def web_gen():
             yield None
 
     assert "streaming" in str(excinfo.value).lower()
 
 
@@ -98,41 +99,41 @@
     stderr = ret.stderr.decode()
     assert "absent_minded_function" in stderr
     assert "@stub.function" in stderr
 
 
 @pytest.mark.asyncio
 async def test_webhook_decorator_in_wrong_order(servicer, client):
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError) as excinfo:
 
         @web_endpoint()  # type: ignore
-        @app.function(serialized=True)
+        @stub.function(serialized=True)
         async def g(x):
             pass
 
     assert "wrong order" in str(excinfo.value).lower()
 
 
 @pytest.mark.asyncio
 async def test_asgi_wsgi(servicer, client):
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @asgi_app()
     async def my_asgi(x):
         pass
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @wsgi_app()
     async def my_wsgi(x):
         pass
 
-    async with app.run(client=client):
+    async with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 2
     assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP
     assert servicer.app_functions["fu-2"].webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP
```

## Comparing `modal/_container_io_manager.py` & `modal/stub.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,666 +1,783 @@
-# Copyright Modal Labs 2024
-import asyncio
-import json
-import math
+# Copyright Modal Labs 2022
+import inspect
 import os
-import signal
-import time
-import traceback
-from pathlib import Path
-from typing import Any, AsyncGenerator, AsyncIterator, Callable, ClassVar, List, Optional, Set, Tuple
-
-from google.protobuf.empty_pb2 import Empty
-from google.protobuf.message import Message
-from grpclib import Status
+import typing
+from pathlib import PurePosixPath
+from typing import Any, AsyncGenerator, Callable, ClassVar, Dict, List, Optional, Sequence, Tuple, Union
+
 from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_pb2
 
-from ._serialization import deserialize, deserialize_data_format, serialize, serialize_data_format
-from ._traceback import extract_traceback
-from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
-from ._utils.blob_utils import MAX_OBJECT_SIZE_BYTES, blob_download, blob_upload
-from ._utils.function_utils import _stream_function_call_data
-from ._utils.grpc_utils import get_proto_oneof, retry_transient_errors
-from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
-from .config import config, logger
-from .exception import InputCancellation, InvalidError
-from .running_app import RunningApp
+from ._ipython import is_notebook
+from ._output import OutputManager
+from ._resolver import Resolver
+from ._utils.async_utils import synchronize_api
+from ._utils.function_utils import FunctionInfo
+from ._utils.mount_utils import validate_volumes
+from .app import _ContainerApp, _LocalApp
+from .client import _Client
+from .cls import _Cls
+from .config import logger
+from .exception import InvalidError, deprecation_error, deprecation_warning
+from .functions import _Function
+from .gpu import GPU_T
+from .image import _Image
+from .mount import _Mount
+from .network_file_system import _NetworkFileSystem
+from .object import _Object
+from .partial_function import PartialFunction, _find_callables_for_cls, _PartialFunction, _PartialFunctionFlags
+from .proxy import _Proxy
+from .retries import Retries
+from .runner import _run_stub
+from .sandbox import _Sandbox
+from .schedule import Schedule
+from .scheduler_placement import SchedulerPlacement
+from .secret import _Secret
+from .volume import _Volume
+
+_default_image: _Image = _Image.debian_slim()
+
+
+class _LocalEntrypoint:
+    _info: FunctionInfo
+    _stub: "_Stub"
+
+    def __init__(self, info, stub):
+        self._info = info  # type: ignore
+        self._stub = stub
+
+    def __call__(self, *args, **kwargs):
+        return self._info.raw_f(*args, **kwargs)
+
+    @property
+    def info(self) -> FunctionInfo:
+        return self._info
+
+    @property
+    def stub(self) -> "_Stub":
+        return self._stub
+
+
+LocalEntrypoint = synchronize_api(_LocalEntrypoint)
+
+
+def check_sequence(items: typing.Sequence[typing.Any], item_type: typing.Type[typing.Any], error_msg: str):
+    if not isinstance(items, (list, tuple)):
+        raise InvalidError(error_msg)
+    if not all(isinstance(v, item_type) for v in items):
+        raise InvalidError(error_msg)
+
+
+CLS_T = typing.TypeVar("CLS_T", bound=typing.Type)
+
+
+class _Stub:
+    """A `Stub` is a description of how to create a Modal application.
+
+    The stub object principally describes Modal objects (`Function`, `Image`,
+    `Secret`, etc.) associated with the application. It has three responsibilities:
+
+    * Syncing of identities across processes (your local Python interpreter and
+      every Modal worker active in your application).
+    * Making Objects stay alive and not be garbage collected for as long as the
+      app lives (see App lifetime below).
+    * Manage log collection for everything that happens inside your code.
+
+    **Registering functions with an app**
+
+    The most common way to explicitly register an Object with an app is through the
+    `@stub.function()` decorator. It both registers the annotated function itself and
+    other passed objects, like schedules and secrets, with the app:
+
+    ```python
+    import modal
+
+    stub = modal.Stub()
+
+    @stub.function(
+        secrets=[modal.Secret.from_name("some_secret")],
+        schedule=modal.Period(days=1),
+    )
+    def foo():
+        pass
+    ```
 
-MAX_OUTPUT_BATCH_SIZE: int = 49
+    In this example, the secret and schedule are registered with the app.
+    """
 
-RTT_S: float = 0.5  # conservative estimate of RTT in seconds.
+    _name: Optional[str]
+    _description: Optional[str]
+    _indexed_objects: Dict[str, _Object]
+    _function_mounts: Dict[str, _Mount]
+    _mounts: Sequence[_Mount]
+    _secrets: Sequence[_Secret]
+    _volumes: Dict[Union[str, PurePosixPath], _Volume]
+    _web_endpoints: List[str]  # Used by the CLI
+    _local_entrypoints: Dict[str, _LocalEntrypoint]
+    _container_app: Optional[_ContainerApp]
+    _local_app: Optional[_LocalApp]
+    _all_stubs: ClassVar[Dict[Optional[str], List["_Stub"]]] = {}
 
+    def __init__(
+        self,
+        name: Optional[str] = None,
+        *,
+        image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)
+        mounts: Sequence[_Mount] = [],  # default mounts for all functions
+        secrets: Sequence[_Secret] = [],  # default secrets for all functions
+        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # default volumes for all functions
+        **indexed_objects: _Object,  # any Modal Object dependencies (Dict, Queue, etc.)
+    ) -> None:
+        """Construct a new app stub, optionally with default image, mounts, secrets
 
-class UserException(Exception):
-    """Used to shut down the task gracefully."""
+        Any "indexed_objects" objects are loaded as part of running or deploying the app,
+        and are accessible by name on the running container app, e.g.:
+        ```python
+        stub = modal.Stub(key_value_store=modal.Dict.new())
+
+        @stub.function()
+        def store_something(key: str, value: str):
+            stub.app.key_value_store.put(key, value)
+        ```
+        """
 
+        self._name = name
+        self._description = name
 
-class Sentinel:
-    """Used to get type-stubs to work with this object."""
+        check_sequence(mounts, _Mount, "mounts has to be a list or tuple of Mount objects")
+        check_sequence(secrets, _Secret, "secrets has to be a list or tuple of Secret objects")
+        validate_volumes(volumes)
+
+        if image is not None and not isinstance(image, _Image):
+            raise InvalidError("image has to be a modal Image or AioImage object")
+
+        if indexed_objects:
+            deprecation_warning(
+                (2023, 12, 13),
+                "Passing **kwargs to a stub is deprecated. In most cases, you can just define the objects in global scope.",
+            )
 
+        for k, v in indexed_objects.items():
+            self._validate_blueprint_value(k, v)
 
-class _ContainerIOManager:
-    """Synchronizes all RPC calls and network operations for a running container.
+        self._indexed_objects = indexed_objects
+        if image is not None:
+            self._indexed_objects["image"] = image  # backward compatibility since "image" used to be on the blueprint
+
+        self._mounts = mounts
+
+        self._secrets = secrets
+        self._volumes = volumes
+        self._local_entrypoints = {}
+        self._web_endpoints = []
+        self._local_app = None  # when this is the launcher process
+        self._container_app = None  # when this is inside a container
+
+        # Register this stub. This is used to look up the stub in the container, when we can't get it from the function
+        _Stub._all_stubs.setdefault(self._name, []).append(self)
+
+    @property
+    def name(self) -> Optional[str]:
+        """The user-provided name of the Stub."""
+        return self._name
+
+    @property
+    def is_interactive(self) -> bool:
+        """Whether the current app for the stub is running in interactive mode."""
+        # return self._name
+        if self._local_app:
+            return self._local_app.is_interactive
+        else:
+            return False
 
-    TODO: maybe we shouldn't synchronize the whole class.
-    Then we could potentially move a bunch of the global functions onto it.
-    """
+    @property
+    def app_id(self) -> Optional[str]:
+        """Return the app_id, if the stub is running."""
+        if self._container_app:
+            return self._container_app._app_id
+        elif self._local_app:
+            return self._local_app._app_id
+        else:
+            return None
 
-    cancelled_input_ids: Set[str]
-    task_id: str
-    function_id: str
-    app_id: str
-    function_def: api_pb2.Function
-    checkpoint_id: Optional[str]
-
-    calls_completed: int
-    total_user_time: float
-    current_input_id: Optional[str]
-    current_input_started_at: Optional[float]
-
-    _input_concurrency: Optional[int]
-    _semaphore: Optional[asyncio.Semaphore]
-    _environment_name: str
-    _waiting_for_checkpoint: bool
-    _heartbeat_loop: Optional[asyncio.Task]
-
-    _is_interactivity_enabled: bool
-    _fetching_inputs: bool
-
-    _client: _Client
-
-    _GENERATOR_STOP_SENTINEL: ClassVar[Sentinel] = Sentinel()
-    _singleton: ClassVar[Optional["_ContainerIOManager"]] = None
-
-    def _init(self, container_args: api_pb2.ContainerArguments, client: _Client):
-        self.cancelled_input_ids = set()
-        self.task_id = container_args.task_id
-        self.function_id = container_args.function_id
-        self.app_id = container_args.app_id
-        self.function_def = container_args.function_def
-        self.checkpoint_id = container_args.checkpoint_id or None
-
-        self.calls_completed = 0
-        self.total_user_time = 0.0
-        self.current_input_id = None
-        self.current_input_started_at = None
-
-        self._input_concurrency = None
-
-        self._semaphore = None
-        self._environment_name = container_args.environment_name
-        self._waiting_for_checkpoint = False
-        self._heartbeat_loop = None
-
-        self._is_interactivity_enabled = False
-        self._fetching_inputs = True
-
-        self._client = client
-        assert isinstance(self._client, _Client)
-
-    def __new__(cls, container_args: api_pb2.ContainerArguments, client: _Client) -> "_ContainerIOManager":
-        cls._singleton = super().__new__(cls)
-        cls._singleton._init(container_args, client)
-        return cls._singleton
-
-    @classmethod
-    def _reset_singleton(cls):
-        """Only used for tests."""
-        cls._singleton = None
-
-    async def _run_heartbeat_loop(self):
-        while 1:
-            t0 = time.monotonic()
-            try:
-                if await self._heartbeat_handle_cancellations():
-                    # got a cancellation event, fine to start another heartbeat immediately
-                    # since the cancellation queue should be empty on the worker server
-                    # however, we wait at least 1s to prevent short-circuiting the heartbeat loop
-                    # in case there is ever a bug. This means it will take at least 1s between
-                    # two subsequent cancellations on the same task at the moment
-                    await asyncio.sleep(1.0)
-                    continue
-            except Exception as exc:
-                # don't stop heartbeat loop if there are transient exceptions!
-                time_elapsed = time.monotonic() - t0
-                error = exc
-                logger.warning(f"Heartbeat attempt failed ({time_elapsed=}, {error=})")
-
-            heartbeat_duration = time.monotonic() - t0
-            time_until_next_hearbeat = max(0.0, HEARTBEAT_INTERVAL - heartbeat_duration)
-            await asyncio.sleep(time_until_next_hearbeat)
-
-    async def _heartbeat_handle_cancellations(self) -> bool:
-        # Return True if a cancellation event was received, in that case we shouldn't wait too long for another heartbeat
-
-        # Don't send heartbeats for tasks waiting to be checkpointed.
-        # Calling gRPC methods open new connections which block the
-        # checkpointing process.
-        if self._waiting_for_checkpoint:
-            return False
+    @property
+    def description(self) -> Optional[str]:
+        """The Stub's `name`, if available, or a fallback descriptive identifier."""
+        return self._description
+
+    def set_description(self, description: str):
+        self._description = description
+
+    def _validate_blueprint_value(self, key: str, value: Any):
+        if not isinstance(value, _Object):
+            raise InvalidError(f"Stub attribute {key} with value {value} is not a valid Modal object")
+
+    def _add_object(self, tag, obj):
+        if self._container_app:
+            # If this is inside a container, then objects can be defined after app initialization.
+            # So we may have to initialize objects once they get bound to the stub.
+            if self._container_app._has_object(tag):
+                self._container_app._hydrate_object(obj, tag)
+
+        self._indexed_objects[tag] = obj
+
+    def __getitem__(self, tag: str):
+        """Stub assignments of the form `stub.x` or `stub["x"]` are deprecated!
+
+        The only use cases for these assignments is in conjunction with `.new()`, which is now
+        in itself deprecated. If you are constructing objects with `.from_name(...)`, there is no
+        need to assign those objects to the stub. Example:
+
+        ```python
+        d = modal.Dict.from_name("my-dict", create_if_missing=True)
+
+        @stub.function()
+        def f(x, y):
+            d[x] = y  # Refer to d in global scope
+        ```
+        """
+        deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+        return self._indexed_objects[tag]
 
-        request = api_pb2.ContainerHeartbeatRequest(supports_graceful_input_cancellation=True)
-        if self.current_input_id is not None:
-            request.current_input_id = self.current_input_id
-        if self.current_input_started_at is not None:
-            request.current_input_started_at = self.current_input_started_at
-
-        # TODO(erikbern): capture exceptions?
-        response = await retry_transient_errors(
-            self._client.stub.ContainerHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT
-        )
+    def __setitem__(self, tag: str, obj: _Object):
+        deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+        self._validate_blueprint_value(tag, obj)
+        # Deprecated ?
+        self._add_object(tag, obj)
+
+    def __getattr__(self, tag: str) -> _Object:
+        # TODO(erikbern): remove this method later
+        assert isinstance(tag, str)
+        if tag.startswith("__"):
+            # Hacky way to avoid certain issues, e.g. pickle will try to look this up
+            raise AttributeError(f"Stub has no member {tag}")
+        if tag not in self._indexed_objects:
+            # Primarily to make hasattr work
+            raise AttributeError(f"Stub has no member {tag}")
+        obj: _Object = self._indexed_objects[tag]
+        deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+        return obj
+
+    def __setattr__(self, tag: str, obj: _Object):
+        # TODO(erikbern): remove this method later
+        # Note that only attributes defined in __annotations__ are set on the object itself,
+        # everything else is registered on the indexed_objects
+        if tag in self.__annotations__:
+            object.__setattr__(self, tag, obj)
+        elif tag == "image":
+            self._indexed_objects["image"] = obj
+        else:
+            self._validate_blueprint_value(tag, obj)
+            deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+            self._add_object(tag, obj)
+
+    @property
+    def image(self) -> _Image:
+        # Exists to get the type inference working for `stub.image`
+        # Will also keep this one after we remove [get/set][item/attr]
+        return self._indexed_objects["image"]
+
+    @image.setter
+    def image(self, value):
+        self._indexed_objects["image"] = value
+
+    def get_objects(self) -> List[Tuple[str, _Object]]:
+        """Used by the container app to initialize objects."""
+        return list(self._indexed_objects.items())
+
+    def _uncreate_all_objects(self):
+        # TODO(erikbern): this doesn't unhydrate objects that aren't tagged
+        for obj in self._indexed_objects.values():
+            obj._unhydrate()
+
+    def is_inside(self, image: Optional[_Image] = None):
+        """Deprecated: use `Image.imports()` instead! Usage:
+        ```
+        my_image = modal.Image.debian_slim().pip_install("torch")
+        with my_image.imports():
+            import torch
+        ```
+        """
+        deprecation_error((2023, 11, 8), _Stub.is_inside.__doc__)
 
-        if response.HasField("cancel_input_event"):
-            # Pause processing of the current input by signaling self a SIGUSR1.
-            input_ids_to_cancel = response.cancel_input_event.input_ids
-            if input_ids_to_cancel:
-                if self._input_concurrency > 1:
-                    logger.info(
-                        "Shutting down task to stop some subset of inputs (concurrent functions don't support fine-grained cancellation)"
-                    )
-                    # This is equivalent to a task cancellation or preemption from worker code,
-                    # except we do not send a SIGKILL to forcefully exit after 30 seconds.
-                    #
-                    # SIGINT always interrupts the main thread, but not any auxiliary threads. On a
-                    # sync function without concurrent inputs, this raises a KeyboardInterrupt. When
-                    # there are concurrent inputs, we cannot interrupt the thread pool, but the
-                    # interpreter stops waiting for daemon threads and exits. On async functions,
-                    # this signal lands outside the event loop, stopping `run_until_complete()`.
-                    os.kill(os.getpid(), signal.SIGINT)
-
-                elif self.current_input_id in input_ids_to_cancel:
-                    # This goes to a registered signal handler for sync Modal functions, or to the
-                    # `SignalHandlingEventLoop` for async functions.
-                    #
-                    # We only send this signal on functions that do not have concurrent inputs enabled.
-                    # This allows us to do fine-grained input cancellation. On sync functions, the
-                    # SIGUSR1 signal should interrupt the main thread where user code is running,
-                    # raising an InputCancellation() exception. On async functions, the signal should
-                    # reach a handler in SignalHandlingEventLoop, which cancels the task.
-                    os.kill(os.getpid(), signal.SIGUSR1)
-            return True
-        return False
+    @asynccontextmanager
+    async def _set_local_app(self, app: _LocalApp) -> AsyncGenerator[None, None]:
+        self._local_app = app
+        try:
+            yield
+        finally:
+            self._local_app = None
 
     @asynccontextmanager
-    async def heartbeats(self) -> AsyncGenerator[None, None]:
-        async with TaskContext() as tc:
-            self._heartbeat_loop = t = tc.create_task(self._run_heartbeat_loop())
-            t.set_name("heartbeat loop")
-            try:
-                yield
-            finally:
-                t.cancel()
-
-    def stop_heartbeat(self):
-        if self._heartbeat_loop:
-            self._heartbeat_loop.cancel()
-
-    async def get_app_objects(self) -> RunningApp:
-        req = api_pb2.AppGetObjectsRequest(app_id=self.app_id, include_unindexed=True)
-        resp = await retry_transient_errors(self._client.stub.AppGetObjects, req)
-        logger.debug(f"AppGetObjects received {len(resp.items)} objects for app {self.app_id}")
-
-        tag_to_object_id = {}
-        object_handle_metadata = {}
-        for item in resp.items:
-            handle_metadata: Optional[Message] = get_proto_oneof(item.object, "handle_metadata_oneof")
-            object_handle_metadata[item.object.object_id] = handle_metadata
-            if item.tag:
-                tag_to_object_id[item.tag] = item.object.object_id
-
-        return RunningApp(
-            self.app_id,
-            environment_name=self._environment_name,
-            tag_to_object_id=tag_to_object_id,
-            object_handle_metadata=object_handle_metadata,
-        )
+    async def run(
+        self,
+        client: Optional[_Client] = None,
+        stdout=None,
+        show_progress: bool = True,
+        detach: bool = False,
+        output_mgr: Optional[OutputManager] = None,
+    ) -> AsyncGenerator["_Stub", None]:
+        """Context manager that runs an app on Modal.
+
+        Use this as the main entry point for your Modal application. All calls
+        to Modal functions should be made within the scope of this context
+        manager, and they will correspond to the current app.
+
+        Note that this method used to return a separate "App" object. This is
+        no longer useful since you can use the stub itself for access to all
+        objects. For backwards compatibility reasons, it returns the same stub.
+        """
+        # TODO(erikbern): deprecate this one too?
+        async with _run_stub(self, client, stdout, show_progress, detach, output_mgr):
+            yield self
+
+    def _get_default_image(self):
+        if "image" in self._indexed_objects:
+            return self._indexed_objects["image"]
+        else:
+            return _default_image
 
-    async def get_serialized_function(self) -> Tuple[Optional[Any], Callable]:
-        # Fetch the serialized function definition
-        request = api_pb2.FunctionGetSerializedRequest(function_id=self.function_id)
-        response = await self._client.stub.FunctionGetSerialized(request)
-        fun = self.deserialize(response.function_serialized)
+    def _get_watch_mounts(self):
+        all_mounts = [
+            *self._mounts,
+        ]
+        for function in self.registered_functions.values():
+            all_mounts.extend(function._all_mounts)
+
+        return [m for m in all_mounts if m.is_local()]
+
+    def _add_function(self, function: _Function):
+        if function.tag in self._indexed_objects:
+            old_function = self._indexed_objects[function.tag]
+            if isinstance(old_function, _Function):
+                if not is_notebook():
+                    logger.warning(
+                        f"Warning: Tag '{function.tag}' collision!"
+                        f" Overriding existing function [{old_function._info.module_name}].{old_function._info.function_name}"
+                        f" with new function [{function._info.module_name}].{function._info.function_name}"
+                    )
+            else:
+                logger.warning(f"Warning: tag {function.tag} exists but is overridden by function")
 
-        if response.class_serialized:
-            cls = self.deserialize(response.class_serialized)
-        else:
-            cls = None
+        self._add_object(function.tag, function)
 
-        return cls, fun
+    @property
+    def registered_functions(self) -> Dict[str, _Function]:
+        """All modal.Function objects registered on the stub."""
+        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Function)}
 
-    def serialize(self, obj: Any) -> bytes:
-        return serialize(obj)
+    @property
+    def registered_classes(self) -> Dict[str, _Function]:
+        """All modal.Cls objects registered on the stub."""
+        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Cls)}
 
-    def deserialize(self, data: bytes) -> Any:
-        return deserialize(data, self._client)
+    @property
+    def registered_entrypoints(self) -> Dict[str, _LocalEntrypoint]:
+        """All local CLI entrypoints registered on the stub."""
+        return self._local_entrypoints
 
-    @synchronizer.no_io_translation
-    def serialize_data_format(self, obj: Any, data_format: int) -> bytes:
-        return serialize_data_format(obj, data_format)
+    @property
+    def indexed_objects(self) -> Dict[str, _Object]:
+        return self._indexed_objects
 
-    def deserialize_data_format(self, data: bytes, data_format: int) -> Any:
-        return deserialize_data_format(data, data_format, self._client)
+    @property
+    def registered_web_endpoints(self) -> List[str]:
+        """Names of web endpoint (ie. webhook) functions registered on the stub."""
+        return self._web_endpoints
 
-    async def get_data_in(self, function_call_id: str) -> AsyncIterator[Any]:
-        """Read from the `data_in` stream of a function call."""
-        async for data in _stream_function_call_data(self._client, function_call_id, "data_in"):
-            yield data
+    def local_entrypoint(
+        self, _warn_parentheses_missing=None, *, name: Optional[str] = None
+    ) -> Callable[[Callable[..., Any]], None]:
+        """Decorate a function to be used as a CLI entrypoint for a Modal App.
 
-    async def put_data_out(
-        self,
-        function_call_id: str,
-        start_index: int,
-        data_format: int,
-        messages_bytes: List[Any],
-    ) -> None:
-        """Put data onto the `data_out` stream of a function call.
+        These functions can be used to define code that runs locally to set up the app,
+        and act as an entrypoint to start Modal functions from. Note that regular
+        Modal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,
+        those functions are executed remotely directly.
 
-        This is used for generator outputs, which includes web endpoint responses. Note that this
-        was introduced as a performance optimization in client version 0.57, so older clients will
-        still use the previous Postgres-backed system based on `FunctionPutOutputs()`.
-        """
-        data_chunks: List[api_pb2.DataChunk] = []
-        for i, message_bytes in enumerate(messages_bytes):
-            chunk = api_pb2.DataChunk(data_format=data_format, index=start_index + i)  # type: ignore
-            if len(message_bytes) > MAX_OBJECT_SIZE_BYTES:
-                chunk.data_blob_id = await blob_upload(message_bytes, self._client.stub)
-            else:
-                chunk.data = message_bytes
-            data_chunks.append(chunk)
+        **Example**
 
-        req = api_pb2.FunctionCallPutDataRequest(function_call_id=function_call_id, data_chunks=data_chunks)
-        await retry_transient_errors(self._client.stub.FunctionCallPutDataOut, req)
+        ```python
+        @stub.local_entrypoint()
+        def main():
+            some_modal_function.remote()
+        ```
 
-    async def generator_output_task(self, function_call_id: str, data_format: int, message_rx: asyncio.Queue) -> None:
-        """Task that feeds generator outputs into a function call's `data_out` stream."""
-        index = 1
-        received_sentinel = False
-        while not received_sentinel:
-            message = await message_rx.get()
-            if message is self._GENERATOR_STOP_SENTINEL:
-                break
-            # ASGI 'http.response.start' and 'http.response.body' msgs are observed to be separated by 1ms.
-            # If we don't sleep here for 1ms we end up with an extra call to .put_data_out().
-            if index == 1:
-                await asyncio.sleep(0.001)
-            messages_bytes = [serialize_data_format(message, data_format)]
-            total_size = len(messages_bytes[0]) + 512
-            while total_size < 16 * 1024 * 1024:  # 16 MiB, maximum size in a single message
-                try:
-                    message = message_rx.get_nowait()
-                except asyncio.QueueEmpty:
-                    break
-                if message is self._GENERATOR_STOP_SENTINEL:
-                    received_sentinel = True
-                    break
-                else:
-                    messages_bytes.append(serialize_data_format(message, data_format))
-                    total_size += len(messages_bytes[-1]) + 512  # 512 bytes for estimated framing overhead
-            await self.put_data_out(function_call_id, index, data_format, messages_bytes)
-            index += len(messages_bytes)
-
-    async def _queue_create(self, size: int) -> asyncio.Queue:
-        """Create a queue, on the synchronicity event loop (needed on Python 3.8 and 3.9)."""
-        return asyncio.Queue(size)
-
-    async def _queue_put(self, queue: asyncio.Queue, value: Any) -> None:
-        """Put a value onto a queue, using the synchronicity event loop."""
-        await queue.put(value)
-
-    async def populate_input_blobs(self, item: api_pb2.FunctionInput):
-        args = await blob_download(item.args_blob_id, self._client.stub)
-
-        # Mutating
-        item.ClearField("args_blob_id")
-        item.args = args
-        return item
-
-    def get_average_call_time(self) -> float:
-        if self.calls_completed == 0:
-            return 0
-
-        return self.total_user_time / self.calls_completed
-
-    def get_max_inputs_to_fetch(self):
-        if self.calls_completed == 0:
-            return 1
-
-        return math.ceil(RTT_S / max(self.get_average_call_time(), 1e-6))
-
-    @synchronizer.no_io_translation
-    async def _generate_inputs(self) -> AsyncIterator[Tuple[str, str, api_pb2.FunctionInput]]:
-        request = api_pb2.FunctionGetInputsRequest(function_id=self.function_id)
-        eof_received = False
-        iteration = 0
-        while not eof_received and self._fetching_inputs:
-            request.average_call_time = self.get_average_call_time()
-            request.max_values = self.get_max_inputs_to_fetch()  # Deprecated; remove.
-            request.input_concurrency = self._input_concurrency
-
-            await self._semaphore.acquire()
-            yielded = False
-            try:
-                # If number of active inputs is at max queue size, this will block.
-                iteration += 1
-                response: api_pb2.FunctionGetInputsResponse = await retry_transient_errors(
-                    self._client.stub.FunctionGetInputs, request
-                )
+        You can call the function using `modal run` directly from the CLI:
 
-                if response.rate_limit_sleep_duration:
-                    logger.info(
-                        "Task exceeded rate limit, sleeping for %.2fs before trying again."
-                        % response.rate_limit_sleep_duration
-                    )
-                    await asyncio.sleep(response.rate_limit_sleep_duration)
-                elif response.inputs:
-                    # for input cancellations and concurrency logic we currently assume
-                    # that there is no input buffering in the container
-                    assert len(response.inputs) == 1
-
-                    for item in response.inputs:
-                        if item.kill_switch:
-                            logger.debug(f"Task {self.task_id} input kill signal input.")
-                            eof_received = True
-                            break
-                        if item.input_id in self.cancelled_input_ids:
-                            continue
-
-                        # If we got a pointer to a blob, download it from S3.
-                        if item.input.WhichOneof("args_oneof") == "args_blob_id":
-                            input_pb = await self.populate_input_blobs(item.input)
-                        else:
-                            input_pb = item.input
-
-                        # If yielded, allow semaphore to be released via complete_call
-                        yield (item.input_id, item.function_call_id, input_pb)
-                        yielded = True
-
-                        # We only support max_inputs = 1 at the moment
-                        if item.input.final_input or self.function_def.max_inputs == 1:
-                            eof_received = True
-                            break
-            finally:
-                if not yielded:
-                    self._semaphore.release()
-
-    @synchronizer.no_io_translation
-    async def run_inputs_outputs(self, input_concurrency: int = 1) -> AsyncIterator[Tuple[str, str, Any, Any]]:
-        # Ensure we do not fetch new inputs when container is too busy.
-        # Before trying to fetch an input, acquire the semaphore:
-        # - if no input is fetched, release the semaphore.
-        # - or, when the output for the fetched input is sent, release the semaphore.
-        self._input_concurrency = input_concurrency
-        self._semaphore = asyncio.Semaphore(input_concurrency)
+        ```shell
+        modal run stub_module.py
+        ```
 
-        try:
-            async for input_id, function_call_id, input_pb in self._generate_inputs():
-                args, kwargs = self.deserialize(input_pb.args) if input_pb.args else ((), {})
-                self.current_input_id, self.current_input_started_at = (input_id, time.time())
-                yield input_id, function_call_id, args, kwargs
-                self.current_input_id, self.current_input_started_at = (None, None)
-        finally:
-            # collect all active input slots, meaning all inputs have wrapped up.
-            for _ in range(input_concurrency):
-                await self._semaphore.acquire()
-
-    async def _push_output(self, input_id, started_at: float, data_format=api_pb2.DATA_FORMAT_UNSPECIFIED, **kwargs):
-        # upload data to S3 if too big.
-        if "data" in kwargs and kwargs["data"] and len(kwargs["data"]) > MAX_OBJECT_SIZE_BYTES:
-            data_blob_id = await blob_upload(kwargs["data"], self._client.stub)
-            # mutating kwargs.
-            del kwargs["data"]
-            kwargs["data_blob_id"] = data_blob_id
-
-        output = api_pb2.FunctionPutOutputsItem(
-            input_id=input_id,
-            input_started_at=started_at,
-            output_created_at=time.time(),
-            result=api_pb2.GenericResult(**kwargs),
-            data_format=data_format,
-        )
+        Note that an explicit [`stub.run()`](/docs/reference/modal.Stub#run) is not needed, as an
+        [app](/docs/guide/apps) is automatically created for you.
 
-        await retry_transient_errors(
-            self._client.stub.FunctionPutOutputs,
-            api_pb2.FunctionPutOutputsRequest(outputs=[output]),
-            additional_status_codes=[Status.RESOURCE_EXHAUSTED],
-            max_retries=None,  # Retry indefinitely, trying every 1s.
-        )
+        **Multiple Entrypoints**
 
-    def serialize_exception(self, exc: BaseException) -> Optional[bytes]:
-        try:
-            return self.serialize(exc)
-        except Exception as serialization_exc:
-            logger.info(f"Failed to serialize exception {exc}: {serialization_exc}")
-            # We can't always serialize exceptions.
-            return None
+        If you have multiple `local_entrypoint` functions, you can qualify the name of your stub and function:
 
-    def serialize_traceback(self, exc: BaseException) -> Tuple[Optional[bytes], Optional[bytes]]:
-        serialized_tb, tb_line_cache = None, None
+        ```shell
+        modal run stub_module.py::stub.some_other_function
+        ```
 
-        try:
-            tb_dict, line_cache = extract_traceback(exc, self.task_id)
-            serialized_tb = self.serialize(tb_dict)
-            tb_line_cache = self.serialize(line_cache)
-        except Exception:
-            logger.info("Failed to serialize exception traceback.")
+        **Parsing Arguments**
 
-        return serialized_tb, tb_line_cache
+        If your entrypoint function take arguments with primitive types, `modal run` automatically parses them as
+        CLI options. For example, the following function can be called with `modal run stub_module.py --foo 1 --bar "hello"`:
 
-    @asynccontextmanager
-    async def handle_user_exception(self) -> AsyncGenerator[None, None]:
-        """Sets the task as failed in a way where it's not retried.
+        ```python
+        @stub.local_entrypoint()
+        def main(foo: int, bar: str):
+            some_modal_function.call(foo, bar)
+        ```
+
+        Currently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported. Use `modal run stub_module.py --help` for more
+        information on usage.
 
-        Used for handling exceptions from container lifecycle methods at the moment, which should
-        trigger a task failure state.
         """
-        try:
-            yield
-        except KeyboardInterrupt:
-            # Send no task result in case we get sigint:ed by the runner
-            # The status of the input should have been handled externally already in that case
-            raise
-        except BaseException as exc:
-            # Since this is on a different thread, sys.exc_info() can't find the exception in the stack.
-            traceback.print_exception(type(exc), exc, exc.__traceback__)
-
-            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
-
-            result = api_pb2.GenericResult(
-                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
-                data=self.serialize_exception(exc),
-                exception=repr(exc),
-                traceback="".join(traceback.format_exception(type(exc), exc, exc.__traceback__)),
-                serialized_tb=serialized_tb,
-                tb_line_cache=tb_line_cache,
+        if _warn_parentheses_missing:
+            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.local_entrypoint()`.")
+        if name is not None and not isinstance(name, str):
+            raise InvalidError("Invalid value for `name`: Must be string.")
+
+        def wrapped(raw_f: Callable[..., Any]) -> None:
+            info = FunctionInfo(raw_f)
+            tag = name if name is not None else raw_f.__qualname__
+            if tag in self._local_entrypoints:
+                # TODO: get rid of this limitation.
+                raise InvalidError(f"Duplicate local entrypoint name: {tag}. Local entrypoint names must be unique.")
+            entrypoint = self._local_entrypoints[tag] = _LocalEntrypoint(info, self)
+            return entrypoint
+
+        return wrapped
+
+    def function(
+        self,
+        _warn_parentheses_missing=None,
+        *,
+        image: Optional[_Image] = None,  # The image to run as the container for the function
+        schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
+        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
+        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
+        serialized: bool = False,  # Whether to send the function over using cloudpickle.
+        mounts: Sequence[_Mount] = (),  # Modal Mounts added to the container
+        network_file_systems: Dict[
+            Union[str, PurePosixPath], _NetworkFileSystem
+        ] = {},  # Mountpoints for Modal NetworkFileSystems
+        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # Mountpoints for Modal Volumes
+        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
+        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
+        memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
+        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
+        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
+        concurrency_limit: Optional[
+            int
+        ] = None,  # An optional maximum number of concurrent containers running the function (use keep_warm for minimum).
+        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
+        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
+        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
+        keep_warm: Optional[
+            int
+        ] = None,  # An optional minimum number of containers to always keep warm (use concurrency_limit for maximum).
+        name: Optional[str] = None,  # Sets the Modal name of the function within the stub
+        is_generator: Optional[
+            bool
+        ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
+        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
+        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
+        checkpointing_enabled: Optional[bool] = None,  # Deprecated
+        block_network: bool = False,  # Whether to block network access
+        max_inputs: Optional[
+            int
+        ] = None,  # Maximum number of inputs a container should handle before shutting down. With `max_inputs = 1`, containers will be single-use.
+        # The next group of parameters are deprecated; do not use in any new code
+        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
+        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
+        # Parameters below here are experimental. Use with caution!
+        _allow_background_volume_commits: bool = False,  # Experimental flag
+        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
+        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
+        _experimental_scheduler_placement: Optional[
+            SchedulerPlacement
+        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
+    ) -> Callable[..., _Function]:
+        """Decorator to register a new Modal function with this stub."""
+        if isinstance(_warn_parentheses_missing, _Image):
+            # Handle edge case where maybe (?) some users passed image as a positional arg
+            raise InvalidError("`image` needs to be a keyword argument: `@stub.function(image=image)`.")
+        if _warn_parentheses_missing:
+            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.function()`.")
+
+        if interactive:
+            deprecation_error(
+                (2024, 2, 29), "interactive=True has been deprecated. Set MODAL_INTERACTIVE_FUNCTIONS=1 instead."
             )
 
-            req = api_pb2.TaskResultRequest(result=result)
-            await retry_transient_errors(self._client.stub.TaskResult, req)
+        if image is None:
+            image = self._get_default_image()
 
-            # Shut down the task gracefully
-            raise UserException()
+        secrets = [*self._secrets, *secrets]
 
-    @asynccontextmanager
-    async def handle_input_exception(self, input_id, started_at: float) -> AsyncGenerator[None, None]:
-        """Handle an exception while processing a function input."""
-        try:
-            yield
-        except KeyboardInterrupt:
-            raise
-        except (InputCancellation, asyncio.CancelledError):
-            # just skip creating any output for this input and keep going with the next instead
-            # it should have been marked as cancelled already in the backend at this point so it
-            # won't be retried
-            logger.warning(f"The current input ({input_id=}) was cancelled by a user request")
-            await self.complete_call(started_at)
-            return
-        except BaseException as exc:
-            # print exception so it's logged
-            traceback.print_exc()
-            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
-
-            # Note: we're not serializing the traceback since it contains
-            # local references that means we can't unpickle it. We *are*
-            # serializing the exception, which may have some issues (there
-            # was an earlier note about it that it might not be possible
-            # to unpickle it in some cases). Let's watch out for issues.
-            await self._push_output(
-                input_id,
-                started_at=started_at,
-                data_format=api_pb2.DATA_FORMAT_PICKLE,
-                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
-                data=self.serialize_exception(exc),
-                exception=repr(exc),
-                traceback=traceback.format_exc(),
-                serialized_tb=serialized_tb,
-                tb_line_cache=tb_line_cache,
+        def wrapped(
+            f: Union[_PartialFunction, Callable[..., Any]],
+            _cls: Optional[type] = None,  # Used for methods only
+        ) -> _Function:
+            nonlocal keep_warm, is_generator
+
+            if isinstance(f, _PartialFunction):
+                f.wrapped = True
+                info = FunctionInfo(f.raw_f, serialized=serialized, name_override=name, cls=_cls)
+                raw_f = f.raw_f
+                webhook_config = f.webhook_config
+                is_generator = f.is_generator
+                keep_warm = f.keep_warm or keep_warm
+
+                if webhook_config:
+                    if interactive:
+                        raise InvalidError("interactive=True is not supported with web endpoint functions")
+                    self._web_endpoints.append(info.get_tag())
+            else:
+                info = FunctionInfo(f, serialized=serialized, name_override=name, cls=_cls)
+                webhook_config = None
+                raw_f = f
+
+            if not _cls and not info.is_serialized() and "." in info.function_name:  # This is a method
+                raise InvalidError(
+                    "`stub.function` on methods is not allowed. See https://modal.com/docs/guide/lifecycle-functions instead"
+                )
+
+            if is_generator is None:
+                is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)
+
+            function = _Function.from_args(
+                info,
+                stub=self,
+                image=image,
+                secret=secret,
+                secrets=secrets,
+                schedule=schedule,
+                is_generator=is_generator,
+                gpu=gpu,
+                mounts=[*self._mounts, *mounts],
+                network_file_systems=network_file_systems,
+                allow_cross_region_volumes=allow_cross_region_volumes,
+                volumes={**self._volumes, **volumes},
+                memory=memory,
+                proxy=proxy,
+                retries=retries,
+                concurrency_limit=concurrency_limit,
+                allow_concurrent_inputs=allow_concurrent_inputs,
+                container_idle_timeout=container_idle_timeout,
+                timeout=timeout,
+                cpu=cpu,
+                keep_warm=keep_warm,
+                cloud=cloud,
+                webhook_config=webhook_config,
+                enable_memory_snapshot=enable_memory_snapshot,
+                checkpointing_enabled=checkpointing_enabled,
+                allow_background_volume_commits=_allow_background_volume_commits,
+                block_network=block_network,
+                max_inputs=max_inputs,
+                _experimental_boost=_experimental_boost,
+                _experimental_scheduler=_experimental_scheduler,
+                _experimental_scheduler_placement=_experimental_scheduler_placement,
             )
-            await self.complete_call(started_at)
 
-    async def complete_call(self, started_at):
-        self.total_user_time += time.time() - started_at
-        self.calls_completed += 1
-        self._semaphore.release()
-
-    @synchronizer.no_io_translation
-    async def push_output(self, input_id, started_at: float, data: Any, data_format: int) -> None:
-        await self._push_output(
-            input_id,
-            started_at=started_at,
-            data_format=data_format,
-            status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
-            data=self.serialize_data_format(data, data_format),
-        )
-        await self.complete_call(started_at)
+            self._add_function(function)
+            return function
 
-    async def restore(self) -> None:
-        # Busy-wait for restore. `/__modal/restore-state.json` is created
-        # by the worker process with updates to the container config.
-        restored_path = Path(config.get("restore_state_path"))
-        start = time.perf_counter()
-        while not restored_path.exists():
-            logger.debug(f"Waiting for restore (elapsed={time.perf_counter() - start:.3f}s)")
-            await asyncio.sleep(0.01)
-            continue
-
-        logger.debug("Container: restored")
-
-        # Look for state file and create new client with updated credentials.
-        # State data is serialized with key-value pairs, example: {"task_id": "tk-000"}
-        with restored_path.open("r") as file:
-            restored_state = json.load(file)
-
-        # Local ContainerIOManager state.
-        for key in ["task_id", "function_id"]:
-            if value := restored_state.get(key):
-                logger.debug(f"Updating ContainerIOManager.{key} = {value}")
-                setattr(self, key, restored_state[key])
-
-        # Env vars and global state.
-        for key, value in restored_state.items():
-            # Empty string indicates that value does not need to be updated.
-            if value != "":
-                config.override_locally(key, value)
-
-        # Restore input to default state.
-        self.current_input_id = None
-        self.current_input_started_at = None
-
-        self._client = await _Client.from_env()
-        self._waiting_for_checkpoint = False
-
-    async def checkpoint(self) -> None:
-        """Message server indicating that function is ready to be checkpointed."""
-        if self.checkpoint_id:
-            logger.debug(f"Checkpoint ID: {self.checkpoint_id}")
+        return wrapped
 
-        await self._client.stub.ContainerCheckpoint(
-            api_pb2.ContainerCheckpointRequest(checkpoint_id=self.checkpoint_id)
+    def cls(
+        self,
+        _warn_parentheses_missing=None,
+        *,
+        image: Optional[_Image] = None,  # The image to run as the container for the function
+        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
+        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
+        serialized: bool = False,  # Whether to send the function over using cloudpickle.
+        mounts: Sequence[_Mount] = (),
+        network_file_systems: Dict[
+            Union[str, PurePosixPath], _NetworkFileSystem
+        ] = {},  # Mountpoints for Modal NetworkFileSystems
+        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # Mountpoints for Modal Volumes
+        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
+        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
+        memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
+        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
+        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
+        concurrency_limit: Optional[int] = None,  # Limit for max concurrent containers running the function.
+        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
+        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
+        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
+        keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm.
+        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
+        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
+        checkpointing_enabled: Optional[bool] = None,  # Deprecated
+        block_network: bool = False,  # Whether to block network access
+        _allow_background_volume_commits: bool = False,
+        max_inputs: Optional[
+            int
+        ] = None,  # Limits the number of inputs a container handles before shutting down. Use `max_inputs = 1` for single-use containers.
+        # The next group of parameters are deprecated; do not use in any new code
+        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
+        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
+        # Parameters below here are experimental. Use with caution!
+        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
+        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
+        _experimental_scheduler_placement: Optional[
+            SchedulerPlacement
+        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
+    ) -> Callable[[CLS_T], _Cls]:
+        if _warn_parentheses_missing:
+            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.cls()`.")
+
+        decorator: Callable[[PartialFunction, type], _Function] = self.function(
+            image=image,
+            secret=secret,
+            secrets=secrets,
+            gpu=gpu,
+            serialized=serialized,
+            mounts=mounts,
+            network_file_systems=network_file_systems,
+            allow_cross_region_volumes=allow_cross_region_volumes,
+            volumes=volumes,
+            cpu=cpu,
+            memory=memory,
+            proxy=proxy,
+            retries=retries,
+            concurrency_limit=concurrency_limit,
+            allow_concurrent_inputs=allow_concurrent_inputs,
+            container_idle_timeout=container_idle_timeout,
+            timeout=timeout,
+            interactive=interactive,
+            keep_warm=keep_warm,
+            cloud=cloud,
+            enable_memory_snapshot=enable_memory_snapshot,
+            checkpointing_enabled=checkpointing_enabled,
+            block_network=block_network,
+            _allow_background_volume_commits=_allow_background_volume_commits,
+            max_inputs=max_inputs,
+            _experimental_boost=_experimental_boost,
+            _experimental_scheduler=_experimental_scheduler,
+            _experimental_scheduler_placement=_experimental_scheduler_placement,
         )
 
-        self._waiting_for_checkpoint = True
-        await self._client._close()
+        def wrapper(user_cls: CLS_T) -> _Cls:
+            cls: _Cls = _Cls.from_local(user_cls, self, decorator)
 
-        logger.debug("Checkpointing request sent. Connection closed.")
-        await self.restore()
-
-    async def volume_commit(self, volume_ids: List[str]) -> None:
-        """
-        Perform volume commit for given `volume_ids`.
-        Only used on container exit to persist uncommitted changes on behalf of user.
-        """
-        if not volume_ids:
-            return
-        await asyncify(os.sync)()
-        results = await asyncio.gather(
-            *[
-                retry_transient_errors(
-                    self._client.stub.VolumeCommit,
-                    api_pb2.VolumeCommitRequest(volume_id=v_id),
-                    max_retries=9,
-                    base_delay=0.25,
-                    max_delay=256,
-                    delay_factor=2,
+            if (
+                _find_callables_for_cls(user_cls, _PartialFunctionFlags.ENTER_PRE_CHECKPOINT)
+                and not enable_memory_snapshot
+            ):
+                raise InvalidError("A class must have `enable_memory_snapshot=True` to use `snap=True` on its methods.")
+
+            if len(cls._functions) > 1 and keep_warm is not None:
+                deprecation_warning(
+                    (2023, 10, 20),
+                    "`@stub.cls(keep_warm=...)` is deprecated when there is more than 1 method."
+                    " Use `@method(keep_warm=...)` on each method instead!",
                 )
-                for v_id in volume_ids
-            ],
-            return_exceptions=True,
-        )
-        for volume_id, res in zip(volume_ids, results):
-            if isinstance(res, Exception):
-                logger.error(f"modal.Volume background commit failed for {volume_id}. Exception: {res}")
-            else:
-                logger.debug(f"modal.Volume background commit success for {volume_id}.")
 
-    async def interact(self):
-        if self._is_interactivity_enabled:
-            # Currently, interactivity is enabled forever
-            return
-        self._is_interactivity_enabled = True
-
-        if not self.function_def.pty_info:
-            raise InvalidError(
-                "Interactivity is not enabled in this function. Use MODAL_INTERACTIVE_FUNCTIONS=1 to enable interactivity."
-            )
+            tag: str = user_cls.__name__
+            self._add_object(tag, cls)
+            return cls
 
-        if self.function_def.concurrency_limit > 1:
-            print(
-                "Warning: Interactivity is not supported on functions with concurrency > 1. You may experience unexpected behavior."
-            )
+        return wrapper
 
-        # todo(nathan): add warning if concurrency limit > 1. but idk how to check this here
-        # todo(nathan): check if function interactivity is enabled
-        try:
-            await self._client.stub.FunctionStartPtyShell(Empty())
-        except Exception as e:
-            print("Error: Failed to start PTY shell.")
-            raise e
+    async def spawn_sandbox(
+        self,
+        *entrypoint_args: str,
+        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
+        mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.
+        secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
+        network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
+        timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
+        workdir: Optional[str] = None,  # Working directory of the sandbox.
+        gpu: GPU_T = None,
+        cloud: Optional[str] = None,
+        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
+        memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
+        block_network: bool = False,  # Whether to block network access
+        volumes: Dict[Union[str, os.PathLike], _Volume] = {},  # Volumes to mount in the sandbox.
+        _allow_background_volume_commits: bool = False,
+        pty_info: Optional[api_pb2.PTYInfo] = None,
+    ) -> _Sandbox:
+        """Sandboxes are a way to run arbitrary commands in dynamically defined environments.
 
-    @classmethod
-    def stop_fetching_inputs(cls):
-        assert cls._singleton
-        cls._singleton._fetching_inputs = False
+        This function returns a [SandboxHandle](/docs/reference/modal.Sandbox#modalsandboxsandbox), which can be used to interact with the running sandbox.
 
+        Refer to the [docs](/docs/guide/sandbox) on how to spawn and use sandboxes.
+        """
+        from .sandbox import _Sandbox
+        from .stub import _default_image
 
-ContainerIOManager = synchronize_api(_ContainerIOManager)
+        if self._local_app:
+            app_id = self._local_app.app_id
+            environment_name = self._local_app._environment_name
+            client = self._local_app.client
+        elif self._container_app:
+            app_id = self._container_app.app_id
+            environment_name = self._container_app._environment_name
+            client = self._container_app.client
+        else:
+            raise InvalidError("`stub.spawn_sandbox` requires a running app.")
 
+        # TODO(erikbern): pulling a lot of app internals here, let's clean up shortly
+        resolver = Resolver(client, environment_name=environment_name, app_id=app_id)
+        obj = _Sandbox._new(
+            entrypoint_args,
+            image=image or _default_image,
+            mounts=mounts,
+            secrets=secrets,
+            timeout=timeout,
+            workdir=workdir,
+            gpu=gpu,
+            cloud=cloud,
+            cpu=cpu,
+            memory=memory,
+            network_file_systems=network_file_systems,
+            block_network=block_network,
+            volumes=volumes,
+            allow_background_volume_commits=_allow_background_volume_commits,
+            pty_info=pty_info,
+        )
+        await resolver.load(obj)
+        return obj
 
-def is_local() -> bool:
-    """Returns if we are currently on the machine launching/deploying a Modal app
+    def include(self, /, other_stub: "_Stub"):
+        """Include another stub's objects in this one.
 
-    Returns `True` when executed locally on the user's machine.
-    Returns `False` when executed from a Modal container in the cloud.
-    """
-    return not _ContainerIOManager._singleton
+        Useful splitting up Modal apps across different self-contained files
 
+        ```python
+        stub_a = modal.Stub("a")
+        @stub.function()
+        def foo():
+            ...
+
+        stub_b = modal.Stub("b")
+        @stub.function()
+        def bar():
+            ...
+
+        stub_a.include(stub_b)
+
+        @stub_a.local_entrypoint()
+        def main():
+            # use function declared on the included stub
+            bar.remote()
+        ```
+        """
+        for tag, object in other_stub._indexed_objects.items():
+            existing_object = self._indexed_objects.get(tag)
+            if existing_object and existing_object != object:
+                logger.warning(
+                    f"Named app object {tag} with existing value {existing_object} is being overwritten by a different object {object}"
+                )
 
-async def _interact() -> None:
-    container_io_manager = _ContainerIOManager._singleton
-    if not container_io_manager:
-        raise InvalidError("Interactivity only works inside a Modal container.")
-    else:
-        await container_io_manager.interact()
+            self._add_object(tag, object)
 
 
-interact = synchronize_api(_interact)
+Stub = synchronize_api(_Stub)
```

## Comparing `modal/_container_io_manager.pyi` & `modal/_container_entrypoint.pyi`

 * *Files 20% similar despite different names*

```diff
@@ -1,71 +1,50 @@
-import _asyncio
-import asyncio.locks
 import asyncio.queues
+import collections.abc
+import modal.app
 import modal.client
-import modal.running_app
+import modal.functions
+import modal.stub
 import modal_proto.api_pb2
-import synchronicity.combined_types
 import typing
 import typing_extensions
 
 class UserException(Exception):
     ...
 
-class Sentinel:
-    ...
+class UserCodeEventLoop:
+    def __enter__(self):
+        ...
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        ...
 
-class _ContainerIOManager:
-    cancelled_input_ids: typing.Set[str]
-    task_id: str
-    function_id: str
-    app_id: str
-    function_def: modal_proto.api_pb2.Function
-    checkpoint_id: typing.Union[str, None]
-    calls_completed: int
-    total_user_time: float
-    current_input_id: typing.Union[str, None]
-    current_input_started_at: typing.Union[float, None]
-    _input_concurrency: typing.Union[int, None]
-    _semaphore: typing.Union[asyncio.locks.Semaphore, None]
-    _environment_name: str
-    _waiting_for_checkpoint: bool
-    _heartbeat_loop: typing.Union[_asyncio.Task, None]
-    _is_interactivity_enabled: bool
-    _fetching_inputs: bool
-    _client: modal.client._Client
-    _GENERATOR_STOP_SENTINEL: typing.ClassVar[Sentinel]
-    _singleton: typing.ClassVar[typing.Union[_ContainerIOManager, None]]
-
-    def _init(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client._Client):
+    def run(self, coro):
         ...
 
-    @staticmethod
-    def __new__(cls, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client._Client) -> _ContainerIOManager:
+
+class _FunctionIOManager:
+    def __init__(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client._Client):
         ...
 
-    @classmethod
-    def _reset_singleton(cls):
+    async def initialize_app(self) -> modal.app._ContainerApp:
         ...
 
     async def _run_heartbeat_loop(self):
         ...
 
     async def _heartbeat_handle_cancellations(self) -> bool:
         ...
 
-    def heartbeats(self) -> typing.AsyncContextManager[None]:
+    def heartbeats(self):
         ...
 
     def stop_heartbeat(self):
         ...
 
-    async def get_app_objects(self) -> modal.running_app.RunningApp:
-        ...
-
     async def get_serialized_function(self) -> typing.Tuple[typing.Union[typing.Any, None], typing.Callable]:
         ...
 
     def serialize(self, obj: typing.Any) -> bytes:
         ...
 
     def deserialize(self, data: bytes) -> typing.Any:
@@ -112,18 +91,18 @@
 
     def serialize_exception(self, exc: BaseException) -> typing.Union[bytes, None]:
         ...
 
     def serialize_traceback(self, exc: BaseException) -> typing.Tuple[typing.Union[bytes, None], typing.Union[bytes, None]]:
         ...
 
-    def handle_user_exception(self) -> typing.AsyncContextManager[None]:
+    def handle_user_exception(self) -> typing.AsyncGenerator[None, None]:
         ...
 
-    def handle_input_exception(self, input_id, started_at: float) -> typing.AsyncContextManager[None]:
+    def handle_input_exception(self, input_id, started_at: float) -> typing.AsyncGenerator[None, None]:
         ...
 
     async def complete_call(self, started_at):
         ...
 
     async def push_output(self, input_id, started_at: float, data: typing.Any, data_format: int) -> None:
         ...
@@ -133,53 +112,27 @@
 
     async def checkpoint(self) -> None:
         ...
 
     async def volume_commit(self, volume_ids: typing.List[str]) -> None:
         ...
 
-    async def interact(self):
-        ...
 
-    @classmethod
-    def stop_fetching_inputs(cls):
+class FunctionIOManager:
+    def __init__(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client.Client):
         ...
 
+    class __initialize_app_spec(typing_extensions.Protocol):
+        def __call__(self) -> modal.app.ContainerApp:
+            ...
 
-class ContainerIOManager:
-    cancelled_input_ids: typing.Set[str]
-    task_id: str
-    function_id: str
-    app_id: str
-    function_def: modal_proto.api_pb2.Function
-    checkpoint_id: typing.Union[str, None]
-    calls_completed: int
-    total_user_time: float
-    current_input_id: typing.Union[str, None]
-    current_input_started_at: typing.Union[float, None]
-    _input_concurrency: typing.Union[int, None]
-    _semaphore: typing.Union[asyncio.locks.Semaphore, None]
-    _environment_name: str
-    _waiting_for_checkpoint: bool
-    _heartbeat_loop: typing.Union[_asyncio.Task, None]
-    _is_interactivity_enabled: bool
-    _fetching_inputs: bool
-    _client: modal.client.Client
-    _GENERATOR_STOP_SENTINEL: typing.ClassVar[Sentinel]
-    _singleton: typing.ClassVar[typing.Union[ContainerIOManager, None]]
-
-    def __init__(self, /, *args, **kwargs):
-        ...
-
-    def _init(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client.Client):
-        ...
+        async def aio(self, *args, **kwargs) -> modal.app.ContainerApp:
+            ...
 
-    @classmethod
-    def _reset_singleton(cls):
-        ...
+    initialize_app: __initialize_app_spec
 
     class ___run_heartbeat_loop_spec(typing_extensions.Protocol):
         def __call__(self):
             ...
 
         async def aio(self, *args, **kwargs):
             ...
@@ -191,35 +144,20 @@
             ...
 
         async def aio(self, *args, **kwargs) -> bool:
             ...
 
     _heartbeat_handle_cancellations: ___heartbeat_handle_cancellations_spec
 
-    class __heartbeats_spec(typing_extensions.Protocol):
-        def __call__(self) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
-            ...
-
-        def aio(self) -> typing.AsyncContextManager[None]:
-            ...
-
-    heartbeats: __heartbeats_spec
+    def heartbeats(self):
+        ...
 
     def stop_heartbeat(self):
         ...
 
-    class __get_app_objects_spec(typing_extensions.Protocol):
-        def __call__(self) -> modal.running_app.RunningApp:
-            ...
-
-        async def aio(self, *args, **kwargs) -> modal.running_app.RunningApp:
-            ...
-
-    get_app_objects: __get_app_objects_spec
-
     class __get_serialized_function_spec(typing_extensions.Protocol):
         def __call__(self) -> typing.Tuple[typing.Union[typing.Any, None], typing.Callable]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.Tuple[typing.Union[typing.Any, None], typing.Callable]:
             ...
 
@@ -327,27 +265,27 @@
     def serialize_exception(self, exc: BaseException) -> typing.Union[bytes, None]:
         ...
 
     def serialize_traceback(self, exc: BaseException) -> typing.Tuple[typing.Union[bytes, None], typing.Union[bytes, None]]:
         ...
 
     class __handle_user_exception_spec(typing_extensions.Protocol):
-        def __call__(self) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
+        def __call__(self) -> typing.Generator[None, None, None]:
             ...
 
-        def aio(self) -> typing.AsyncContextManager[None]:
+        def aio(self) -> typing.AsyncGenerator[None, None]:
             ...
 
     handle_user_exception: __handle_user_exception_spec
 
     class __handle_input_exception_spec(typing_extensions.Protocol):
-        def __call__(self, input_id, started_at: float) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
+        def __call__(self, input_id, started_at: float) -> typing.Generator[None, None, None]:
             ...
 
-        def aio(self, input_id, started_at: float) -> typing.AsyncContextManager[None]:
+        def aio(self, input_id, started_at: float) -> typing.AsyncGenerator[None, None]:
             ...
 
     handle_input_exception: __handle_input_exception_spec
 
     class __complete_call_spec(typing_extensions.Protocol):
         def __call__(self, started_at):
             ...
@@ -389,42 +327,52 @@
             ...
 
         async def aio(self, *args, **kwargs) -> None:
             ...
 
     volume_commit: __volume_commit_spec
 
-    class __interact_spec(typing_extensions.Protocol):
-        def __call__(self):
-            ...
 
-        async def aio(self, *args, **kwargs):
-            ...
+def call_function_sync(function_io_manager, imp_fun: ImportedFunction):
+    ...
+
 
-    interact: __interact_spec
+async def call_function_async(function_io_manager, imp_fun: ImportedFunction):
+    ...
+
+
+class ImportedFunction:
+    obj: typing.Any
+    fun: typing.Callable
+    stub: typing.Union[modal.stub._Stub, None]
+    is_async: bool
+    is_generator: bool
+    data_format: int
+    input_concurrency: int
+    is_auto_snapshot: bool
+    function: modal.functions._Function
 
-    @classmethod
-    def stop_fetching_inputs(cls):
+    def __init__(self, obj: typing.Any, fun: typing.Callable, stub: typing.Union[modal.stub._Stub, None], is_async: bool, is_generator: bool, data_format: int, input_concurrency: int, is_auto_snapshot: bool, function: modal.functions._Function) -> None:
         ...
 
+    def __repr__(self):
+        ...
 
-def is_local() -> bool:
-    ...
+    def __eq__(self, other):
+        ...
 
 
-async def _interact() -> None:
+def import_function(function_def: modal_proto.api_pb2.Function, ser_cls, ser_fun, ser_params: typing.Union[bytes, None], function_io_manager, client: modal.client.Client) -> ImportedFunction:
     ...
 
 
-class __interact_spec(typing_extensions.Protocol):
-    def __call__(self) -> None:
-        ...
+def call_lifecycle_functions(event_loop: UserCodeEventLoop, function_io_manager, funcs: collections.abc.Iterable[typing.Callable]) -> None:
+    ...
 
-    async def aio(self, *args, **kwargs) -> None:
-        ...
 
-interact: __interact_spec
+def main(container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client.Client):
+    ...
 
 
-MAX_OUTPUT_BATCH_SIZE: int
+MAX_OUTPUT_BATCH_SIZE: 'int'
 
-RTT_S: float
+RTT_S: 'float'
```

## Comparing `modal-0.62.87.dist-info/LICENSE` & `modal-0.62.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `modal-0.62.87.dist-info/METADATA` & `modal-0.62.9.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: modal
-Version: 0.62.87
+Version: 0.62.9
 Summary: Python client library for Modal
 Author: Modal Labs
 Author-email: support@modal.com
 Project-URL: Homepage, https://modal.com
 Keywords: modal,client,cloud,serverless,infrastructure
 Classifier: Topic :: System :: Distributed Computing
 Classifier: Operating System :: OS Independent
@@ -15,17 +15,17 @@
 License-File: LICENSE
 Requires-Dist: aiohttp
 Requires-Dist: aiostream (~=0.5.2)
 Requires-Dist: certifi
 Requires-Dist: click (>=8.1.0)
 Requires-Dist: fastapi
 Requires-Dist: grpclib (==0.4.7)
-Requires-Dist: protobuf (!=4.24.0,<6.0,>=3.19)
+Requires-Dist: protobuf (!=4.24.0,<5.0,>=3.19)
 Requires-Dist: rich (>=12.0.0)
-Requires-Dist: synchronicity (~=0.6.6)
+Requires-Dist: synchronicity (~=0.6.3)
 Requires-Dist: toml
 Requires-Dist: typer (~=0.9.0)
 Requires-Dist: types-certifi
 Requires-Dist: types-toml
 Requires-Dist: watchfiles
 Requires-Dist: typing-extensions (~=4.6)
```

## Comparing `modal-0.62.87.dist-info/RECORD` & `modal-0.62.9.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,203 +1,197 @@
-modal/__init__.py,sha256=6W6XMQXIVGGIZVUXHk5xrk1vrllk6OndXMs_SC-zjX8,2100
+modal/__init__.py,sha256=IM8hIP9NlXodRITGUlnQDpnyGuOmmJEo7M6Ci7FVQY8,2102
 modal/__main__.py,sha256=EKalcwy_6N0L5iqIvlYpXihi3zxy9HNuUMvnbraoTrk,1141
 modal/_asgi.py,sha256=CEiJv4CBgJ30UtZ6KTlisNSwRJswV_ccDTMUDUn-g_8,15476
-modal/_container_entrypoint.py,sha256=axJu_riLjkcjwgYauBLq9PpcirbwIIVQlCcBuSp_sTQ,28448
+modal/_container_entrypoint.py,sha256=YilbMiHSVMlZd7HjuqRGgKFWLO9GjqqqCl0rt5L-CvI,52214
+modal/_container_entrypoint.pyi,sha256=pYKqWTac9q1VPQT6D62YOok6mwMIvgbGWv5wx5MYGek,11148
 modal/_container_exec.py,sha256=FiaSBUD0UkQRF8hW0AFf0rbqC8lb87f3EGeg7Mr5dx4,4349
-modal/_container_io_manager.py,sha256=eISAzNK-3mU3z_fnW_k9aJo0GOLdwS9LSoHuwW-8G30,28709
-modal/_container_io_manager.pyi,sha256=BALNBQ4JKjOC8SSkyGV1SM6sjPIqGXWGJAHLLhH9cU4,12668
 modal/_ipython.py,sha256=HF_DYy0e0qM9WnGDmTY30s1RxzGya9GeORCauCEpRaE,450
 modal/_location.py,sha256=_SdCPzVOl7HRwLWIxzTJjhdpOo1sIl4flfEJ-bAbiDE,929
 modal/_output.py,sha256=1GXKp-Qr-C5oe0e4A5epTgcIFhcp5Cko3HG_ircLFWg,20565
 modal/_proxy_tunnel.py,sha256=gnKyCfmVB7x2d1A6c-JDysNIP3kEFxmXzhcXhPrzPn0,1906
 modal/_pty.py,sha256=GhzrHKZpoI-YHMDN7LoySlSYLpoJ4yGPSF-fqiNsFrM,1336
-modal/_resolver.py,sha256=FAkA_3_VK74MEiNOmYH9s2mThD38Z7QNWhNNY3r7qcI,7077
-modal/_resources.py,sha256=4pOUFIgF2nwiKm744Q49fU4ZBZABywOAScttxUhGAgs,1142
+modal/_resolver.py,sha256=_Wi3MbgOCnYfFv9ytHdanpZQvm51oqWdoNEKRsTNgZc,7208
 modal/_sandbox_shell.py,sha256=OtkeMqOymFVqDBWt3kmT6_4mslPu0ZCfjqYjwUb6cGE,1658
-modal/_serialization.py,sha256=evTjYWEqh3A7dGnvrFf8sPRyYgn53rBUPHL7QJLyw2o,12133
+modal/_serialization.py,sha256=nC2-Wz4tyg1va_4l2SutdiLaW0glmONxxnf4er0E4eY,12329
 modal/_traceback.py,sha256=l6y-flU7rQK3YelSthHv4Paurw3rA5q9KofBRRVHCVM,10029
 modal/_tunnel.py,sha256=acWThUm-ct-Ry_nvytxEhCH-djdttGC_qbDdz2y4ddM,5073
 modal/_tunnel.pyi,sha256=MhcFhkpt-Fo9dA59qhXtoNA9ktyJzwtmGj0vWaUdFiM,1337
 modal/_watcher.py,sha256=RWkOgkg3FXpa1RgNr86ZrdrMpD9RB6I1_uuUMy7_yjY,3609
-modal/app.py,sha256=lHMTFiEK76QGGfKtWcTB0BbYiwvcvsuLEaZTv2pzYRs,35268
-modal/app.pyi,sha256=vgD1uIUoU73eLuNt37ZIWJ-x9Xawwc1xrFqfcVC5QSY,18642
-modal/app_utils.py,sha256=EuRot0Xhrq5m7bQgM8yVZkRxiKfnBMJSFguu-lsJ5Vs,748
-modal/app_utils.pyi,sha256=9u0YQMoIt158Uh-HxUGrnuBeMFdBLdi_DLHHZNSGU-M,613
+modal/app.py,sha256=FYFIFoKPBt8vCRfNNEUmTbtJIOD637jkzjXpvBS7k9k,15555
+modal/app.pyi,sha256=PkwSmPsDnXXD0ooZtVdynC5xXI95rUhEgbtBIJy0lxk,8360
 modal/call_graph.py,sha256=KhqmbJPlcpL-PO0N_lrK8VHoXK_xAHq5IhZ9A9rW9Zk,2524
-modal/client.py,sha256=ORQojL4QndWBnCB8MjNsv8OAPbKVp4X1od9ESHRW1-g,10828
-modal/client.pyi,sha256=paxFVJMCBCTHgEdONI8ZWU5NMg3tl1IiRlfwoLc1mfw,4006
-modal/cloud_bucket_mount.py,sha256=H7tbC4tH22mOm3t41Zz5KCtKBmKir10xYOftR0u7o0Y,5550
-modal/cloud_bucket_mount.pyi,sha256=HQy0m3uXIziIrTbftNY_tja5m0tlYro1Q5vQ2fQq2lM,1217
-modal/cls.py,sha256=qjpzsWBGDQzSwpKCKW1o1mO9mdaaUcz1AFQomj_8Po8,12738
-modal/cls.pyi,sha256=Wv4AqJ96Hc8YOUx75z28HCXpsN8rm6WLcMjJvNtK_Rs,6525
-modal/config.py,sha256=_5PGiL2HsxDAL1THv1-FpZXvaiFIaNnT0xd1vGjBhTI,9980
-modal/dict.py,sha256=fPJaHaAiJQauYBvHIrtgHV_qmTCzY7qkqHebqbN5CWs,10777
+modal/client.py,sha256=BO2i7-tCPtnSBLYmuRm8xSVpYWYc4Z51dNoDrSJLiN0,9751
+modal/cloud_bucket_mount.py,sha256=gti25rJPIfeapokZixcsQ-f19l0gSDQrRAquSRQPd_I,2942
+modal/cloud_bucket_mount.pyi,sha256=8J673RncR2-JqjXnXA65O7PZe8VzfU46INEu6k8flkc,1418
+modal/cls.py,sha256=x-oGCqBZi45-zX1bNIvMsHKmrxOhkZZciwrB6B9g2IA,12799
+modal/cls.pyi,sha256=3cpx8HcGgRImPn6fsaq9GSWKicEc_p__2UK2q5xP_Z8,6482
+modal/config.py,sha256=8FfI-0LTPNa0kSz5lPLjQQBnlc27uOF30M-GRH1tRrM,9934
+modal/dict.py,sha256=hZsP-5rXnJ3CNKudKHYfhZwSjD8GqWlA8fxDf51Prck,10194
 modal/dict.pyi,sha256=6BZ8LU4NXrrA_amPbTf1tgPn0FgH64cr2oJnQ4Zr4i4,5809
 modal/environments.py,sha256=xOsBpVpjyBwA-0PVYAQAV_qUtEMxnYzu3Qpmkcqsbeo,2452
 modal/environments.pyi,sha256=HMSB6AeWCXeQcrsEyNiWwSzou3lD0G6h6R_4-AFU3gg,1439
-modal/exception.py,sha256=TtBYDwV3_JfAVGBIKcmaeyV28CFbsVsRwtVhVdd8BuE,5868
-modal/experimental.py,sha256=-nXs5bQoFlQHsJWf6YfIO41BrMoYJuVKr1Pil-EUOOU,316
-modal/functions.py,sha256=TWAxJU8Ov8Rb3_Q6lfiPIo0jobSABfXWJnQ_KfFu63M,67936
-modal/functions.pyi,sha256=aKGi_l3y9YX9kRuVjxW9QBjdzEl6dq4n45THNhUVSls,23960
+modal/exception.py,sha256=h-LDmGhdwViC_yxBulNQHJpwhuKnkh40fAbwX-Y4D0c,5865
+modal/experimental.py,sha256=R-3pEE_X_NB891MIaUxWMkqlW7FMjpmqp2IrI_Tds10,293
+modal/functions.py,sha256=vG8qmvdl0k1D-BdNmaEVZYsyIs6cUCBqOnQ2Ns1yxKc,63450
+modal/functions.pyi,sha256=biuQhH788mNh_0tGL3JhM_hkywFme6IuYhrW7tvYhk8,19548
 modal/gpu.py,sha256=QDfghe3SgOecMXAc8DRbUuGKhhaLeuG8pVEpO-rQvA0,8038
-modal/image.py,sha256=a1_Ps__TG6vCQ5y32wy7Kfjzske_Ju8Xmmq1-rlkWy4,68298
-modal/image.pyi,sha256=hA0O-YEJKOyyUo3ie9AaNpCgnkjUBXhTIfx0xcfSlMo,18339
-modal/mount.py,sha256=Kgu6NkWsSVfnEGEgpSjopxanrkx_qfAV59JnWGGpm3I,23177
+modal/image.py,sha256=JWYROEpW1wuX7SO752yXkmTbBw12P4MV2YZHkPPF1jI,62658
+modal/image.pyi,sha256=0oF1q7sTr5ce2KZxNz5eVNXlP48P8fe8HmUUNsYKBmQ,17457
+modal/mount.py,sha256=FTsWO_IgcJXlq1tCIWNMH2CuFH-GJT5Tzrz3-zN7cj8,23182
 modal/mount.pyi,sha256=xhLXRhKkMmlBiSiNNtA03_4S8m1NzUW6TUhM2FpME2k,9588
-modal/network_file_system.py,sha256=yt_GNgUln7PCXUS4fNo9WPS6EnAp5446gIF3SeV3BUQ,14380
-modal/network_file_system.pyi,sha256=TI04zO5AUG58HwPIGCD0aB3G53DjUx-XdZy3EkbxvDo,6280
-modal/object.py,sha256=Ja5F9kJlATJvJM7WGBG26cQwW--DICSq8N0aSJL2oTA,8329
+modal/network_file_system.py,sha256=iOj897r3SXbv4fnWiGjusYAyS8VQoIpjRGboFQ9Psds,14356
+modal/network_file_system.pyi,sha256=2Bp38k_zZsN8UhhsJ8dF3GpKEpXqhcsVNSLG1hIaxJw,6404
+modal/object.py,sha256=T5ORM_18kMSrVMy2wWsfefXzZbgaGdGdTFOl2c4LSgc,8258
 modal/object.pyi,sha256=NVYTuTldnB5j8724Qx-dd0R8tWeuXBfh9IGSBMaw2aw,7573
 modal/partial_function.py,sha256=n4qGXD_DdZlWJNNzjwXoh2ovX3Oxl4XhrfnmzAPBdw4,19988
-modal/partial_function.pyi,sha256=1nfByULFi2rNIiCaFQR3YTv-FwD76aHpN_7iiO1o30E,6172
+modal/partial_function.pyi,sha256=U-hSV_-KXRoaygOMGd6G3kBaMMbwrHqXctrbgH9t_WY,6270
 modal/proxy.py,sha256=ey5IlDkPfb0zHGSsIQjgtFsLZwdjCoCp-ecIJlbDo5w,1307
 modal/proxy.pyi,sha256=Db8boRetc0K7sAUuKw-mg4eoRAX0GB7vyHOHlwuCEs8,428
 modal/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-modal/queue.py,sha256=BZVTc4IYUIgQu1v4VzIluyvn7oXLriNWgKVVFyUN-6s,17136
-modal/queue.pyi,sha256=FytGwS0VK2jUffqZb90cAIuQR7-bcykazijgbm4ohe4,7397
+modal/queue.py,sha256=c8JJZc8wjjYofXVkTyRNCIdMGa7ttzN43R_iThWPazw,12296
+modal/queue.pyi,sha256=dLrgajEzVAx74ekC56MuZ3n0qEhCezClOH4-MDRDRSU,5673
+modal/requirements.312.txt,sha256=zWWUVgVQ92GXBKNYYr2-5vn9rlnXcmkqlwlX5u1eTYw,400
+modal/requirements.txt,sha256=OjsbXFkCSdkzzryZP82Q73osr5wxQ6EUzmGcK7twfkA,502
 modal/retries.py,sha256=bRDYRTUNFzmjRAggCcoMViqwu-PNnqbokc_QYCf_2nc,3730
-modal/runner.py,sha256=N2XxMWzZNVFYxBq4GbOLxY-l5bRnUzejrHZeaBOXVBk,19091
-modal/runner.pyi,sha256=JlwrUukT0NGQWVz8KDn-0X3edalbTPKCpqM8uFYErwc,5753
-modal/running_app.py,sha256=MRLSMzdAryNbDTI9ox0KbY4X8WBsCxZnhdYje9PP6Ts,461
-modal/sandbox.py,sha256=TL7t0dFRBJdm0eCn8B1Yp_l2Uh48Ait8B0T1GujdaPM,15040
-modal/sandbox.pyi,sha256=LMiJPsxEfw-w9omPCpWkohZBUONN4AJ1fN_chuDPmKM,6514
+modal/runner.py,sha256=IAO4oyybCTDSQY-eZFY_VKjifSHz5CUCmKZ5a4bub2E,12295
+modal/runner.pyi,sha256=3BxRQ-P-5DRvzz_ufaQiu6hpuGbhaomha7JNdaYFEI8,3114
+modal/sandbox.py,sha256=QzqU08-_iF3W6yKuvzrP11mwvFs8j1zDRhu8EZt22u4,15301
+modal/sandbox.pyi,sha256=MDvAfhxeQckL6CDYo0ccDl_Dxv2MsdY3pLO06MXav8Y,6466
 modal/schedule.py,sha256=GjvGQxXhAf0oPvHlwyWsppanel7LrSBuarWEINB_xTY,2621
 modal/scheduler_placement.py,sha256=15NhfoI1W6qklc8BabM5t19oVNhx9ZbPByaQO7YWSYw,662
-modal/secret.py,sha256=gzCDPPozRH9Yey17f1Zu2yy_vVdVSWy6cZjVZXJ_jcY,8916
+modal/secret.py,sha256=GKyTm8Bo7DAvCyzG8nSr4bIaWyOf-egEp8CDHlgewe0,8898
 modal/secret.pyi,sha256=kh0LL3VkJpZHQc71c-6xE2HcnrgSsw9xOHMDWXMatwk,2225
-modal/serving.py,sha256=YMHrWH1lcoKSgsJqRSBnWMtpgGWvyRDpEYv67NGDKRo,4800
-modal/serving.pyi,sha256=Sr_7eADe4cCg3cZqnKj0HsDgeytsWqh9KJk2XXMtt9c,2977
+modal/serving.py,sha256=h0gPQYbLixn8qP64OLJjAGdcuCbbS2sCZqTn9-0etrA,4681
+modal/serving.pyi,sha256=S5yq9KgrC2ece1gBzPFlwas2XEFTqnfA6YSDf3bjl5U,1957
 modal/shared_volume.py,sha256=vB-QLl7EMeusFWT91bVxsmYhzlBNYYYJukLmbF_5Rgk,888
 modal/shared_volume.pyi,sha256=JSrQyY3M0nn6cbhEV_Xd8AqN7VdzWr2_jsHEGH7HYOQ,405
-modal/token_flow.py,sha256=M28JpcN2KCeIaXmlGqtj6NBPcb8qvjEd59_ojR3tMDo,6742
+modal/stub.py,sha256=heGve7FVwsnnv4Nn8QLgU_4rL2uEbEb4wKPYg9GlGuQ,33852
+modal/stub.pyi,sha256=egq2GR_n9FCtKwmb4FwHcQjb41VvarDmRzzy5E-EKhM,17567
+modal/token_flow.py,sha256=sCsSCtBDuKxJXyNbUPbSW7uc2Q7elUkkfsjIzpFilWw,6771
 modal/token_flow.pyi,sha256=F5_ty7M18ulmLG9I-DJoqcg7CrbEgLDndEWvL_p9708,1890
-modal/volume.py,sha256=KAoNUZUt9AcDdflCuo781V0tqR43_wotAoUEZ9v-j9E,28278
-modal/volume.pyi,sha256=h5NY-9l_FDCn_2-w9xepr60pdPXzt4Dhbr1xDUj_SGU,9748
+modal/volume.py,sha256=YrZ3bAN_YwXaEknXl-f9XSGGWktjQewewFKBJd6ui0c,24410
+modal/volume.pyi,sha256=OpJxb-xQX0gCnnfcX-etFwE1zNkF5O1LqpAbfaI59o4,9111
 modal/_utils/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
 modal/_utils/app_utils.py,sha256=Uf9oNKcC6wJg3wcHNygWxMlkXuIMVRYIE5nutDSafkQ,465
-modal/_utils/async_utils.py,sha256=9w5DHi61IwHA18z1_gHgHyo4U7rgQXg7S9OITNYNJr0,16049
+modal/_utils/async_utils.py,sha256=XqGtqrgtOFsW-NBAIxBgKpGSRXWoBbHPlB-f4LSDN58,12930
 modal/_utils/blob_utils.py,sha256=oHWcx13W9NNXx0Das_x50jvVbA29jpGnLJM7c74WfxQ,15054
-modal/_utils/function_utils.py,sha256=hLFTGZ20k8ZV8Od-VqzTcZapO2NyqU87kxnh4XfTJiw,15339
-modal/_utils/grpc_testing.py,sha256=n_HsO6pkTx2CStvGvLCl2-Rrj25X35wQ8Jln83QUySM,7927
-modal/_utils/grpc_utils.py,sha256=HinIKh1t4GznJwQ_OS1SDWLWa3IiV_uBdePXJSJx_dk,9518
-modal/_utils/hash_utils.py,sha256=HefF7zPQPxFxyx3fpz-AdSm4QsHZNNvgL9-iQHY-_F4,1790
+modal/_utils/function_utils.py,sha256=Orkons1_SFHxfI7NIE3-O-FhQ9wA1CxjC_or4PR_bm8,13520
+modal/_utils/grpc_testing.py,sha256=LLwhZ4XJgnWSxnTEFd1OxgUjJ-z91xrMiWm9i44H54Q,7780
+modal/_utils/grpc_utils.py,sha256=OssGLNWGUpc8PHxd8UvTGvZAalW9BKgJ8phKeZlm_SM,9322
+modal/_utils/hash_utils.py,sha256=YOE1LZwyqaTQSZajrF-H_5_iJkjxMclJewC0SgH_8xA,1597
 modal/_utils/http_utils.py,sha256=DGKvrSQxAHjP_LNdM6EaL-TIQPGDX5vt3gcMGg2xkz4,1426
 modal/_utils/logger.py,sha256=0QvxZpyhhZwKZ5xOcMC9btS_XBE1wAKhtQmISU3gdd0,1311
-modal/_utils/mount_utils.py,sha256=f2q_tlDLVHcttJBkuoGfy99G-krZ-s5vZARi-bYzv-E,2341
+modal/_utils/mount_utils.py,sha256=oAbmYe3RT29EoysJou_NmmmQ5FwxkdWd2qbOK1cZgXA,2327
 modal/_utils/package_utils.py,sha256=mh5zRXmY9wSmvqaCqFBAdPntLP1EmSkDWJt3GB4ey4s,1640
-modal/_utils/rand_pb_testing.py,sha256=_dRz09XhVoY9ZO7SN7xqi0MDVT22sY_ONnS0ts_3sUg,3857
+modal/_utils/rand_pb_testing.py,sha256=KESWmCi8lwmRNllknQKM2Y67bm7XwRIFZUW_X2mOesE,3871
 modal/_utils/shell_utils.py,sha256=_nQIZb4jbwcfjg-qCLgtlXEx2I9TCsmY1G0IQY8fFg4,3633
 modal/_vendor/__init__.py,sha256=MIEP8jhXUeGq_eCjYFcqN5b1bxBM4fdk0VESpjWR0fc,28
 modal/_vendor/a2wsgi_wsgi.py,sha256=2AnQcS5VhLZxod0trVNxnIYGH1SRbzT3dmJ4owVXzxA,22144
 modal/_vendor/cloudpickle.py,sha256=CcpkVlNqP3rtFiPK1Ffpub_i0bc7EThN8kU7nC0WXlc,55225
 modal/_vendor/tblib.py,sha256=g1O7QUDd3sDoLd8YPFltkXkih7r_fyZOjgmGuligv3s,9722
 modal/cli/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
-modal/cli/_download.py,sha256=TMlfUFfo-YS18SqUciBKtMIBaVBj4N6AO24I35fgjSU,2234
-modal/cli/app.py,sha256=KDadUG5ZP9IALjeQ2a4p0L5TOewhVhuL-W13niONko0,3041
+modal/cli/_download.py,sha256=wPhkhENC24x2cZcdkeaKdAacncNXIhIZcrEUPpdhsEU,2601
+modal/cli/app.py,sha256=C-4a7FJY6-KbS-blwqHKfk0rKo2zYbcnm6KMwsS_9J4,3068
 modal/cli/config.py,sha256=cSTH2oy0HTT-8GgH0tMvjqMKk_zYi_UcRGqRXkqCH3w,1294
 modal/cli/container.py,sha256=LC9VnnidDmMF0lr-MclewJAqqByVdC-CQqi6kdlIwsE,1748
 modal/cli/entry_point.py,sha256=aUB_UYJeqbapCBN0bX4SyCagk1PIm94-oPan2PIsAwk,3538
 modal/cli/environment.py,sha256=Jgen64Wu6MUdvX8iDoqE3aJtiCD8PiHVjd3dfDOjT_U,3414
-modal/cli/import_refs.py,sha256=LJxySja_NrCUEA59sBmSszv6_Lx43oNqjn0nuqjy_us,9529
-modal/cli/launch.py,sha256=Y1qiy_Q-25puFt4bmlfnD4XorBNxiKsevq6Yqq2Z7CY,2136
-modal/cli/network_file_system.py,sha256=5aNRGbHFZEVWmuLvEUkVyttoHch2XgBl4wv7OwX-kSQ,8256
+modal/cli/import_refs.py,sha256=OEVxtlarpItVu9ifi8V_Lt3Fmr3MKrufnBz0pKH684c,9083
+modal/cli/launch.py,sha256=qunqsZ26u60tY5fLAz1hneWGXfkJHBIML_qnUf5u2UY,1702
+modal/cli/network_file_system.py,sha256=wQDlACewu7ucYRAKvgW2r1SlhTJnXYhsT1tvEVbDf6I,8449
 modal/cli/profile.py,sha256=s4jCYHwriOorEFCKxeGZoSWX8rXTR_hDTNFZhOA565s,3109
-modal/cli/run.py,sha256=c83V78I79HBU86tWeY_RqhsGZpRw6OoqyFcSj-uRTac,13769
+modal/cli/run.py,sha256=3Hbztt4nw5PrJdNpextY5jg-m9rON-G4BTZOWc2p0iE,13798
 modal/cli/secret.py,sha256=oylN52070uWS4NLT8bbHmt93KWV1DBmmuAWds677amE,4181
 modal/cli/token.py,sha256=Vsxy1ViEfuauWgPnpV8R3mRtKFQUtNmmTc2FYOY82Tg,1875
 modal/cli/utils.py,sha256=GUG0Vb_hYv6yEGm2Q9fM84PnRniFo2W2FygN7lyWm8c,1330
-modal/cli/volume.py,sha256=6X_RdLsttT_LqP3nkSrKiwBRwPrnHeRGVYIKSFRBJQI,10775
+modal/cli/volume.py,sha256=W8dMgYMJ3x7VDRpWJf1bI5lEnn4kNgWyQrVnxiRSMOQ,10871
 modal/cli/programs/__init__.py,sha256=svYKtV8HDwDCN86zbdWqyq5T8sMdGDj0PVlzc2tIxDM,28
-modal/cli/programs/run_jupyter.py,sha256=UqUIE6PY6RfcdsRgwTRfAq3sLGDZhnwPwK5sxTUdol8,2143
-modal/cli/programs/vscode.py,sha256=EAmPQ3Q9eKojrvdIvawudNZevJt3IjcdfPTrfIVvSdE,1898
+modal/cli/programs/run_jupyter.py,sha256=FJ6tmscw2CNaEZeD-14VUDZPMvUuF3T13emGPHpm5bk,2010
+modal/cli/programs/vscode.py,sha256=sh7iGUqOHOplsow9-2iopHT0jsiPW3ie1CwZAq3tqOM,1765
 modal/extensions/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
 modal/extensions/ipython.py,sha256=Q66tGs_PWzuL1M6zZ8yNplv25qHog_aXyy8NED0csh0,988
-modal/requirements/2023.12.312.txt,sha256=zWWUVgVQ92GXBKNYYr2-5vn9rlnXcmkqlwlX5u1eTYw,400
-modal/requirements/2023.12.txt,sha256=OjsbXFkCSdkzzryZP82Q73osr5wxQ6EUzmGcK7twfkA,502
-modal/requirements/2024.04.txt,sha256=ahcvUgDTsw7slMDPneAX6rZdeIc6fsW00ZzODccAC4A,520
 modal_docs/__init__.py,sha256=svYKtV8HDwDCN86zbdWqyq5T8sMdGDj0PVlzc2tIxDM,28
 modal_docs/gen_cli_docs.py,sha256=c1yfBS_x--gL5bs0N4ihMwqwX8l3IBWSkBAKNNIi6bQ,3801
 modal_docs/gen_reference_docs.py,sha256=g37KpE5P5ZxZl-vj0ie3Ewx1waTRMKCGIXQ_iYSy0e0,6555
 modal_docs/mdmd/__init__.py,sha256=svYKtV8HDwDCN86zbdWqyq5T8sMdGDj0PVlzc2tIxDM,28
 modal_docs/mdmd/mdmd.py,sha256=F9J0KdYVz8WmdLDnInTIlm8SmNZJLAiu3ZAZcxVIZ0k,6268
-modal_docs/mdmd/signatures.py,sha256=Jqy5AosHsQLAQJJe5cgYbciyFvb8xVwPIYwyBn-6RzU,3243
+modal_docs/mdmd/signatures.py,sha256=cTum8S5ydixC_HaniltEoTwadtgQU9IJrE4FwF9XzQg,3095
 modal_global_objects/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
 modal_global_objects/images/__init__.py,sha256=MIEP8jhXUeGq_eCjYFcqN5b1bxBM4fdk0VESpjWR0fc,28
 modal_global_objects/images/conda.py,sha256=5Tqd_62p7zGwVezJj1Qp2Vcxtt2WHWVBMzMNbjuNW-M,324
 modal_global_objects/images/debian_slim.py,sha256=9iB8L0tuEDWas3Mge1jyGvfBXiKsJpcjqJT5M1fWRos,330
 modal_global_objects/images/micromamba.py,sha256=k5m5P4xSBzliiTrdMITHx6iLhdQGIBuhPnP65JlBdG0,329
 modal_global_objects/mounts/__init__.py,sha256=MIEP8jhXUeGq_eCjYFcqN5b1bxBM4fdk0VESpjWR0fc,28
 modal_global_objects/mounts/modal_client_package.py,sha256=W0E_yShsRojPzWm6LtIQqNVolapdnrZkm2hVEQuZK_4,767
 modal_global_objects/mounts/python_standalone.py,sha256=_vTEX3PECUsatzhDs8lyJmDK0LbFetT1sJB6MIDfFAo,1870
 modal_proto/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
-modal_proto/api.proto,sha256=r5yfVhNpSCveGAVOkzvJaaP1LrjS4mh3taQnbXFnbQw,56704
-modal_proto/api_grpc.py,sha256=-YaCdYIMUPYMArNyLGOs2ie3FS0MblcePF2EwhZOIS0,85985
-modal_proto/api_pb2.py,sha256=6EZ2ky0RTzWXTdwshReo6IpWbA0YVqnISdabAJY2G6I,219214
-modal_proto/api_pb2_grpc.py,sha256=DIYSL60RpXh7DW0fx4kBAGyaMEYCTFgrSwuSXqdh3Nw,185802
+modal_proto/api.proto,sha256=C12wjpZgioFcFVnRDbirdkyVsvs9Wo-b20M9utOV8NY,56021
+modal_proto/api_grpc.py,sha256=3eorhXe6eSERza1QPwaGDFui5ClzVoGkO-JPL2c_tEU,85328
+modal_proto/api_pb2.py,sha256=RaCwh-9sW254itYxe0JdclclWsxkk1VuVafHpTt-Z1I,217994
+modal_proto/api_pb2_grpc.py,sha256=__5-RlGE2VhrKxgvhbIcmfX5EwNee9OKYuy1MgOSlts,184288
 modal_proto/options.proto,sha256=a-siq4swVbZPfaFRXAipRZzGP2bq8OsdUvjlyzAeodQ,488
 modal_proto/options_grpc.py,sha256=M18X3d-8F_cNYSVM3I25dUTO5rZ0rd-vCCfynfh13Nc,125
 modal_proto/options_pb2.py,sha256=OC2Oob8Yz_3Gs58hwpS_jSFWpGsWMcxlgXbJCyw3gMk,1827
 modal_proto/options_pb2_grpc.py,sha256=1oboBPFxaTEXt9Aw7EAj8gXHDCNMhZD2VXqocC9l_gk,159
 modal_version/__init__.py,sha256=HTM4O90gT-ndgKJIFiD8JRMsdauRAWxc9TDGhBuhkYI,470
 modal_version/__main__.py,sha256=2FO0yYQQwDTh6udt1h-cBnGd1c4ZyHnHSI4BksxzVac,105
-modal_version/_version_generated.py,sha256=J637MP_oxW1YKieMZ2bK93TyPyRPjdeZaFbJ15xoDLA,149
+modal_version/_version_generated.py,sha256=sEzCzfiHo79C_7v_AspiYbHiJ5mN549B2PJO7byP7Go,148
 test/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
-test/aio_test.py,sha256=u8nTraWcjAxPU7mV5w3m5hI2Xi9U1rXEEYyBnXTIEVs,199
-test/async_utils_test.py,sha256=T4HJlfMNe1cL5cEzFxv2e4rJoaJkqygvZYSgS75FAo4,7247
+test/aio_test.py,sha256=YOmAEt1WaBh0VxlmCa7gzjM-VEvIQNQ6By959y2WwSk,203
+test/async_utils_test.py,sha256=1NGazu9XBc1YJYhKPB2vt49dJyDheisA1YDl3iYVlGc,6792
 test/blob_test.py,sha256=72LpqPJreb0mtLlkS14IN6rsvsQUWr1yrM97uTANUQo,2539
-test/cli_imports_test.py,sha256=qngK4HaXnOfq8IBgYKjkDa7jpZ1qrxRlUkt6Pkp5QjM,4665
-test/cli_test.py,sha256=mF6Ij8SGzBjMMIDaNsvSYeBC6hHdwKLVB7aUQXWcRSU,27017
+test/cli_imports_test.py,sha256=hayUSViVXhY0Vxl-WJ1b3dapx8TGi76I8ql-Snm5bJI,4680
+test/cli_test.py,sha256=lqbjyOBMH-u_ixx646LUnpqrrDBPBKbGVW2lQn3x5to,26607
 test/client_test.py,sha256=E9lfLgW9EFnA_stbw0vX8vUA7djIiO5fW-x-JR7tKeM,6426
-test/cloud_bucket_mount_test.py,sha256=5BwaEZbiWaDQ-9rzifhB7Uq6vzaqgTI6sL17SnAZqHA,530
-test/cls_test.py,sha256=pYwU78XRpXCgCt4ag3FINjvClmLObHy1ezm5NbiBWAA,16131
-test/config_test.py,sha256=oQgLnKLMZYTLxz_LNvE_Re99gYrAjR0DcfivI-U1btc,5121
-test/conftest.py,sha256=46o0GkzaF4YF0Csp3FkhbfIr_Q1uIJy6fVnOf705tls,61518
-test/container_app_test.py,sha256=MDFTc_J9R1i4nXWD-k2Rk9SRiyGhxaBLFB6SwbG2GGs,1506
-test/container_test.py,sha256=cOfVv7vsCs4EBVFdGv2X-dFIWv627A2MVmgebg2xbf0,50596
-test/cpu_test.py,sha256=36StHgZlfblNGYxzwRvqL137x1Dk6C7oEb6y6cDWQi0,398
-test/decorator_test.py,sha256=ec0oAO-oBE0fMVehr83Gj7gnRtqL4pz8jN6vwMDYXaw,1850
+test/cls_test.py,sha256=MSRzjUJAngQVHPXglYcwxl-nNwGgRlgXfAEWWJfWkEk,15906
+test/config_test.py,sha256=mhacVtiOB5jAwLjoiDy_IUqP7z3n5DKzW6ugNtwmwnQ,4768
+test/conftest.py,sha256=dZL3RbDkgQyY1TlTLI_jRWIe0pZCCp8g7kay5DoC9w0,59766
+test/container_app_test.py,sha256=hWP6DZTZx6zJYudOfNq5386-rEvRGXSdU7XImaUtKYE,895
+test/container_test.py,sha256=-zS-Pf5ao9mwnTJEFBwT1wvyU4IBRxUiGkxXxBEwCEI,50061
+test/cpu_test.py,sha256=CoJhxhHH9ZjZh3wnpKFfPku2Vq6xIZJoB6j7O-v2278,405
+test/decorator_test.py,sha256=MS90m-9IiVzNgTMfr_2pVlemR3nwMQKAhuJktckq9Xo,1871
 test/deprecation_test.py,sha256=IS_pracoNgt0vuEdLThukZpBkPBX4LUCQP5sAUMhjiY,1055
-test/dict_test.py,sha256=-i4aZjw-W8N5TRlB2lMbLFIVd9hEt4VDpl14UxAg58g,1286
+test/dict_test.py,sha256=FQyWl6ZZ0ktUExxh8xBEuu1LCKsworW6Io1bAD2z1Ug,851
 test/e2e_test.py,sha256=OBBTB8p422PBD0iCoOfNTQQsIs_sX6_SkT1dNv3DiY4,2152
 test/error_test.py,sha256=bDxKUwE4WJ1qwAfm0Jlzqkb3WetF0YLxplrgjevAv4U,165
-test/function_serialization_test.py,sha256=4SJM8EdH9YXJ1woBla5L2VdnxWqSkOiL_EzDUU85KKY,966
-test/function_test.py,sha256=CgPO7oGt3hXU-iXBD5n-frDkebdCf7-guRpbT0T8lPE,23757
+test/function_serialization_test.py,sha256=1rili8CT3bdOfIMabIAEBn699GTBWqc11xCe-wu5_bY,971
+test/function_test.py,sha256=pWtmXnVHSVWoSd9EUc37oA2q9HIPX2O-g-J-7NY3mYk,19697
 test/function_utils_test.py,sha256=AZr8ZPh2h152epv-kn0rWZXwhM8MbPcaVSsOa5thzhY,1892
-test/gpu_test.py,sha256=lwL0nvQfcMHzS1tld9Pj3ElsujI6GaZTRA09Ik85lWA,4571
+test/gpu_test.py,sha256=i8Zp0NRI6LhXAaPc1hATJJjQLlFtmxZ-RxUEDD6pTls,4611
 test/grpc_utils_test.py,sha256=me9sGIvRO3bmj4M4UmwU3dbHj5IpKCF7aKy6W5JaP3U,5048
-test/helpers.py,sha256=MCCSkKM0Qw7_TjhIUB-5zgcJ7z8M3K0c-n6KLjmqn2M,1709
-test/image_test.py,sha256=NUTBREVJlohusHCpwZBqtiNGiceYwbCRC1cwOvF9P1M,32798
-test/live_reload_test.py,sha256=iP3TdMu4JjkS6rI7oaIPfgNqDBRpkg2v9OQD9iae5mA,2722
-test/lookup_test.py,sha256=uGauFaxUBNJPl3unMgqM8lHhQMWDAGM8W_r_aBotZgQ,2160
+test/helpers.py,sha256=-kxMLEjyQeBj6K0AHLrGrzWGw_ZGj2ST4eJaLMX9Rbo,1443
+test/image_test.py,sha256=DsKT8hiZDnaxkrnzG2YeeLaYfeyz5EgqCpTNnmW79mk,25515
+test/live_reload_test.py,sha256=8bk6MAbrzUe9spuQ_-0zQE1p4fvLsbAOhL5g41qEJt4,2742
+test/lookup_test.py,sha256=jcjGt7lIf6iC_HOg0sQE06yg_KcgmHc2pvOYg956chw,2172
 test/mdmd_test.py,sha256=G3B0986QneKtIrui34hu95h09No9mM06O7mPp2sTm50,5118
-test/mount_test.py,sha256=n7Qar7sDVMl1kTKfN2Je0kXbQ8DWPsAQ5q_v-VbrE3o,5475
-test/mounted_files_test.py,sha256=-6k2ATfdTxZsuepNefYW2_h5vCsGAmMnQDl0utxQj0M,12461
-test/network_file_system_test.py,sha256=EBg_Tqtg0ryZeGANjSvwSeRFy1FGDdP29b6XTxNJGYM,6265
+test/mount_test.py,sha256=-76FWFPCRDJpmG4UXdTWpOwG6YgoF6W94F6MAlFB578,5489
+test/mounted_files_test.py,sha256=Noasj7JaCVjVAyTEZIjq-MJkelnVES9FpbE2Ldhe8lQ,12471
+test/network_file_system_test.py,sha256=GxP0fklqOgcXFiqaPnpvN5ZJ8tlqT6e8vNwV9jB9KQI,6073
 test/notebook_test.py,sha256=IkOFP9Tat8pEazSOV73OU7QMGgOlw3sU_5vTzOigrZU,2157
-test/object_test.py,sha256=jyLxikU4quvVnDhdhIC9-rKRie1WOnB3hjaD-u383BM,1291
+test/object_test.py,sha256=Ncv3B3-Sd78MeHnhmvuyBN1qXRICONW_qckzVndFWc4,1298
 test/package_utils_test.py,sha256=ic3cJqwVbe7p2jOwnnTuGTwP9aDduz9QueqVwcA8r9I,795
-test/queue_test.py,sha256=YZMJePJSsq5pGoRK1Vam8MZhJIggP_6kxxSfHXBSddg,3667
-test/resolver_test.py,sha256=YlwYPYZO6ZNBH9lQ8ynABwMUfvIs0Mg1h3lKz3PsmAs,1782
-test/retries_test.py,sha256=4x43pJc0Xt6VoPbk9ScnO2Xo_qFVX8QdtuoemO-prLk,1806
-test/runner_test.py,sha256=DG0WVFKidX0KGIIOxn3AEiYRbNDge7wiu7lOISJytt0,2909
-test/sandbox_test.py,sha256=SRsZbZdZjaQwv7VfsUu2MjT7P23m4YmaVMNlvDm77mA,5254
-test/schedule_test.py,sha256=ZDFaaG7WAA6mx6qwlYPNlcf0LHE96uvGFcxfmrnqi78,354
-test/scheduler_placement_test.py,sha256=nd0OizTcsfMiTA6F8KholcZfUlD8Np-QjCReaT2P8us,737
-test/secret_test.py,sha256=QrS6hudMghVU61Bg5N2C9aqvGl85jzIzzRkin2JAJ8E,2506
+test/queue_test.py,sha256=5a8RQMH6DBvm2BvUmbSRGy8JykDtmqOguYTlmvmyJcE,3270
+test/resolver_test.py,sha256=pmQeuSodsGd8tZg3Ja6bKB8lgg-HNxx6Q1aGn2BjQ0Q,1751
+test/retries_test.py,sha256=05W440EOUJ-aSNGyu46SgcwCCS-Gt3NkFnI_8m5aRCo,1817
+test/runner_test.py,sha256=0tke6A5B7AqDpRkCbcPL2sU4O2wuOY9Y1tsQlodVEcU,2582
+test/sandbox_test.py,sha256=U2qFwmHmm4rnVDZOU_MP2irqbSx5dtf_pEIXjBdVODQ,5282
+test/schedule_test.py,sha256=lyj6pHchOg_CqyJNQY8NbRUTfh-RxwAugcX5-P4b-0k,359
+test/scheduler_placement_test.py,sha256=efdPOIy0MxrrioSzzEBTufM-ea5DXUbYiJ-UTdbj-fE,742
+test/secret_test.py,sha256=NGa3HrHvu97XlyOquf-ZanqV6s2gclC9lZarkYSu7O8,2527
 test/serialization_test.py,sha256=d_r5xDn85WkAK3ovMiNGtSxHC90_WM2kS2GSzhRjAnU,1867
-test/stub_composition_test.py,sha256=yMW7P0JqqwvKP_cP6kj7GTRiJl3dUjOd-nSED9mQItk,555
-test/stub_test.py,sha256=LDYpGaCNmeCY73bZcHH_RQDD-ElRCG3Om0IS2cpczUI,10547
+test/stub_composition_test.py,sha256=8JncwauejwbPM4ttm_IBhp6bQwabVdY7_hf6P_XCV44,558
+test/stub_test.py,sha256=CzKjxrKCRawOeWroKTq2wEvbhRcuukvgy154LeSMngQ,11284
 test/test_asgi_wrapper.py,sha256=NzhKYIVVQhL9mTGJh8AWBEv_cTZ1uT9W9ngoLufL09M,7320
 test/token_flow_test.py,sha256=tbxqsDFzRiMCkxfexDzxbbEomrmNlqJjOpPm8ndPE_c,614
 test/traceback_test.py,sha256=3FNfnb00ek1gMVj8xcT8veAcyF4j4Fdrz3G9_giljsc,4747
 test/tunnel_test.py,sha256=5v3FGmYS3VAcQB1xMjoXnmU1jLGvKu27TgFkH42N09w,768
 test/utils_test.py,sha256=VRBxxpIAl3TvvY8X_rumvqCR-5ZozV2tfm_1Gy4ok6M,2679
 test/version_test.py,sha256=lii24csueJzXzv0HM0fvHD83N_9FOOFRl-VRmDQZnDA,403
-test/volume_test.py,sha256=auMLjBdHB7Zti1Mo8JXnngBTMwcIbVGya9a-5cHNZbc,14309
+test/volume_test.py,sha256=j7B6tKwvURCHAahaeaI-nFFItKXDihui5xWHeXmzSNw,12461
 test/watcher_test.py,sha256=RdtFZVsdQEdKo7WUy98SK6UJ795u8v--x3nuOTNEghc,1193
-test/webhook_test.py,sha256=FDD-gR0LUP3tTSZKHDq9313CG7lxZ8LL1fakU_P7zj0,4172
-modal-0.62.87.dist-info/LICENSE,sha256=psuoW8kuDP96RQsdhzwOqi6fyWv0ct8CR6Jr7He_P_k,10173
-modal-0.62.87.dist-info/METADATA,sha256=VZOCuuzLsfvAbR8YjwlxW3nqGspvgrdrXnu88OeqD4Q,2302
-modal-0.62.87.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-modal-0.62.87.dist-info/entry_points.txt,sha256=An-wYgeEUnm6xzrAP9_NTSTSciYvvEWsMZILtYrvpAI,46
-modal-0.62.87.dist-info/top_level.txt,sha256=AfUN7pr_AWNHp5av5c2h5ntw_83jrhQ2-ytzdC-S4Z0,69
-modal-0.62.87.dist-info/RECORD,,
+test/webhook_test.py,sha256=wyG3nG93En4-tXyo3GMMX__0GKfKnH0ZpChssyKlTXI,4205
+modal-0.62.9.dist-info/LICENSE,sha256=psuoW8kuDP96RQsdhzwOqi6fyWv0ct8CR6Jr7He_P_k,10173
+modal-0.62.9.dist-info/METADATA,sha256=oRZjAhkdxCJpmzkD5tmsIIMdGT0L93zTJwQ-FE5yjUc,2301
+modal-0.62.9.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+modal-0.62.9.dist-info/entry_points.txt,sha256=An-wYgeEUnm6xzrAP9_NTSTSciYvvEWsMZILtYrvpAI,46
+modal-0.62.9.dist-info/top_level.txt,sha256=AfUN7pr_AWNHp5av5c2h5ntw_83jrhQ2-ytzdC-S4Z0,69
+modal-0.62.9.dist-info/RECORD,,
```

