# Comparing `tmp/modal-0.62.87-py3-none-any.whl.zip` & `tmp/modal-0.62.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,205 +1,199 @@
-Zip file size: 453278 bytes, number of entries: 203
--rw-r--r--  2.0 unx     2100 b- defN 24-Apr-18 02:16 modal/__init__.py
--rw-r--r--  2.0 unx     1141 b- defN 24-Apr-18 02:16 modal/__main__.py
--rw-r--r--  2.0 unx    15476 b- defN 24-Apr-18 02:16 modal/_asgi.py
--rw-r--r--  2.0 unx    28448 b- defN 24-Apr-18 02:16 modal/_container_entrypoint.py
--rw-r--r--  2.0 unx     4349 b- defN 24-Apr-18 02:16 modal/_container_exec.py
--rw-r--r--  2.0 unx    28709 b- defN 24-Apr-18 02:16 modal/_container_io_manager.py
--rw-r--r--  2.0 unx    12668 b- defN 24-Apr-18 02:16 modal/_container_io_manager.pyi
--rw-r--r--  2.0 unx      450 b- defN 24-Apr-18 02:16 modal/_ipython.py
--rw-r--r--  2.0 unx      929 b- defN 24-Apr-18 02:16 modal/_location.py
--rw-r--r--  2.0 unx    20565 b- defN 24-Apr-18 02:16 modal/_output.py
--rw-r--r--  2.0 unx     1906 b- defN 24-Apr-18 02:16 modal/_proxy_tunnel.py
--rw-r--r--  2.0 unx     1336 b- defN 24-Apr-18 02:16 modal/_pty.py
--rw-r--r--  2.0 unx     7077 b- defN 24-Apr-18 02:16 modal/_resolver.py
--rw-r--r--  2.0 unx     1142 b- defN 24-Apr-18 02:16 modal/_resources.py
--rw-r--r--  2.0 unx     1658 b- defN 24-Apr-18 02:16 modal/_sandbox_shell.py
--rw-r--r--  2.0 unx    12133 b- defN 24-Apr-18 02:16 modal/_serialization.py
--rw-r--r--  2.0 unx    10029 b- defN 24-Apr-18 02:16 modal/_traceback.py
--rw-r--r--  2.0 unx     5073 b- defN 24-Apr-18 02:16 modal/_tunnel.py
--rw-r--r--  2.0 unx     1337 b- defN 24-Apr-18 02:16 modal/_tunnel.pyi
--rw-r--r--  2.0 unx     3609 b- defN 24-Apr-18 02:16 modal/_watcher.py
--rw-r--r--  2.0 unx    35268 b- defN 24-Apr-18 02:16 modal/app.py
--rw-r--r--  2.0 unx    18642 b- defN 24-Apr-18 02:16 modal/app.pyi
--rw-r--r--  2.0 unx      748 b- defN 24-Apr-18 02:16 modal/app_utils.py
--rw-r--r--  2.0 unx      613 b- defN 24-Apr-18 02:16 modal/app_utils.pyi
--rw-r--r--  2.0 unx     2524 b- defN 24-Apr-18 02:16 modal/call_graph.py
--rw-r--r--  2.0 unx    10828 b- defN 24-Apr-18 02:16 modal/client.py
--rw-r--r--  2.0 unx     4006 b- defN 24-Apr-18 02:16 modal/client.pyi
--rw-r--r--  2.0 unx     5550 b- defN 24-Apr-18 02:16 modal/cloud_bucket_mount.py
--rw-r--r--  2.0 unx     1217 b- defN 24-Apr-18 02:16 modal/cloud_bucket_mount.pyi
--rw-r--r--  2.0 unx    12738 b- defN 24-Apr-18 02:16 modal/cls.py
--rw-r--r--  2.0 unx     6525 b- defN 24-Apr-18 02:16 modal/cls.pyi
--rw-r--r--  2.0 unx     9980 b- defN 24-Apr-18 02:16 modal/config.py
--rw-r--r--  2.0 unx    10777 b- defN 24-Apr-18 02:16 modal/dict.py
--rw-r--r--  2.0 unx     5809 b- defN 24-Apr-18 02:16 modal/dict.pyi
--rw-r--r--  2.0 unx     2452 b- defN 24-Apr-18 02:16 modal/environments.py
--rw-r--r--  2.0 unx     1439 b- defN 24-Apr-18 02:16 modal/environments.pyi
--rw-r--r--  2.0 unx     5868 b- defN 24-Apr-18 02:16 modal/exception.py
--rw-r--r--  2.0 unx      316 b- defN 24-Apr-18 02:16 modal/experimental.py
--rw-r--r--  2.0 unx    67936 b- defN 24-Apr-18 02:16 modal/functions.py
--rw-r--r--  2.0 unx    23960 b- defN 24-Apr-18 02:16 modal/functions.pyi
--rw-r--r--  2.0 unx     8038 b- defN 24-Apr-18 02:16 modal/gpu.py
--rw-r--r--  2.0 unx    68298 b- defN 24-Apr-18 02:16 modal/image.py
--rw-r--r--  2.0 unx    18339 b- defN 24-Apr-18 02:16 modal/image.pyi
--rw-r--r--  2.0 unx    23177 b- defN 24-Apr-18 02:16 modal/mount.py
--rw-r--r--  2.0 unx     9588 b- defN 24-Apr-18 02:16 modal/mount.pyi
--rw-r--r--  2.0 unx    14380 b- defN 24-Apr-18 02:16 modal/network_file_system.py
--rw-r--r--  2.0 unx     6280 b- defN 24-Apr-18 02:16 modal/network_file_system.pyi
--rw-r--r--  2.0 unx     8329 b- defN 24-Apr-18 02:16 modal/object.py
--rw-r--r--  2.0 unx     7573 b- defN 24-Apr-18 02:16 modal/object.pyi
--rw-r--r--  2.0 unx    19988 b- defN 24-Apr-18 02:16 modal/partial_function.py
--rw-r--r--  2.0 unx     6172 b- defN 24-Apr-18 02:16 modal/partial_function.pyi
--rw-r--r--  2.0 unx     1307 b- defN 24-Apr-18 02:16 modal/proxy.py
--rw-r--r--  2.0 unx      428 b- defN 24-Apr-18 02:16 modal/proxy.pyi
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-18 02:16 modal/py.typed
--rw-r--r--  2.0 unx    17136 b- defN 24-Apr-18 02:16 modal/queue.py
--rw-r--r--  2.0 unx     7397 b- defN 24-Apr-18 02:16 modal/queue.pyi
--rw-r--r--  2.0 unx     3730 b- defN 24-Apr-18 02:16 modal/retries.py
--rw-r--r--  2.0 unx    19091 b- defN 24-Apr-18 02:16 modal/runner.py
--rw-r--r--  2.0 unx     5753 b- defN 24-Apr-18 02:16 modal/runner.pyi
--rw-r--r--  2.0 unx      461 b- defN 24-Apr-18 02:16 modal/running_app.py
--rw-r--r--  2.0 unx    15040 b- defN 24-Apr-18 02:16 modal/sandbox.py
--rw-r--r--  2.0 unx     6514 b- defN 24-Apr-18 02:16 modal/sandbox.pyi
--rw-r--r--  2.0 unx     2621 b- defN 24-Apr-18 02:16 modal/schedule.py
--rw-r--r--  2.0 unx      662 b- defN 24-Apr-18 02:16 modal/scheduler_placement.py
--rw-r--r--  2.0 unx     8916 b- defN 24-Apr-18 02:16 modal/secret.py
--rw-r--r--  2.0 unx     2225 b- defN 24-Apr-18 02:16 modal/secret.pyi
--rw-r--r--  2.0 unx     4800 b- defN 24-Apr-18 02:16 modal/serving.py
--rw-r--r--  2.0 unx     2977 b- defN 24-Apr-18 02:16 modal/serving.pyi
--rw-r--r--  2.0 unx      888 b- defN 24-Apr-18 02:16 modal/shared_volume.py
--rw-r--r--  2.0 unx      405 b- defN 24-Apr-18 02:16 modal/shared_volume.pyi
--rw-r--r--  2.0 unx     6742 b- defN 24-Apr-18 02:16 modal/token_flow.py
--rw-r--r--  2.0 unx     1890 b- defN 24-Apr-18 02:16 modal/token_flow.pyi
--rw-r--r--  2.0 unx    28278 b- defN 24-Apr-18 02:16 modal/volume.py
--rw-r--r--  2.0 unx     9748 b- defN 24-Apr-18 02:16 modal/volume.pyi
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/_utils/__init__.py
--rw-r--r--  2.0 unx      465 b- defN 24-Apr-18 02:16 modal/_utils/app_utils.py
--rw-r--r--  2.0 unx    16049 b- defN 24-Apr-18 02:16 modal/_utils/async_utils.py
--rw-r--r--  2.0 unx    15054 b- defN 24-Apr-18 02:16 modal/_utils/blob_utils.py
--rw-r--r--  2.0 unx    15339 b- defN 24-Apr-18 02:16 modal/_utils/function_utils.py
--rw-r--r--  2.0 unx     7927 b- defN 24-Apr-18 02:16 modal/_utils/grpc_testing.py
--rw-r--r--  2.0 unx     9518 b- defN 24-Apr-18 02:16 modal/_utils/grpc_utils.py
--rw-r--r--  2.0 unx     1790 b- defN 24-Apr-18 02:16 modal/_utils/hash_utils.py
--rw-r--r--  2.0 unx     1426 b- defN 24-Apr-18 02:16 modal/_utils/http_utils.py
--rw-r--r--  2.0 unx     1311 b- defN 24-Apr-18 02:16 modal/_utils/logger.py
--rw-r--r--  2.0 unx     2341 b- defN 24-Apr-18 02:16 modal/_utils/mount_utils.py
--rw-r--r--  2.0 unx     1640 b- defN 24-Apr-18 02:16 modal/_utils/package_utils.py
--rw-r--r--  2.0 unx     3857 b- defN 24-Apr-18 02:16 modal/_utils/rand_pb_testing.py
--rw-r--r--  2.0 unx     3633 b- defN 24-Apr-18 02:16 modal/_utils/shell_utils.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/_vendor/__init__.py
--rw-r--r--  2.0 unx    22144 b- defN 24-Apr-18 02:16 modal/_vendor/a2wsgi_wsgi.py
--rw-r--r--  2.0 unx    55225 b- defN 24-Apr-18 02:16 modal/_vendor/cloudpickle.py
--rw-r--r--  2.0 unx     9722 b- defN 24-Apr-18 02:16 modal/_vendor/tblib.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/cli/__init__.py
--rw-r--r--  2.0 unx     2234 b- defN 24-Apr-18 02:16 modal/cli/_download.py
--rw-r--r--  2.0 unx     3041 b- defN 24-Apr-18 02:16 modal/cli/app.py
--rw-r--r--  2.0 unx     1294 b- defN 24-Apr-18 02:16 modal/cli/config.py
--rw-r--r--  2.0 unx     1748 b- defN 24-Apr-18 02:16 modal/cli/container.py
--rw-r--r--  2.0 unx     3538 b- defN 24-Apr-18 02:16 modal/cli/entry_point.py
--rw-r--r--  2.0 unx     3414 b- defN 24-Apr-18 02:16 modal/cli/environment.py
--rw-r--r--  2.0 unx     9529 b- defN 24-Apr-18 02:16 modal/cli/import_refs.py
--rw-r--r--  2.0 unx     2136 b- defN 24-Apr-18 02:16 modal/cli/launch.py
--rw-r--r--  2.0 unx     8256 b- defN 24-Apr-18 02:16 modal/cli/network_file_system.py
--rw-r--r--  2.0 unx     3109 b- defN 24-Apr-18 02:16 modal/cli/profile.py
--rw-r--r--  2.0 unx    13769 b- defN 24-Apr-18 02:16 modal/cli/run.py
--rw-r--r--  2.0 unx     4181 b- defN 24-Apr-18 02:16 modal/cli/secret.py
--rw-r--r--  2.0 unx     1875 b- defN 24-Apr-18 02:16 modal/cli/token.py
--rw-r--r--  2.0 unx     1330 b- defN 24-Apr-18 02:16 modal/cli/utils.py
--rw-r--r--  2.0 unx    10775 b- defN 24-Apr-18 02:16 modal/cli/volume.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/cli/programs/__init__.py
--rw-r--r--  2.0 unx     2143 b- defN 24-Apr-18 02:16 modal/cli/programs/run_jupyter.py
--rw-r--r--  2.0 unx     1898 b- defN 24-Apr-18 02:16 modal/cli/programs/vscode.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal/extensions/__init__.py
--rw-r--r--  2.0 unx      988 b- defN 24-Apr-18 02:16 modal/extensions/ipython.py
--rw-r--r--  2.0 unx      400 b- defN 24-Apr-18 02:16 modal/requirements/2023.12.312.txt
--rw-r--r--  2.0 unx      502 b- defN 24-Apr-18 02:16 modal/requirements/2023.12.txt
--rw-r--r--  2.0 unx      520 b- defN 24-Apr-18 02:16 modal/requirements/2024.04.txt
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_docs/__init__.py
--rw-r--r--  2.0 unx     3801 b- defN 24-Apr-18 02:16 modal_docs/gen_cli_docs.py
--rw-r--r--  2.0 unx     6555 b- defN 24-Apr-18 02:16 modal_docs/gen_reference_docs.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_docs/mdmd/__init__.py
--rw-r--r--  2.0 unx     6268 b- defN 24-Apr-18 02:16 modal_docs/mdmd/mdmd.py
--rw-r--r--  2.0 unx     3243 b- defN 24-Apr-18 02:16 modal_docs/mdmd/signatures.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_global_objects/__init__.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_global_objects/images/__init__.py
--rw-r--r--  2.0 unx      324 b- defN 24-Apr-18 02:16 modal_global_objects/images/conda.py
--rw-r--r--  2.0 unx      330 b- defN 24-Apr-18 02:16 modal_global_objects/images/debian_slim.py
--rw-r--r--  2.0 unx      329 b- defN 24-Apr-18 02:16 modal_global_objects/images/micromamba.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_global_objects/mounts/__init__.py
--rw-r--r--  2.0 unx      767 b- defN 24-Apr-18 02:16 modal_global_objects/mounts/modal_client_package.py
--rw-r--r--  2.0 unx     1870 b- defN 24-Apr-18 02:16 modal_global_objects/mounts/python_standalone.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 modal_proto/__init__.py
--rw-r--r--  2.0 unx    56704 b- defN 24-Apr-18 02:16 modal_proto/api.proto
--rw-r--r--  2.0 unx    85985 b- defN 24-Apr-18 02:16 modal_proto/api_grpc.py
--rw-r--r--  2.0 unx   219214 b- defN 24-Apr-18 02:16 modal_proto/api_pb2.py
--rw-r--r--  2.0 unx   185802 b- defN 24-Apr-18 02:16 modal_proto/api_pb2_grpc.py
--rw-r--r--  2.0 unx      488 b- defN 24-Apr-18 02:16 modal_proto/options.proto
--rw-r--r--  2.0 unx      125 b- defN 24-Apr-18 02:16 modal_proto/options_grpc.py
--rw-r--r--  2.0 unx     1827 b- defN 24-Apr-18 02:16 modal_proto/options_pb2.py
--rw-r--r--  2.0 unx      159 b- defN 24-Apr-18 02:16 modal_proto/options_pb2_grpc.py
--rw-r--r--  2.0 unx      470 b- defN 24-Apr-18 02:16 modal_version/__init__.py
--rw-r--r--  2.0 unx      105 b- defN 24-Apr-18 02:16 modal_version/__main__.py
--rw-r--r--  2.0 unx      149 b- defN 24-Apr-18 02:16 modal_version/_version_generated.py
--rw-r--r--  2.0 unx       28 b- defN 24-Apr-18 02:16 test/__init__.py
--rw-r--r--  2.0 unx      199 b- defN 24-Apr-18 02:16 test/aio_test.py
--rw-r--r--  2.0 unx     7247 b- defN 24-Apr-18 02:16 test/async_utils_test.py
--rw-r--r--  2.0 unx     2539 b- defN 24-Apr-18 02:16 test/blob_test.py
--rw-r--r--  2.0 unx     4665 b- defN 24-Apr-18 02:16 test/cli_imports_test.py
--rw-r--r--  2.0 unx    27017 b- defN 24-Apr-18 02:16 test/cli_test.py
--rw-r--r--  2.0 unx     6426 b- defN 24-Apr-18 02:16 test/client_test.py
--rw-r--r--  2.0 unx      530 b- defN 24-Apr-18 02:16 test/cloud_bucket_mount_test.py
--rw-r--r--  2.0 unx    16131 b- defN 24-Apr-18 02:16 test/cls_test.py
--rw-r--r--  2.0 unx     5121 b- defN 24-Apr-18 02:16 test/config_test.py
--rw-r--r--  2.0 unx    61518 b- defN 24-Apr-18 02:16 test/conftest.py
--rw-r--r--  2.0 unx     1506 b- defN 24-Apr-18 02:16 test/container_app_test.py
--rw-r--r--  2.0 unx    50596 b- defN 24-Apr-18 02:16 test/container_test.py
--rw-r--r--  2.0 unx      398 b- defN 24-Apr-18 02:16 test/cpu_test.py
--rw-r--r--  2.0 unx     1850 b- defN 24-Apr-18 02:16 test/decorator_test.py
--rw-r--r--  2.0 unx     1055 b- defN 24-Apr-18 02:16 test/deprecation_test.py
--rw-r--r--  2.0 unx     1286 b- defN 24-Apr-18 02:16 test/dict_test.py
--rw-r--r--  2.0 unx     2152 b- defN 24-Apr-18 02:16 test/e2e_test.py
--rw-r--r--  2.0 unx      165 b- defN 24-Apr-18 02:16 test/error_test.py
--rw-r--r--  2.0 unx      966 b- defN 24-Apr-18 02:16 test/function_serialization_test.py
--rw-r--r--  2.0 unx    23757 b- defN 24-Apr-18 02:16 test/function_test.py
--rw-r--r--  2.0 unx     1892 b- defN 24-Apr-18 02:16 test/function_utils_test.py
--rw-r--r--  2.0 unx     4571 b- defN 24-Apr-18 02:16 test/gpu_test.py
--rw-r--r--  2.0 unx     5048 b- defN 24-Apr-18 02:16 test/grpc_utils_test.py
--rw-r--r--  2.0 unx     1709 b- defN 24-Apr-18 02:16 test/helpers.py
--rw-r--r--  2.0 unx    32798 b- defN 24-Apr-18 02:16 test/image_test.py
--rw-r--r--  2.0 unx     2722 b- defN 24-Apr-18 02:16 test/live_reload_test.py
--rw-r--r--  2.0 unx     2160 b- defN 24-Apr-18 02:16 test/lookup_test.py
--rw-r--r--  2.0 unx     5118 b- defN 24-Apr-18 02:16 test/mdmd_test.py
--rw-r--r--  2.0 unx     5475 b- defN 24-Apr-18 02:16 test/mount_test.py
--rw-r--r--  2.0 unx    12461 b- defN 24-Apr-18 02:16 test/mounted_files_test.py
--rw-r--r--  2.0 unx     6265 b- defN 24-Apr-18 02:16 test/network_file_system_test.py
--rw-r--r--  2.0 unx     2157 b- defN 24-Apr-18 02:16 test/notebook_test.py
--rw-r--r--  2.0 unx     1291 b- defN 24-Apr-18 02:16 test/object_test.py
--rw-r--r--  2.0 unx      795 b- defN 24-Apr-18 02:16 test/package_utils_test.py
--rw-r--r--  2.0 unx     3667 b- defN 24-Apr-18 02:16 test/queue_test.py
--rw-r--r--  2.0 unx     1782 b- defN 24-Apr-18 02:16 test/resolver_test.py
--rw-r--r--  2.0 unx     1806 b- defN 24-Apr-18 02:16 test/retries_test.py
--rw-r--r--  2.0 unx     2909 b- defN 24-Apr-18 02:16 test/runner_test.py
--rw-r--r--  2.0 unx     5254 b- defN 24-Apr-18 02:16 test/sandbox_test.py
--rw-r--r--  2.0 unx      354 b- defN 24-Apr-18 02:16 test/schedule_test.py
--rw-r--r--  2.0 unx      737 b- defN 24-Apr-18 02:16 test/scheduler_placement_test.py
--rw-r--r--  2.0 unx     2506 b- defN 24-Apr-18 02:16 test/secret_test.py
--rw-r--r--  2.0 unx     1867 b- defN 24-Apr-18 02:16 test/serialization_test.py
--rw-r--r--  2.0 unx      555 b- defN 24-Apr-18 02:16 test/stub_composition_test.py
--rw-r--r--  2.0 unx    10547 b- defN 24-Apr-18 02:16 test/stub_test.py
--rw-r--r--  2.0 unx     7320 b- defN 24-Apr-18 02:16 test/test_asgi_wrapper.py
--rw-r--r--  2.0 unx      614 b- defN 24-Apr-18 02:16 test/token_flow_test.py
--rw-r--r--  2.0 unx     4747 b- defN 24-Apr-18 02:16 test/traceback_test.py
--rw-r--r--  2.0 unx      768 b- defN 24-Apr-18 02:16 test/tunnel_test.py
--rw-r--r--  2.0 unx     2679 b- defN 24-Apr-18 02:16 test/utils_test.py
--rw-r--r--  2.0 unx      403 b- defN 24-Apr-18 02:16 test/version_test.py
--rw-r--r--  2.0 unx    14309 b- defN 24-Apr-18 02:16 test/volume_test.py
--rw-r--r--  2.0 unx     1193 b- defN 24-Apr-18 02:16 test/watcher_test.py
--rw-r--r--  2.0 unx     4172 b- defN 24-Apr-18 02:16 test/webhook_test.py
--rw-r--r--  2.0 unx    10173 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/LICENSE
--rw-r--r--  2.0 unx     2302 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/WHEEL
--rw-r--r--  2.0 unx       46 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       69 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    16058 b- defN 24-Apr-18 02:16 modal-0.62.87.dist-info/RECORD
-203 files, 1939989 bytes uncompressed, 428698 bytes compressed:  77.9%
+Zip file size: 436952 bytes, number of entries: 197
+-rw-r--r--  2.0 unx     2102 b- defN 24-Mar-28 16:25 modal/__init__.py
+-rw-r--r--  2.0 unx     1141 b- defN 24-Mar-28 16:25 modal/__main__.py
+-rw-r--r--  2.0 unx    15476 b- defN 24-Mar-28 16:25 modal/_asgi.py
+-rw-r--r--  2.0 unx    52214 b- defN 24-Mar-28 16:25 modal/_container_entrypoint.py
+-rw-r--r--  2.0 unx    11148 b- defN 24-Mar-28 16:25 modal/_container_entrypoint.pyi
+-rw-r--r--  2.0 unx     4349 b- defN 24-Mar-28 16:25 modal/_container_exec.py
+-rw-r--r--  2.0 unx      450 b- defN 24-Mar-28 16:25 modal/_ipython.py
+-rw-r--r--  2.0 unx      929 b- defN 24-Mar-28 16:25 modal/_location.py
+-rw-r--r--  2.0 unx    20565 b- defN 24-Mar-28 16:25 modal/_output.py
+-rw-r--r--  2.0 unx     1906 b- defN 24-Mar-28 16:25 modal/_proxy_tunnel.py
+-rw-r--r--  2.0 unx     1336 b- defN 24-Mar-28 16:25 modal/_pty.py
+-rw-r--r--  2.0 unx     7208 b- defN 24-Mar-28 16:25 modal/_resolver.py
+-rw-r--r--  2.0 unx     1658 b- defN 24-Mar-28 16:25 modal/_sandbox_shell.py
+-rw-r--r--  2.0 unx    12329 b- defN 24-Mar-28 16:25 modal/_serialization.py
+-rw-r--r--  2.0 unx    10029 b- defN 24-Mar-28 16:25 modal/_traceback.py
+-rw-r--r--  2.0 unx     5073 b- defN 24-Mar-28 16:25 modal/_tunnel.py
+-rw-r--r--  2.0 unx     1337 b- defN 24-Mar-28 16:25 modal/_tunnel.pyi
+-rw-r--r--  2.0 unx     3609 b- defN 24-Mar-28 16:25 modal/_watcher.py
+-rw-r--r--  2.0 unx    15555 b- defN 24-Mar-28 16:25 modal/app.py
+-rw-r--r--  2.0 unx     8360 b- defN 24-Mar-28 16:25 modal/app.pyi
+-rw-r--r--  2.0 unx     2524 b- defN 24-Mar-28 16:25 modal/call_graph.py
+-rw-r--r--  2.0 unx     9751 b- defN 24-Mar-28 16:25 modal/client.py
+-rw-r--r--  2.0 unx     2942 b- defN 24-Mar-28 16:25 modal/cloud_bucket_mount.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-Mar-28 16:25 modal/cloud_bucket_mount.pyi
+-rw-r--r--  2.0 unx    12799 b- defN 24-Mar-28 16:25 modal/cls.py
+-rw-r--r--  2.0 unx     6482 b- defN 24-Mar-28 16:25 modal/cls.pyi
+-rw-r--r--  2.0 unx     9934 b- defN 24-Mar-28 16:25 modal/config.py
+-rw-r--r--  2.0 unx    10194 b- defN 24-Mar-28 16:25 modal/dict.py
+-rw-r--r--  2.0 unx     5809 b- defN 24-Mar-28 16:25 modal/dict.pyi
+-rw-r--r--  2.0 unx     2452 b- defN 24-Mar-28 16:25 modal/environments.py
+-rw-r--r--  2.0 unx     1439 b- defN 24-Mar-28 16:25 modal/environments.pyi
+-rw-r--r--  2.0 unx     5865 b- defN 24-Mar-28 16:25 modal/exception.py
+-rw-r--r--  2.0 unx      293 b- defN 24-Mar-28 16:25 modal/experimental.py
+-rw-r--r--  2.0 unx    63450 b- defN 24-Mar-28 16:25 modal/functions.py
+-rw-r--r--  2.0 unx    19548 b- defN 24-Mar-28 16:25 modal/functions.pyi
+-rw-r--r--  2.0 unx     8038 b- defN 24-Mar-28 16:25 modal/gpu.py
+-rw-r--r--  2.0 unx    62658 b- defN 24-Mar-28 16:25 modal/image.py
+-rw-r--r--  2.0 unx    17457 b- defN 24-Mar-28 16:25 modal/image.pyi
+-rw-r--r--  2.0 unx    23182 b- defN 24-Mar-28 16:25 modal/mount.py
+-rw-r--r--  2.0 unx     9588 b- defN 24-Mar-28 16:25 modal/mount.pyi
+-rw-r--r--  2.0 unx    14356 b- defN 24-Mar-28 16:25 modal/network_file_system.py
+-rw-r--r--  2.0 unx     6404 b- defN 24-Mar-28 16:25 modal/network_file_system.pyi
+-rw-r--r--  2.0 unx     8258 b- defN 24-Mar-28 16:25 modal/object.py
+-rw-r--r--  2.0 unx     7573 b- defN 24-Mar-28 16:25 modal/object.pyi
+-rw-r--r--  2.0 unx    19988 b- defN 24-Mar-28 16:25 modal/partial_function.py
+-rw-r--r--  2.0 unx     6270 b- defN 24-Mar-28 16:25 modal/partial_function.pyi
+-rw-r--r--  2.0 unx     1307 b- defN 24-Mar-28 16:25 modal/proxy.py
+-rw-r--r--  2.0 unx      428 b- defN 24-Mar-28 16:25 modal/proxy.pyi
+-rw-r--r--  2.0 unx        0 b- defN 24-Mar-28 16:25 modal/py.typed
+-rw-r--r--  2.0 unx    12296 b- defN 24-Mar-28 16:25 modal/queue.py
+-rw-r--r--  2.0 unx     5673 b- defN 24-Mar-28 16:25 modal/queue.pyi
+-rw-r--r--  2.0 unx      400 b- defN 24-Mar-28 16:25 modal/requirements.312.txt
+-rw-r--r--  2.0 unx      502 b- defN 24-Mar-28 16:25 modal/requirements.txt
+-rw-r--r--  2.0 unx     3730 b- defN 24-Mar-28 16:25 modal/retries.py
+-rw-r--r--  2.0 unx    12295 b- defN 24-Mar-28 16:25 modal/runner.py
+-rw-r--r--  2.0 unx     3114 b- defN 24-Mar-28 16:25 modal/runner.pyi
+-rw-r--r--  2.0 unx    15301 b- defN 24-Mar-28 16:25 modal/sandbox.py
+-rw-r--r--  2.0 unx     6466 b- defN 24-Mar-28 16:25 modal/sandbox.pyi
+-rw-r--r--  2.0 unx     2621 b- defN 24-Mar-28 16:25 modal/schedule.py
+-rw-r--r--  2.0 unx      662 b- defN 24-Mar-28 16:25 modal/scheduler_placement.py
+-rw-r--r--  2.0 unx     8898 b- defN 24-Mar-28 16:25 modal/secret.py
+-rw-r--r--  2.0 unx     2225 b- defN 24-Mar-28 16:25 modal/secret.pyi
+-rw-r--r--  2.0 unx     4681 b- defN 24-Mar-28 16:25 modal/serving.py
+-rw-r--r--  2.0 unx     1957 b- defN 24-Mar-28 16:25 modal/serving.pyi
+-rw-r--r--  2.0 unx      888 b- defN 24-Mar-28 16:25 modal/shared_volume.py
+-rw-r--r--  2.0 unx      405 b- defN 24-Mar-28 16:25 modal/shared_volume.pyi
+-rw-r--r--  2.0 unx    33852 b- defN 24-Mar-28 16:25 modal/stub.py
+-rw-r--r--  2.0 unx    17567 b- defN 24-Mar-28 16:25 modal/stub.pyi
+-rw-r--r--  2.0 unx     6771 b- defN 24-Mar-28 16:25 modal/token_flow.py
+-rw-r--r--  2.0 unx     1890 b- defN 24-Mar-28 16:25 modal/token_flow.pyi
+-rw-r--r--  2.0 unx    24410 b- defN 24-Mar-28 16:25 modal/volume.py
+-rw-r--r--  2.0 unx     9111 b- defN 24-Mar-28 16:25 modal/volume.pyi
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/_utils/__init__.py
+-rw-r--r--  2.0 unx      465 b- defN 24-Mar-28 16:25 modal/_utils/app_utils.py
+-rw-r--r--  2.0 unx    12930 b- defN 24-Mar-28 16:25 modal/_utils/async_utils.py
+-rw-r--r--  2.0 unx    15054 b- defN 24-Mar-28 16:25 modal/_utils/blob_utils.py
+-rw-r--r--  2.0 unx    13520 b- defN 24-Mar-28 16:25 modal/_utils/function_utils.py
+-rw-r--r--  2.0 unx     7780 b- defN 24-Mar-28 16:25 modal/_utils/grpc_testing.py
+-rw-r--r--  2.0 unx     9322 b- defN 24-Mar-28 16:25 modal/_utils/grpc_utils.py
+-rw-r--r--  2.0 unx     1597 b- defN 24-Mar-28 16:25 modal/_utils/hash_utils.py
+-rw-r--r--  2.0 unx     1426 b- defN 24-Mar-28 16:25 modal/_utils/http_utils.py
+-rw-r--r--  2.0 unx     1311 b- defN 24-Mar-28 16:25 modal/_utils/logger.py
+-rw-r--r--  2.0 unx     2327 b- defN 24-Mar-28 16:25 modal/_utils/mount_utils.py
+-rw-r--r--  2.0 unx     1640 b- defN 24-Mar-28 16:25 modal/_utils/package_utils.py
+-rw-r--r--  2.0 unx     3871 b- defN 24-Mar-28 16:25 modal/_utils/rand_pb_testing.py
+-rw-r--r--  2.0 unx     3633 b- defN 24-Mar-28 16:25 modal/_utils/shell_utils.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/_vendor/__init__.py
+-rw-r--r--  2.0 unx    22144 b- defN 24-Mar-28 16:25 modal/_vendor/a2wsgi_wsgi.py
+-rw-r--r--  2.0 unx    55225 b- defN 24-Mar-28 16:25 modal/_vendor/cloudpickle.py
+-rw-r--r--  2.0 unx     9722 b- defN 24-Mar-28 16:25 modal/_vendor/tblib.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/cli/__init__.py
+-rw-r--r--  2.0 unx     2601 b- defN 24-Mar-28 16:25 modal/cli/_download.py
+-rw-r--r--  2.0 unx     3068 b- defN 24-Mar-28 16:25 modal/cli/app.py
+-rw-r--r--  2.0 unx     1294 b- defN 24-Mar-28 16:25 modal/cli/config.py
+-rw-r--r--  2.0 unx     1748 b- defN 24-Mar-28 16:25 modal/cli/container.py
+-rw-r--r--  2.0 unx     3538 b- defN 24-Mar-28 16:25 modal/cli/entry_point.py
+-rw-r--r--  2.0 unx     3414 b- defN 24-Mar-28 16:25 modal/cli/environment.py
+-rw-r--r--  2.0 unx     9083 b- defN 24-Mar-28 16:25 modal/cli/import_refs.py
+-rw-r--r--  2.0 unx     1702 b- defN 24-Mar-28 16:25 modal/cli/launch.py
+-rw-r--r--  2.0 unx     8449 b- defN 24-Mar-28 16:25 modal/cli/network_file_system.py
+-rw-r--r--  2.0 unx     3109 b- defN 24-Mar-28 16:25 modal/cli/profile.py
+-rw-r--r--  2.0 unx    13798 b- defN 24-Mar-28 16:25 modal/cli/run.py
+-rw-r--r--  2.0 unx     4181 b- defN 24-Mar-28 16:25 modal/cli/secret.py
+-rw-r--r--  2.0 unx     1875 b- defN 24-Mar-28 16:25 modal/cli/token.py
+-rw-r--r--  2.0 unx     1330 b- defN 24-Mar-28 16:25 modal/cli/utils.py
+-rw-r--r--  2.0 unx    10871 b- defN 24-Mar-28 16:25 modal/cli/volume.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/cli/programs/__init__.py
+-rw-r--r--  2.0 unx     2010 b- defN 24-Mar-28 16:25 modal/cli/programs/run_jupyter.py
+-rw-r--r--  2.0 unx     1765 b- defN 24-Mar-28 16:25 modal/cli/programs/vscode.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal/extensions/__init__.py
+-rw-r--r--  2.0 unx      988 b- defN 24-Mar-28 16:25 modal/extensions/ipython.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_docs/__init__.py
+-rw-r--r--  2.0 unx     3801 b- defN 24-Mar-28 16:25 modal_docs/gen_cli_docs.py
+-rw-r--r--  2.0 unx     6555 b- defN 24-Mar-28 16:25 modal_docs/gen_reference_docs.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_docs/mdmd/__init__.py
+-rw-r--r--  2.0 unx     6268 b- defN 24-Mar-28 16:25 modal_docs/mdmd/mdmd.py
+-rw-r--r--  2.0 unx     3095 b- defN 24-Mar-28 16:25 modal_docs/mdmd/signatures.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_global_objects/__init__.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_global_objects/images/__init__.py
+-rw-r--r--  2.0 unx      324 b- defN 24-Mar-28 16:25 modal_global_objects/images/conda.py
+-rw-r--r--  2.0 unx      330 b- defN 24-Mar-28 16:25 modal_global_objects/images/debian_slim.py
+-rw-r--r--  2.0 unx      329 b- defN 24-Mar-28 16:25 modal_global_objects/images/micromamba.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_global_objects/mounts/__init__.py
+-rw-r--r--  2.0 unx      767 b- defN 24-Mar-28 16:25 modal_global_objects/mounts/modal_client_package.py
+-rw-r--r--  2.0 unx     1870 b- defN 24-Mar-28 16:25 modal_global_objects/mounts/python_standalone.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 modal_proto/__init__.py
+-rw-r--r--  2.0 unx    56021 b- defN 24-Mar-28 16:25 modal_proto/api.proto
+-rw-r--r--  2.0 unx    85328 b- defN 24-Mar-28 16:25 modal_proto/api_grpc.py
+-rw-r--r--  2.0 unx   217994 b- defN 24-Mar-28 16:25 modal_proto/api_pb2.py
+-rw-r--r--  2.0 unx   184288 b- defN 24-Mar-28 16:25 modal_proto/api_pb2_grpc.py
+-rw-r--r--  2.0 unx      488 b- defN 24-Mar-28 16:25 modal_proto/options.proto
+-rw-r--r--  2.0 unx      125 b- defN 24-Mar-28 16:25 modal_proto/options_grpc.py
+-rw-r--r--  2.0 unx     1827 b- defN 24-Mar-28 16:25 modal_proto/options_pb2.py
+-rw-r--r--  2.0 unx      159 b- defN 24-Mar-28 16:25 modal_proto/options_pb2_grpc.py
+-rw-r--r--  2.0 unx      470 b- defN 24-Mar-28 16:25 modal_version/__init__.py
+-rw-r--r--  2.0 unx      105 b- defN 24-Mar-28 16:25 modal_version/__main__.py
+-rw-r--r--  2.0 unx      148 b- defN 24-Mar-28 16:25 modal_version/_version_generated.py
+-rw-r--r--  2.0 unx       28 b- defN 24-Mar-28 16:25 test/__init__.py
+-rw-r--r--  2.0 unx      203 b- defN 24-Mar-28 16:25 test/aio_test.py
+-rw-r--r--  2.0 unx     6792 b- defN 24-Mar-28 16:25 test/async_utils_test.py
+-rw-r--r--  2.0 unx     2539 b- defN 24-Mar-28 16:25 test/blob_test.py
+-rw-r--r--  2.0 unx     4680 b- defN 24-Mar-28 16:25 test/cli_imports_test.py
+-rw-r--r--  2.0 unx    26607 b- defN 24-Mar-28 16:25 test/cli_test.py
+-rw-r--r--  2.0 unx     6426 b- defN 24-Mar-28 16:25 test/client_test.py
+-rw-r--r--  2.0 unx    15906 b- defN 24-Mar-28 16:25 test/cls_test.py
+-rw-r--r--  2.0 unx     4768 b- defN 24-Mar-28 16:25 test/config_test.py
+-rw-r--r--  2.0 unx    59766 b- defN 24-Mar-28 16:25 test/conftest.py
+-rw-r--r--  2.0 unx      895 b- defN 24-Mar-28 16:25 test/container_app_test.py
+-rw-r--r--  2.0 unx    50061 b- defN 24-Mar-28 16:25 test/container_test.py
+-rw-r--r--  2.0 unx      405 b- defN 24-Mar-28 16:25 test/cpu_test.py
+-rw-r--r--  2.0 unx     1871 b- defN 24-Mar-28 16:25 test/decorator_test.py
+-rw-r--r--  2.0 unx     1055 b- defN 24-Mar-28 16:25 test/deprecation_test.py
+-rw-r--r--  2.0 unx      851 b- defN 24-Mar-28 16:25 test/dict_test.py
+-rw-r--r--  2.0 unx     2152 b- defN 24-Mar-28 16:25 test/e2e_test.py
+-rw-r--r--  2.0 unx      165 b- defN 24-Mar-28 16:25 test/error_test.py
+-rw-r--r--  2.0 unx      971 b- defN 24-Mar-28 16:25 test/function_serialization_test.py
+-rw-r--r--  2.0 unx    19697 b- defN 24-Mar-28 16:25 test/function_test.py
+-rw-r--r--  2.0 unx     1892 b- defN 24-Mar-28 16:25 test/function_utils_test.py
+-rw-r--r--  2.0 unx     4611 b- defN 24-Mar-28 16:25 test/gpu_test.py
+-rw-r--r--  2.0 unx     5048 b- defN 24-Mar-28 16:25 test/grpc_utils_test.py
+-rw-r--r--  2.0 unx     1443 b- defN 24-Mar-28 16:25 test/helpers.py
+-rw-r--r--  2.0 unx    25515 b- defN 24-Mar-28 16:25 test/image_test.py
+-rw-r--r--  2.0 unx     2742 b- defN 24-Mar-28 16:25 test/live_reload_test.py
+-rw-r--r--  2.0 unx     2172 b- defN 24-Mar-28 16:25 test/lookup_test.py
+-rw-r--r--  2.0 unx     5118 b- defN 24-Mar-28 16:25 test/mdmd_test.py
+-rw-r--r--  2.0 unx     5489 b- defN 24-Mar-28 16:25 test/mount_test.py
+-rw-r--r--  2.0 unx    12471 b- defN 24-Mar-28 16:25 test/mounted_files_test.py
+-rw-r--r--  2.0 unx     6073 b- defN 24-Mar-28 16:25 test/network_file_system_test.py
+-rw-r--r--  2.0 unx     2157 b- defN 24-Mar-28 16:25 test/notebook_test.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-Mar-28 16:25 test/object_test.py
+-rw-r--r--  2.0 unx      795 b- defN 24-Mar-28 16:25 test/package_utils_test.py
+-rw-r--r--  2.0 unx     3270 b- defN 24-Mar-28 16:25 test/queue_test.py
+-rw-r--r--  2.0 unx     1751 b- defN 24-Mar-28 16:25 test/resolver_test.py
+-rw-r--r--  2.0 unx     1817 b- defN 24-Mar-28 16:25 test/retries_test.py
+-rw-r--r--  2.0 unx     2582 b- defN 24-Mar-28 16:25 test/runner_test.py
+-rw-r--r--  2.0 unx     5282 b- defN 24-Mar-28 16:25 test/sandbox_test.py
+-rw-r--r--  2.0 unx      359 b- defN 24-Mar-28 16:25 test/schedule_test.py
+-rw-r--r--  2.0 unx      742 b- defN 24-Mar-28 16:25 test/scheduler_placement_test.py
+-rw-r--r--  2.0 unx     2527 b- defN 24-Mar-28 16:25 test/secret_test.py
+-rw-r--r--  2.0 unx     1867 b- defN 24-Mar-28 16:25 test/serialization_test.py
+-rw-r--r--  2.0 unx      558 b- defN 24-Mar-28 16:25 test/stub_composition_test.py
+-rw-r--r--  2.0 unx    11284 b- defN 24-Mar-28 16:25 test/stub_test.py
+-rw-r--r--  2.0 unx     7320 b- defN 24-Mar-28 16:25 test/test_asgi_wrapper.py
+-rw-r--r--  2.0 unx      614 b- defN 24-Mar-28 16:25 test/token_flow_test.py
+-rw-r--r--  2.0 unx     4747 b- defN 24-Mar-28 16:25 test/traceback_test.py
+-rw-r--r--  2.0 unx      768 b- defN 24-Mar-28 16:25 test/tunnel_test.py
+-rw-r--r--  2.0 unx     2679 b- defN 24-Mar-28 16:25 test/utils_test.py
+-rw-r--r--  2.0 unx      403 b- defN 24-Mar-28 16:25 test/version_test.py
+-rw-r--r--  2.0 unx    12461 b- defN 24-Mar-28 16:25 test/volume_test.py
+-rw-r--r--  2.0 unx     1193 b- defN 24-Mar-28 16:25 test/watcher_test.py
+-rw-r--r--  2.0 unx     4205 b- defN 24-Mar-28 16:25 test/webhook_test.py
+-rw-r--r--  2.0 unx    10173 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2301 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx       46 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       69 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    15540 b- defN 24-Mar-28 16:25 modal-0.62.9.dist-info/RECORD
+197 files, 1877599 bytes uncompressed, 413184 bytes compressed:  78.0%
```

## zipnote {}

```diff
@@ -6,21 +6,18 @@
 
 Filename: modal/_asgi.py
 Comment: 
 
 Filename: modal/_container_entrypoint.py
 Comment: 
 
-Filename: modal/_container_exec.py
-Comment: 
-
-Filename: modal/_container_io_manager.py
+Filename: modal/_container_entrypoint.pyi
 Comment: 
 
-Filename: modal/_container_io_manager.pyi
+Filename: modal/_container_exec.py
 Comment: 
 
 Filename: modal/_ipython.py
 Comment: 
 
 Filename: modal/_location.py
 Comment: 
@@ -33,17 +30,14 @@
 
 Filename: modal/_pty.py
 Comment: 
 
 Filename: modal/_resolver.py
 Comment: 
 
-Filename: modal/_resources.py
-Comment: 
-
 Filename: modal/_sandbox_shell.py
 Comment: 
 
 Filename: modal/_serialization.py
 Comment: 
 
 Filename: modal/_traceback.py
@@ -60,29 +54,20 @@
 
 Filename: modal/app.py
 Comment: 
 
 Filename: modal/app.pyi
 Comment: 
 
-Filename: modal/app_utils.py
-Comment: 
-
-Filename: modal/app_utils.pyi
-Comment: 
-
 Filename: modal/call_graph.py
 Comment: 
 
 Filename: modal/client.py
 Comment: 
 
-Filename: modal/client.pyi
-Comment: 
-
 Filename: modal/cloud_bucket_mount.py
 Comment: 
 
 Filename: modal/cloud_bucket_mount.pyi
 Comment: 
 
 Filename: modal/cls.py
@@ -162,26 +147,29 @@
 
 Filename: modal/queue.py
 Comment: 
 
 Filename: modal/queue.pyi
 Comment: 
 
+Filename: modal/requirements.312.txt
+Comment: 
+
+Filename: modal/requirements.txt
+Comment: 
+
 Filename: modal/retries.py
 Comment: 
 
 Filename: modal/runner.py
 Comment: 
 
 Filename: modal/runner.pyi
 Comment: 
 
-Filename: modal/running_app.py
-Comment: 
-
 Filename: modal/sandbox.py
 Comment: 
 
 Filename: modal/sandbox.pyi
 Comment: 
 
 Filename: modal/schedule.py
@@ -204,14 +192,20 @@
 
 Filename: modal/shared_volume.py
 Comment: 
 
 Filename: modal/shared_volume.pyi
 Comment: 
 
+Filename: modal/stub.py
+Comment: 
+
+Filename: modal/stub.pyi
+Comment: 
+
 Filename: modal/token_flow.py
 Comment: 
 
 Filename: modal/token_flow.pyi
 Comment: 
 
 Filename: modal/volume.py
@@ -333,23 +327,14 @@
 
 Filename: modal/extensions/__init__.py
 Comment: 
 
 Filename: modal/extensions/ipython.py
 Comment: 
 
-Filename: modal/requirements/2023.12.312.txt
-Comment: 
-
-Filename: modal/requirements/2023.12.txt
-Comment: 
-
-Filename: modal/requirements/2024.04.txt
-Comment: 
-
 Filename: modal_docs/__init__.py
 Comment: 
 
 Filename: modal_docs/gen_cli_docs.py
 Comment: 
 
 Filename: modal_docs/gen_reference_docs.py
@@ -441,17 +426,14 @@
 
 Filename: test/cli_test.py
 Comment: 
 
 Filename: test/client_test.py
 Comment: 
 
-Filename: test/cloud_bucket_mount_test.py
-Comment: 
-
 Filename: test/cls_test.py
 Comment: 
 
 Filename: test/config_test.py
 Comment: 
 
 Filename: test/conftest.py
@@ -585,26 +567,26 @@
 
 Filename: test/watcher_test.py
 Comment: 
 
 Filename: test/webhook_test.py
 Comment: 
 
-Filename: modal-0.62.87.dist-info/LICENSE
+Filename: modal-0.62.9.dist-info/LICENSE
 Comment: 
 
-Filename: modal-0.62.87.dist-info/METADATA
+Filename: modal-0.62.9.dist-info/METADATA
 Comment: 
 
-Filename: modal-0.62.87.dist-info/WHEEL
+Filename: modal-0.62.9.dist-info/WHEEL
 Comment: 
 
-Filename: modal-0.62.87.dist-info/entry_points.txt
+Filename: modal-0.62.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: modal-0.62.87.dist-info/top_level.txt
+Filename: modal-0.62.9.dist-info/top_level.txt
 Comment: 
 
-Filename: modal-0.62.87.dist-info/RECORD
+Filename: modal-0.62.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## modal/__init__.py

```diff
@@ -5,18 +5,16 @@
     raise RuntimeError("This version of Modal requires at least Python 3.8")
 if sys.version_info[:2] >= (3, 13):
     raise RuntimeError("This version of Modal does not support Python 3.13+")
 
 from modal_version import __version__
 
 try:
-
-    from ._container_io_manager import interact, is_local
     from ._tunnel import Tunnel, forward
-    from .app import App, Stub
+    from .app import container_app, interact, is_local
     from .client import Client
     from .cloud_bucket_mount import CloudBucketMount
     from .cls import Cls
     from .dict import Dict
     from .exception import Error
     from .functions import Function, current_function_call_id, current_input_id
     from .image import Image
@@ -27,27 +25,27 @@
     from .queue import Queue
     from .retries import Retries
     from .sandbox import Sandbox
     from .schedule import Cron, Period
     from .scheduler_placement import SchedulerPlacement
     from .secret import Secret
     from .shared_volume import SharedVolume
+    from .stub import Stub
     from .volume import Volume
 except Exception:
     print()
     print("#" * 80)
     print("#" + "Something with the Modal installation seems broken.".center(78) + "#")
     print("#" + "Please email support@modal.com and we will try to help!".center(78) + "#")
     print("#" * 80)
     print()
     raise
 
 __all__ = [
     "__version__",
-    "App",
     "Client",
     "Cls",
     "Cron",
     "Dict",
     "Error",
     "Function",
     "Image",
@@ -63,14 +61,15 @@
     "Secret",
     "SharedVolume",
     "Stub",
     "Tunnel",
     "Volume",
     "asgi_app",
     "build",
+    "container_app",
     "current_function_call_id",
     "current_input_id",
     "enter",
     "exit",
     "forward",
     "is_local",
     "method",
```

## modal/_container_entrypoint.py

```diff
@@ -1,61 +1,67 @@
 # Copyright Modal Labs 2022
+from __future__ import annotations
+
 import asyncio
 import base64
+import contextlib
 import importlib
 import inspect
+import json
+import math
+import os
 import signal
 import sys
 import threading
 import time
+import traceback
+from collections.abc import Iterable
 from dataclasses import dataclass
-from typing import TYPE_CHECKING, Any, Callable, List, Optional, Sequence, Type
+from pathlib import Path
+from typing import TYPE_CHECKING, Any, AsyncGenerator, AsyncIterator, Callable, List, Optional, Set, Tuple, Type
 
-from google.protobuf.message import Message
-from synchronicity import Interface
+from grpclib import Status
 
 from modal_proto import api_pb2
 
 from ._asgi import (
     asgi_app_wrapper,
     get_ip_address,
     wait_for_web_server,
     web_server_proxy,
     webhook_asgi_app,
     wsgi_app_wrapper,
 )
-from ._container_io_manager import ContainerIOManager, UserException, _ContainerIOManager, interact
 from ._proxy_tunnel import proxy_tunnel
-from ._serialization import deserialize
-from ._utils.async_utils import TaskContext, synchronizer
+from ._serialization import deserialize, deserialize_data_format, serialize, serialize_data_format
+from ._traceback import extract_traceback
+from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
+from ._utils.blob_utils import MAX_OBJECT_SIZE_BYTES, blob_download, blob_upload
 from ._utils.function_utils import LocalFunctionError, is_async as get_is_async, is_global_function, method_has_params
-from .app import App, _App
-from .client import Client, _Client
+from ._utils.grpc_utils import retry_transient_errors
+from .app import ContainerApp, _container_app, _ContainerApp, interact
+from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, Client, _Client
 from .cls import Cls
-from .config import logger
-from .exception import ExecutionError, InputCancellation, InvalidError
-from .functions import Function, _Function, _set_current_context_ids
+from .config import config, logger
+from .exception import InputCancellation, InvalidError
+from .functions import Function, _Function, _set_current_context_ids, _stream_function_call_data
 from .partial_function import _find_callables_for_obj, _PartialFunctionFlags
-from .running_app import RunningApp
+from .stub import _Stub
 
 if TYPE_CHECKING:
     from types import ModuleType
 
+MAX_OUTPUT_BATCH_SIZE: int = 49
 
-@dataclass
-class ImportedFunction:
-    obj: Any
-    fun: Callable
-    app: Optional[_App]
-    is_async: bool
-    is_generator: bool
-    data_format: int  # api_pb2.DataFormat
-    input_concurrency: int
-    is_auto_snapshot: bool
-    function: _Function
+RTT_S: float = 0.5  # conservative estimate of RTT in seconds.
+
+
+class UserException(Exception):
+    # Used to shut down the task gracefully
+    pass
 
 
 class UserCodeEventLoop:
     """Run an async event loop as a context manager and handle signals.
 
     This will run all *user supplied* async code, i.e. async functions, as well as async enter/exit managers
 
@@ -112,56 +118,581 @@
                 raise KeyboardInterrupt()
         finally:
             self.loop.remove_signal_handler(signal.SIGUSR1)
             if not ignore_sigint:
                 self.loop.remove_signal_handler(signal.SIGINT)
 
 
+class _FunctionIOManager:
+    """Synchronizes all RPC calls and network operations for a running container.
+
+    TODO: maybe we shouldn't synchronize the whole class.
+    Then we could potentially move a bunch of the global functions onto it.
+    """
+
+    _GENERATOR_STOP_SENTINEL = object()
+
+    def __init__(self, container_args: api_pb2.ContainerArguments, client: _Client):
+        self.cancelled_input_ids: Set[str] = set()
+        self.task_id = container_args.task_id
+        self.function_id = container_args.function_id
+        self.app_id = container_args.app_id
+        self.function_def = container_args.function_def
+        self.checkpoint_id = container_args.checkpoint_id
+
+        self.calls_completed = 0
+        self.total_user_time: float = 0.0
+        self.current_input_id: Optional[str] = None
+        self.current_input_started_at: Optional[float] = None
+
+        self._stub_name = self.function_def.stub_name
+        self._input_concurrency: Optional[int] = None
+
+        self._semaphore: Optional[asyncio.Semaphore] = None
+        self._environment_name = container_args.environment_name
+        self._waiting_for_checkpoint = False
+        self._heartbeat_loop = None
+
+        self._client = client
+        assert isinstance(self._client, _Client)
+
+    async def initialize_app(self) -> _ContainerApp:
+        await _container_app.init(self._client, self.app_id, self._stub_name, self._environment_name, self.function_def)
+        return _container_app
+
+    async def _run_heartbeat_loop(self):
+        while 1:
+            t0 = time.monotonic()
+            try:
+                if await self._heartbeat_handle_cancellations():
+                    # got a cancellation event, fine to start another heartbeat immediately
+                    # since the cancellation queue should be empty on the worker server
+                    # however, we wait at least 1s to prevent short-circuiting the heartbeat loop
+                    # in case there is ever a bug. This means it will take at least 1s between
+                    # two subsequent cancellations on the same task at the moment
+                    await asyncio.sleep(1.0)
+                    continue
+            except Exception as exc:
+                # don't stop heartbeat loop if there are transient exceptions!
+                time_elapsed = time.monotonic() - t0
+                error = exc
+                logger.warning(f"Heartbeat attempt failed ({time_elapsed=}, {error=})")
+
+            heartbeat_duration = time.monotonic() - t0
+            time_until_next_hearbeat = max(0.0, HEARTBEAT_INTERVAL - heartbeat_duration)
+            await asyncio.sleep(time_until_next_hearbeat)
+
+    async def _heartbeat_handle_cancellations(self) -> bool:
+        # Return True if a cancellation event was received, in that case we shouldn't wait too long for another heartbeat
+
+        # Don't send heartbeats for tasks waiting to be checkpointed.
+        # Calling gRPC methods open new connections which block the
+        # checkpointing process.
+        if self._waiting_for_checkpoint:
+            return False
+
+        request = api_pb2.ContainerHeartbeatRequest(supports_graceful_input_cancellation=True)
+        if self.current_input_id is not None:
+            request.current_input_id = self.current_input_id
+        if self.current_input_started_at is not None:
+            request.current_input_started_at = self.current_input_started_at
+
+        # TODO(erikbern): capture exceptions?
+        response = await retry_transient_errors(
+            self._client.stub.ContainerHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT
+        )
+
+        if response.HasField("cancel_input_event"):
+            # Pause processing of the current input by signaling self a SIGUSR1.
+            input_ids_to_cancel = response.cancel_input_event.input_ids
+            if input_ids_to_cancel:
+                if self._input_concurrency > 1:
+                    logger.info(
+                        "Shutting down task to stop some subset of inputs (concurrent functions don't support fine-grained cancellation)"
+                    )
+                    # This is equivalent to a task cancellation or preemption from worker code,
+                    # except we do not send a SIGKILL to forcefully exit after 30 seconds.
+                    #
+                    # SIGINT always interrupts the main thread, but not any auxiliary threads. On a
+                    # sync function without concurrent inputs, this raises a KeyboardInterrupt. When
+                    # there are concurrent inputs, we cannot interrupt the thread pool, but the
+                    # interpreter stops waiting for daemon threads and exits. On async functions,
+                    # this signal lands outside the event loop, stopping `run_until_complete()`.
+                    os.kill(os.getpid(), signal.SIGINT)
+
+                elif self.current_input_id in input_ids_to_cancel:
+                    # This goes to a registered signal handler for sync Modal functions, or to the
+                    # `SignalHandlingEventLoop` for async functions.
+                    #
+                    # We only send this signal on functions that do not have concurrent inputs enabled.
+                    # This allows us to do fine-grained input cancellation. On sync functions, the
+                    # SIGUSR1 signal should interrupt the main thread where user code is running,
+                    # raising an InputCancellation() exception. On async functions, the signal should
+                    # reach a handler in SignalHandlingEventLoop, which cancels the task.
+                    os.kill(os.getpid(), signal.SIGUSR1)
+            return True
+        return False
+
+    @contextlib.asynccontextmanager
+    async def heartbeats(self):
+        async with TaskContext() as tc:
+            self._heartbeat_loop = t = tc.create_task(self._run_heartbeat_loop())
+            t.set_name("heartbeat loop")
+            try:
+                yield
+            finally:
+                t.cancel()
+
+    def stop_heartbeat(self):
+        if self._heartbeat_loop:
+            self._heartbeat_loop.cancel()
+
+    async def get_serialized_function(self) -> Tuple[Optional[Any], Callable]:
+        # Fetch the serialized function definition
+        request = api_pb2.FunctionGetSerializedRequest(function_id=self.function_id)
+        response = await self._client.stub.FunctionGetSerialized(request)
+        fun = self.deserialize(response.function_serialized)
+
+        if response.class_serialized:
+            cls = self.deserialize(response.class_serialized)
+        else:
+            cls = None
+
+        return cls, fun
+
+    def serialize(self, obj: Any) -> bytes:
+        return serialize(obj)
+
+    def deserialize(self, data: bytes) -> Any:
+        return deserialize(data, self._client)
+
+    @synchronizer.no_io_translation
+    def serialize_data_format(self, obj: Any, data_format: int) -> bytes:
+        return serialize_data_format(obj, data_format)
+
+    def deserialize_data_format(self, data: bytes, data_format: int) -> Any:
+        return deserialize_data_format(data, data_format, self._client)
+
+    async def get_data_in(self, function_call_id: str) -> AsyncIterator[Any]:
+        """Read from the `data_in` stream of a function call."""
+        async for data in _stream_function_call_data(self._client, function_call_id, "data_in"):
+            yield data
+
+    async def put_data_out(
+        self,
+        function_call_id: str,
+        start_index: int,
+        data_format: int,
+        messages_bytes: List[Any],
+    ) -> None:
+        """Put data onto the `data_out` stream of a function call.
+
+        This is used for generator outputs, which includes web endpoint responses. Note that this
+        was introduced as a performance optimization in client version 0.57, so older clients will
+        still use the previous Postgres-backed system based on `FunctionPutOutputs()`.
+        """
+        data_chunks: List[api_pb2.DataChunk] = []
+        for i, message_bytes in enumerate(messages_bytes):
+            chunk = api_pb2.DataChunk(data_format=data_format, index=start_index + i)  # type: ignore
+            if len(message_bytes) > MAX_OBJECT_SIZE_BYTES:
+                chunk.data_blob_id = await blob_upload(message_bytes, self._client.stub)
+            else:
+                chunk.data = message_bytes
+            data_chunks.append(chunk)
+
+        req = api_pb2.FunctionCallPutDataRequest(function_call_id=function_call_id, data_chunks=data_chunks)
+        await retry_transient_errors(self._client.stub.FunctionCallPutDataOut, req)
+
+    async def generator_output_task(self, function_call_id: str, data_format: int, message_rx: asyncio.Queue) -> None:
+        """Task that feeds generator outputs into a function call's `data_out` stream."""
+        index = 1
+        received_sentinel = False
+        while not received_sentinel:
+            message = await message_rx.get()
+            if message is self._GENERATOR_STOP_SENTINEL:
+                break
+            # ASGI 'http.response.start' and 'http.response.body' msgs are observed to be separated by 1ms.
+            # If we don't sleep here for 1ms we end up with an extra call to .put_data_out().
+            if index == 1:
+                await asyncio.sleep(0.001)
+            messages_bytes = [serialize_data_format(message, data_format)]
+            total_size = len(messages_bytes[0]) + 512
+            while total_size < 16 * 1024 * 1024:  # 16 MiB, maximum size in a single message
+                try:
+                    message = message_rx.get_nowait()
+                except asyncio.QueueEmpty:
+                    break
+                if message is self._GENERATOR_STOP_SENTINEL:
+                    received_sentinel = True
+                    break
+                else:
+                    messages_bytes.append(serialize_data_format(message, data_format))
+                    total_size += len(messages_bytes[-1]) + 512  # 512 bytes for estimated framing overhead
+            await self.put_data_out(function_call_id, index, data_format, messages_bytes)
+            index += len(messages_bytes)
+
+    async def _queue_create(self, size: int) -> asyncio.Queue:
+        """Create a queue, on the synchronicity event loop (needed on Python 3.8 and 3.9)."""
+        return asyncio.Queue(size)
+
+    async def _queue_put(self, queue: asyncio.Queue, value: Any) -> None:
+        """Put a value onto a queue, using the synchronicity event loop."""
+        await queue.put(value)
+
+    async def populate_input_blobs(self, item: api_pb2.FunctionInput):
+        args = await blob_download(item.args_blob_id, self._client.stub)
+
+        # Mutating
+        item.ClearField("args_blob_id")
+        item.args = args
+        return item
+
+    def get_average_call_time(self) -> float:
+        if self.calls_completed == 0:
+            return 0
+
+        return self.total_user_time / self.calls_completed
+
+    def get_max_inputs_to_fetch(self):
+        if self.calls_completed == 0:
+            return 1
+
+        return math.ceil(RTT_S / max(self.get_average_call_time(), 1e-6))
+
+    @synchronizer.no_io_translation
+    async def _generate_inputs(self) -> AsyncIterator[Tuple[str, str, api_pb2.FunctionInput]]:
+        request = api_pb2.FunctionGetInputsRequest(function_id=self.function_id)
+        eof_received = False
+        iteration = 0
+        while not eof_received and _container_app.fetching_inputs:
+            request.average_call_time = self.get_average_call_time()
+            request.max_values = self.get_max_inputs_to_fetch()  # Deprecated; remove.
+            request.input_concurrency = self._input_concurrency
+
+            await self._semaphore.acquire()
+            yielded = False
+            try:
+                # If number of active inputs is at max queue size, this will block.
+                iteration += 1
+                response: api_pb2.FunctionGetInputsResponse = await retry_transient_errors(
+                    self._client.stub.FunctionGetInputs, request
+                )
+
+                if response.rate_limit_sleep_duration:
+                    logger.info(
+                        "Task exceeded rate limit, sleeping for %.2fs before trying again."
+                        % response.rate_limit_sleep_duration
+                    )
+                    await asyncio.sleep(response.rate_limit_sleep_duration)
+                elif response.inputs:
+                    # for input cancellations and concurrency logic we currently assume
+                    # that there is no input buffering in the container
+                    assert len(response.inputs) == 1
+
+                    for item in response.inputs:
+                        if item.kill_switch:
+                            logger.debug(f"Task {self.task_id} input kill signal input.")
+                            eof_received = True
+                            break
+                        if item.input_id in self.cancelled_input_ids:
+                            continue
+
+                        # If we got a pointer to a blob, download it from S3.
+                        if item.input.WhichOneof("args_oneof") == "args_blob_id":
+                            input_pb = await self.populate_input_blobs(item.input)
+                        else:
+                            input_pb = item.input
+
+                        # If yielded, allow semaphore to be released via complete_call
+                        yield (item.input_id, item.function_call_id, input_pb)
+                        yielded = True
+
+                        # We only support max_inputs = 1 at the moment
+                        if item.input.final_input or self.function_def.max_inputs == 1:
+                            eof_received = True
+                            break
+            finally:
+                if not yielded:
+                    self._semaphore.release()
+
+    @synchronizer.no_io_translation
+    async def run_inputs_outputs(self, input_concurrency: int = 1) -> AsyncIterator[Tuple[str, str, Any, Any]]:
+        # Ensure we do not fetch new inputs when container is too busy.
+        # Before trying to fetch an input, acquire the semaphore:
+        # - if no input is fetched, release the semaphore.
+        # - or, when the output for the fetched input is sent, release the semaphore.
+        self._input_concurrency = input_concurrency
+        self._semaphore = asyncio.Semaphore(input_concurrency)
+
+        try:
+            async for input_id, function_call_id, input_pb in self._generate_inputs():
+                args, kwargs = self.deserialize(input_pb.args) if input_pb.args else ((), {})
+                self.current_input_id, self.current_input_started_at = (input_id, time.time())
+                yield input_id, function_call_id, args, kwargs
+                self.current_input_id, self.current_input_started_at = (None, None)
+        finally:
+            # collect all active input slots, meaning all inputs have wrapped up.
+            for _ in range(input_concurrency):
+                await self._semaphore.acquire()
+
+    async def _push_output(self, input_id, started_at: float, data_format=api_pb2.DATA_FORMAT_UNSPECIFIED, **kwargs):
+        # upload data to S3 if too big.
+        if "data" in kwargs and kwargs["data"] and len(kwargs["data"]) > MAX_OBJECT_SIZE_BYTES:
+            data_blob_id = await blob_upload(kwargs["data"], self._client.stub)
+            # mutating kwargs.
+            del kwargs["data"]
+            kwargs["data_blob_id"] = data_blob_id
+
+        output = api_pb2.FunctionPutOutputsItem(
+            input_id=input_id,
+            input_started_at=started_at,
+            output_created_at=time.time(),
+            result=api_pb2.GenericResult(**kwargs),
+            data_format=data_format,
+        )
+
+        await retry_transient_errors(
+            self._client.stub.FunctionPutOutputs,
+            api_pb2.FunctionPutOutputsRequest(outputs=[output]),
+            additional_status_codes=[Status.RESOURCE_EXHAUSTED],
+            max_retries=None,  # Retry indefinitely, trying every 1s.
+        )
+
+    def serialize_exception(self, exc: BaseException) -> Optional[bytes]:
+        try:
+            return self.serialize(exc)
+        except Exception as serialization_exc:
+            logger.info(f"Failed to serialize exception {exc}: {serialization_exc}")
+            # We can't always serialize exceptions.
+            return None
+
+    def serialize_traceback(self, exc: BaseException) -> Tuple[Optional[bytes], Optional[bytes]]:
+        serialized_tb, tb_line_cache = None, None
+
+        try:
+            tb_dict, line_cache = extract_traceback(exc, self.task_id)
+            serialized_tb = self.serialize(tb_dict)
+            tb_line_cache = self.serialize(line_cache)
+        except Exception:
+            logger.info("Failed to serialize exception traceback.")
+
+        return serialized_tb, tb_line_cache
+
+    @contextlib.asynccontextmanager
+    async def handle_user_exception(self) -> AsyncGenerator[None, None]:
+        """Sets the task as failed in a way where it's not retried.
+
+        Used for handling exceptions from container lifecycle methods at the moment, which should
+        trigger a task failure state.
+        """
+        try:
+            yield
+        except KeyboardInterrupt:
+            # Send no task result in case we get sigint:ed by the runner
+            # The status of the input should have been handled externally already in that case
+            raise
+        except BaseException as exc:
+            # Since this is on a different thread, sys.exc_info() can't find the exception in the stack.
+            traceback.print_exception(type(exc), exc, exc.__traceback__)
+
+            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
+
+            result = api_pb2.GenericResult(
+                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
+                data=self.serialize_exception(exc),
+                exception=repr(exc),
+                traceback="".join(traceback.format_exception(type(exc), exc, exc.__traceback__)),
+                serialized_tb=serialized_tb,
+                tb_line_cache=tb_line_cache,
+            )
+
+            req = api_pb2.TaskResultRequest(result=result)
+            await retry_transient_errors(self._client.stub.TaskResult, req)
+
+            # Shut down the task gracefully
+            raise UserException()
+
+    @contextlib.asynccontextmanager
+    async def handle_input_exception(self, input_id, started_at: float) -> AsyncGenerator[None, None]:
+        """Handle an exception while processing a function input."""
+        try:
+            yield
+        except KeyboardInterrupt:
+            raise
+        except (InputCancellation, asyncio.CancelledError):
+            # just skip creating any output for this input and keep going with the next instead
+            # it should have been marked as cancelled already in the backend at this point so it
+            # won't be retried
+            logger.warning(f"The current input ({input_id=}) was cancelled by a user request")
+            await self.complete_call(started_at)
+            return
+        except BaseException as exc:
+            # print exception so it's logged
+            traceback.print_exc()
+            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
+
+            # Note: we're not serializing the traceback since it contains
+            # local references that means we can't unpickle it. We *are*
+            # serializing the exception, which may have some issues (there
+            # was an earlier note about it that it might not be possible
+            # to unpickle it in some cases). Let's watch out for issues.
+            await self._push_output(
+                input_id,
+                started_at=started_at,
+                data_format=api_pb2.DATA_FORMAT_PICKLE,
+                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
+                data=self.serialize_exception(exc),
+                exception=repr(exc),
+                traceback=traceback.format_exc(),
+                serialized_tb=serialized_tb,
+                tb_line_cache=tb_line_cache,
+            )
+            await self.complete_call(started_at)
+
+    async def complete_call(self, started_at):
+        self.total_user_time += time.time() - started_at
+        self.calls_completed += 1
+        self._semaphore.release()
+
+    @synchronizer.no_io_translation
+    async def push_output(self, input_id, started_at: float, data: Any, data_format: int) -> None:
+        await self._push_output(
+            input_id,
+            started_at=started_at,
+            data_format=data_format,
+            status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
+            data=self.serialize_data_format(data, data_format),
+        )
+        await self.complete_call(started_at)
+
+    async def restore(self) -> None:
+        # Busy-wait for restore. `/__modal/restore-state.json` is created
+        # by the worker process with updates to the container config.
+        restored_path = Path(config.get("restore_state_path"))
+        start = time.perf_counter()
+        while not restored_path.exists():
+            logger.debug(f"Waiting for restore (elapsed={time.perf_counter() - start:.3f}s)")
+            await asyncio.sleep(0.01)
+            continue
+
+        logger.debug("Container: restored")
+
+        # Look for state file and create new client with updated credentials.
+        # State data is serialized with key-value pairs, example: {"task_id": "tk-000"}
+        with restored_path.open("r") as file:
+            restored_state = json.load(file)
+
+        # Local FunctionIOManager state.
+        for key in ["task_id", "function_id"]:
+            if value := restored_state.get(key):
+                logger.debug(f"Updating FunctionIOManager.{key} = {value}")
+                setattr(self, key, restored_state[key])
+
+        # Env vars and global state.
+        for key, value in restored_state.items():
+            # Empty string indicates that value does not need to be updated.
+            if value != "":
+                config.override_locally(key, value)
+
+        # Restore input to default state.
+        self.current_input_id = None
+        self.current_input_started_at = None
+
+        self._client = await _Client.from_env()
+        self._waiting_for_checkpoint = False
+
+    async def checkpoint(self) -> None:
+        """Message server indicating that function is ready to be checkpointed."""
+        if self.checkpoint_id:
+            logger.debug(f"Checkpoint ID: {self.checkpoint_id}")
+
+        await self._client.stub.ContainerCheckpoint(
+            api_pb2.ContainerCheckpointRequest(checkpoint_id=self.checkpoint_id)
+        )
+
+        self._waiting_for_checkpoint = True
+        await self._client._close()
+
+        logger.debug("Checkpointing request sent. Connection closed.")
+        await self.restore()
+
+    async def volume_commit(self, volume_ids: List[str]) -> None:
+        """
+        Perform volume commit for given `volume_ids`.
+        Only used on container exit to persist uncommitted changes on behalf of user.
+        """
+        if not volume_ids:
+            return
+        await asyncify(os.sync)()
+        results = await asyncio.gather(
+            *[
+                retry_transient_errors(
+                    self._client.stub.VolumeCommit,
+                    api_pb2.VolumeCommitRequest(volume_id=v_id),
+                    max_retries=9,
+                    base_delay=0.25,
+                    max_delay=256,
+                    delay_factor=2,
+                )
+                for v_id in volume_ids
+            ],
+            return_exceptions=True,
+        )
+        for volume_id, res in zip(volume_ids, results):
+            if isinstance(res, Exception):
+                logger.error(f"modal.Volume background commit failed for {volume_id}. Exception: {res}")
+            else:
+                logger.debug(f"modal.Volume background commit success for {volume_id}.")
+
+
+FunctionIOManager = synchronize_api(_FunctionIOManager)
+
+
 def call_function_sync(
-    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
+    function_io_manager,  #: FunctionIOManager,  TODO: this type is generated at runtime
     imp_fun: ImportedFunction,
 ):
     def run_input(input_id: str, function_call_id: str, args: Any, kwargs: Any) -> None:
         started_at = time.time()
         reset_context = _set_current_context_ids(input_id, function_call_id)
-        with container_io_manager.handle_input_exception(input_id, started_at):
+        with function_io_manager.handle_input_exception(input_id, started_at):
             logger.debug(f"Starting input {input_id} (sync)")
             res = imp_fun.fun(*args, **kwargs)
             logger.debug(f"Finished input {input_id} (sync)")
 
             # TODO(erikbern): any exception below shouldn't be considered a user exception
             if imp_fun.is_generator:
                 if not inspect.isgenerator(res):
                     raise InvalidError(f"Generator function returned value of type {type(res)}")
 
                 # Send up to this many outputs at a time.
-                generator_queue: asyncio.Queue[Any] = container_io_manager._queue_create(1024)
-                generator_output_task = container_io_manager.generator_output_task(
+                generator_queue: asyncio.Queue[Any] = function_io_manager._queue_create(1024)
+                generator_output_task = function_io_manager.generator_output_task(
                     function_call_id,
                     imp_fun.data_format,
                     generator_queue,
                     _future=True,  # Synchronicity magic to return a future.
                 )
 
                 item_count = 0
                 for value in res:
-                    container_io_manager._queue_put(generator_queue, value)
+                    function_io_manager._queue_put(generator_queue, value)
                     item_count += 1
 
-                container_io_manager._queue_put(generator_queue, _ContainerIOManager._GENERATOR_STOP_SENTINEL)
+                function_io_manager._queue_put(generator_queue, _FunctionIOManager._GENERATOR_STOP_SENTINEL)
                 generator_output_task.result()  # Wait to finish sending generator outputs.
                 message = api_pb2.GeneratorDone(items_total=item_count)
-                container_io_manager.push_output(input_id, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE)
+                function_io_manager.push_output(input_id, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE)
             else:
                 if inspect.iscoroutine(res) or inspect.isgenerator(res) or inspect.isasyncgen(res):
                     raise InvalidError(
                         f"Sync (non-generator) function return value of type {type(res)}."
                         " You might need to use @stub.function(..., is_generator=True)."
                     )
-                container_io_manager.push_output(input_id, started_at, res, imp_fun.data_format)
+                function_io_manager.push_output(input_id, started_at, res, imp_fun.data_format)
         reset_context()
 
     if imp_fun.input_concurrency > 1:
         # We can't use `concurrent.futures.ThreadPoolExecutor` here because in Python 3.11+, this
         # class has no workaround that allows us to exit the Python interpreter process without
         # waiting for the worker threads to finish. We need this behavior on SIGINT.
 
@@ -182,109 +713,122 @@
                     run_input(*args)
                 except BaseException:
                     # This should basically never happen, since only KeyboardInterrupt is the only error that can
                     # bubble out of from handle_input_exception and those wouldn't be raised outside the main thread
                     pass
                 inputs.task_done()
 
-        for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs(
+        for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs(
             imp_fun.input_concurrency
         ):
             if spawned_workers < imp_fun.input_concurrency:
                 threading.Thread(target=worker_thread, daemon=True).start()
                 spawned_workers += 1
             inputs.put((input_id, function_call_id, args, kwargs))
 
         finished.set()
         inputs.join()
 
     else:
-        for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs(
+        for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs(
             imp_fun.input_concurrency
         ):
             try:
                 run_input(input_id, function_call_id, args, kwargs)
             except:
                 raise
 
 
 async def call_function_async(
-    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
+    function_io_manager,  #: FunctionIOManager,  TODO: this type is generated at runtime
     imp_fun: ImportedFunction,
 ):
     async def run_input(input_id: str, function_call_id: str, args: Any, kwargs: Any) -> None:
         started_at = time.time()
         reset_context = _set_current_context_ids(input_id, function_call_id)
-        async with container_io_manager.handle_input_exception.aio(input_id, started_at):
+        async with function_io_manager.handle_input_exception.aio(input_id, started_at):
             logger.debug(f"Starting input {input_id} (async)")
             res = imp_fun.fun(*args, **kwargs)
             logger.debug(f"Finished input {input_id} (async)")
 
             # TODO(erikbern): any exception below shouldn't be considered a user exception
             if imp_fun.is_generator:
                 if not inspect.isasyncgen(res):
                     raise InvalidError(f"Async generator function returned value of type {type(res)}")
 
                 # Send up to this many outputs at a time.
-                generator_queue: asyncio.Queue[Any] = await container_io_manager._queue_create.aio(1024)
+                generator_queue: asyncio.Queue[Any] = await function_io_manager._queue_create.aio(1024)
                 generator_output_task = asyncio.create_task(
-                    container_io_manager.generator_output_task.aio(
+                    function_io_manager.generator_output_task.aio(
                         function_call_id,
                         imp_fun.data_format,
                         generator_queue,
                     )
                 )
 
                 item_count = 0
                 async for value in res:
-                    await container_io_manager._queue_put.aio(generator_queue, value)
+                    await function_io_manager._queue_put.aio(generator_queue, value)
                     item_count += 1
 
-                await container_io_manager._queue_put.aio(generator_queue, _ContainerIOManager._GENERATOR_STOP_SENTINEL)
+                await function_io_manager._queue_put.aio(generator_queue, _FunctionIOManager._GENERATOR_STOP_SENTINEL)
                 await generator_output_task  # Wait to finish sending generator outputs.
                 message = api_pb2.GeneratorDone(items_total=item_count)
-                await container_io_manager.push_output.aio(
+                await function_io_manager.push_output.aio(
                     input_id, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE
                 )
             else:
                 if not inspect.iscoroutine(res) or inspect.isgenerator(res) or inspect.isasyncgen(res):
                     raise InvalidError(
                         f"Async (non-generator) function returned value of type {type(res)}"
                         " You might need to use @stub.function(..., is_generator=True)."
                     )
                 value = await res
-                await container_io_manager.push_output.aio(input_id, started_at, value, imp_fun.data_format)
+                await function_io_manager.push_output.aio(input_id, started_at, value, imp_fun.data_format)
         reset_context()
 
     if imp_fun.input_concurrency > 1:
         # all run_input coroutines will have completed by the time we leave the execution context
         # but the wrapping *tasks* may not yet have been resolved, so we add a 0.01s
         # for them to resolve gracefully:
         async with TaskContext(0.01) as execution_context:
-            async for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs.aio(
+            async for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs.aio(
                 imp_fun.input_concurrency
             ):
                 # Note that run_inputs_outputs will not return until the concurrency semaphore has
                 # released all its slots so that they can be acquired by the run_inputs_outputs finalizer
                 # This prevents leaving the execution_context before outputs have been created
                 # TODO: refactor to make this a bit more easy to follow?
                 execution_context.create_task(run_input(input_id, function_call_id, args, kwargs))
     else:
-        async for input_id, function_call_id, args, kwargs in container_io_manager.run_inputs_outputs.aio(
+        async for input_id, function_call_id, args, kwargs in function_io_manager.run_inputs_outputs.aio(
             imp_fun.input_concurrency
         ):
             await run_input(input_id, function_call_id, args, kwargs)
 
 
+@dataclass
+class ImportedFunction:
+    obj: Any
+    fun: Callable
+    stub: Optional[_Stub]
+    is_async: bool
+    is_generator: bool
+    data_format: int  # api_pb2.DataFormat
+    input_concurrency: int
+    is_auto_snapshot: bool
+    function: _Function
+
+
 def import_function(
     function_def: api_pb2.Function,
     ser_cls,
     ser_fun,
     ser_params: Optional[bytes],
-    container_io_manager,
+    function_io_manager,
     client: Client,
 ) -> ImportedFunction:
     """Imports a function dynamically, and locates the stub.
 
     This is somewhat complex because we're dealing with 3 quite different type of functions:
     1. Functions defined in global scope and decorated in global scope (Function objects)
     2. Functions defined in global scope but decorated elsewhere (these will be raw callables)
@@ -305,15 +849,15 @@
     thread. This is so that any user code running in global scope (which executes as a part of
     the import) runs on the right thread.
     """
     module: Optional[ModuleType] = None
     cls: Optional[Type] = None
     fun: Callable
     function: Optional[_Function] = None
-    active_app: Optional[_App] = None
+    active_stub: Optional[_Stub] = None
     pty_info: api_pb2.PTYInfo = function_def.pty_info
 
     if ser_fun is not None:
         # This is a serialized function we already fetched from the server
         cls, fun = ser_cls, ser_fun
     else:
         # Load the module dynamically
@@ -327,49 +871,49 @@
         if len(parts) == 1:
             # This is a function
             cls = None
             f = getattr(module, qual_name)
             if isinstance(f, Function):
                 function = synchronizer._translate_in(f)
                 fun = function.get_raw_f()
-                active_app = function._stub
+                active_stub = function._stub
             else:
                 fun = f
         elif len(parts) == 2:
             # This is a method on a class
             cls_name, fun_name = parts
             cls = getattr(module, cls_name)
             if isinstance(cls, Cls):
                 # The cls decorator is in global scope
                 _cls = synchronizer._translate_in(cls)
                 fun = _cls._callables[fun_name]
                 function = _cls._functions.get(fun_name)
-                active_app = _cls._stub
+                active_stub = _cls._stub
             else:
                 # This is a raw class
                 fun = getattr(cls, fun_name)
         else:
             raise InvalidError(f"Invalid function qualname {qual_name}")
 
     # If the cls/function decorator was applied in local scope, but the stub is global, we can look it up
-    if active_app is None:
+    if active_stub is None:
         # This branch is reached in the special case that the imported function is 1) not serialized, and 2) isn't a FunctionHandle - i.e, not decorated at definition time
         # Look at all instantiated stubs - if there is only one with the indicated name, use that one
         stub_name: Optional[str] = function_def.stub_name or None  # coalesce protobuf field to None
-        matching_stubs = _App._all_apps.get(stub_name, [])
+        matching_stubs = _Stub._all_stubs.get(stub_name, [])
         if len(matching_stubs) > 1:
             if stub_name is not None:
                 warning_sub_message = f"stub with the same name ('{stub_name}')"
             else:
                 warning_sub_message = "unnamed stub"
             logger.warning(
                 f"You have more than one {warning_sub_message}. It's recommended to name all your Stubs uniquely when using multiple stubs"
             )
         elif len(matching_stubs) == 1:
-            (active_app,) = matching_stubs
+            (active_stub,) = matching_stubs
         # there could also technically be zero found stubs, but that should probably never be an issue since that would mean user won't use is_inside or other function handles anyway
 
     # Check this property before we turn it into a method (overriden by webhooks)
     is_async = get_is_async(fun)
 
     # Use the function definition for whether this is a generator (overriden by webhooks)
     is_generator = function_def.function_type == api_pb2.Function.FUNCTION_TYPE_GENERATOR
@@ -402,133 +946,122 @@
     if function_def.webhook_config.type:
         is_async = True
         is_generator = True
         data_format = api_pb2.DATA_FORMAT_ASGI
 
         if function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP:
             # Function returns an asgi_app, which we can use as a callable.
-            fun = asgi_app_wrapper(fun(), container_io_manager)
+            fun = asgi_app_wrapper(fun(), function_io_manager)
 
         elif function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP:
             # Function returns an wsgi_app, which we can use as a callable.
-            fun = wsgi_app_wrapper(fun(), container_io_manager)
+            fun = wsgi_app_wrapper(fun(), function_io_manager)
 
         elif function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION:
             # Function is a webhook without an ASGI app. Create one for it.
             fun = asgi_app_wrapper(
                 webhook_asgi_app(fun, function_def.webhook_config.method),
-                container_io_manager,
+                function_io_manager,
             )
 
         elif function_def.webhook_config.type == api_pb2.WEBHOOK_TYPE_WEB_SERVER:
             # Function spawns an HTTP web server listening at a port.
             fun()
 
             # We intentionally try to connect to the external interface instead of the loopback
             # interface here so users are forced to expose the server. This allows us to potentially
             # change the implementation to use an external bridge in the future.
             host = get_ip_address(b"eth0")
             port = function_def.webhook_config.web_server_port
             startup_timeout = function_def.webhook_config.web_server_startup_timeout
             wait_for_web_server(host, port, timeout=startup_timeout)
-            fun = asgi_app_wrapper(web_server_proxy(host, port), container_io_manager)
+            fun = asgi_app_wrapper(web_server_proxy(host, port), function_io_manager)
 
         else:
             raise InvalidError(f"Unrecognized web endpoint type {function_def.webhook_config.type}")
 
     return ImportedFunction(
         obj,
         fun,
-        active_app,
+        active_stub,
         is_async,
         is_generator,
         data_format,
         input_concurrency,
         function_def.is_auto_snapshot,
         function,
     )
 
 
 def call_lifecycle_functions(
     event_loop: UserCodeEventLoop,
-    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
-    funcs: Sequence[Callable],
+    function_io_manager,  #: FunctionIOManager,  TODO: this type is generated at runtime
+    funcs: Iterable[Callable],
 ) -> None:
     """Call function(s), can be sync or async, but any return values are ignored."""
-    with container_io_manager.handle_user_exception():
+    with function_io_manager.handle_user_exception():
         for func in funcs:
             # We are deprecating parameterized exit methods but want to gracefully handle old code.
             # We can remove this once the deprecation in the actual @exit decorator is enforced.
             args = (None, None, None) if method_has_params(func) else ()
             res = func(
                 *args
             )  # in case func is non-async, it's executed here and sigint will by default interrupt it using a KeyboardInterrupt exception
             if inspect.iscoroutine(res):
                 # if however func is async, we have to jump through some hoops
                 event_loop.run(res)
 
 
 def main(container_args: api_pb2.ContainerArguments, client: Client):
-    # This is a bit weird but we need both the blocking and async versions of ContainerIOManager.
+    # This is a bit weird but we need both the blocking and async versions of FunctionIOManager.
     # At some point, we should fix that by having built-in support for running "user code"
-    container_io_manager = ContainerIOManager(container_args, client)
+    function_io_manager = FunctionIOManager(container_args, client)
+
+    # Define a global app (need to do this before imports).
+    container_app: ContainerApp = function_io_manager.initialize_app()
 
-    with container_io_manager.heartbeats(), UserCodeEventLoop() as event_loop:
+    with function_io_manager.heartbeats(), UserCodeEventLoop() as event_loop:
         # If this is a serialized function, fetch the definition from the server
         if container_args.function_def.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED:
-            ser_cls, ser_fun = container_io_manager.get_serialized_function()
+            ser_cls, ser_fun = function_io_manager.get_serialized_function()
         else:
             ser_cls, ser_fun = None, None
 
         # Initialize the function, importing user code.
-        with container_io_manager.handle_user_exception():
+        with function_io_manager.handle_user_exception():
             imp_fun = import_function(
                 container_args.function_def,
                 ser_cls,
                 ser_fun,
                 container_args.serialized_params,
-                container_io_manager,
+                function_io_manager,
                 client,
             )
 
-        # Get ids and metadata for objects (primarily functions and classes) on the app
-        container_app: RunningApp = container_io_manager.get_app_objects()
-
         # Initialize objects on the stub.
-        # This is basically only functions and classes - anything else is deprecated and will be unsupported soon
-        if imp_fun.app is not None:
-            stub: App = synchronizer._translate_out(imp_fun.app, Interface.BLOCKING)
-            stub._init_container(client, container_app)
+        if imp_fun.stub is not None:
+            container_app.associate_stub_container(imp_fun.stub)
 
         # Hydrate all function dependencies.
         # TODO(erikbern): we an remove this once we
         # 1. Enable lazy hydration for all objects
         # 2. Fully deprecate .new() objects
         if imp_fun.function:
-            _client: _Client = synchronizer._translate_in(client)  # TODO(erikbern): ugly
             dep_object_ids: List[str] = [dep.object_id for dep in container_args.function_def.object_dependencies]
-            function_deps = imp_fun.function.deps(only_explicit_mounts=True)
-            if len(function_deps) != len(dep_object_ids):
-                raise ExecutionError(
-                    f"Function has {len(function_deps)} dependencies"
-                    f" but container got {len(dep_object_ids)} object ids."
-                )
-            for object_id, obj in zip(dep_object_ids, function_deps):
-                metadata: Message = container_app.object_handle_metadata[object_id]
-                obj._hydrate(object_id, _client, metadata)
+            container_app.hydrate_function_deps(imp_fun.function, dep_object_ids)
 
         # Identify all "enter" methods that need to run before we checkpoint.
         if imp_fun.obj is not None and not imp_fun.is_auto_snapshot:
             pre_checkpoint_methods = _find_callables_for_obj(imp_fun.obj, _PartialFunctionFlags.ENTER_PRE_CHECKPOINT)
-            call_lifecycle_functions(event_loop, container_io_manager, list(pre_checkpoint_methods.values()))
+            call_lifecycle_functions(event_loop, function_io_manager, pre_checkpoint_methods.values())
 
         # If this container is being used to create a checkpoint, checkpoint the container after
         # global imports and innitialization. Checkpointed containers run from this point onwards.
         if container_args.function_def.is_checkpointing_function:
-            container_io_manager.checkpoint()
+            function_io_manager.checkpoint()
 
         # Install hooks for interactive functions.
         if container_args.function_def.pty_info.pty_type != api_pb2.PTYInfo.PTY_TYPE_UNSPECIFIED:
 
             def breakpoint_wrapper():
                 # note: it would be nice to not have breakpoint_wrapper() included in the backtrace
                 interact()
@@ -537,45 +1070,45 @@
                 pdb.set_trace()
 
             sys.breakpointhook = breakpoint_wrapper
 
         # Identify the "enter" methods to run after resuming from a checkpoint.
         if imp_fun.obj is not None and not imp_fun.is_auto_snapshot:
             post_checkpoint_methods = _find_callables_for_obj(imp_fun.obj, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
-            call_lifecycle_functions(event_loop, container_io_manager, list(post_checkpoint_methods.values()))
+            call_lifecycle_functions(event_loop, function_io_manager, post_checkpoint_methods.values())
 
         # Execute the function.
         try:
             if imp_fun.is_async:
-                event_loop.run(call_function_async(container_io_manager, imp_fun))
+                event_loop.run(call_function_async(function_io_manager, imp_fun))
             else:
                 # Set up a signal handler for `SIGUSR1`, which gets translated to an InputCancellation
                 # during function execution. This is sent to cancel inputs from the user.
                 def _cancel_input_signal_handler(signum, stackframe):
                     raise InputCancellation("Input was cancelled by user")
 
                 signal.signal(signal.SIGUSR1, _cancel_input_signal_handler)
 
-                call_function_sync(container_io_manager, imp_fun)
+                call_function_sync(function_io_manager, imp_fun)
         finally:
             # Run exit handlers. From this point onward, ignore all SIGINT signals that come from
             # graceful shutdowns originating on the worker, as well as stray SIGUSR1 signals that
             # may have been sent to cancel inputs.
             int_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
             usr1_handler = signal.signal(signal.SIGUSR1, signal.SIG_IGN)
 
             try:
                 # Identify "exit" methods and run them.
                 if imp_fun.obj is not None and not imp_fun.is_auto_snapshot:
                     exit_methods = _find_callables_for_obj(imp_fun.obj, _PartialFunctionFlags.EXIT)
-                    call_lifecycle_functions(event_loop, container_io_manager, list(exit_methods.values()))
+                    call_lifecycle_functions(event_loop, function_io_manager, exit_methods.values())
 
                 # Finally, commit on exit to catch uncommitted volume changes and surface background
                 # commit errors.
-                container_io_manager.volume_commit(
+                function_io_manager.volume_commit(
                     [v.volume_id for v in container_args.function_def.volume_mounts if v.allow_background_commits]
                 )
             finally:
                 # Restore the original signal handler, needed for container_test hygiene since the
                 # test runs `main()` multiple times in the same process.
                 signal.signal(signal.SIGINT, int_handler)
                 signal.signal(signal.SIGUSR1, usr1_handler)
```

## modal/_resolver.py

```diff
@@ -1,27 +1,31 @@
 # Copyright Modal Labs 2023
 import asyncio
 import contextlib
 from asyncio import Future
-from typing import TYPE_CHECKING, Dict, Hashable, List, Optional
+from typing import TYPE_CHECKING, Dict, Hashable, List, Optional, TypeVar
 
 from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
-from .exception import ExecutionError, NotFoundError
-
 if TYPE_CHECKING:
+    from rich.spinner import Spinner
     from rich.tree import Tree
 
     from modal.object import _Object
+else:
+    Spinner = TypeVar("Spinner")
+    Tree = TypeVar("Tree")
+
+from modal.exception import ExecutionError, NotFoundError
 
 
 class StatusRow:
-    def __init__(self, progress: "Optional[Tree]"):
+    def __init__(self, progress: Optional[Tree]):
         from ._output import (
             step_progress,
         )
 
         self._spinner = None
         self._step_node = None
         if progress is not None:
@@ -39,14 +43,15 @@
 
         if self._step_node is not None:
             step_progress_update(self._spinner, message)
             self._step_node.label = step_completed(message, is_substep=True)
 
 
 class Resolver:
+    _tree: Tree
     _local_uuid_to_future: Dict[str, Future]
     _environment_name: Optional[str]
     _app_id: Optional[str]
     _deduplication_cache: Dict[Hashable, Future]
 
     def __init__(
         self,
```

## modal/_serialization.py

```diff
@@ -290,14 +290,17 @@
         assert isinstance(obj, api_pb2.GeneratorDone)
         return obj.SerializeToString(deterministic=True)
     else:
         raise InvalidError(f"Unknown data format {data_format!r}")
 
 
 def deserialize_data_format(s: bytes, data_format: int, client) -> Any:
+    if data_format == api_pb2.DATA_FORMAT_UNSPECIFIED:
+        # TODO: Remove this after Modal client version 0.52, when the data_format field is always set.
+        return deserialize(s, client)
     if data_format == api_pb2.DATA_FORMAT_PICKLE:
         return deserialize(s, client)
     elif data_format == api_pb2.DATA_FORMAT_ASGI:
         return _deserialize_asgi(api_pb2.Asgi.FromString(s))
     elif data_format == api_pb2.DATA_FORMAT_GENERATOR_DONE:
         return api_pb2.GeneratorDone.FromString(s)
     else:
```

## modal/app.py

```diff
@@ -1,816 +1,396 @@
 # Copyright Modal Labs 2022
-import inspect
-import typing
-from pathlib import PurePosixPath
-from typing import Any, AsyncGenerator, Callable, ClassVar, Dict, List, Optional, Sequence, Tuple, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, TypeVar
 
+from google.protobuf.empty_pb2 import Empty
 from google.protobuf.message import Message
-from synchronicity.async_wrap import asynccontextmanager
+from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
-from ._ipython import is_notebook
 from ._output import OutputManager
 from ._resolver import Resolver
 from ._utils.async_utils import synchronize_api
-from ._utils.function_utils import FunctionInfo
-from ._utils.mount_utils import validate_volumes
-from .app_utils import (  # noqa: F401
-    _list_apps,
-    list_apps,
-)
+from ._utils.grpc_utils import get_proto_oneof, retry_transient_errors
 from .client import _Client
-from .cloud_bucket_mount import _CloudBucketMount
-from .cls import _Cls
 from .config import logger
-from .exception import InvalidError, deprecation_error, deprecation_warning
-from .functions import _Function
-from .gpu import GPU_T
-from .image import _Image
-from .mount import _Mount
-from .network_file_system import _NetworkFileSystem
+from .exception import ExecutionError, InvalidError
 from .object import _Object
-from .partial_function import PartialFunction, _find_callables_for_cls, _PartialFunction, _PartialFunctionFlags
-from .proxy import _Proxy
-from .retries import Retries
-from .runner import _run_stub
-from .running_app import RunningApp
-from .sandbox import _Sandbox
-from .schedule import Schedule
-from .scheduler_placement import SchedulerPlacement
-from .secret import _Secret
-from .volume import _Volume
-
-_default_image: _Image = _Image.debian_slim()
-
-
-class _LocalEntrypoint:
-    _info: FunctionInfo
-    _stub: "_App"
-
-    def __init__(self, info, stub):
-        self._info = info  # type: ignore
-        self._stub = stub
 
-    def __call__(self, *args, **kwargs):
-        return self._info.raw_f(*args, **kwargs)
+if TYPE_CHECKING:
+    from .functions import _Function
 
-    @property
-    def info(self) -> FunctionInfo:
-        return self._info
-
-    @property
-    def stub(self) -> "_App":
-        return self._stub
-
-
-LocalEntrypoint = synchronize_api(_LocalEntrypoint)
-
-
-def check_sequence(items: typing.Sequence[typing.Any], item_type: typing.Type[typing.Any], error_msg: str):
-    if not isinstance(items, (list, tuple)):
-        raise InvalidError(error_msg)
-    if not all(isinstance(v, item_type) for v in items):
-        raise InvalidError(error_msg)
-
-
-CLS_T = typing.TypeVar("CLS_T", bound=typing.Type)
-
-
-class _App:
-    """A Modal app (formerly known as "stub") is a group of functions and classes
-    deployed together.
-
-    The stub object principally describes Modal objects (`Function`, `Image`,
-    `Secret`, etc.) associated with the application. It has three responsibilities:
-
-    * Syncing of identities across processes (your local Python interpreter and
-      every Modal worker active in your application).
-    * Making Objects stay alive and not be garbage collected for as long as the
-      app lives (see App lifetime below).
-    * Manage log collection for everything that happens inside your code.
+else:
+    _Function = TypeVar("_Function")
 
-    **Registering functions with an app**
 
-    The most common way to explicitly register an Object with an app is through the
-    `@stub.function()` decorator. It both registers the annotated function itself and
-    other passed objects, like schedules and secrets, with the app:
-
-    ```python
-    import modal
-
-    stub = modal.Stub()
-
-    @stub.function(
-        secrets=[modal.Secret.from_name("some_secret")],
-        schedule=modal.Period(days=1),
-    )
-    def foo():
-        pass
-    ```
-
-    In this example, the secret and schedule are registered with the app.
-    """
-
-    _name: Optional[str]
-    _description: Optional[str]
-    _indexed_objects: Dict[str, _Object]
-    _function_mounts: Dict[str, _Mount]
-    _mounts: Sequence[_Mount]
-    _secrets: Sequence[_Secret]
-    _volumes: Dict[Union[str, PurePosixPath], _Volume]
-    _web_endpoints: List[str]  # Used by the CLI
-    _local_entrypoints: Dict[str, _LocalEntrypoint]
-    _running_app: Optional[RunningApp]
-    _client: Optional[_Client]
-    _all_apps: ClassVar[Dict[Optional[str], List["_App"]]] = {}
+class _LocalApp:
+    _tag_to_object_id: Dict[str, str]
+    _client: _Client
+    _app_id: str
+    _app_page_url: str
+    _environment_name: str
+    _interactive: bool
 
     def __init__(
         self,
-        name: Optional[str] = None,
-        *,
-        image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)
-        mounts: Sequence[_Mount] = [],  # default mounts for all functions
-        secrets: Sequence[_Secret] = [],  # default secrets for all functions
-        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # default volumes for all functions
-        **kwargs: _Object,  # DEPRECATED: passing additional objects to the stub as kwargs is no longer supported
-    ) -> None:
-        """Construct a new app stub, optionally with default image, mounts, secrets, or volumes.
-
-        ```python notest
-        image = modal.Image.debian_slim().pip_install(...)
-        mount = modal.Mount.from_local_dir("./config")
-        secret = modal.Secret.from_name("my-secret")
-        volume = modal.Volume.from_name("my-data")
-        stub = modal.Stub(image=image, mounts=[mount], secrets=[secret], volumes={"/mnt/data": volume})
-        ```
-        """
-
-        self._name = name
-        self._description = name
-
-        check_sequence(mounts, _Mount, "`mounts=` has to be a list or tuple of Mount objects")
-        check_sequence(secrets, _Secret, "`secrets=` has to be a list or tuple of Secret objects")
-        validate_volumes(volumes)
-
-        if image is not None and not isinstance(image, _Image):
-            raise InvalidError("image has to be a modal Image or AioImage object")
-
-        if kwargs:
-            deprecation_error(
-                (2023, 12, 13),
-                "Passing additional objects to the stub constructor is deprecated."
-                f" Please remove the following parameters from your stub definition: {', '.join(kwargs)}."
-                " In most cases, persistent (named) objects can just be defined in the global scope.",
-            )
-
-        for k, v in kwargs.items():
-            self._validate_blueprint_value(k, v)
-
-        self._indexed_objects = kwargs
-        if image is not None:
-            self._indexed_objects["image"] = image  # backward compatibility since "image" used to be on the blueprint
-
-        self._mounts = mounts
-
-        self._secrets = secrets
-        self._volumes = volumes
-        self._local_entrypoints = {}
-        self._web_endpoints = []
-        self._running_app = None  # Set inside container, OR during the time an app is running locally
-        self._client = None
-
-        # Register this stub. This is used to look up the stub in the container, when we can't get it from the function
-        _App._all_apps.setdefault(self._name, []).append(self)
-
-    @property
-    def name(self) -> Optional[str]:
-        """The user-provided name of the Stub."""
-        return self._name
-
-    @property
-    def is_interactive(self) -> bool:
-        """Whether the current app for the stub is running in interactive mode."""
-        # return self._name
-        if self._running_app:
-            return self._running_app.interactive
-        else:
-            return False
+        client: _Client,
+        app_id: str,
+        app_page_url: str,
+        tag_to_object_id: Optional[Dict[str, str]] = None,
+        stub_name: Optional[str] = None,
+        environment_name: Optional[str] = None,
+        interactive: bool = False,
+    ):
+        """mdmd:hidden This is the app constructor. Users should not call this directly."""
+        self._app_id = app_id
+        self._app_page_url = app_page_url
+        self._client = client
+        self._tag_to_object_id = tag_to_object_id or {}
+        self._stub_name = stub_name
+        self._environment_name = environment_name
+        self._interactive = interactive
 
     @property
-    def app_id(self) -> Optional[str]:
-        """Return the app_id, if the stub is running."""
-        if self._running_app:
-            return self._running_app.app_id
-        else:
-            return None
+    def client(self) -> _Client:
+        """A reference to the running App's server client."""
+        return self._client
 
     @property
-    def description(self) -> Optional[str]:
-        """The Stub's `name`, if available, or a fallback descriptive identifier."""
-        return self._description
-
-    def set_description(self, description: str):
-        self._description = description
-
-    def _validate_blueprint_value(self, key: str, value: Any):
-        if not isinstance(value, _Object):
-            raise InvalidError(f"Stub attribute `{key}` with value {value!r} is not a valid Modal object")
-
-    def _add_object(self, tag, obj):
-        if self._running_app:
-            # If this is inside a container, then objects can be defined after app initialization.
-            # So we may have to initialize objects once they get bound to the stub.
-            if tag in self._running_app.tag_to_object_id:
-                object_id: str = self._running_app.tag_to_object_id[tag]
-                metadata: Message = self._running_app.object_handle_metadata[object_id]
-                obj._hydrate(object_id, self._client, metadata)
-
-        self._indexed_objects[tag] = obj
-
-    def __getitem__(self, tag: str):
-        """Stub assignments of the form `stub.x` or `stub["x"]` are deprecated!
-
-        The only use cases for these assignments is in conjunction with `.new()`, which is now
-        in itself deprecated. If you are constructing objects with `.from_name(...)`, there is no
-        need to assign those objects to the stub. Example:
-
-        ```python
-        d = modal.Dict.from_name("my-dict", create_if_missing=True)
-
-        @stub.function()
-        def f(x, y):
-            d[x] = y  # Refer to d in global scope
-        ```
-        """
-        deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-        return self._indexed_objects[tag]
-
-    def __setitem__(self, tag: str, obj: _Object):
-        deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-        self._validate_blueprint_value(tag, obj)
-        # Deprecated ?
-        self._add_object(tag, obj)
-
-    def __getattr__(self, tag: str) -> _Object:
-        # TODO(erikbern): remove this method later
-        assert isinstance(tag, str)
-        if tag.startswith("__"):
-            # Hacky way to avoid certain issues, e.g. pickle will try to look this up
-            raise AttributeError(f"App has no member {tag}")
-        if tag not in self._indexed_objects:
-            # Primarily to make hasattr work
-            raise AttributeError(f"App has no member {tag}")
-        obj: _Object = self._indexed_objects[tag]
-        deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-        return obj
-
-    def __setattr__(self, tag: str, obj: _Object):
-        # TODO(erikbern): remove this method later
-        # Note that only attributes defined in __annotations__ are set on the object itself,
-        # everything else is registered on the indexed_objects
-        if tag in self.__annotations__:
-            object.__setattr__(self, tag, obj)
-        elif tag == "image":
-            self._indexed_objects["image"] = obj
-        else:
-            self._validate_blueprint_value(tag, obj)
-            deprecation_warning((2024, 3, 25), _App.__getitem__.__doc__)
-            self._add_object(tag, obj)
+    def app_id(self) -> str:
+        """A unique identifier for this running App."""
+        return self._app_id
 
     @property
-    def image(self) -> _Image:
-        # Exists to get the type inference working for `stub.image`
-        # Will also keep this one after we remove [get/set][item/attr]
-        return self._indexed_objects["image"]
-
-    @image.setter
-    def image(self, value):
-        self._indexed_objects["image"] = value
-
-    def _uncreate_all_objects(self):
-        # TODO(erikbern): this doesn't unhydrate objects that aren't tagged
-        for obj in self._indexed_objects.values():
-            obj._unhydrate()
-
-    def is_inside(self, image: Optional[_Image] = None):
-        """Deprecated: use `Image.imports()` instead! Usage:
-        ```
-        my_image = modal.Image.debian_slim().pip_install("torch")
-        with my_image.imports():
-            import torch
-        ```
-        """
-        deprecation_error((2023, 11, 8), _App.is_inside.__doc__)
-
-    @asynccontextmanager
-    async def _set_local_app(self, client: _Client, app: RunningApp) -> AsyncGenerator[None, None]:
-        self._client = client
-        self._running_app = app
-        try:
-            yield
-        finally:
-            self._client = None
-            self._running_app = None
+    def is_interactive(self) -> bool:
+        return self._interactive
 
-    @asynccontextmanager
-    async def run(
+    async def _create_all_objects(
         self,
-        client: Optional[_Client] = None,
-        stdout=None,
-        show_progress: bool = True,
-        detach: bool = False,
+        indexed_objects: Dict[str, _Object],
+        new_app_state: int,
+        environment_name: str,
         output_mgr: Optional[OutputManager] = None,
-    ) -> AsyncGenerator["_App", None]:
-        """Context manager that runs an app on Modal.
+    ):  # api_pb2.AppState.V
+        """Create objects that have been defined but not created on the server."""
+        resolver = Resolver(
+            self._client,
+            output_mgr=output_mgr,
+            environment_name=environment_name,
+            app_id=self.app_id,
+        )
+        with resolver.display():
+            # Get current objects, and reset all objects
+            tag_to_object_id = self._tag_to_object_id
+            self._tag_to_object_id = {}
+
+            # Assign all objects
+            for tag, obj in indexed_objects.items():
+                # Reset object_id in case the app runs twice
+                # TODO(erikbern): clean up the interface
+                obj._unhydrate()
+
+            # Preload all functions to make sure they have ids assigned before they are loaded.
+            # This is important to make sure any enclosed function handle references in serialized
+            # functions have ids assigned to them when the function is serialized.
+            # Note: when handles/objs are merged, all objects will need to get ids pre-assigned
+            # like this in order to be referrable within serialized functions
+            for tag, obj in indexed_objects.items():
+                existing_object_id = tag_to_object_id.get(tag)
+                # Note: preload only currently implemented for Functions, returns None otherwise
+                # this is to ensure that directly referenced functions from the global scope has
+                # ids associated with them when they are serialized into other functions
+                await resolver.preload(obj, existing_object_id)
+                if obj.object_id is not None:
+                    tag_to_object_id[tag] = obj.object_id
+
+            for tag, obj in indexed_objects.items():
+                existing_object_id = tag_to_object_id.get(tag)
+                await resolver.load(obj, existing_object_id)
+                self._tag_to_object_id[tag] = obj.object_id
+
+        # Create the app (and send a list of all tagged obs)
+        # TODO(erikbern): we should delete objects from a previous version that are no longer needed
+        # We just delete them from the app, but the actual objects will stay around
+        indexed_object_ids = self._tag_to_object_id
+        assert indexed_object_ids == self._tag_to_object_id
+        all_objects = resolver.objects()
+
+        unindexed_object_ids = list(set(obj.object_id for obj in all_objects) - set(self._tag_to_object_id.values()))
+        req_set = api_pb2.AppSetObjectsRequest(
+            app_id=self._app_id,
+            indexed_object_ids=indexed_object_ids,
+            unindexed_object_ids=unindexed_object_ids,
+            new_app_state=new_app_state,  # type: ignore
+        )
+        await retry_transient_errors(self._client.stub.AppSetObjects, req_set)
 
-        Use this as the main entry point for your Modal application. All calls
-        to Modal functions should be made within the scope of this context
-        manager, and they will correspond to the current app.
-
-        Note that this method used to return a separate "App" object. This is
-        no longer useful since you can use the stub itself for access to all
-        objects. For backwards compatibility reasons, it returns the same stub.
-        """
-        # TODO(erikbern): deprecate this one too?
-        async with _run_stub(self, client, stdout, show_progress, detach, output_mgr):
-            yield self
-
-    def _get_default_image(self):
-        if "image" in self._indexed_objects:
-            return self._indexed_objects["image"]
-        else:
-            return _default_image
+    async def disconnect(
+        self, reason: "Optional[api_pb2.AppDisconnectReason.ValueType]" = None, exc_str: Optional[str] = None
+    ):
+        """Tell the server the client has disconnected for this app. Terminates all running tasks
+        for ephemeral apps."""
+
+        if exc_str:
+            exc_str = exc_str[:1000]  # Truncate to 1000 chars
+
+        logger.debug("Sending app disconnect/stop request")
+        req_disconnect = api_pb2.AppClientDisconnectRequest(app_id=self._app_id, reason=reason, exception=exc_str)
+        await retry_transient_errors(self._client.stub.AppClientDisconnect, req_disconnect)
+        logger.debug("App disconnected")
+
+    async def stop(self):
+        """Tell the server to stop this app, terminating all running tasks."""
+        req_disconnect = api_pb2.AppStopRequest(app_id=self._app_id, source=api_pb2.APP_STOP_SOURCE_PYTHON_CLIENT)
+        await retry_transient_errors(self._client.stub.AppStop, req_disconnect)
+
+    def log_url(self):
+        """URL link to a running app's logs page in the Modal dashboard."""
+        return self._app_page_url
+
+    @staticmethod
+    async def _init_existing(client: _Client, existing_app_id: str) -> "_LocalApp":
+        # Get all the objects first
+        obj_req = api_pb2.AppGetObjectsRequest(app_id=existing_app_id)
+        obj_resp = await retry_transient_errors(client.stub.AppGetObjects, obj_req)
+        app_page_url = f"https://modal.com/apps/{existing_app_id}"  # TODO (elias): this should come from the backend
+        object_ids = {item.tag: item.object.object_id for item in obj_resp.items}
+        return _LocalApp(client, existing_app_id, app_page_url, tag_to_object_id=object_ids)
+
+    @staticmethod
+    async def _init_new(
+        client: _Client,
+        description: str,
+        app_state: int,
+        environment_name: str = "",
+        interactive=False,
+    ) -> "_LocalApp":
+        app_req = api_pb2.AppCreateRequest(
+            description=description,
+            environment_name=environment_name,
+            app_state=app_state,
+        )
+        app_resp = await retry_transient_errors(client.stub.AppCreate, app_req)
+        app_page_url = app_resp.app_logs_url
+        logger.debug(f"Created new app with id {app_resp.app_id}")
+        return _LocalApp(
+            client, app_resp.app_id, app_page_url, environment_name=environment_name, interactive=interactive
+        )
 
-    def _get_watch_mounts(self):
-        all_mounts = [
-            *self._mounts,
-        ]
-        for function in self.registered_functions.values():
-            all_mounts.extend(function._all_mounts)
-
-        return [m for m in all_mounts if m.is_local()]
-
-    def _add_function(self, function: _Function):
-        if function.tag in self._indexed_objects:
-            old_function = self._indexed_objects[function.tag]
-            if isinstance(old_function, _Function):
-                if not is_notebook():
-                    logger.warning(
-                        f"Warning: Tag '{function.tag}' collision!"
-                        f" Overriding existing function [{old_function._info.module_name}].{old_function._info.function_name}"
-                        f" with new function [{function._info.module_name}].{function._info.function_name}"
-                    )
-            else:
-                logger.warning(f"Warning: tag {function.tag} exists but is overridden by function")
+    @staticmethod
+    async def _init_from_name(
+        client: _Client,
+        name: str,
+        namespace,
+        environment_name: str = "",
+    ):
+        # Look up any existing deployment
+        app_req = api_pb2.AppGetByDeploymentNameRequest(
+            name=name,
+            namespace=namespace,
+            environment_name=environment_name,
+        )
+        app_resp = await retry_transient_errors(client.stub.AppGetByDeploymentName, app_req)
+        existing_app_id = app_resp.app_id or None
 
-        self._add_object(function.tag, function)
+        # Grab the app
+        if existing_app_id is not None:
+            return await _LocalApp._init_existing(client, existing_app_id)
+        else:
+            return await _LocalApp._init_new(
+                client, name, api_pb2.APP_STATE_INITIALIZING, environment_name=environment_name
+            )
 
-    def _init_container(self, client: _Client, running_app: RunningApp):
-        self._client = client
-        self._running_app = running_app
+    async def deploy(self, name: str, namespace, public: bool) -> str:
+        """`App.deploy` is deprecated in favor of `modal.runner.deploy_stub`."""
 
-        # Hydrate objects on stub
-        for tag, object_id in running_app.tag_to_object_id.items():
-            if tag in self._indexed_objects:
-                obj = self._indexed_objects[tag]
-                handle_metadata = running_app.object_handle_metadata[object_id]
-                obj._hydrate(object_id, client, handle_metadata)
+        deploy_req = api_pb2.AppDeployRequest(
+            app_id=self.app_id,
+            name=name,
+            namespace=namespace,
+            object_entity="ap",
+            visibility=(api_pb2.APP_DEPLOY_VISIBILITY_PUBLIC if public else api_pb2.APP_DEPLOY_VISIBILITY_WORKSPACE),
+        )
+        try:
+            deploy_response = await retry_transient_errors(self._client.stub.AppDeploy, deploy_req)
+        except GRPCError as exc:
+            if exc.status == Status.INVALID_ARGUMENT:
+                raise InvalidError(exc.message)
+            if exc.status == Status.FAILED_PRECONDITION:
+                raise InvalidError(exc.message)
+            raise
+        return deploy_response.url
 
-    @property
-    def registered_functions(self) -> Dict[str, _Function]:
-        """All modal.Function objects registered on the stub."""
-        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Function)}
 
-    @property
-    def registered_classes(self) -> Dict[str, _Function]:
-        """All modal.Cls objects registered on the stub."""
-        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Cls)}
+class _ContainerApp:
+    _client: Optional[_Client]
+    _app_id: Optional[str]
+    _associated_stub: Optional[Any]  # TODO(erikbern): type
+    _environment_name: Optional[str]
+    _tag_to_object_id: Dict[str, str]
+    _object_handle_metadata: Dict[str, Optional[Message]]
+    _stub_name: Optional[str]
+    # if true, there's an active PTY shell session connected to this process.
+    _is_interactivity_enabled: bool
+    _function_def: Optional[api_pb2.Function]
+    _fetching_inputs: bool
 
-    @property
-    def registered_entrypoints(self) -> Dict[str, _LocalEntrypoint]:
-        """All local CLI entrypoints registered on the stub."""
-        return self._local_entrypoints
+    def __init__(self):
+        self._client = None
+        self._app_id = None
+        self._associated_stub = None
+        self._stub_name = None
+        self._environment_name = None
+        self._tag_to_object_id = {}
+        self._object_handle_metadata = {}
+        self._is_interactivity_enabled = False
+        self._fetching_inputs = True
 
     @property
-    def indexed_objects(self) -> Dict[str, _Object]:
-        return self._indexed_objects
+    def client(self) -> Optional[_Client]:
+        """A reference to the running App's server client."""
+        return self._client
 
     @property
-    def registered_web_endpoints(self) -> List[str]:
-        """Names of web endpoint (ie. webhook) functions registered on the stub."""
-        return self._web_endpoints
-
-    def local_entrypoint(
-        self, _warn_parentheses_missing=None, *, name: Optional[str] = None
-    ) -> Callable[[Callable[..., Any]], None]:
-        """Decorate a function to be used as a CLI entrypoint for a Modal App.
-
-        These functions can be used to define code that runs locally to set up the app,
-        and act as an entrypoint to start Modal functions from. Note that regular
-        Modal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,
-        those functions are executed remotely directly.
-
-        **Example**
-
-        ```python
-        @stub.local_entrypoint()
-        def main():
-            some_modal_function.remote()
-        ```
-
-        You can call the function using `modal run` directly from the CLI:
-
-        ```shell
-        modal run stub_module.py
-        ```
-
-        Note that an explicit [`stub.run()`](/docs/reference/modal.Stub#run) is not needed, as an
-        [app](/docs/guide/apps) is automatically created for you.
-
-        **Multiple Entrypoints**
-
-        If you have multiple `local_entrypoint` functions, you can qualify the name of your stub and function:
-
-        ```shell
-        modal run stub_module.py::stub.some_other_function
-        ```
-
-        **Parsing Arguments**
-
-        If your entrypoint function take arguments with primitive types, `modal run` automatically parses them as
-        CLI options. For example, the following function can be called with `modal run stub_module.py --foo 1 --bar "hello"`:
-
-        ```python
-        @stub.local_entrypoint()
-        def main(foo: int, bar: str):
-            some_modal_function.call(foo, bar)
-        ```
-
-        Currently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported. Use `modal run stub_module.py --help` for more
-        information on usage.
-
-        """
-        if _warn_parentheses_missing:
-            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.local_entrypoint()`.")
-        if name is not None and not isinstance(name, str):
-            raise InvalidError("Invalid value for `name`: Must be string.")
-
-        def wrapped(raw_f: Callable[..., Any]) -> None:
-            info = FunctionInfo(raw_f)
-            tag = name if name is not None else raw_f.__qualname__
-            if tag in self._local_entrypoints:
-                # TODO: get rid of this limitation.
-                raise InvalidError(f"Duplicate local entrypoint name: {tag}. Local entrypoint names must be unique.")
-            entrypoint = self._local_entrypoints[tag] = _LocalEntrypoint(info, self)
-            return entrypoint
-
-        return wrapped
-
-    def function(
-        self,
-        _warn_parentheses_missing=None,
-        *,
-        image: Optional[_Image] = None,  # The image to run as the container for the function
-        schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
-        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
-        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
-        serialized: bool = False,  # Whether to send the function over using cloudpickle.
-        mounts: Sequence[_Mount] = (),  # Modal Mounts added to the container
-        network_file_systems: Dict[
-            Union[str, PurePosixPath], _NetworkFileSystem
-        ] = {},  # Mountpoints for Modal NetworkFileSystems
-        volumes: Dict[
-            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
-        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
-        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
-        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
-        memory: Optional[
-            Union[int, Tuple[int, int]]
-        ] = None,  # Specify, in MiB, a memory request which is the minimum memory required. Or, pass (request, limit) to additionally specify a hard limit in MiB.
-        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
-        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
-        concurrency_limit: Optional[
-            int
-        ] = None,  # An optional maximum number of concurrent containers running the function (use keep_warm for minimum).
-        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
-        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
-        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
-        keep_warm: Optional[
-            int
-        ] = None,  # An optional minimum number of containers to always keep warm (use concurrency_limit for maximum).
-        name: Optional[str] = None,  # Sets the Modal name of the function within the stub
-        is_generator: Optional[
-            bool
-        ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
-        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
-        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
-        checkpointing_enabled: Optional[bool] = None,  # Deprecated
-        block_network: bool = False,  # Whether to block network access
-        max_inputs: Optional[
-            int
-        ] = None,  # Maximum number of inputs a container should handle before shutting down. With `max_inputs = 1`, containers will be single-use.
-        # The next group of parameters are deprecated; do not use in any new code
-        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
-        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
-        # Parameters below here are experimental. Use with caution!
-        _allow_background_volume_commits: bool = False,  # Experimental flag
-        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
-        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
-        _experimental_scheduler_placement: Optional[
-            SchedulerPlacement
-        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
-    ) -> Callable[..., _Function]:
-        """Decorator to register a new Modal function with this stub."""
-        if isinstance(_warn_parentheses_missing, _Image):
-            # Handle edge case where maybe (?) some users passed image as a positional arg
-            raise InvalidError("`image` needs to be a keyword argument: `@stub.function(image=image)`.")
-        if _warn_parentheses_missing:
-            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.function()`.")
-
-        if interactive:
-            deprecation_error(
-                (2024, 2, 29), "interactive=True has been deprecated. Set MODAL_INTERACTIVE_FUNCTIONS=1 instead."
-            )
-
-        if image is None:
-            image = self._get_default_image()
+    def app_id(self) -> Optional[str]:
+        """A unique identifier for this running App."""
+        return self._app_id
 
-        secrets = [*self._secrets, *secrets]
+    @property
+    def fetching_inputs(self) -> bool:
+        return self._fetching_inputs
 
-        def wrapped(
-            f: Union[_PartialFunction, Callable[..., Any]],
-            _cls: Optional[type] = None,  # Used for methods only
-        ) -> _Function:
-            nonlocal keep_warm, is_generator
-
-            if isinstance(f, _PartialFunction):
-                f.wrapped = True
-                info = FunctionInfo(f.raw_f, serialized=serialized, name_override=name, cls=_cls)
-                raw_f = f.raw_f
-                webhook_config = f.webhook_config
-                is_generator = f.is_generator
-                keep_warm = f.keep_warm or keep_warm
-
-                if webhook_config:
-                    if interactive:
-                        raise InvalidError("interactive=True is not supported with web endpoint functions")
-                    self._web_endpoints.append(info.get_tag())
-            else:
-                info = FunctionInfo(f, serialized=serialized, name_override=name, cls=_cls)
-                webhook_config = None
-                raw_f = f
-
-            if not _cls and not info.is_serialized() and "." in info.function_name:  # This is a method
-                raise InvalidError(
-                    "`stub.function` on methods is not allowed. See https://modal.com/docs/guide/lifecycle-functions instead"
-                )
-
-            if is_generator is None:
-                is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)
-
-            function = _Function.from_args(
-                info,
-                stub=self,
-                image=image,
-                secret=secret,
-                secrets=secrets,
-                schedule=schedule,
-                is_generator=is_generator,
-                gpu=gpu,
-                mounts=[*self._mounts, *mounts],
-                network_file_systems=network_file_systems,
-                allow_cross_region_volumes=allow_cross_region_volumes,
-                volumes={**self._volumes, **volumes},
-                memory=memory,
-                proxy=proxy,
-                retries=retries,
-                concurrency_limit=concurrency_limit,
-                allow_concurrent_inputs=allow_concurrent_inputs,
-                container_idle_timeout=container_idle_timeout,
-                timeout=timeout,
-                cpu=cpu,
-                keep_warm=keep_warm,
-                cloud=cloud,
-                webhook_config=webhook_config,
-                enable_memory_snapshot=enable_memory_snapshot,
-                checkpointing_enabled=checkpointing_enabled,
-                allow_background_volume_commits=_allow_background_volume_commits,
-                block_network=block_network,
-                max_inputs=max_inputs,
-                _experimental_boost=_experimental_boost,
-                _experimental_scheduler=_experimental_scheduler,
-                _experimental_scheduler_placement=_experimental_scheduler_placement,
+    def associate_stub_container(self, stub):
+        # TODO(erikbern): the fact that we need to set two-way references strongly indicate that
+        # we should just merge these two objects!
+        self._associated_stub = stub
+        stub._container_app = self
+
+        # Initialize objects on stub
+        stub_objects: dict[str, _Object] = dict(stub.get_objects())
+        for tag, object_id in self._tag_to_object_id.items():
+            obj = stub_objects.get(tag)
+            if obj is not None:
+                handle_metadata = self._object_handle_metadata[object_id]
+                obj._hydrate(object_id, self._client, handle_metadata)
+
+    def _has_object(self, tag: str) -> bool:
+        return tag in self._tag_to_object_id
+
+    def _hydrate_object(self, obj, tag: str):
+        object_id: str = self._tag_to_object_id[tag]
+        metadata: Message = self._object_handle_metadata[object_id]
+        obj._hydrate(object_id, self._client, metadata)
+
+    def hydrate_function_deps(self, function: _Function, dep_object_ids: List[str]):
+        function_deps = function.deps(only_explicit_mounts=True)
+        if len(function_deps) != len(dep_object_ids):
+            raise ExecutionError(
+                f"Function has {len(function_deps)} dependencies"
+                f" but container got {len(dep_object_ids)} object ids."
             )
+        for object_id, obj in zip(dep_object_ids, function_deps):
+            metadata: Message = self._object_handle_metadata[object_id]
+            obj._hydrate(object_id, self._client, metadata)
 
-            self._add_function(function)
-            return function
-
-        return wrapped
-
-    def cls(
-        self,
-        _warn_parentheses_missing=None,
-        *,
-        image: Optional[_Image] = None,  # The image to run as the container for the function
-        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
-        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
-        serialized: bool = False,  # Whether to send the function over using cloudpickle.
-        mounts: Sequence[_Mount] = (),
-        network_file_systems: Dict[
-            Union[str, PurePosixPath], _NetworkFileSystem
-        ] = {},  # Mountpoints for Modal NetworkFileSystems
-        volumes: Dict[
-            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
-        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
-        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
-        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
-        memory: Optional[
-            Union[int, Tuple[int, int]]
-        ] = None,  # Specify, in MiB, a memory request which is the minimum memory required. Or, pass (request, limit) to additionally specify a hard limit in MiB.
-        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
-        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
-        concurrency_limit: Optional[int] = None,  # Limit for max concurrent containers running the function.
-        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
-        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
-        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
-        keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm.
-        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
-        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
-        checkpointing_enabled: Optional[bool] = None,  # Deprecated
-        block_network: bool = False,  # Whether to block network access
-        _allow_background_volume_commits: bool = False,
-        max_inputs: Optional[
-            int
-        ] = None,  # Limits the number of inputs a container handles before shutting down. Use `max_inputs = 1` for single-use containers.
-        # The next group of parameters are deprecated; do not use in any new code
-        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
-        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
-        # Parameters below here are experimental. Use with caution!
-        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
-        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
-        _experimental_scheduler_placement: Optional[
-            SchedulerPlacement
-        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
-    ) -> Callable[[CLS_T], _Cls]:
-        if _warn_parentheses_missing:
-            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.cls()`.")
-
-        decorator: Callable[[PartialFunction, type], _Function] = self.function(
-            image=image,
-            secret=secret,
-            secrets=secrets,
-            gpu=gpu,
-            serialized=serialized,
-            mounts=mounts,
-            network_file_systems=network_file_systems,
-            allow_cross_region_volumes=allow_cross_region_volumes,
-            volumes=volumes,
-            cpu=cpu,
-            memory=memory,
-            proxy=proxy,
-            retries=retries,
-            concurrency_limit=concurrency_limit,
-            allow_concurrent_inputs=allow_concurrent_inputs,
-            container_idle_timeout=container_idle_timeout,
-            timeout=timeout,
-            interactive=interactive,
-            keep_warm=keep_warm,
-            cloud=cloud,
-            enable_memory_snapshot=enable_memory_snapshot,
-            checkpointing_enabled=checkpointing_enabled,
-            block_network=block_network,
-            _allow_background_volume_commits=_allow_background_volume_commits,
-            max_inputs=max_inputs,
-            _experimental_boost=_experimental_boost,
-            _experimental_scheduler=_experimental_scheduler,
-            _experimental_scheduler_placement=_experimental_scheduler_placement,
-        )
-
-        def wrapper(user_cls: CLS_T) -> _Cls:
-            cls: _Cls = _Cls.from_local(user_cls, self, decorator)
-
-            if (
-                _find_callables_for_cls(user_cls, _PartialFunctionFlags.ENTER_PRE_CHECKPOINT)
-                and not enable_memory_snapshot
-            ):
-                raise InvalidError("A class must have `enable_memory_snapshot=True` to use `snap=True` on its methods.")
-
-            if len(cls._functions) > 1 and keep_warm is not None:
-                deprecation_warning(
-                    (2023, 10, 20),
-                    "`@stub.cls(keep_warm=...)` is deprecated when there is more than 1 method."
-                    " Use `@method(keep_warm=...)` on each method instead!",
-                )
-
-            tag: str = user_cls.__name__
-            self._add_object(tag, cls)
-            return cls
-
-        return wrapper
-
-    async def spawn_sandbox(
+    async def init(
         self,
-        *entrypoint_args: str,
-        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
-        mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.
-        secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
-        network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
-        timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
-        workdir: Optional[str] = None,  # Working directory of the sandbox.
-        gpu: GPU_T = None,
-        cloud: Optional[str] = None,
-        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
-        memory: Optional[
-            Union[int, Tuple[int, int]]
-        ] = None,  # Specify, in MiB, a memory request which is the minimum memory required. Or, pass (request, limit) to additionally specify a hard limit in MiB.
-        block_network: bool = False,  # Whether to block network access
-        volumes: Dict[
-            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
-        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
-        _allow_background_volume_commits: bool = False,
-        pty_info: Optional[api_pb2.PTYInfo] = None,
-    ) -> _Sandbox:
-        """Sandboxes are a way to run arbitrary commands in dynamically defined environments.
-
-        This function returns a [SandboxHandle](/docs/reference/modal.Sandbox#modalsandboxsandbox), which can be used to interact with the running sandbox.
-
-        Refer to the [docs](/docs/guide/sandbox) on how to spawn and use sandboxes.
-        """
-        if self._running_app:
-            app_id = self._running_app.app_id
-            environment_name = self._running_app.environment_name
-            client = self._client
-        else:
-            raise InvalidError("`stub.spawn_sandbox` requires a running app.")
-
-        # TODO(erikbern): pulling a lot of app internals here, let's clean up shortly
-        resolver = Resolver(client, environment_name=environment_name, app_id=app_id)
-        obj = _Sandbox._new(
-            entrypoint_args,
-            image=image or _default_image,
-            mounts=mounts,
-            secrets=secrets,
-            timeout=timeout,
-            workdir=workdir,
-            gpu=gpu,
-            cloud=cloud,
-            cpu=cpu,
-            memory=memory,
-            network_file_systems=network_file_systems,
-            block_network=block_network,
-            volumes=volumes,
-            allow_background_volume_commits=_allow_background_volume_commits,
-            pty_info=pty_info,
-        )
-        await resolver.load(obj)
-        return obj
-
-    def include(self, /, other_stub: "_App"):
-        """Include another stub's objects in this one.
-
-        Useful splitting up Modal apps across different self-contained files
+        client: _Client,
+        app_id: str,
+        stub_name: str = "",
+        environment_name: str = "",
+        function_def: Optional[api_pb2.Function] = None,
+    ):
+        """Used by the container to bootstrap the app and all its objects. Not intended to be called by Modal users."""
+        global _is_container_app
+        _is_container_app = True
 
-        ```python
-        stub_a = modal.Stub("a")
-        @stub.function()
-        def foo():
-            ...
-
-        stub_b = modal.Stub("b")
-        @stub.function()
-        def bar():
-            ...
-
-        stub_a.include(stub_b)
+        self._client = client
+        self._app_id = app_id
+        self._stub_name = stub_name
+        self._environment_name = environment_name
+        self._function_def = function_def
+        self._tag_to_object_id = {}
+        self._object_handle_metadata = {}
+        req = api_pb2.AppGetObjectsRequest(app_id=app_id, include_unindexed=True)
+        resp = await retry_transient_errors(client.stub.AppGetObjects, req)
+        logger.debug(f"AppGetObjects received {len(resp.items)} objects for app {app_id}")
+        for item in resp.items:
+            handle_metadata: Optional[Message] = get_proto_oneof(item.object, "handle_metadata_oneof")
+            self._object_handle_metadata[item.object.object_id] = handle_metadata
+            logger.debug(f"Setting metadata for {item.object.object_id} ({item.tag})")
+            if item.tag:
+                self._tag_to_object_id[item.tag] = item.object.object_id
+
+    @staticmethod
+    def _reset_container():
+        # Just used for tests
+        global _is_container_app, _container_app
+        _is_container_app = False
+        _container_app.__init__()  # type: ignore
+
+    def stop_fetching_inputs(self):
+        self._fetching_inputs = False
+
+
+LocalApp = synchronize_api(_LocalApp)
+ContainerApp = synchronize_api(_ContainerApp)
+
+_is_container_app = False
+_container_app = _ContainerApp()
+container_app = synchronize_api(_container_app)
+assert isinstance(container_app, ContainerApp)
+
+
+async def _interact(client: Optional[_Client] = None) -> None:
+    if _container_app._is_interactivity_enabled:
+        # Currently, interactivity is enabled forever
+        return
+    _container_app._is_interactivity_enabled = True
+
+    if not client:
+        client = await _Client.from_env()
+
+    if client.client_type != api_pb2.CLIENT_TYPE_CONTAINER:
+        raise InvalidError("Interactivity only works inside a Modal Container.")
+
+    if _container_app._function_def is not None:
+        if not _container_app._function_def.pty_info:
+            raise InvalidError(
+                "Interactivity is not enabled in this function. Use MODAL_INTERACTIVE_FUNCTIONS=1 to enable interactivity."
+            )
 
-        @stub_a.local_entrypoint()
-        def main():
-            # use function declared on the included stub
-            bar.remote()
-        ```
-        """
-        for tag, object in other_stub._indexed_objects.items():
-            existing_object = self._indexed_objects.get(tag)
-            if existing_object and existing_object != object:
-                logger.warning(
-                    f"Named app object {tag} with existing value {existing_object} is being overwritten by a different object {object}"
-                )
+        if _container_app._function_def.concurrency_limit > 1:
+            print(
+                "Warning: Interactivity is not supported on functions with concurrency > 1. You may experience unexpected behavior."
+            )
 
-            self._add_object(tag, object)
+    # todo(nathan): add warning if concurrency limit > 1. but idk how to check this here
+    # todo(nathan): check if function interactivity is enabled
+    try:
+        await client.stub.FunctionStartPtyShell(Empty())
+    except Exception as e:
+        print("Error: Failed to start PTY shell.")
+        raise e
 
 
-App = synchronize_api(_App)
+interact = synchronize_api(_interact)
 
 
-class _Stub(_App):
-    """This enables using an "Stub" class instead of "App".
+def is_local() -> bool:
+    """Returns if we are currently on the machine launching/deploying a Modal app
 
-    For most of Modal's history, the app class was called "Stub", so this exists for
-    backwards compatibility, in order to facilitate moving from "Stub" to "App".
+    Returns `True` when executed locally on the user's machine.
+    Returns `False` when executed from a Modal container in the cloud.
     """
+    return not _is_container_app
+
 
-    pass
+async def _list_apps(env: str, client: Optional[_Client] = None) -> List[api_pb2.AppStats]:
+    """List apps in a given Modal environment."""
+    if client is None:
+        client = await _Client.from_env()
+    resp: api_pb2.AppListResponse = await client.stub.AppList(api_pb2.AppListRequest(environment_name=env))
+    return list(resp.apps)
 
 
-Stub = synchronize_api(_Stub)
+list_apps = synchronize_api(_list_apps)
```

## modal/app.pyi

```diff
@@ -1,340 +1,291 @@
+import google.protobuf.message
 import modal._output
-import modal._utils.function_utils
 import modal.client
-import modal.cloud_bucket_mount
-import modal.cls
-import modal.functions
-import modal.gpu
-import modal.image
-import modal.mount
-import modal.network_file_system
 import modal.object
-import modal.proxy
-import modal.retries
-import modal.running_app
-import modal.sandbox
-import modal.schedule
-import modal.scheduler_placement
-import modal.secret
-import modal.volume
 import modal_proto.api_pb2
-import pathlib
-import synchronicity.combined_types
 import typing
 import typing_extensions
 
-class _LocalEntrypoint:
-    _info: modal._utils.function_utils.FunctionInfo
-    _stub: _App
+_Function = typing.TypeVar("_Function")
 
-    def __init__(self, info, stub):
-        ...
+class _LocalApp:
+    _tag_to_object_id: typing.Dict[str, str]
+    _client: modal.client._Client
+    _app_id: str
+    _app_page_url: str
+    _environment_name: str
+    _interactive: bool
 
-    def __call__(self, *args, **kwargs):
+    def __init__(self, client: modal.client._Client, app_id: str, app_page_url: str, tag_to_object_id: typing.Union[typing.Dict[str, str], None] = None, stub_name: typing.Union[str, None] = None, environment_name: typing.Union[str, None] = None, interactive: bool = False):
         ...
 
     @property
-    def info(self) -> modal._utils.function_utils.FunctionInfo:
+    def client(self) -> modal.client._Client:
         ...
 
     @property
-    def stub(self) -> _App:
+    def app_id(self) -> str:
         ...
 
+    @property
+    def is_interactive(self) -> bool:
+        ...
 
-class LocalEntrypoint:
-    _info: modal._utils.function_utils.FunctionInfo
-    _stub: App
+    async def _create_all_objects(self, indexed_objects: typing.Dict[str, modal.object._Object], new_app_state: int, environment_name: str, output_mgr: typing.Union[modal._output.OutputManager, None] = None):
+        ...
 
-    def __init__(self, info, stub):
+    async def disconnect(self, reason: typing.Union[int, None] = None, exc_str: typing.Union[str, None] = None):
         ...
 
-    def __call__(self, *args, **kwargs):
+    async def stop(self):
         ...
 
-    @property
-    def info(self) -> modal._utils.function_utils.FunctionInfo:
+    def log_url(self):
         ...
 
-    @property
-    def stub(self) -> App:
+    @staticmethod
+    async def _init_existing(client: modal.client._Client, existing_app_id: str) -> _LocalApp:
         ...
 
+    @staticmethod
+    async def _init_new(client: modal.client._Client, description: str, app_state: int, environment_name: str = '', interactive=False) -> _LocalApp:
+        ...
 
-def check_sequence(items: typing.Sequence[typing.Any], item_type: typing.Type[typing.Any], error_msg: str):
-    ...
+    @staticmethod
+    async def _init_from_name(client: modal.client._Client, name: str, namespace, environment_name: str = ''):
+        ...
 
+    async def deploy(self, name: str, namespace, public: bool) -> str:
+        ...
 
-CLS_T = typing.TypeVar("CLS_T", bound="typing.Type")
 
-class _App:
-    _name: typing.Union[str, None]
-    _description: typing.Union[str, None]
-    _indexed_objects: typing.Dict[str, modal.object._Object]
-    _function_mounts: typing.Dict[str, modal.mount._Mount]
-    _mounts: typing.Sequence[modal.mount._Mount]
-    _secrets: typing.Sequence[modal.secret._Secret]
-    _volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume._Volume]
-    _web_endpoints: typing.List[str]
-    _local_entrypoints: typing.Dict[str, _LocalEntrypoint]
-    _running_app: typing.Union[modal.running_app.RunningApp, None]
+class _ContainerApp:
     _client: typing.Union[modal.client._Client, None]
-    _all_apps: typing.ClassVar[typing.Dict[typing.Union[str, None], typing.List[_App]]]
-
-    def __init__(self, name: typing.Union[str, None] = None, *, image: typing.Union[modal.image._Image, None] = None, mounts: typing.Sequence[modal.mount._Mount] = [], secrets: typing.Sequence[modal.secret._Secret] = [], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume._Volume] = {}, **kwargs: modal.object._Object) -> None:
-        ...
+    _app_id: typing.Union[str, None]
+    _associated_stub: typing.Union[typing.Any, None]
+    _environment_name: typing.Union[str, None]
+    _tag_to_object_id: typing.Dict[str, str]
+    _object_handle_metadata: typing.Dict[str, typing.Union[google.protobuf.message.Message, None]]
+    _stub_name: typing.Union[str, None]
+    _is_interactivity_enabled: bool
+    _function_def: typing.Union[modal_proto.api_pb2.Function, None]
+    _fetching_inputs: bool
 
-    @property
-    def name(self) -> typing.Union[str, None]:
+    def __init__(self):
         ...
 
     @property
-    def is_interactive(self) -> bool:
+    def client(self) -> typing.Union[modal.client._Client, None]:
         ...
 
     @property
     def app_id(self) -> typing.Union[str, None]:
         ...
 
     @property
-    def description(self) -> typing.Union[str, None]:
+    def fetching_inputs(self) -> bool:
         ...
 
-    def set_description(self, description: str):
+    def associate_stub_container(self, stub):
         ...
 
-    def _validate_blueprint_value(self, key: str, value: typing.Any):
+    def _has_object(self, tag: str) -> bool:
         ...
 
-    def _add_object(self, tag, obj):
+    def _hydrate_object(self, obj, tag: str):
         ...
 
-    def __getitem__(self, tag: str):
+    def hydrate_function_deps(self, function: _Function, dep_object_ids: typing.List[str]):
         ...
 
-    def __setitem__(self, tag: str, obj: modal.object._Object):
+    async def init(self, client: modal.client._Client, app_id: str, stub_name: str = '', environment_name: str = '', function_def: typing.Union[modal_proto.api_pb2.Function, None] = None):
         ...
 
-    def __getattr__(self, tag: str) -> modal.object._Object:
+    @staticmethod
+    def _reset_container():
         ...
 
-    def __setattr__(self, tag: str, obj: modal.object._Object):
+    def stop_fetching_inputs(self):
         ...
 
-    @property
-    def image(self) -> modal.image._Image:
-        ...
 
-    @image.setter
-    def image(self, value):
-        ...
-
-    def _uncreate_all_objects(self):
-        ...
+class LocalApp:
+    _tag_to_object_id: typing.Dict[str, str]
+    _client: modal.client.Client
+    _app_id: str
+    _app_page_url: str
+    _environment_name: str
+    _interactive: bool
 
-    def is_inside(self, image: typing.Union[modal.image._Image, None] = None):
+    def __init__(self, client: modal.client.Client, app_id: str, app_page_url: str, tag_to_object_id: typing.Union[typing.Dict[str, str], None] = None, stub_name: typing.Union[str, None] = None, environment_name: typing.Union[str, None] = None, interactive: bool = False):
         ...
 
-    def _set_local_app(self, client: modal.client._Client, app: modal.running_app.RunningApp) -> typing.AsyncContextManager[None]:
+    @property
+    def client(self) -> modal.client.Client:
         ...
 
-    def run(self, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None) -> typing.AsyncContextManager[_App]:
+    @property
+    def app_id(self) -> str:
         ...
 
-    def _get_default_image(self):
+    @property
+    def is_interactive(self) -> bool:
         ...
 
-    def _get_watch_mounts(self):
-        ...
+    class ___create_all_objects_spec(typing_extensions.Protocol):
+        def __call__(self, indexed_objects: typing.Dict[str, modal.object.Object], new_app_state: int, environment_name: str, output_mgr: typing.Union[modal._output.OutputManager, None] = None):
+            ...
 
-    def _add_function(self, function: modal.functions._Function):
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    def _init_container(self, client: modal.client._Client, running_app: modal.running_app.RunningApp):
-        ...
+    _create_all_objects: ___create_all_objects_spec
 
-    @property
-    def registered_functions(self) -> typing.Dict[str, modal.functions._Function]:
-        ...
+    class __disconnect_spec(typing_extensions.Protocol):
+        def __call__(self, reason: typing.Union[int, None] = None, exc_str: typing.Union[str, None] = None):
+            ...
 
-    @property
-    def registered_classes(self) -> typing.Dict[str, modal.functions._Function]:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    @property
-    def registered_entrypoints(self) -> typing.Dict[str, _LocalEntrypoint]:
-        ...
+    disconnect: __disconnect_spec
 
-    @property
-    def indexed_objects(self) -> typing.Dict[str, modal.object._Object]:
-        ...
+    class __stop_spec(typing_extensions.Protocol):
+        def __call__(self):
+            ...
 
-    @property
-    def registered_web_endpoints(self) -> typing.List[str]:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    def local_entrypoint(self, _warn_parentheses_missing=None, *, name: typing.Union[str, None] = None) -> typing.Callable[[typing.Callable[..., typing.Any]], None]:
-        ...
+    stop: __stop_spec
 
-    def function(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image._Image, None] = None, schedule: typing.Union[modal.schedule.Schedule, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, name: typing.Union[str, None] = None, is_generator: typing.Union[bool, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret._Secret, None] = None, _allow_background_volume_commits: bool = False, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[..., modal.functions._Function]:
+    def log_url(self):
         ...
 
-    def cls(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image._Image, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, _allow_background_volume_commits: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret._Secret, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[[CLS_T], modal.cls._Cls]:
-        ...
+    class ___init_existing_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, existing_app_id: str) -> LocalApp:
+            ...
 
-    async def spawn_sandbox(self, *entrypoint_args: str, image: typing.Union[modal.image._Image, None] = None, mounts: typing.Sequence[modal.mount._Mount] = (), secrets: typing.Sequence[modal.secret._Secret] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, block_network: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, _allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> modal.sandbox._Sandbox:
-        ...
+        async def aio(self, *args, **kwargs) -> LocalApp:
+            ...
 
-    def include(self, /, other_stub: _App):
-        ...
+    _init_existing: ___init_existing_spec
 
+    class ___init_new_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, description: str, app_state: int, environment_name: str = '', interactive=False) -> LocalApp:
+            ...
 
-class App:
-    _name: typing.Union[str, None]
-    _description: typing.Union[str, None]
-    _indexed_objects: typing.Dict[str, modal.object.Object]
-    _function_mounts: typing.Dict[str, modal.mount.Mount]
-    _mounts: typing.Sequence[modal.mount.Mount]
-    _secrets: typing.Sequence[modal.secret.Secret]
-    _volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume.Volume]
-    _web_endpoints: typing.List[str]
-    _local_entrypoints: typing.Dict[str, LocalEntrypoint]
-    _running_app: typing.Union[modal.running_app.RunningApp, None]
-    _client: typing.Union[modal.client.Client, None]
-    _all_apps: typing.ClassVar[typing.Dict[typing.Union[str, None], typing.List[App]]]
+        async def aio(self, *args, **kwargs) -> LocalApp:
+            ...
 
-    def __init__(self, name: typing.Union[str, None] = None, *, image: typing.Union[modal.image.Image, None] = None, mounts: typing.Sequence[modal.mount.Mount] = [], secrets: typing.Sequence[modal.secret.Secret] = [], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume.Volume] = {}, **kwargs: modal.object.Object) -> None:
-        ...
+    _init_new: ___init_new_spec
 
-    @property
-    def name(self) -> typing.Union[str, None]:
-        ...
+    class ___init_from_name_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, name: str, namespace, environment_name: str = ''):
+            ...
 
-    @property
-    def is_interactive(self) -> bool:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    @property
-    def app_id(self) -> typing.Union[str, None]:
-        ...
+    _init_from_name: ___init_from_name_spec
 
-    @property
-    def description(self) -> typing.Union[str, None]:
-        ...
+    class __deploy_spec(typing_extensions.Protocol):
+        def __call__(self, name: str, namespace, public: bool) -> str:
+            ...
 
-    def set_description(self, description: str):
-        ...
+        async def aio(self, *args, **kwargs) -> str:
+            ...
 
-    def _validate_blueprint_value(self, key: str, value: typing.Any):
-        ...
+    deploy: __deploy_spec
 
-    def _add_object(self, tag, obj):
-        ...
 
-    def __getitem__(self, tag: str):
-        ...
+class ContainerApp:
+    _client: typing.Union[modal.client.Client, None]
+    _app_id: typing.Union[str, None]
+    _associated_stub: typing.Union[typing.Any, None]
+    _environment_name: typing.Union[str, None]
+    _tag_to_object_id: typing.Dict[str, str]
+    _object_handle_metadata: typing.Dict[str, typing.Union[google.protobuf.message.Message, None]]
+    _stub_name: typing.Union[str, None]
+    _is_interactivity_enabled: bool
+    _function_def: typing.Union[modal_proto.api_pb2.Function, None]
+    _fetching_inputs: bool
 
-    def __setitem__(self, tag: str, obj: modal.object.Object):
+    def __init__(self):
         ...
 
-    def __getattr__(self, tag: str) -> modal.object.Object:
+    @property
+    def client(self) -> typing.Union[modal.client.Client, None]:
         ...
 
-    def __setattr__(self, tag: str, obj: modal.object.Object):
+    @property
+    def app_id(self) -> typing.Union[str, None]:
         ...
 
     @property
-    def image(self) -> modal.image.Image:
+    def fetching_inputs(self) -> bool:
         ...
 
-    @image.setter
-    def image(self, value):
+    def associate_stub_container(self, stub):
         ...
 
-    def _uncreate_all_objects(self):
+    def _has_object(self, tag: str) -> bool:
         ...
 
-    def is_inside(self, image: typing.Union[modal.image.Image, None] = None):
+    def _hydrate_object(self, obj, tag: str):
         ...
 
-    class ___set_local_app_spec(typing_extensions.Protocol):
-        def __call__(self, client: modal.client.Client, app: modal.running_app.RunningApp) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
-            ...
-
-        def aio(self, client: modal.client.Client, app: modal.running_app.RunningApp) -> typing.AsyncContextManager[None]:
-            ...
-
-    _set_local_app: ___set_local_app_spec
+    def hydrate_function_deps(self, function: _Function, dep_object_ids: typing.List[str]):
+        ...
 
-    class __run_spec(typing_extensions.Protocol):
-        def __call__(self, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[App]:
+    class __init_spec(typing_extensions.Protocol):
+        def __call__(self, client: modal.client.Client, app_id: str, stub_name: str = '', environment_name: str = '', function_def: typing.Union[modal_proto.api_pb2.Function, None] = None):
             ...
 
-        def aio(self, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None) -> typing.AsyncContextManager[App]:
+        async def aio(self, *args, **kwargs):
             ...
 
-    run: __run_spec
+    init: __init_spec
 
-    def _get_default_image(self):
+    @staticmethod
+    def _reset_container():
         ...
 
-    def _get_watch_mounts(self):
+    def stop_fetching_inputs(self):
         ...
 
-    def _add_function(self, function: modal.functions.Function):
-        ...
 
-    def _init_container(self, client: modal.client.Client, running_app: modal.running_app.RunningApp):
-        ...
+_container_app: _ContainerApp
 
-    @property
-    def registered_functions(self) -> typing.Dict[str, modal.functions.Function]:
-        ...
+container_app: ContainerApp
 
-    @property
-    def registered_classes(self) -> typing.Dict[str, modal.functions.Function]:
-        ...
+async def _interact(client: typing.Union[modal.client._Client, None] = None) -> None:
+    ...
 
-    @property
-    def registered_entrypoints(self) -> typing.Dict[str, LocalEntrypoint]:
-        ...
 
-    @property
-    def indexed_objects(self) -> typing.Dict[str, modal.object.Object]:
+class __interact_spec(typing_extensions.Protocol):
+    def __call__(self, client: typing.Union[modal.client.Client, None] = None) -> None:
         ...
 
-    @property
-    def registered_web_endpoints(self) -> typing.List[str]:
+    async def aio(self, *args, **kwargs) -> None:
         ...
 
-    def local_entrypoint(self, _warn_parentheses_missing=None, *, name: typing.Union[str, None] = None) -> typing.Callable[[typing.Callable[..., typing.Any]], None]:
-        ...
+interact: __interact_spec
 
-    def function(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image.Image, None] = None, schedule: typing.Union[modal.schedule.Schedule, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, name: typing.Union[str, None] = None, is_generator: typing.Union[bool, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret.Secret, None] = None, _allow_background_volume_commits: bool = False, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[..., modal.functions.Function]:
-        ...
 
-    def cls(self, _warn_parentheses_missing=None, *, image: typing.Union[modal.image.Image, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, serialized: bool = False, mounts: typing.Sequence[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_cross_region_volumes: bool = False, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, timeout: typing.Union[int, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, block_network: bool = False, _allow_background_volume_commits: bool = False, max_inputs: typing.Union[int, None] = None, interactive: bool = False, secret: typing.Union[modal.secret.Secret, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None) -> typing.Callable[[CLS_T], modal.cls.Cls]:
-        ...
+def is_local() -> bool:
+    ...
 
-    class __spawn_sandbox_spec(typing_extensions.Protocol):
-        def __call__(self, *entrypoint_args: str, image: typing.Union[modal.image.Image, None] = None, mounts: typing.Sequence[modal.mount.Mount] = (), secrets: typing.Sequence[modal.secret.Secret] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, block_network: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, _allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> modal.sandbox.Sandbox:
-            ...
 
-        async def aio(self, *args, **kwargs) -> modal.sandbox.Sandbox:
-            ...
+async def _list_apps(env: str, client: typing.Union[modal.client._Client, None] = None) -> typing.List[modal_proto.api_pb2.AppStats]:
+    ...
 
-    spawn_sandbox: __spawn_sandbox_spec
 
-    def include(self, other_stub: App):
+class __list_apps_spec(typing_extensions.Protocol):
+    def __call__(self, env: str, client: typing.Union[modal.client.Client, None] = None) -> typing.List[modal_proto.api_pb2.AppStats]:
         ...
 
-
-class _Stub(_App):
-    ...
-
-class Stub(App):
-    def __init__(self, name: typing.Union[str, None] = None, *, image: typing.Union[modal.image.Image, None] = None, mounts: typing.Sequence[modal.mount.Mount] = [], secrets: typing.Sequence[modal.secret.Secret] = [], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.volume.Volume] = {}, **kwargs: modal.object.Object) -> None:
+    async def aio(self, *args, **kwargs) -> typing.List[modal_proto.api_pb2.AppStats]:
         ...
 
-
-_default_image: modal.image._Image
+list_apps: __list_apps_spec
```

## modal/client.py

```diff
@@ -1,18 +1,16 @@
 # Copyright Modal Labs 2022
 import asyncio
 import platform
 import warnings
-from typing import AsyncIterator, Awaitable, Callable, ClassVar, Dict, Optional, Tuple
+from typing import Awaitable, Callable, Dict, Optional, Tuple
 
-import grpclib.client
 from aiohttp import ClientConnectorError, ClientResponseError
 from google.protobuf import empty_pb2
 from grpclib import GRPCError, Status
-from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_grpc, api_pb2
 from modal_version import __version__
 
 from ._utils import async_utils
 from ._utils.async_utils import synchronize_api
 from ._utils.grpc_utils import create_channel, retry_transient_errors
@@ -73,53 +71,54 @@
         return f"HTTP exception: {exc.os_error.__class__.__name__}"
     except Exception as exc:
         return f"HTTP exception: {exc.__class__.__name__}"
 
 
 async def _grpc_exc_string(exc: GRPCError, method_name: str, server_url: str, timeout: float) -> str:
     http_status = await _http_check(server_url, timeout=timeout)
-    return f"{method_name}: {exc.message} [gRPC status: {exc.status.name}, {http_status}]"
+    return f"{method_name}: {exc.message} [GRPC status: {exc.status.name}, {http_status}]"
 
 
 class _Client:
-    _client_from_env: ClassVar[Optional["_Client"]] = None
-    _client_from_env_lock: ClassVar[asyncio.Lock] = asyncio.Lock()
+    _client_from_env = None
+    _client_from_env_lock = None
+
+    client_type: int
 
     def __init__(
         self,
-        server_url: str,
-        client_type: int,
-        credentials: Optional[Tuple[str, str]],
-        version: str = __version__,
+        server_url,
+        client_type,
+        credentials,
+        version=__version__,
+        *,
+        no_verify=False,
     ):
         """The Modal client object is not intended to be instantiated directly by users."""
         self.server_url = server_url
         self.client_type = client_type
         self.credentials = credentials
         self.version = version
-        self._authenticated = False
-        self.image_builder_version: Optional[str] = None
+        self.no_verify = no_verify
         self._pre_stop: Optional[Callable[[], Awaitable[None]]] = None
-        self._channel: Optional[grpclib.client.Channel] = None
+        self._channel = None
         self._stub: Optional[api_grpc.ModalClientStub] = None
 
     @property
     def stub(self) -> Optional[api_grpc.ModalClientStub]:
         """mdmd:hidden"""
         return self._stub
 
-    @property
-    def authenticated(self) -> bool:
-        """mdmd:hidden"""
-        return self._authenticated
-
     async def _open(self):
         assert self._stub is None
         metadata = _get_metadata(self.client_type, self.credentials, self.version)
-        self._channel = create_channel(self.server_url, metadata=metadata)
+        self._channel = create_channel(
+            self.server_url,
+            metadata=metadata,
+        )
         self._stub = api_grpc.ModalClientStub(self._channel)  # type: ignore
 
     async def _close(self):
         if self._pre_stop is not None:
             logger.debug("Client: running pre-stop coroutine before shutting down")
             await self._pre_stop()  # type: ignore
 
@@ -135,82 +134,77 @@
         # teardown when an interrupt signal is received (eg. KeyboardInterrupt).
         # By registering a pre-stop fn stub.serve() can have its teardown
         # performed before the client is disconnected.
         #
         # ref: github.com/modal-labs/modal-client/pull/108
         self._pre_stop = pre_stop
 
-    async def _init(self):
-        """Connect to server and retrieve version information; raise appropriate error for various failures."""
+    async def _verify(self):
         logger.debug("Client: Starting")
         _check_config()
         try:
             req = empty_pb2.Empty()
             resp = await retry_transient_errors(
                 self.stub.ClientHello,
                 req,
                 attempt_timeout=CLIENT_CREATE_ATTEMPT_TIMEOUT,
                 total_timeout=CLIENT_CREATE_TOTAL_TIMEOUT,
             )
             if resp.warning:
                 ALARM_EMOJI = chr(0x1F6A8)
                 warnings.warn(f"{ALARM_EMOJI} {resp.warning} {ALARM_EMOJI}", DeprecationError)
-            self._authenticated = True
-            self.image_builder_version = resp.image_builder_version
         except GRPCError as exc:
             if exc.status == Status.FAILED_PRECONDITION:
                 raise VersionError(
-                    f"The client version ({self.version}) is too old. Please update (pip install --upgrade modal)."
+                    f"The client version {self.version} is too old. Please update to the latest package on PyPi: https://pypi.org/project/modal"
                 )
             elif exc.status == Status.UNAUTHENTICATED:
                 raise AuthError(exc.message)
             else:
                 exc_string = await _grpc_exc_string(exc, "ClientHello", self.server_url, CLIENT_CREATE_TOTAL_TIMEOUT)
                 raise ConnectionError(exc_string)
         except (OSError, asyncio.TimeoutError) as exc:
             raise ConnectionError(str(exc))
 
     async def __aenter__(self):
         await self._open()
-        try:
-            await self._init()
-        except BaseException:
-            await self._close()
-            raise
+        if not self.no_verify:
+            try:
+                await self._verify()
+            except BaseException:
+                await self._close()
+                raise
         return self
 
     async def __aexit__(self, exc_type, exc, tb):
         await self._close()
 
     @classmethod
-    @asynccontextmanager
-    async def anonymous(cls, server_url: str) -> AsyncIterator["_Client"]:
-        """mdmd:hidden
-        Create a connection with no credentials; to be used for token creation.
-        """
-        logger.debug("Client: Starting client without authentication")
-        client = cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials=None)
-        try:
-            await client._open()
-            # Skip client._init
-            yield client
-        finally:
-            await client._close()
+    async def verify(cls, server_url, credentials):
+        """mdmd:hidden"""
+        async with _Client(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials):
+            pass  # Will call ClientHello
+
+    @classmethod
+    async def unauthenticated_client(cls, server_url: str):
+        """mdmd:hidden"""
+        # Create a connection with no credentials
+        # To be used with the token flow
+        return _Client(server_url, api_pb2.CLIENT_TYPE_CLIENT, None, no_verify=True)
 
     @classmethod
     async def from_env(cls, _override_config=None) -> "_Client":
-        """mdmd:hidden
-        Singleton that is instantiated from the Modal config and reused on subsequent calls.
-        """
+        """mdmd:hidden"""
         if _override_config:
             # Only used for testing
             c = _override_config
         else:
             c = config
 
+        # Sets server_url to socket file path if proxy is available.
         server_url = c["server_url"]
 
         token_id = c["token_id"]
         token_secret = c["token_secret"]
         task_id = c["task_id"]
         task_secret = c["task_secret"]
 
@@ -220,63 +214,52 @@
         elif token_id and token_secret:
             client_type = api_pb2.CLIENT_TYPE_CLIENT
             credentials = (token_id, token_secret)
         else:
             client_type = api_pb2.CLIENT_TYPE_CLIENT
             credentials = None
 
+        if cls._client_from_env_lock is None:
+            cls._client_from_env_lock = asyncio.Lock()
+
         async with cls._client_from_env_lock:
             if cls._client_from_env:
                 return cls._client_from_env
             else:
                 client = _Client(server_url, client_type, credentials)
                 await client._open()
                 async_utils.on_shutdown(client._close())
                 try:
-                    await client._init()
+                    await client._verify()
                 except AuthError:
                     if not credentials:
                         creds_missing_msg = (
-                            "Token missing. Could not authenticate client."
-                            " If you have token credentials, see modal.com/docs/reference/modal.config for setup help."
-                            " If you are a new user, register an account at modal.com, then run `modal token new`."
+                            "Token missing. Could not authenticate client. "
+                            "If you have token credentials, see modal.com/docs/reference/modal.config for setup help. "
+                            "If you are a new user, register an account at modal.com, then run `modal token new`."
                         )
                         raise AuthError(creds_missing_msg)
                     else:
                         raise
                 cls._client_from_env = client
                 return client
 
     @classmethod
     async def from_credentials(cls, token_id: str, token_secret: str) -> "_Client":
-        """mdmd:hidden
-        Constructor based on token credentials; useful for managing Modal on behalf of third-party users.
-        """
-        server_url = config["server_url"]
+        """mdmd:hidden"""
         client_type = api_pb2.CLIENT_TYPE_CLIENT
         credentials = (token_id, token_secret)
+        server_url = config["server_url"]
+
         client = _Client(server_url, client_type, credentials)
         await client._open()
-        try:
-            await client._init()
-        except BaseException:
-            await client._close()
-            raise
         async_utils.on_shutdown(client._close())
         return client
 
     @classmethod
-    async def verify(cls, server_url: str, credentials: Tuple[str, str]) -> None:
-        """mdmd:hidden
-        Check whether can the client can connect to this server with these credentials; raise if not.
-        """
-        async with cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials):
-            pass  # Will call ClientHello RPC and possibly raise AuthError or ConnectionError
-
-    @classmethod
     def set_env_client(cls, client: Optional["_Client"]):
         """mdmd:hidden"""
         # Just used from tests.
         cls._client_from_env = client
 
 
 Client = synchronize_api(_Client)
```

## modal/cloud_bucket_mount.py

```diff
@@ -1,150 +1,89 @@
 # Copyright Modal Labs 2022
 from dataclasses import dataclass
-from typing import List, Optional, Tuple
-from urllib.parse import urlparse
+from enum import Enum
+from typing import List, Optional, Tuple, Union
 
 from modal_proto import api_pb2
 
 from ._utils.async_utils import synchronize_api
 from .secret import _Secret
 
 
+class BucketType(Enum):
+    S3 = "s3"
+
+    @property
+    def proto(self):
+        if self.value == "s3":
+            return api_pb2.CloudBucketMount.BucketType.S3
+
+
 @dataclass
 class _CloudBucketMount:
     """Mounts a cloud bucket to your container. Currently supports AWS S3 buckets.
 
-    S3 buckets are mounted using [AWS S3 Mountpoint](https://github.com/awslabs/mountpoint-s3).
+    S3 buckets are mounted using [AWS' S3 Mountpoint](https://github.com/awslabs/mountpoint-s3).
     S3 mounts are optimized for reading large files sequentially. It does not support every file operation; consult
-    [the AWS S3 Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md) for more information.
-
-    **AWS S3 Usage**
-
-    ```python
-    import subprocess
-
-    stub = modal.Stub()
-    secret = modal.Secret.from_dict({
-        "AWS_ACCESS_KEY_ID": "...",
-        "AWS_SECRET_ACCESS_KEY": "...",
-    })
-    @stub.function(
-        volumes={
-            "/my-mount": modal.CloudBucketMount(
-                bucket_name="s3-bucket-name",
-                secret=secret,
-                read_only=True
-            )
-        }
-    )
-    def f():
-        subprocess.run(["ls", "/my-mount"], check=True)
-    ```
-
-    **Cloudflare R2 Usage**
+    [the AWS S3 Mountpoin documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md) for more information.
 
-    Cloudflare R2 is [S3-compatible](https://developers.cloudflare.com/r2/api/s3/api/) so its setup looks very similar to S3.
-    But additionally the `bucket_endpoint_url` argument must be passed.
+    **Usage**
 
     ```python
+    import modal
     import subprocess
 
     stub = modal.Stub()
-    secret = modal.Secret.from_dict({
-        "AWS_ACCESS_KEY_ID": "...",
-        "AWS_SECRET_ACCESS_KEY": "...",
-    })
-    @stub.function(
-        volumes={
-            "/my-mount": modal.CloudBucketMount(
-                bucket_name="my-r2-bucket",
-                bucket_endpoint_url="https://<ACCOUNT ID>.r2.cloudflarestorage.com",
-                secret=secret,
-                read_only=True
-            )
-        }
-    )
-    def f():
-        subprocess.run(["ls", "/my-mount"], check=True)
-    ```
-
-    **Google GCS Usage**
 
-    Google Cloud Storage (GCS) is partially [S3-compatible](https://cloud.google.com/storage/docs/interoperability).
-    Currently **only `read_only=True`** is supported for GCS buckets. GCS Buckets also require a secret with Google-specific
-    key names (see below) populated with a [HMAC key](https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create).
-
-    ```python
-    import subprocess
-
-    stub = modal.Stub()
-    gcp_hmac_secret = modal.Secret.from_dict({
-        "GOOGLE_ACCESS_KEY_ID": "GOOG1ERM12345...",
-        "GOOGLE_ACCESS_KEY_SECRET": "HTJ123abcdef...",
-    })
     @stub.function(
         volumes={
-            "/my-mount": modal.CloudBucketMount(
-                bucket_name="my-gcs-bucket",
-                bucket_endpoint_url="https://storage.googleapis.com",
-                secret=gcp_hmac_secret,
-                read_only=True,  # writing to bucket currently unsupported
-            )
+            "/my-mount": modal.CloudBucketMount("s3-bucket-name", secret=modal.Secret.from_dict({
+                "AWS_ACCESS_KEY_ID": "...",
+                "AWS_SECRET_ACCESS_KEY": "...",
+            }), read_only=True)
         }
     )
     def f():
-        subprocess.run(["ls", "/my-mount"], check=True)
+        subprocess.run(["ls", "/my-mount"])
     ```
     """
 
     bucket_name: str
-    # Endpoint URL is used to support Cloudflare R2 and Google Cloud Platform GCS.
-    bucket_endpoint_url: Optional[str] = None
 
-    # Credentials used to access a cloud bucket.
-    # If the bucket is private, the secret **must** contain AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
-    # If the bucket is publicly accessible, the secret is unnecessary and can be omitted.
+    # Credentials used to access a cloud bucket. When
+    # The given secret can contain AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
+    # AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY can be omitted if the bucket is publicly accessible.
     secret: Optional[_Secret] = None
 
     read_only: bool = False
     requester_pays: bool = False
+    bucket_type: Union[
+        BucketType, str
+    ] = BucketType.S3.value  # S3 is the default bucket type until other cloud buckets are supported
 
 
 def cloud_bucket_mounts_to_proto(mounts: List[Tuple[str, _CloudBucketMount]]) -> List[api_pb2.CloudBucketMount]:
     """Helper function to convert `CloudBucketMount` to a list of protobufs that can be passed to the server."""
     cloud_bucket_mounts: List[api_pb2.CloudBucketMount] = []
 
     for path, mount in mounts:
-        # crude mapping from mount arguments to type.
-        if mount.bucket_endpoint_url:
-            parse_result = urlparse(mount.bucket_endpoint_url)
-            if parse_result.hostname.endswith("r2.cloudflarestorage.com"):
-                bucket_type = api_pb2.CloudBucketMount.BucketType.R2
-            elif parse_result.hostname.endswith("storage.googleapis.com"):
-                bucket_type = api_pb2.CloudBucketMount.BucketType.GCP
-                if not mount.read_only:
-                    raise ValueError(
-                        f"CloudBucketMount of '{mount.bucket_name}' is invalid. Writing to GCP buckets with modal.CloudBucketMount in currently unsupported."
-                    )
-            else:
-                raise ValueError(f"Unsupported bucket endpoint hostname '{parse_result.hostname}'")
+        if isinstance(mount.bucket_type, str):
+            bucket_type = BucketType(mount.bucket_type)
         else:
-            # just assume S3; this is backwards and forwards compatible.
-            bucket_type = api_pb2.CloudBucketMount.BucketType.S3
+            bucket_type = mount.bucket_type
 
         if mount.requester_pays and not mount.secret:
             raise ValueError("Credentials required in order to use Requester Pays.")
 
         cloud_bucket_mount = api_pb2.CloudBucketMount(
             bucket_name=mount.bucket_name,
-            bucket_endpoint_url=mount.bucket_endpoint_url,
             mount_path=path,
             credentials_secret_id=mount.secret.object_id if mount.secret else "",
             read_only=mount.read_only,
-            bucket_type=bucket_type,
+            bucket_type=bucket_type.proto,
             requester_pays=mount.requester_pays,
         )
         cloud_bucket_mounts.append(cloud_bucket_mount)
 
     return cloud_bucket_mounts
```

## modal/cloud_bucket_mount.pyi

```diff
@@ -1,19 +1,32 @@
+import enum
 import modal.secret
 import modal_proto.api_pb2
 import typing
 
+class BucketType(enum.Enum):
+    def _generate_next_value_(name, start, count, last_values):
+        ...
+
+    @property
+    def proto(self):
+        ...
+
+    def __new__(cls, value):
+        ...
+
+
 class _CloudBucketMount:
     bucket_name: str
-    bucket_endpoint_url: typing.Union[str, None]
     secret: typing.Union[modal.secret._Secret, None]
     read_only: bool
     requester_pays: bool
+    bucket_type: typing.Union[BucketType, str]
 
-    def __init__(self, bucket_name: str, bucket_endpoint_url: typing.Union[str, None] = None, secret: typing.Union[modal.secret._Secret, None] = None, read_only: bool = False, requester_pays: bool = False) -> None:
+    def __init__(self, bucket_name: str, secret: typing.Union[modal.secret._Secret, None] = None, read_only: bool = False, requester_pays: bool = False, bucket_type: typing.Union[BucketType, str] = 's3') -> None:
         ...
 
     def __repr__(self):
         ...
 
     def __eq__(self, other):
         ...
@@ -21,20 +34,20 @@
 
 def cloud_bucket_mounts_to_proto(mounts: typing.List[typing.Tuple[str, _CloudBucketMount]]) -> typing.List[modal_proto.api_pb2.CloudBucketMount]:
     ...
 
 
 class CloudBucketMount:
     bucket_name: str
-    bucket_endpoint_url: typing.Union[str, None]
     secret: typing.Union[modal.secret.Secret, None]
     read_only: bool
     requester_pays: bool
+    bucket_type: typing.Union[BucketType, str]
 
-    def __init__(self, bucket_name: str, bucket_endpoint_url: typing.Union[str, None] = None, secret: typing.Union[modal.secret.Secret, None] = None, read_only: bool = False, requester_pays: bool = False) -> None:
+    def __init__(self, bucket_name: str, secret: typing.Union[modal.secret.Secret, None] = None, read_only: bool = False, requester_pays: bool = False, bucket_type: typing.Union[BucketType, str] = 's3') -> None:
         ...
 
     def __repr__(self):
         ...
 
     def __eq__(self, other):
         ...
```

## modal/cls.py

```diff
@@ -1,30 +1,29 @@
 # Copyright Modal Labs 2022
 import os
 import typing
-from typing import Any, Callable, Collection, Dict, List, Optional, Tuple, Type, TypeVar, Union
+from typing import Any, Callable, Collection, Dict, List, Optional, Type, TypeVar, Union
 
 from google.protobuf.message import Message
 from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
 from ._output import OutputManager
 from ._resolver import Resolver
-from ._resources import convert_fn_config_to_resources_config
 from ._serialization import check_valid_cls_constructor_arg
 from ._utils.async_utils import synchronize_api, synchronizer
 from ._utils.grpc_utils import retry_transient_errors
 from ._utils.mount_utils import validate_volumes
 from .client import _Client
 from .exception import InvalidError, NotFoundError
 from .functions import (
     _parse_retries,
 )
-from .gpu import GPU_T
+from .gpu import GPU_T, parse_gpu_config
 from .object import _get_environment_name, _Object
 from .partial_function import (
     PartialFunction,
     _find_callables_for_cls,
     _find_partial_methods_for_cls,
     _Function,
     _PartialFunctionFlags,
@@ -33,15 +32,15 @@
 from .secret import _Secret
 from .volume import _Volume
 
 T = TypeVar("T")
 
 
 if typing.TYPE_CHECKING:
-    import modal.app
+    import modal.stub
 
 
 class _Obj:
     """An instance of a `Cls`, i.e. `Cls("foo", 42)` returns an `Obj`.
 
     All this class does is to return `Function` objects."""
 
@@ -134,15 +133,15 @@
 
 class _Cls(_Object, type_prefix="cs"):
     _user_cls: Optional[type]
     _functions: Dict[str, _Function]
     _options: Optional[api_pb2.FunctionOptions]
     _callables: Dict[str, Callable]
     _from_other_workspace: Optional[bool]  # Functions require FunctionBindParams before invocation.
-    _stub: Optional["modal.app._App"] = None  # not set for lookups
+    _stub: Optional["modal.stub._Stub"] = None  # not set for lookups
 
     def _initialize_from_empty(self):
         self._user_cls = None
         self._functions = {}
         self._options = None
         self._callables = {}
         self._from_other_workspace = None
@@ -255,29 +254,28 @@
         cls = cls._from_loader(_load_remote, rep, is_another_app=True)
         cls._from_other_workspace = bool(workspace is not None)
         return cls
 
     def with_options(
         self: "_Cls",
         cpu: Optional[float] = None,
-        memory: Optional[Union[int, Tuple[int, int]]] = None,
+        memory: Optional[int] = None,
         gpu: GPU_T = None,
         secrets: Collection[_Secret] = (),
         volumes: Dict[Union[str, os.PathLike], _Volume] = {},
         retries: Optional[Union[int, Retries]] = None,
         timeout: Optional[int] = None,
         concurrency_limit: Optional[int] = None,
         allow_concurrent_inputs: Optional[int] = None,
         container_idle_timeout: Optional[int] = None,
         allow_background_volume_commits: bool = False,
     ) -> "_Cls":
         """
-        Beta: Allows for the runtime modification of a modal.Cls's configuration.
-
-        This is a beta feature and may be unstable.
+        Allows for the runtime modification of a modal.Cls's configuration.
+        Designed for usage in the [MK1 Flywheel](/docs/guide/mk1).
 
         **Usage:**
 
         ```python notest
         import modal
         Model = modal.Cls.lookup(
             "flywheel-generic", "Model", workspace="mk-1"
@@ -287,15 +285,17 @@
             volumes={"/models": models_vol}
         )
         Model2().generate.remote(42)
         ```
         """
         retry_policy = _parse_retries(retries)
         if gpu or cpu or memory:
-            resources = convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu)
+            milli_cpu = int(1000 * cpu) if cpu is not None else None
+            gpu_config = parse_gpu_config(gpu)
+            resources = api_pb2.Resources(milli_cpu=milli_cpu, gpu_config=gpu_config, memory_mb=memory)
         else:
             resources = None
 
         volume_mounts = [
             api_pb2.VolumeMount(
                 mount_path=path,
                 volume_id=volume.object_id,
```

## modal/cls.pyi

```diff
@@ -1,17 +1,17 @@
 import google.protobuf.message
 import modal._output
-import modal.app
 import modal.client
 import modal.functions
 import modal.gpu
 import modal.object
 import modal.partial_function
 import modal.retries
 import modal.secret
+import modal.stub
 import modal.volume
 import modal_proto.api_pb2
 import os
 import typing
 import typing_extensions
 
 T = typing.TypeVar("T")
@@ -86,15 +86,15 @@
 
 class _Cls(modal.object._Object):
     _user_cls: typing.Union[type, None]
     _functions: typing.Dict[str, modal.functions._Function]
     _options: typing.Union[modal_proto.api_pb2.FunctionOptions, None]
     _callables: typing.Dict[str, typing.Callable]
     _from_other_workspace: typing.Union[bool, None]
-    _stub: typing.Union[modal.app._App, None]
+    _stub: typing.Union[modal.stub._Stub, None]
 
     def _initialize_from_empty(self):
         ...
 
     def _initialize_from_other(self, other: _Cls):
         ...
 
@@ -111,15 +111,15 @@
     def from_local(user_cls, stub, decorator: typing.Callable[[modal.partial_function.PartialFunction, type], modal.functions._Function]) -> _Cls:
         ...
 
     @classmethod
     def from_name(cls: typing.Type[_Cls], app_name: str, tag: typing.Union[str, None] = None, namespace=1, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> _Cls:
         ...
 
-    def with_options(self: _Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret._Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume._Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> _Cls:
+    def with_options(self: _Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret._Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume._Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> _Cls:
         ...
 
     @staticmethod
     async def lookup(app_name: str, tag: typing.Union[str, None] = None, namespace=1, client: typing.Union[modal.client._Client, None] = None, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> _Cls:
         ...
 
     def __call__(self, *args, **kwargs) -> _Obj:
@@ -131,15 +131,15 @@
 
 class Cls(modal.object.Object):
     _user_cls: typing.Union[type, None]
     _functions: typing.Dict[str, modal.functions.Function]
     _options: typing.Union[modal_proto.api_pb2.FunctionOptions, None]
     _callables: typing.Dict[str, typing.Callable]
     _from_other_workspace: typing.Union[bool, None]
-    _stub: typing.Union[modal.app.App, None]
+    _stub: typing.Union[modal.stub.Stub, None]
 
     def __init__(self, *args, **kwargs):
         ...
 
     def _initialize_from_empty(self):
         ...
 
@@ -159,15 +159,15 @@
     def from_local(user_cls, stub, decorator: typing.Callable[[modal.partial_function.PartialFunction, type], modal.functions.Function]) -> Cls:
         ...
 
     @classmethod
     def from_name(cls: typing.Type[Cls], app_name: str, tag: typing.Union[str, None] = None, namespace=1, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> Cls:
         ...
 
-    def with_options(self: Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret.Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume.Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> Cls:
+    def with_options(self: Cls, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, secrets: typing.Collection[modal.secret.Secret] = (), volumes: typing.Dict[typing.Union[str, os.PathLike], modal.volume.Volume] = {}, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, allow_background_volume_commits: bool = False) -> Cls:
         ...
 
     class __lookup_spec(typing_extensions.Protocol):
         def __call__(self, app_name: str, tag: typing.Union[str, None] = None, namespace=1, client: typing.Union[modal.client.Client, None] = None, environment_name: typing.Union[str, None] = None, workspace: typing.Union[str, None] = None) -> Cls:
             ...
 
         async def aio(self, *args, **kwargs) -> Cls:
```

## modal/config.py

```diff
@@ -169,18 +169,14 @@
 
 
 _profile = os.environ.get("MODAL_PROFILE") or _config_active_profile()
 
 # Define settings
 
 
-def _to_boolean(x: object) -> bool:
-    return str(x).lower() not in {"", "0", "false"}
-
-
 class _Setting(typing.NamedTuple):
     default: typing.Any = None
     transform: typing.Callable[[str], typing.Any] = lambda x: x  # noqa: E731
 
 
 _SETTINGS = {
     "loglevel": _Setting("WARNING", lambda s: s.upper()),
@@ -190,26 +186,25 @@
     "token_secret": _Setting(),
     "task_id": _Setting(),
     "task_secret": _Setting(),
     "serve_timeout": _Setting(transform=float),
     "sync_entrypoint": _Setting(),
     "logs_timeout": _Setting(10, float),
     "image_id": _Setting(),
-    "automount": _Setting(True, transform=_to_boolean),
-    "profiling_enabled": _Setting(False, transform=_to_boolean),
+    "automount": _Setting(True, transform=lambda x: x not in ("", "0")),
+    "profiling_enabled": _Setting(False, transform=lambda x: x not in ("", "0")),
     "heartbeat_interval": _Setting(15, float),
     "function_runtime": _Setting(),
-    "function_runtime_debug": _Setting(False, transform=_to_boolean),  # For internal debugging use.
+    "function_runtime_debug": _Setting(False, transform=lambda x: x not in ("", "0")),  # For internal debugging use.
     "environment": _Setting(),
     "default_cloud": _Setting(None, transform=lambda x: x if x else None),
     "worker_id": _Setting(),  # For internal debugging use.
     "restore_state_path": _Setting("/__modal/restore-state.json"),
-    "force_build": _Setting(False, transform=_to_boolean),
-    "traceback": _Setting(False, transform=_to_boolean),
-    "image_builder_version": _Setting(),
+    "force_build": _Setting(False, transform=lambda x: x not in ("", "0")),
+    "traceback": _Setting(False, transform=lambda x: x not in ("", "0")),
 }
 
 
 class Config:
     """Singleton that holds configuration used by Modal internally."""
 
     def __init__(self):
```

## modal/dict.py

```diff
@@ -18,46 +18,40 @@
 def _serialize_dict(data):
     return [api_pb2.DictEntry(key=serialize(k), value=serialize(v)) for k, v in data.items()]
 
 
 class _Dict(_Object, type_prefix="di"):
     """Distributed dictionary for storage in Modal apps.
 
-    Keys and values can be essentially any object, so long as they can be serialized by
-    `cloudpickle`, which includes other Modal objects.
+    Keys and values can be essentially any object, so long as they can be
+    serialized by `cloudpickle`, including Modal objects.
 
     **Lifetime of a Dict and its items**
 
     An individual dict entry will expire 30 days after it was last added to its Dict object.
-    Additionally, data are stored in memory on the Modal server and could be lost due to
-    unexpected server restarts. Because of this, `Dict` is best suited for storing short-term
-    state and is not recommended for durable storage.
+    Because of this, `Dict`s are best not used for
+    long-term storage. All data is deleted when the app is stopped.
 
     **Usage**
 
     ```python
-    import modal
+    from modal import Dict, Stub
 
-    stub = modal.Stub()
-    my_dict = modal.Dict.from_name("my-persisted_dict", create_if_missing=True)
+    stub = Stub()
+    my_dict = Dict.from_name("my-persisted_dict", create_if_missing=True)
 
     @stub.local_entrypoint()
     def main():
         my_dict["some key"] = "some value"
         my_dict[123] = 456
 
         assert my_dict["some key"] == "some value"
         assert my_dict[123] == 456
     ```
 
-    The `Dict` class offers a few methods for operations that are usually accomplished
-    in Python with operators, such as `Dict.put` and `Dict.contains`. The advantage of
-    these methods is that they can be safely called in an asynchronous context, whereas
-    their operator-based analogues will block the event loop.
-
     For more examples, see the [guide](/docs/guide/dicts-and-queues#modal-dicts).
     """
 
     @staticmethod
     def new(data: Optional[dict] = None) -> "_Dict":
         """`Dict.new` is deprecated.
 
@@ -146,15 +140,15 @@
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
                 data=serialized,
             )
             response = await resolver.client.stub.DictGetOrCreate(req)
             logger.debug(f"Created dict with id {response.dict_id}")
             self._hydrate(response.dict_id, resolver.client, None)
 
-        return _Dict._from_loader(_load, "Dict()", is_another_app=True, hydrate_lazily=True)
+        return _Dict._from_loader(_load, "Dict()", is_another_app=True)
 
     @staticmethod
     def persisted(
         label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None
     ) -> "_Dict":
         """Deprecated! Use `Dict.from_name(name, create_if_missing=True)`."""
         deprecation_warning((2024, 3, 1), _Dict.persisted.__doc__)
@@ -221,15 +215,15 @@
         resp = await retry_transient_errors(self._client.stub.DictLen, req)
         return resp.len
 
     @live_method
     async def __getitem__(self, key: Any) -> Any:
         """Get the value associated with a key.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         NOT_FOUND = object()
         value = await self.get(key, NOT_FOUND)
         if value is NOT_FOUND:
             raise KeyError(f"{key} not in dict {self.object_id}")
 
         return value
@@ -249,15 +243,15 @@
         req = api_pb2.DictUpdateRequest(dict_id=self.object_id, updates=serialized)
         await retry_transient_errors(self._client.stub.DictUpdate, req)
 
     @live_method
     async def __setitem__(self, key: Any, value: Any) -> None:
         """Set a specific key-value pair to the dictionary.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         return await self.put(key, value)
 
     @live_method
     async def pop(self, key: Any) -> Any:
         """Remove a key from the dictionary, returning the value if it exists."""
         req = api_pb2.DictPopRequest(dict_id=self.object_id, key=serialize(key))
@@ -266,21 +260,21 @@
             raise KeyError(f"{key} not in dict {self.object_id}")
         return deserialize(resp.value, self._client)
 
     @live_method
     async def __delitem__(self, key: Any) -> Any:
         """Delete a key from the dictionary.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         return await self.pop(key)
 
     @live_method
     async def __contains__(self, key: Any) -> bool:
         """Return if a key is present.
 
-        Note: this function will block the event loop when called in an async context.
+        This function only works in a synchronous context.
         """
         return await self.contains(key)
 
 
 Dict = synchronize_api(_Dict)
```

## modal/exception.py

```diff
@@ -117,15 +117,15 @@
 
 
 def deprecation_error(deprecated_on: Tuple[int, int, int], msg: str):
     raise DeprecationError(f"Deprecated on {date(*deprecated_on)}: {msg}")
 
 
 def deprecation_warning(
-    deprecated_on: Tuple[int, int, int], msg: str, *, pending: bool = False, show_source: bool = True
+    deprecated_on: Tuple[int, int, int], msg: str, pending: bool = False, show_source: bool = True
 ) -> None:
     """Utility for getting the proper stack entry.
 
     See the implementation of the built-in [warnings.warn](https://docs.python.org/3/library/warnings.html#available-functions).
     """
     filename, lineno = "<unknown>", 0
     if show_source:
```

## modal/experimental.py

```diff
@@ -1,9 +1,10 @@
 # Copyright Modal Labs 2022
-from ._container_io_manager import _ContainerIOManager
 
 
 def stop_fetching_inputs():
     """Don't fetch any more inputs from the server, after the current one.
     The container will exit gracefully after the current input is processed."""
 
-    _ContainerIOManager.stop_fetching_inputs()
+    from .app import _container_app
+
+    _container_app.stop_fetching_inputs()
```

## modal/functions.py

```diff
@@ -1,67 +1,63 @@
 # Copyright Modal Labs 2023
 import asyncio
 import inspect
 import time
-import typing
 import warnings
 from contextvars import ContextVar
 from dataclasses import dataclass
 from pathlib import PurePosixPath
 from typing import (
     TYPE_CHECKING,
     Any,
     AsyncGenerator,
     AsyncIterable,
     AsyncIterator,
     Callable,
     Collection,
     Dict,
-    Iterable,
     List,
+    Literal,
     Optional,
     Sequence,
     Set,
     Sized,
     Tuple,
     Type,
     Union,
 )
 
 from aiostream import pipe, stream
 from google.protobuf.message import Message
 from grpclib import GRPCError, Status
-from synchronicity.combined_types import MethodWithAio
+from grpclib.exceptions import StreamTerminatedError
 from synchronicity.exceptions import UserCodeException
 
+from modal import _pty, is_local
 from modal_proto import api_grpc, api_pb2
 
-from ._container_io_manager import is_local
 from ._location import parse_cloud_provider
 from ._output import OutputManager
-from ._pty import get_pty_info
 from ._resolver import Resolver
-from ._resources import convert_fn_config_to_resources_config
 from ._serialization import deserialize, deserialize_data_format, serialize
 from ._traceback import append_modal_tb
 from ._utils.async_utils import (
-    AsyncOrSyncIteratable,
     queue_batch_iterator,
     synchronize_api,
     synchronizer,
     warn_if_generator_is_not_consumed,
 )
 from ._utils.blob_utils import (
     BLOB_MAX_PARALLELISM,
     MAX_OBJECT_SIZE_BYTES,
     blob_download,
     blob_upload,
 )
-from ._utils.function_utils import FunctionInfo, _stream_function_call_data, get_referred_objects, is_async
-from ._utils.grpc_utils import retry_transient_errors
+from ._utils.function_utils import FunctionInfo, get_referred_objects, is_async
+from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors, unary_stream
 from ._utils.mount_utils import validate_mount_points, validate_volumes
 from .call_graph import InputInfo, _reconstruct_call_graph
 from .client import _Client
 from .cloud_bucket_mount import _CloudBucketMount, cloud_bucket_mounts_to_proto
 from .config import config, logger
 from .exception import (
     ExecutionError,
@@ -84,36 +80,15 @@
 from .volume import _Volume
 
 OUTPUTS_TIMEOUT = 55.0  # seconds
 ATTEMPT_TIMEOUT_GRACE_PERIOD = 5  # seconds
 
 
 if TYPE_CHECKING:
-    import modal.app
-
-
-class _SynchronizedQueue:
-    """mdmd:hidden"""
-
-    # small wrapper around asyncio.Queue to make it cross-thread compatible through synchronicity
-    async def init(self):
-        # in Python 3.8 the asyncio.Queue is bound to the event loop on creation
-        # so it needs to be created in a synchronicity-wrapped init method
-        self.q = asyncio.Queue()
-
-    @synchronizer.no_io_translation
-    async def put(self, item):
-        await self.q.put(item)
-
-    @synchronizer.no_io_translation
-    async def get(self):
-        return await self.q.get()
-
-
-SynchronizedQueue = synchronize_api(_SynchronizedQueue)
+    import modal.stub
 
 
 def exc_with_hints(exc: BaseException):
     """mdmd:hidden"""
     if isinstance(exc, ImportError) and exc.msg == "attempted relative import with no known parent package":
         exc.msg += """\n
 HINT: For relative imports to work, you might need to run your modal app as a module. Try:
@@ -200,14 +175,53 @@
     else:
         return api_pb2.FunctionPutInputsItem(
             input=api_pb2.FunctionInput(args=args_serialized, data_format=api_pb2.DATA_FORMAT_PICKLE),
             idx=idx,
         )
 
 
+async def _stream_function_call_data(
+    client, function_call_id: str, variant: Literal["data_in", "data_out"]
+) -> AsyncIterator[Any]:
+    """Read from the `data_in` or `data_out` stream of a function call."""
+    last_index = 0
+    retries_remaining = 10
+
+    if variant == "data_in":
+        stub_fn = client.stub.FunctionCallGetDataIn
+    elif variant == "data_out":
+        stub_fn = client.stub.FunctionCallGetDataOut
+    else:
+        raise ValueError(f"Invalid variant {variant}")
+
+    while True:
+        req = api_pb2.FunctionCallGetDataRequest(function_call_id=function_call_id, last_index=last_index)
+        try:
+            async for chunk in unary_stream(stub_fn, req):
+                if chunk.index <= last_index:
+                    continue
+                last_index = chunk.index
+                if chunk.data_blob_id:
+                    message_bytes = await blob_download(chunk.data_blob_id, client.stub)
+                else:
+                    message_bytes = chunk.data
+                message = deserialize_data_format(message_bytes, chunk.data_format, client)
+                yield message
+        except (GRPCError, StreamTerminatedError) as exc:
+            if retries_remaining > 0:
+                retries_remaining -= 1
+                if isinstance(exc, GRPCError):
+                    if exc.status in RETRYABLE_GRPC_STATUS_CODES:
+                        await asyncio.sleep(1.0)
+                        continue
+                elif isinstance(exc, StreamTerminatedError):
+                    continue
+            raise
+
+
 @dataclass
 class _OutputValue:
     # box class for distinguishing None results from non-existing/None markers
     value: Any
 
 
 class _Invocation:
@@ -323,15 +337,16 @@
 
 
 MAP_INVOCATION_CHUNK_SIZE = 49
 
 
 async def _map_invocation(
     function_id: str,
-    raw_input_queue: _SynchronizedQueue,
+    input_stream: AsyncIterable[Any],
+    kwargs: Dict[str, Any],
     client: _Client,
     order_outputs: bool,
     return_exceptions: bool,
     count_update_callback: Optional[Callable[[int, int], None]],
 ):
     assert client.stub
     request = api_pb2.FunctionMapRequest(
@@ -343,71 +358,58 @@
     response = await retry_transient_errors(client.stub.FunctionMap, request)
 
     function_call_id = response.function_call_id
 
     have_all_inputs = False
     num_inputs = 0
     num_outputs = 0
-
-    def count_update():
-        if count_update_callback is not None:
-            count_update_callback(num_outputs, num_inputs)
-
     pending_outputs: Dict[str, int] = {}  # Map input_id -> next expected gen_index value
     completed_outputs: Set[str] = set()  # Set of input_ids whose outputs are complete (expecting no more values)
 
     input_queue: asyncio.Queue = asyncio.Queue()
 
-    async def create_input(argskwargs):
+    async def create_input(arg: Any) -> api_pb2.FunctionPutInputsItem:
         nonlocal num_inputs
         idx = num_inputs
         num_inputs += 1
-        (args, kwargs) = argskwargs
-        return await _create_input(args, kwargs, client, idx)
-
-    async def input_iter():
-        while 1:
-            raw_input = await raw_input_queue.get()
-            if raw_input is None:  # end of input sentinel
-                return
-            yield raw_input  # args, kwargs
+        item = await _create_input(arg, kwargs, client, idx=idx)
+        return item
 
     async def drain_input_generator():
         # Parallelize uploading blobs
-        proto_input_stream = stream.iterate(input_iter()) | pipe.map(
+        proto_input_stream = stream.iterate(input_stream) | pipe.map(
             create_input,  # type: ignore[reportArgumentType]
             ordered=True,
             task_limit=BLOB_MAX_PARALLELISM,
         )
         async with proto_input_stream.stream() as streamer:
             async for item in streamer:
                 await input_queue.put(item)
 
         # close queue iterator
         await input_queue.put(None)
         yield
 
     async def pump_inputs():
         assert client.stub
-        nonlocal have_all_inputs, num_inputs
+        nonlocal have_all_inputs
         async for items in queue_batch_iterator(input_queue, MAP_INVOCATION_CHUNK_SIZE):
             request = api_pb2.FunctionPutInputsRequest(
                 function_id=function_id, inputs=items, function_call_id=function_call_id
             )
             logger.debug(
                 f"Pushing {len(items)} inputs to server. Num queued inputs awaiting push is {input_queue.qsize()}."
             )
             resp = await retry_transient_errors(
                 client.stub.FunctionPutInputs,
                 request,
                 max_retries=None,
                 max_delay=10,
                 additional_status_codes=[Status.RESOURCE_EXHAUSTED],
             )
-            count_update()
             for item in resp.inputs:
                 pending_outputs.setdefault(item.input_id, 0)
             logger.debug(
                 f"Successfully pushed {len(items)} inputs to server. Num queued inputs awaiting push is {input_queue.qsize()}."
             )
 
         have_all_inputs = True
@@ -476,15 +478,16 @@
 
         # map to store out-of-order outputs received
         received_outputs = {}
         output_idx = 0
 
         async with outputs_fetched.stream() as streamer:
             async for idx, output in streamer:
-                count_update()
+                if count_update_callback is not None:
+                    count_update_callback(num_outputs, num_inputs)
                 if not order_outputs:
                     yield _OutputValue(output)
                 else:
                     # hold on to outputs for function maps, so we can reorder them correctly.
                     received_outputs[idx] = output
                     while output_idx in received_outputs:
                         output = received_outputs.pop(output_idx)
@@ -529,49 +532,48 @@
         err_object = f"Function {raw_f}" if raw_f else "Function"
         raise InvalidError(
             f"{err_object} retries must be an integer or instance of modal.Retries. Found: {type(retries)}"
         )
 
 
 @dataclass
-class _FunctionSpec:
+class FunctionEnv:
     """
-    Stores information about a Function specification.
-    This is used for `modal shell` to support running shells with
-    the same configuration as a user-defined Function.
+    Stores information about the function environment. This is used for `modal shell` to support
+    running shells in the same environment as a user-defined function.
     """
 
     image: Optional[_Image]
     mounts: Sequence[_Mount]
     secrets: Sequence[_Secret]
     network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem]
     volumes: Dict[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]]
     gpu: GPU_T
     cloud: Optional[str]
     cpu: Optional[float]
-    memory: Optional[Union[int, Tuple[int, int]]]
+    memory: Optional[int]
 
 
 class _Function(_Object, type_prefix="fu"):
     """Functions are the basic units of serverless execution on Modal.
 
     Generally, you will not construct a `Function` directly. Instead, use the
     `@stub.function()` decorator on the `Stub` object for your application.
     """
 
     # TODO: more type annotations
     _info: Optional[FunctionInfo]
     _all_mounts: Collection[_Mount]
-    _stub: "modal.app._App"
+    _stub: "modal.stub._Stub"
     _obj: Any
     _web_url: Optional[str]
     _is_remote_cls_method: bool = False  # TODO(erikbern): deprecated
     _function_name: Optional[str]
     _is_method: bool
-    _spec: _FunctionSpec
+    _env: FunctionEnv
     _tag: str
     _raw_f: Callable[..., Any]
     _build_args: dict
     _parent: "_Function"
 
     @staticmethod
     def from_args(
@@ -585,15 +587,15 @@
         gpu: GPU_T = None,
         # TODO: maybe break this out into a separate decorator for notebooks.
         mounts: Collection[_Mount] = (),
         network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
         allow_cross_region_volumes: bool = False,
         volumes: Dict[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]] = {},
         webhook_config: Optional[api_pb2.WebhookConfig] = None,
-        memory: Optional[Union[int, Tuple[int, int]]] = None,
+        memory: Optional[int] = None,
         proxy: Optional[_Proxy] = None,
         retries: Optional[Union[int, Retries]] = None,
         timeout: Optional[int] = None,
         concurrency_limit: Optional[int] = None,
         allow_concurrent_inputs: Optional[int] = None,
         container_idle_timeout: Optional[int] = None,
         cpu: Optional[float] = None,
@@ -659,15 +661,15 @@
         gpu_config = parse_gpu_config(gpu)
 
         if proxy:
             # HACK: remove this once we stop using ssh tunnels for this.
             if image:
                 image = image.apt_install("autossh")
 
-        function_spec = _FunctionSpec(
+        function_env = FunctionEnv(
             mounts=all_mounts,
             secrets=secrets,
             gpu=gpu,
             network_file_systems=network_file_systems,
             volumes=volumes,
             image=image,
             cloud=cloud,
@@ -796,18 +798,22 @@
             status_row.message(f"Creating {tag}...")
 
             if is_generator:
                 function_type = api_pb2.Function.FUNCTION_TYPE_GENERATOR
             else:
                 function_type = api_pb2.Function.FUNCTION_TYPE_FUNCTION
 
+            if cpu is not None and cpu < 0.25:
+                raise InvalidError(f"Invalid fractional CPU value {cpu}. Cannot have less than 0.25 CPU resources.")
+            milli_cpu = int(1000 * cpu) if cpu is not None else 0
+
             timeout_secs = timeout
 
             if stub and stub.is_interactive and not is_builder_function:
-                pty_info = get_pty_info(shell=False)
+                pty_info = _pty.get_pty_info(shell=False)
             else:
                 pty_info = None
 
             if info.is_serialized():
                 # Use cloudpickle. Used when working w/ Jupyter notebooks.
                 # serialize at _load time, not function decoration time
                 # otherwise we can't capture a surrounding class for lifetime methods etc.
@@ -861,15 +867,15 @@
                 mount_ids=loaded_mount_ids,
                 secret_ids=[secret.object_id for secret in secrets],
                 image_id=(image.object_id if image else ""),
                 definition_type=info.definition_type,
                 function_serialized=function_serialized or b"",
                 class_serialized=class_serialized or b"",
                 function_type=function_type,
-                resources=convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu),
+                resources=api_pb2.Resources(milli_cpu=milli_cpu, gpu_config=gpu_config, memory_mb=memory or 0),
                 webhook_config=webhook_config,
                 shared_volume_mounts=network_file_system_mount_protos(
                     validated_network_file_systems, allow_cross_region_volumes
                 ),
                 volume_mounts=volume_mounts,
                 proxy_id=(proxy.object_id if proxy else None),
                 retry_policy=retry_policy,
@@ -950,15 +956,15 @@
         obj._info = info
         obj._tag = tag
         obj._all_mounts = all_mounts  # needed for modal.serve file watching
         obj._stub = stub  # needed for CLI right now
         obj._obj = None
         obj._is_generator = is_generator
         obj._is_method = bool(info.cls)
-        obj._spec = function_spec  # needed for modal shell
+        obj._env = function_env  # needed for modal shell
 
         # Used to check whether we should rebuild an image using run_function
         # Plaintext source and arg definition for the function, so it's part of the image
         # hash. We can't use the cloudpickle hash because it's not very stable.
         obj._build_args = dict(  # See get_build_def
             secrets=repr(secrets),
             gpu_config=repr(gpu_config),
@@ -1094,28 +1100,28 @@
     @property
     def tag(self) -> str:
         """mdmd:hidden"""
         assert self._tag
         return self._tag
 
     @property
-    def stub(self) -> "modal.app._App":
+    def stub(self) -> "modal.stub._Stub":
         """mdmd:hidden"""
         return self._stub
 
     @property
     def info(self) -> FunctionInfo:
         """mdmd:hidden"""
         assert self._info
         return self._info
 
     @property
-    def spec(self) -> _FunctionSpec:
+    def env(self) -> FunctionEnv:
         """mdmd:hidden"""
-        return self._spec
+        return self._env
 
     def get_build_def(self) -> str:
         """mdmd:hidden"""
         assert hasattr(self, "_raw_f") and hasattr(self, "_build_args")
         return f"{inspect.getsource(self._raw_f)}\n{repr(self._build_args)}"
 
     # Live handle methods
@@ -1170,26 +1176,15 @@
 
     @property
     def is_generator(self) -> bool:
         """mdmd:hidden"""
         assert self._is_generator is not None
         return self._is_generator
 
-    @live_method_gen
-    async def _map(
-        self, input_queue: _SynchronizedQueue, order_outputs: bool, return_exceptions: bool
-    ) -> AsyncGenerator[Any, None]:
-        """mdmd:hidden
-
-        Synchronicity-wrapped map implementation. To be safe against invocations of user code in the synchronicity thread
-        it doesn't accept an [async]iterator, and instead takes a _SynchronizedQueue instance.
-
-        _SynchronizedQueue is used instead of asyncio.Queue so that the main thread can put
-        items in the queue safely.
-        """
+    async def _map(self, input_stream: AsyncIterable[Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
         if self._web_url:
             raise InvalidError(
                 "A web endpoint function cannot be directly invoked for parallel remote execution. "
                 f"Invoke this function via its web url '{self._web_url}' or call it locally: {self._function_name}()."
             )
         if self._is_generator:
             raise InvalidError("A generator function cannot be called with `.map(...)`.")
@@ -1197,15 +1192,16 @@
         assert self._function_name
         count_update_callback = (
             self._output_mgr.function_progress_callback(self._function_name, total=None) if self._output_mgr else None
         )
 
         async for item in _map_invocation(
             self.object_id,
-            input_queue,
+            input_stream,
+            kwargs,
             self._client,
             order_outputs,
             return_exceptions,
             count_update_callback,
         ):
             yield item
 
@@ -1217,38 +1213,36 @@
             # this can happen if the user terminates a program, triggering a cancellation cascade
             if not self._mute_cancellation:
                 raise
 
     async def _call_function_nowait(self, args, kwargs) -> _Invocation:
         return await _Invocation.create(self.object_id, args, kwargs, self._client)
 
-    @warn_if_generator_is_not_consumed()
+    @warn_if_generator_is_not_consumed
     @live_method_gen
     @synchronizer.no_input_translation
     async def _call_generator(self, args, kwargs):
         invocation = await _Invocation.create(self.object_id, args, kwargs, self._client)
         async for res in invocation.run_generator():
             yield res
 
     @synchronizer.no_io_translation
     async def _call_generator_nowait(self, args, kwargs):
         return await _Invocation.create(self.object_id, args, kwargs, self._client)
 
-    # note that `map()` is not synchronicity-wrapped, since it accepts executable code in the form of
-    # iterators that we don't want to run inside the synchronicity thread. We delegate to `._map()` with
-    # a safer Queue as input
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.map")
-    def _map_sync(
+    @warn_if_generator_is_not_consumed
+    @live_method_gen
+    @synchronizer.no_input_translation
+    async def map(
         self,
-        *input_iterators: Iterable[Any],  # one input iterator per argument in the mapped-over function/generator
+        *input_iterators,  # one input iterator per argument in the mapped-over function/generator
         kwargs={},  # any extra keyword arguments for the function
         order_outputs: bool = True,  # return outputs in order
-        return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
-    ) -> AsyncOrSyncIteratable:
+        return_exceptions: bool = False,  # propogate exceptions (False) or aggregate them in the results list (True)
+    ) -> AsyncGenerator[Any, None]:
         """Parallel map over a set of inputs.
 
         Takes one iterator argument per argument in the function being mapped over.
 
         Example:
         ```python
         @stub.function()
@@ -1278,113 +1272,38 @@
         @stub.local_entrypoint()
         def main():
             # [0, 1, UserCodeException(Exception('ohno'))]
             print(list(my_func.map(range(3), return_exceptions=True)))
         ```
         """
 
-        return AsyncOrSyncIteratable(
-            self._map_async(
-                *input_iterators, kwargs=kwargs, order_outputs=order_outputs, return_exceptions=return_exceptions
-            ),
-            nested_async_message="You can't iter(Function.map()) or Function.for_each() from an async function. Use async for ... Function.map.aio() or Function.for_each.aio() instead.",
-        )
-
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.map.aio")
-    async def _map_async(
-        self,
-        *input_iterators: Union[
-            Iterable[Any], AsyncIterable[Any]
-        ],  # one input iterator per argument in the mapped-over function/generator
-        kwargs={},  # any extra keyword arguments for the function
-        order_outputs: bool = True,  # return outputs in order
-        return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
-    ) -> AsyncGenerator[Any, None]:
-        """mdmd:hidden
-        This runs in an event loop on the main thread
-
-        It concurrently feeds new input to the input queue and yields available outputs
-        to the caller.
-        Note that since the iterator(s) can block, it's a bit opaque how often the event
-        loop decides to get a new input vs how often it will emit a new output.
-        We could make this explicit as an improvement or even let users decide what they
-        prefer: throughput (prioritize queueing inputs) or latency (prioritize yielding results)
-        """
-        raw_input_queue: Any = SynchronizedQueue()  # type: ignore
-        raw_input_queue.init()
-
-        async def feed_queue():
-            # This runs in a main thread event loop, so it doesn't block the synchronizer loop
-            async with stream.zip(*[stream.iterate(it) for it in input_iterators]).stream() as streamer:
-                async for args in streamer:
-                    await raw_input_queue.put.aio((args, kwargs))
-            await raw_input_queue.put.aio(None)  # end-of-input sentinel
-
-        feed_input_task = asyncio.create_task(feed_queue())
-
-        try:
-            async for output in self._map.aio(raw_input_queue, order_outputs, return_exceptions):  # type: ignore[reportFunctionMemberAccess]
-                yield output
-        finally:
-            feed_input_task.cancel()  # should only be needed in case of exceptions
+        input_stream = stream.zip(*(stream.iterate(it) for it in input_iterators))
+        async for item in self._map(input_stream, order_outputs, return_exceptions, kwargs):
+            yield item
 
-    def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
+    @synchronizer.no_input_translation
+    async def for_each(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
         """Execute function for all inputs, ignoring outputs.
 
         Convenient alias for `.map()` in cases where the function just needs to be called.
         as the caller doesn't have to consume the generator to process the inputs.
         """
         # TODO(erikbern): it would be better if this is more like a map_spawn that immediately exits
         # rather than iterating over the result
-        for _ in self.map(*input_iterators, kwargs=kwargs, order_outputs=False, return_exceptions=ignore_exceptions):
-            pass
-
-    @synchronizer.nowrap
-    async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        async for _ in self.map.aio(  # type: ignore
+        async for _ in self.map(
             *input_iterators, kwargs=kwargs, order_outputs=False, return_exceptions=ignore_exceptions
         ):
             pass
 
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.starmap")
-    async def _starmap_async(
-        self,
-        input_iterator: Union[Iterable[Sequence[Any]], AsyncIterable[Sequence[Any]]],
-        kwargs={},
-        order_outputs: bool = True,
-        return_exceptions: bool = False,
-    ) -> typing.AsyncIterable[Any]:
-        raw_input_queue: Any = SynchronizedQueue()  # type: ignore
-        raw_input_queue.init()
-
-        async def feed_queue():
-            # This runs in a main thread event loop, so it doesn't block the synchronizer loop
-            async with stream.iterate(input_iterator).stream() as streamer:
-                async for args in streamer:
-                    await raw_input_queue.put.aio((args, kwargs))
-            await raw_input_queue.put.aio(None)  # end-of-input sentinel
-
-        feed_input_task = asyncio.create_task(feed_queue())
-        try:
-            async for output in self._map.aio(raw_input_queue, order_outputs, return_exceptions):  # type: ignore[reportFunctionMemberAccess]
-                yield output
-        finally:
-            feed_input_task.cancel()  # should only be needed in case of exceptions
-
-    @synchronizer.nowrap
-    @warn_if_generator_is_not_consumed(function_name="Function.starmap.aio")
-    def _starmap_sync(
-        self,
-        input_iterator: Iterable[Sequence[Any]],
-        kwargs={},
-        order_outputs: bool = True,
-        return_exceptions: bool = False,
-    ) -> AsyncOrSyncIteratable:
+    @warn_if_generator_is_not_consumed
+    @live_method_gen
+    @synchronizer.no_input_translation
+    async def starmap(
+        self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False
+    ) -> AsyncGenerator[Any, None]:
         """Like `map`, but spreads arguments over multiple function arguments.
 
         Assumes every input is a sequence (e.g. a tuple).
 
         Example:
         ```python
         @stub.function()
@@ -1393,20 +1312,17 @@
 
 
         @stub.local_entrypoint()
         def main():
             assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]
         ```
         """
-        return AsyncOrSyncIteratable(
-            self._starmap_async(
-                input_iterator, kwargs=kwargs, order_outputs=order_outputs, return_exceptions=return_exceptions
-            ),
-            nested_async_message="You can't run Function.map() or Function.for_each() from an async function. Use Function.map.aio()/Function.for_each.aio() instead.",
-        )
+        input_stream = stream.iterate(input_iterator)
+        async for item in self._map(input_stream, order_outputs, return_exceptions, kwargs):
+            yield item
 
     @synchronizer.no_io_translation
     @live_method
     async def remote(self, *args, **kwargs) -> Any:
         """
         Calls the function remotely, executing it with the given arguments and returning the execution's result.
         """
@@ -1466,17 +1382,15 @@
         else:
             return self._obj
 
     @synchronizer.nowrap
     def local(self, *args, **kwargs) -> Any:
         """
         Calls the function locally, executing it with the given arguments and returning the execution's result.
-
-        The function will execute in the same environment as the caller, just like calling the underlying function
-        directly in Python. In particular, secrets will not be available through environment variables.
+        This method allows a caller to execute the standard Python function wrapped by Modal.
         """
         # TODO(erikbern): it would be nice to remove the nowrap thing, but right now that would cause
         # "user code" to run on the synchronicity thread, which seems bad
         info = self._get_info()
         if not info:
             msg = (
                 "The definition for this function is missing so it is not possible to invoke it locally. "
@@ -1537,22 +1451,14 @@
         resp = await self._client.stub.FunctionGetCurrentStats(
             api_pb2.FunctionGetCurrentStatsRequest(function_id=self.object_id)
         )
         return FunctionStats(
             backlog=resp.backlog, num_active_runners=resp.num_active_tasks, num_total_runners=resp.num_total_tasks
         )
 
-    # A bit hacky - but the map-style functions need to not be synchronicity-wrapped
-    # in order to not execute their input iterators on the synchronicity event loop.
-    # We still need to wrap them using MethodWithAio to maintain a synchronicity-like
-    # api with `.aio` and get working type-stubs and reference docs generation:
-    map = MethodWithAio(_map_sync, _map_async, synchronizer)
-    starmap = MethodWithAio(_starmap_sync, _starmap_async, synchronizer)
-    for_each = MethodWithAio(_for_each_sync, _for_each_async, synchronizer)
-
 
 Function = synchronize_api(_Function)
 
 
 class _FunctionCall(_Object, type_prefix="fc"):
     """A reference to an executed function call.
```

## modal/functions.pyi

```diff
@@ -1,88 +1,48 @@
 import _contextvars
 import google.protobuf.message
 import modal._output
-import modal._utils.async_utils
 import modal._utils.function_utils
-import modal.app
 import modal.call_graph
 import modal.client
 import modal.cloud_bucket_mount
 import modal.gpu
 import modal.image
 import modal.mount
 import modal.network_file_system
 import modal.object
 import modal.proxy
 import modal.retries
 import modal.schedule
 import modal.scheduler_placement
 import modal.secret
+import modal.stub
 import modal.volume
 import modal_proto.api_grpc
 import modal_proto.api_pb2
 import pathlib
 import typing
 import typing_extensions
 
-class _SynchronizedQueue:
-    async def init(self):
-        ...
-
-    async def put(self, item):
-        ...
-
-    async def get(self):
-        ...
-
-
-class SynchronizedQueue:
-    def __init__(self, /, *args, **kwargs):
-        ...
-
-    class __init_spec(typing_extensions.Protocol):
-        def __call__(self):
-            ...
-
-        async def aio(self, *args, **kwargs):
-            ...
-
-    init: __init_spec
-
-    class __put_spec(typing_extensions.Protocol):
-        def __call__(self, item):
-            ...
-
-        async def aio(self, *args, **kwargs):
-            ...
-
-    put: __put_spec
-
-    class __get_spec(typing_extensions.Protocol):
-        def __call__(self):
-            ...
-
-        async def aio(self, *args, **kwargs):
-            ...
-
-    get: __get_spec
-
-
 def exc_with_hints(exc: BaseException):
     ...
 
 
 async def _process_result(result: modal_proto.api_pb2.GenericResult, data_format: int, stub, client=None):
     ...
 
 
 async def _create_input(args, kwargs, client, idx: typing.Union[int, None] = None) -> modal_proto.api_pb2.FunctionPutInputsItem:
     ...
 
 
+def _stream_function_call_data(client, function_call_id: str, variant: typing.Literal['data_in', 'data_out']) -> typing.AsyncIterator[typing.Any]:
+    ...
+
+
 class _OutputValue:
     value: typing.Any
 
     def __init__(self, value: typing.Any) -> None:
         ...
 
     def __repr__(self):
@@ -109,15 +69,15 @@
     async def poll_function(self, timeout: typing.Union[float, None] = None):
         ...
 
     def run_generator(self):
         ...
 
 
-def _map_invocation(function_id: str, raw_input_queue: _SynchronizedQueue, client: modal.client._Client, order_outputs: bool, return_exceptions: bool, count_update_callback: typing.Union[typing.Callable[[int, int], None], None]):
+def _map_invocation(function_id: str, input_stream: typing.AsyncIterable[typing.Any], kwargs: typing.Dict[str, typing.Any], client: modal.client._Client, order_outputs: bool, return_exceptions: bool, count_update_callback: typing.Union[typing.Callable[[int, int], None], None]):
     ...
 
 
 class FunctionStats:
     backlog: int
     num_active_runners: int
     num_total_runners: int
@@ -141,52 +101,52 @@
         ...
 
 
 def _parse_retries(retries: typing.Union[int, modal.retries.Retries, None], raw_f: typing.Union[typing.Callable, None] = None) -> typing.Union[modal_proto.api_pb2.FunctionRetryPolicy, None]:
     ...
 
 
-class _FunctionSpec:
+class FunctionEnv:
     image: typing.Union[modal.image._Image, None]
     mounts: typing.Sequence[modal.mount._Mount]
     secrets: typing.Sequence[modal.secret._Secret]
     network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem]
     volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]]
     gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig]
     cloud: typing.Union[str, None]
     cpu: typing.Union[float, None]
-    memory: typing.Union[int, typing.Tuple[int, int], None]
+    memory: typing.Union[int, None]
 
-    def __init__(self, image: typing.Union[modal.image._Image, None], mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig], cloud: typing.Union[str, None], cpu: typing.Union[float, None], memory: typing.Union[int, typing.Tuple[int, int], None]) -> None:
+    def __init__(self, image: typing.Union[modal.image._Image, None], mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem], volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig], cloud: typing.Union[str, None], cpu: typing.Union[float, None], memory: typing.Union[int, None]) -> None:
         ...
 
     def __repr__(self):
         ...
 
     def __eq__(self, other):
         ...
 
 
 class _Function(modal.object._Object):
     _info: typing.Union[modal._utils.function_utils.FunctionInfo, None]
     _all_mounts: typing.Collection[modal.mount._Mount]
-    _stub: modal.app._App
+    _stub: modal.stub._Stub
     _obj: typing.Any
     _web_url: typing.Union[str, None]
     _is_remote_cls_method: bool
     _function_name: typing.Union[str, None]
     _is_method: bool
-    _spec: _FunctionSpec
+    _env: FunctionEnv
     _tag: str
     _raw_f: typing.Callable[..., typing.Any]
     _build_args: dict
     _parent: _Function
 
     @staticmethod
-    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image._Image, secret: typing.Union[modal.secret._Secret, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
+    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image._Image, secret: typing.Union[modal.secret._Secret, None] = None, secrets: typing.Sequence[modal.secret._Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount._Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system._NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, None] = None, proxy: typing.Union[modal.proxy._Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
         ...
 
     def from_parametrized(self, obj, from_other_workspace: bool, options: typing.Union[modal_proto.api_pb2.FunctionOptions, None], args: typing.Sized, kwargs: typing.Dict[str, typing.Any]) -> _Function:
         ...
 
     async def keep_warm(self, warm_pool_size: int) -> None:
         ...
@@ -200,23 +160,23 @@
         ...
 
     @property
     def tag(self) -> str:
         ...
 
     @property
-    def stub(self) -> modal.app._App:
+    def stub(self) -> modal.stub._Stub:
         ...
 
     @property
     def info(self) -> modal._utils.function_utils.FunctionInfo:
         ...
 
     @property
-    def spec(self) -> _FunctionSpec:
+    def env(self) -> FunctionEnv:
         ...
 
     def get_build_def(self) -> str:
         ...
 
     def _initialize_from_empty(self):
         ...
@@ -237,45 +197,36 @@
     def web_url(self) -> str:
         ...
 
     @property
     def is_generator(self) -> bool:
         ...
 
-    def _map(self, input_queue: _SynchronizedQueue, order_outputs: bool, return_exceptions: bool) -> typing.AsyncGenerator[typing.Any, None]:
+    def _map(self, input_stream: typing.AsyncIterable[typing.Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
         ...
 
     async def _call_function(self, args, kwargs):
         ...
 
     async def _call_function_nowait(self, args, kwargs) -> _Invocation:
         ...
 
     def _call_generator(self, args, kwargs):
         ...
 
     async def _call_generator_nowait(self, args, kwargs):
         ...
 
-    def _map_sync(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-        ...
-
-    def _map_async(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
+    def map(self, *input_iterators, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
         ...
 
-    def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
+    async def for_each(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
         ...
 
-    async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        ...
-
-    def _starmap_async(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-        ...
-
-    def _starmap_sync(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
+    def starmap(self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
         ...
 
     async def remote(self, *args, **kwargs) -> typing.Any:
         ...
 
     def remote_gen(self, *args, **kwargs) -> typing.AsyncGenerator[typing.Any, None]:
         ...
@@ -300,62 +251,35 @@
 
     def get_raw_f(self) -> typing.Callable[..., typing.Any]:
         ...
 
     async def get_current_stats(self) -> FunctionStats:
         ...
 
-    class __map_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
-            ...
-
-    map: __map_spec
-
-    class __starmap_spec(typing_extensions.Protocol):
-        def __call__(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-            ...
-
-    starmap: __starmap_spec
-
-    class __for_each_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-        async def aio(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-    for_each: __for_each_spec
-
 
 class Function(modal.object.Object):
     _info: typing.Union[modal._utils.function_utils.FunctionInfo, None]
     _all_mounts: typing.Collection[modal.mount.Mount]
-    _stub: modal.app.App
+    _stub: modal.stub.Stub
     _obj: typing.Any
     _web_url: typing.Union[str, None]
     _is_remote_cls_method: bool
     _function_name: typing.Union[str, None]
     _is_method: bool
-    _spec: _FunctionSpec
+    _env: FunctionEnv
     _tag: str
     _raw_f: typing.Callable[..., typing.Any]
     _build_args: dict
     _parent: Function
 
     def __init__(self, *args, **kwargs):
         ...
 
     @staticmethod
-    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image.Image, secret: typing.Union[modal.secret.Secret, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
+    def from_args(info: modal._utils.function_utils.FunctionInfo, stub, image: modal.image.Image, secret: typing.Union[modal.secret.Secret, None] = None, secrets: typing.Sequence[modal.secret.Secret] = (), schedule: typing.Union[modal.schedule.Schedule, None] = None, is_generator=False, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, mounts: typing.Collection[modal.mount.Mount] = (), network_file_systems: typing.Dict[typing.Union[str, pathlib.PurePosixPath], modal.network_file_system.NetworkFileSystem] = {}, allow_cross_region_volumes: bool = False, volumes: typing.Dict[typing.Union[str, pathlib.PurePosixPath], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None] = None, memory: typing.Union[int, None] = None, proxy: typing.Union[modal.proxy.Proxy, None] = None, retries: typing.Union[int, modal.retries.Retries, None] = None, timeout: typing.Union[int, None] = None, concurrency_limit: typing.Union[int, None] = None, allow_concurrent_inputs: typing.Union[int, None] = None, container_idle_timeout: typing.Union[int, None] = None, cpu: typing.Union[float, None] = None, keep_warm: typing.Union[int, None] = None, cloud: typing.Union[str, None] = None, _experimental_boost: bool = False, _experimental_scheduler: bool = False, _experimental_scheduler_placement: typing.Union[modal.scheduler_placement.SchedulerPlacement, None] = None, is_builder_function: bool = False, is_auto_snapshot: bool = False, enable_memory_snapshot: bool = False, checkpointing_enabled: typing.Union[bool, None] = None, allow_background_volume_commits: bool = False, block_network: bool = False, max_inputs: typing.Union[int, None] = None) -> None:
         ...
 
     def from_parametrized(self, obj, from_other_workspace: bool, options: typing.Union[modal_proto.api_pb2.FunctionOptions, None], args: typing.Sized, kwargs: typing.Dict[str, typing.Any]) -> Function:
         ...
 
     class __keep_warm_spec(typing_extensions.Protocol):
         def __call__(self, warm_pool_size: int) -> None:
@@ -380,23 +304,23 @@
     lookup: __lookup_spec
 
     @property
     def tag(self) -> str:
         ...
 
     @property
-    def stub(self) -> modal.app.App:
+    def stub(self) -> modal.stub.Stub:
         ...
 
     @property
     def info(self) -> modal._utils.function_utils.FunctionInfo:
         ...
 
     @property
-    def spec(self) -> _FunctionSpec:
+    def env(self) -> FunctionEnv:
         ...
 
     def get_build_def(self) -> str:
         ...
 
     def _initialize_from_empty(self):
         ...
@@ -418,18 +342,18 @@
         ...
 
     @property
     def is_generator(self) -> bool:
         ...
 
     class ___map_spec(typing_extensions.Protocol):
-        def __call__(self, input_queue: SynchronizedQueue, order_outputs: bool, return_exceptions: bool) -> typing.Generator[typing.Any, None, None]:
+        def __call__(self, input_stream: typing.Iterable[typing.Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
             ...
 
-        def aio(self, input_queue: SynchronizedQueue, order_outputs: bool, return_exceptions: bool) -> typing.AsyncGenerator[typing.Any, None]:
+        def aio(self, input_stream: typing.AsyncIterable[typing.Any], order_outputs: bool, return_exceptions: bool, kwargs={}):
             ...
 
     _map: ___map_spec
 
     class ___call_function_spec(typing_extensions.Protocol):
         def __call__(self, args, kwargs):
             ...
@@ -456,31 +380,40 @@
             ...
 
         async def aio(self, *args, **kwargs):
             ...
 
     _call_generator_nowait: ___call_generator_nowait_spec
 
-    def _map_sync(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-        ...
+    class __map_spec(typing_extensions.Protocol):
+        def __call__(self, *input_iterators, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.Generator[typing.Any, None, None]:
+            ...
 
-    def _map_async(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
-        ...
+        def aio(self, *input_iterators, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
+            ...
 
-    def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        ...
+    map: __map_spec
 
-    async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-        ...
+    class __for_each_spec(typing_extensions.Protocol):
+        def __call__(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
+            ...
 
-    def _starmap_async(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-        ...
+        async def aio(self, *args, **kwargs):
+            ...
 
-    def _starmap_sync(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-        ...
+    for_each: __for_each_spec
+
+    class __starmap_spec(typing_extensions.Protocol):
+        def __call__(self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.Generator[typing.Any, None, None]:
+            ...
+
+        def aio(self, input_iterator, kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
+            ...
+
+    starmap: __starmap_spec
 
     class __remote_spec(typing_extensions.Protocol):
         def __call__(self, *args, **kwargs) -> typing.Any:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.Any:
             ...
@@ -534,41 +467,14 @@
             ...
 
         async def aio(self, *args, **kwargs) -> FunctionStats:
             ...
 
     get_current_stats: __get_current_stats_spec
 
-    class __map_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators: typing.Iterable[typing.Any], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, *input_iterators: typing.Union[typing.Iterable[typing.Any], typing.AsyncIterable[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncGenerator[typing.Any, None]:
-            ...
-
-    map: __map_spec
-
-    class __starmap_spec(typing_extensions.Protocol):
-        def __call__(self, input_iterator: typing.Iterable[typing.Sequence[typing.Any]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> modal._utils.async_utils.AsyncOrSyncIteratable:
-            ...
-
-        def aio(self, input_iterator: typing.Union[typing.Iterable[typing.Sequence[typing.Any]], typing.AsyncIterable[typing.Sequence[typing.Any]]], kwargs={}, order_outputs: bool = True, return_exceptions: bool = False) -> typing.AsyncIterable[typing.Any]:
-            ...
-
-    starmap: __starmap_spec
-
-    class __for_each_spec(typing_extensions.Protocol):
-        def __call__(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-        async def aio(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
-            ...
-
-    for_each: __for_each_spec
-
 
 class _FunctionCall(modal.object._Object):
     def _invocation(self):
         ...
 
     async def get(self, timeout: typing.Union[float, None] = None):
         ...
```

## modal/image.py

```diff
@@ -1,148 +1,95 @@
 # Copyright Modal Labs 2022
 import contextlib
 import os
-import re
 import shlex
 import sys
 import typing
 import warnings
 from dataclasses import dataclass
 from inspect import isfunction
 from pathlib import Path, PurePosixPath
-from typing import Any, Callable, Dict, List, Literal, Optional, Sequence, Set, Tuple, Union, get_args
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 
 from google.protobuf.message import Message
 from grpclib.exceptions import GRPCError, StreamTerminatedError
 
 from modal_proto import api_pb2
 
 from ._resolver import Resolver
 from ._serialization import serialize
 from ._utils.async_utils import synchronize_api
 from ._utils.blob_utils import MAX_OBJECT_SIZE_BYTES
 from ._utils.function_utils import FunctionInfo
 from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors, unary_stream
-from .config import config, logger, user_config_path
-from .exception import InvalidError, NotFoundError, RemoteError, VersionError, deprecation_error, deprecation_warning
+from .config import config, logger
+from .exception import InvalidError, NotFoundError, RemoteError, deprecation_warning
 from .gpu import GPU_T, parse_gpu_config
 from .mount import _Mount, python_standalone_mount_name
 from .network_file_system import _NetworkFileSystem
 from .object import _Object
 from .secret import _Secret
 
-if typing.TYPE_CHECKING:
-    import modal.functions
-
-
-# This is used for both type checking and runtime validation
-ImageBuilderVersion = Literal["2023.12", "2024.04"]
 
-# Note: we also define supported Python versions via logic at the top of the package __init__.py
-# so that we fail fast / clearly in unsupported containers. Additionally, we enumerate the supported
-# Python versions in mount.py where we specify the "standalone Python versions" we create mounts for.
-# Consider consolidating these multiple sources of truth?
-SUPPORTED_PYTHON_SERIES: Set[str] = {"3.8", "3.9", "3.10", "3.11", "3.12"}
-
-CONTAINER_REQUIREMENTS_PATH = "/modal_requirements.txt"
-
-
-def _validate_python_version(version: Optional[str], allow_micro_granularity: bool = True) -> str:
-    if version is None:
-        # If Python version is unspecified, match the local version, up to the minor component
-        version = series_version = "{0}.{1}".format(*sys.version_info)
-    elif not isinstance(version, str):
-        raise InvalidError(f"Python version must be specified as a string, not {type(version).__name__}")
-    elif not re.match(r"^3(?:\.\d{1,2}){1,2}$", version):
-        raise InvalidError(f"Invalid Python version: {version!r}")
-    else:
-        components = version.split(".")
-        if len(components) == 3 and not allow_micro_granularity:
-            raise InvalidError(
-                "Python version must be specified as 'major.minor' for this interface;"
-                f" micro-level specification ({version!r}) is not valid."
-            )
-        series_version = "{0}.{1}".format(*components)
-
-    if series_version not in SUPPORTED_PYTHON_SERIES:
+def _validate_python_version(version: str) -> None:
+    components = version.split(".")
+    supported_versions = {"3.12", "3.11", "3.10", "3.9", "3.8"}
+    if len(components) == 2 and version in supported_versions:
+        return
+    elif len(components) == 3:
         raise InvalidError(
-            f"Unsupported Python version: {version!r}."
-            f" Modal supports versions in the following series: {SUPPORTED_PYTHON_SERIES!r}."
+            f"major.minor.patch version specification not valid. Supported major.minor versions are {supported_versions}."
         )
-    return version
+    raise InvalidError(f"Unsupported version {version}. Supported versions are {supported_versions}.")
+
 
+def _dockerhub_python_version(python_version=None):
+    if python_version is None:
+        python_version = "%d.%d" % sys.version_info[:2]
 
-def _dockerhub_python_version(builder_version: ImageBuilderVersion, python_version: Optional[str] = None) -> str:
-    python_version = _validate_python_version(python_version)
-    components = python_version.split(".")
+    parts = python_version.split(".")
 
-    # When user specifies a full Python version, use that
-    if len(components) > 2:
+    if len(parts) > 2:
         return python_version
 
-    # Otherwise, use the same series, but a specific micro version, corresponding to the latest
-    # available from https://hub.docker.com/_/python at the time of each image builder release.
+    # We use the same major/minor version, but the highest micro version
+    # See https://hub.docker.com/_/python
     latest_micro_version = {
-        "2023.12": {
-            "3.12": "1",
-            "3.11": "0",
-            "3.10": "8",
-            "3.9": "15",
-            "3.8": "15",
-        },
-        "2024.04": {
-            "3.12": "2",
-            "3.11": "8",
-            "3.10": "14",
-            "3.9": "19",
-            "3.8": "19",
-        },
+        "3.12": "1",
+        "3.11": "0",
+        "3.10": "8",
+        "3.9": "15",
+        "3.8": "15",
     }
-    python_series = "{0}.{1}".format(*components)
-    micro_version = latest_micro_version[builder_version][python_series]
-    python_version = f"{python_series}.{micro_version}"
+    major_minor_version = ".".join(parts[:2])
+    python_version = major_minor_version + "." + latest_micro_version[major_minor_version]
     return python_version
 
 
-def _dockerhub_debian_codename(builder_version: ImageBuilderVersion) -> str:
-    return {"2023.12": "bullseye", "2024.04": "bookworm"}[builder_version]
-
-
-def _get_modal_requirements_path(builder_version: ImageBuilderVersion, python_version: Optional[str] = None) -> str:
-    # Locate Modal client requirements data
+def _get_client_requirements_path(python_version: Optional[str] = None) -> str:
+    # Locate Modal client requirements.txt
     import modal
 
-    modal_path = Path(modal.__path__[0])
-
-    # When we added Python 3.12 support, we needed to update a few dependencies but did not yet
-    # support versioned builds, so we put them in a separate 3.12-specific requirements file.
-    # When the python_version is not specified in the Image API, we fall back to the local version.
-    # Note that this is buggy if you're using a registry or dockerfile Image that (implicitly) contains 3.12
-    # and have a different local version. We can't really fix that; but users can update their image builder.
-    # We can get rid of this complexity entirely when we drop support for 2023.12.
-    python_version = python_version or sys.version
-    suffix = ".312" if builder_version == "2023.12" and python_version.startswith("3.12") else ""
-
-    return str(modal_path / "requirements" / f"{builder_version}{suffix}.txt")
-
-
-def _get_modal_requirements_command(version: ImageBuilderVersion) -> str:
-    command = "pip install"
-    if version <= "2023.12":
-        args = f"-r {CONTAINER_REQUIREMENTS_PATH}"
+    modal_path = modal.__path__[0]
+    if python_version is None:
+        major, minor, *_ = sys.version_info
     else:
-        args = f"--no-cache --no-deps -r {CONTAINER_REQUIREMENTS_PATH}"
-    return f"{command} {args}"
+        major, minor = python_version.split("-")[0].split(".")[:2]
+    suffix = {(3, 12): ".312"}.get((int(major), int(minor)), "")
+    return os.path.join(modal_path, f"requirements{suffix}.txt")
 
 
 def _flatten_str_args(function_name: str, arg_name: str, args: Tuple[Union[str, List[str]], ...]) -> List[str]:
     """Takes a tuple of strings, or string lists, and flattens it.
 
     Raises an error if any of the elements are not strings or string lists.
     """
+    # TODO(erikbern): maybe we can just build somthing intelligent that checks
+    # based on type annotations in real time?
+    # Or use something like this? https://github.com/FelixTheC/strongtyping
 
     def is_str_list(x):
         return isinstance(x, list) and all(isinstance(y, str) for y in x)
 
     ret: List[str] = []
     for x in args:
         if isinstance(x, str):
@@ -162,50 +109,21 @@
 ) -> str:
     flags = [
         ("--find-links", find_links),  # TODO(erikbern): allow multiple?
         ("--index-url", index_url),
         ("--extra-index-url", extra_index_url),  # TODO(erikbern): allow multiple?
     ]
 
-    args = " ".join(f"{flag} {shlex.quote(value)}" for flag, value in flags if value is not None)
+    args = " ".join(flag + " " + shlex.quote(value) for flag, value in flags if value is not None)
     if pre:
         args += " --pre"
 
     return args
 
 
-def _get_image_builder_version(client_version: str) -> ImageBuilderVersion:
-    if config_version := config.get("image_builder_version"):
-        version = config_version
-        if (env_var := "MODAL_IMAGE_BUILDER_VERSION") in os.environ:
-            version_source = f" (based on your `{env_var}` environment variable)"
-        else:
-            version_source = f" (based on your local config file at `{user_config_path}`)"
-    else:
-        version = client_version
-        version_source = ""
-
-    supported_versions: Set[ImageBuilderVersion] = set(get_args(ImageBuilderVersion))
-    if version not in supported_versions:
-        if config_version is not None:
-            update_suggestion = "or remove your local configuration"
-        elif version < min(supported_versions):
-            update_suggestion = "your image builder version using the Modal dashboard"
-        else:
-            update_suggestion = "your client library (pip install --upgrade modal)"
-        raise VersionError(
-            "This version of the modal client supports the following image builder versions:"
-            f" {supported_versions!r}."
-            f"\n\nYou are using {version!r}{version_source}."
-            f" Please update {update_suggestion}."
-        )
-
-    return version
-
-
 class _ImageRegistryConfig:
     """mdmd:hidden"""
 
     def __init__(
         self,
         # TODO: change to _PUBLIC after worker starts handling it.
         registry_auth_type: int = api_pb2.REGISTRY_AUTH_TYPE_UNSPECIFIED,
@@ -217,14 +135,18 @@
     def get_proto(self) -> api_pb2.ImageRegistryConfig:
         return api_pb2.ImageRegistryConfig(
             registry_auth_type=self.registry_auth_type,
             secret_id=(self.secret.object_id if self.secret else None),
         )
 
 
+if typing.TYPE_CHECKING:
+    import modal.functions
+
+
 @dataclass
 class DockerfileSpec:
     # Ideally we would use field() with default_factory=, but doesn't work with synchronicity type-stub gen
     commands: List[str]
     context_files: Dict[str, str]
 
 
@@ -247,15 +169,15 @@
             for exc in self.inside_exceptions:
                 raise exc
 
     @staticmethod
     def _from_args(
         *,
         base_images: Optional[Dict[str, "_Image"]] = None,
-        dockerfile_function: Optional[Callable[[ImageBuilderVersion], DockerfileSpec]] = None,
+        dockerfile_function: Optional[Callable[[], DockerfileSpec]] = None,
         secrets: Optional[Sequence[_Secret]] = None,
         gpu_config: Optional[api_pb2.GPUConfig] = None,
         build_function: Optional["modal.functions._Function"] = None,
         build_function_input: Optional[api_pb2.FunctionInput] = None,
         image_registry_config: Optional[_ImageRegistryConfig] = None,
         context_mount: Optional[_Mount] = None,
         force_build: bool = False,
@@ -285,20 +207,18 @@
             if context_mount:
                 deps.append(context_mount)
             if image_registry_config.secret:
                 deps.append(image_registry_config.secret)
             return deps
 
         async def _load(self: _Image, resolver: Resolver, existing_object_id: Optional[str]):
-            builder_version = _get_image_builder_version(resolver.client.image_builder_version)
-
             if dockerfile_function is None:
                 dockerfile = DockerfileSpec(commands=[], context_files={})
             else:
-                dockerfile = dockerfile_function(builder_version)
+                dockerfile = dockerfile_function()
 
             if not dockerfile.commands and not build_function:
                 raise InvalidError(
                     "No commands were provided for the image  have you tried using modal.Image.debian_slim()?"
                 )
             if dockerfile.commands and build_function:
                 raise InvalidError("Cannot provide both a build function and Dockerfile commands!")
@@ -363,15 +283,14 @@
             req = api_pb2.ImageGetOrCreateRequest(
                 app_id=resolver.app_id,
                 image=image_definition,
                 existing_image_id=existing_object_id,  # TODO: ignored
                 build_function_id=build_function_id,
                 force_build=config.get("force_build") or force_build,
                 namespace=_namespace,
-                builder_version=builder_version,
             )
             resp = await retry_transient_errors(resolver.client.stub.ImageGetOrCreate, req)
             image_id = resp.image_id
 
             logger.debug("Waiting for image %s" % image_id)
             last_entry_id: Optional[str] = None
             result: Optional[api_pb2.GenericResult] = None
@@ -428,15 +347,15 @@
     def extend(self, **kwargs) -> "_Image":
         """Deprecated! This is a low-level method not intended to be part of the public API."""
         deprecation_warning(
             (2024, 3, 7),
             "`Image.extend` is deprecated; please use a higher-level method, such as `Image.dockerfile_commands`.",
         )
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile():
             return DockerfileSpec(
                 commands=kwargs.pop("dockerfile_commands", []),
                 context_files=kwargs.pop("context_files", {}),
             )
 
         return _Image._from_args(base_images={"base": self}, dockerfile_function=build_dockerfile, **kwargs)
 
@@ -454,15 +373,15 @@
         # place mount's contents into /static directory of image.
         image = modal.Image.debian_slim().copy_mount(mount, remote_path="/static")
         ```
         """
         if not isinstance(mount, _Mount):
             raise InvalidError("The mount argument to copy has to be a Modal Mount object")
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = ["FROM base", f"COPY . {remote_path}"]  # copy everything from the supplied mount
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             context_mount=mount,
@@ -472,15 +391,15 @@
         """Copy a file into the image as a part of building it.
 
         This works in a similar way to [`COPY`](https://docs.docker.com/engine/reference/builder/#copy) in a `Dockerfile`.
         """
         basename = str(Path(local_path).name)
         mount = _Mount.from_local_file(local_path, remote_path=f"/{basename}")
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base", f"COPY {basename} {remote_path}"], context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             context_mount=mount,
         )
@@ -488,15 +407,15 @@
     def copy_local_dir(self, local_path: Union[str, Path], remote_path: Union[str, Path] = ".") -> "_Image":
         """Copy a directory into the image as a part of building the image.
 
         This works in a similar way to [`COPY`](https://docs.docker.com/engine/reference/builder/#copy) in a `Dockerfile`.
         """
         mount = _Mount.from_local_dir(local_path, remote_path="/")
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base", f"COPY . {remote_path}"], context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             context_mount=mount,
         )
@@ -520,27 +439,32 @@
         image = modal.Image.debian_slim().pip_install("click", "httpx~=0.23.3")
         ```
         """
         pkgs = _flatten_str_args("pip_install", "packages", packages)
         if not pkgs:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            package_args = " ".join(shlex.quote(pkg) for pkg in sorted(pkgs))
+        def build_dockerfile() -> DockerfileSpec:
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
-            commands = ["FROM base", f"RUN python -m pip install {package_args} {extra_args}"]
-            if version > "2023.12":  # Back-compat for legacy trailing space with empty extra_args
-                commands = [cmd.strip() for cmd in commands]
+            package_args = " ".join(shlex.quote(pkg) for pkg in sorted(pkgs))
+
+            commands = [
+                "FROM base",
+                f"RUN python -m pip install {package_args} {extra_args}",
+                # TODO(erikbern): if extra_args is empty, we add a superfluous space at the end.
+                # However removing it at this point would cause image hashes to change.
+                # Maybe let's remove it later when/if client requirements change.
+            ]
             return DockerfileSpec(commands=commands, context_files={})
 
         gpu_config = parse_gpu_config(gpu)
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
-            force_build=self.force_build or force_build,
+            force_build=self.force_build or force_build,  # TODO shouldn't forcing upstream build always rerun this?
             gpu_config=gpu_config,
             secrets=secrets,
         )
 
     def pip_install_private_repos(
         self,
         *repositories: str,
@@ -606,30 +530,28 @@
             raise InvalidError(
                 f"{len(invalid_repos)} out of {len(repositories)} given repository refs are invalid. "
                 f"Invalid refs: {invalid_repos}. "
             )
 
         secret_names = ",".join([s.app_name if hasattr(s, "app_name") else str(s) for s in secrets])  # type: ignore
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = ["FROM base"]
             if any(r.startswith("github") for r in repositories):
                 commands.append(
                     f"RUN bash -c \"[[ -v GITHUB_TOKEN ]] || (echo 'GITHUB_TOKEN env var not set by provided modal.Secret(s): {secret_names}' && exit 1)\"",
                 )
             if any(r.startswith("gitlab") for r in repositories):
                 commands.append(
                     f"RUN bash -c \"[[ -v GITLAB_TOKEN ]] || (echo 'GITLAB_TOKEN env var not set by provided modal.Secret(s): {secret_names}' && exit 1)\"",
                 )
 
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
             commands.extend(["RUN apt-get update && apt-get install -y git"])
             commands.extend([f'RUN python3 -m pip install "{url}" {extra_args}' for url in install_urls])
-            if version > "2023.12":  # Back-compat for legacy trailing space with empty extra_args
-                commands = [cmd.strip() for cmd in commands]
             return DockerfileSpec(commands=commands, context_files={})
 
         gpu_config = parse_gpu_config(gpu)
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
@@ -648,29 +570,26 @@
         pre: bool = False,  # Passes --pre (allow pre-releases) to pip install
         force_build: bool = False,
         secrets: Sequence[_Secret] = [],
         gpu: GPU_T = None,
     ) -> "_Image":
         """Install a list of Python packages from a local `requirements.txt` file."""
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             requirements_txt_path = os.path.expanduser(requirements_txt)
             context_files = {"/.requirements.txt": requirements_txt_path}
 
-            null_find_links_arg = " " if version == "2023.12" else ""
-            find_links_arg = f" -f {find_links}" if find_links else null_find_links_arg
+            find_links_arg = f"-f {find_links}" if find_links else ""
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
 
             commands = [
                 "FROM base",
                 "COPY /.requirements.txt /.requirements.txt",
-                f"RUN python -m pip install -r /.requirements.txt{find_links_arg} {extra_args}",
+                f"RUN python -m pip install -r /.requirements.txt {find_links_arg} {extra_args}",
             ]
-            if version > "2023.12":  # Back-compat for legacy whitespace with empty find_link / extra args
-                commands = [cmd.strip() for cmd in commands]
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             force_build=self.force_build or force_build,
             gpu_config=parse_gpu_config(gpu),
@@ -694,15 +613,15 @@
 
         `optional_dependencies` is a list of the keys of the
         optional-dependencies section(s) of the `pyproject.toml` file
         (e.g. test, doc, experiment, etc). When provided,
         all of the packages in each listed section are installed as well.
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             # Defer toml import so we don't need it in the container runtime environment
             import toml
 
             config = toml.load(os.path.expanduser(pyproject_toml))
 
             dependencies = []
             if "project" not in config or "dependencies" not in config["project"]:
@@ -718,18 +637,22 @@
                 optionals = config["project"]["optional-dependencies"]
                 for dep_group_name in optional_dependencies:
                     if dep_group_name in optionals:
                         dependencies.extend(optionals[dep_group_name])
 
             extra_args = _make_pip_install_args(find_links, index_url, extra_index_url, pre)
             package_args = " ".join(shlex.quote(pkg) for pkg in sorted(dependencies))
-            commands = ["FROM base", f"RUN python -m pip install {package_args} {extra_args}"]
-            if version > "2023.12":  # Back-compat for legacy trailing space
-                commands = [cmd.strip() for cmd in commands]
 
+            commands = [
+                "FROM base",
+                f"RUN python -m pip install {package_args} {extra_args}",
+                # TODO(erikbern): if extra_args is empty, we add a superfluous space at the end.
+                # However removing it at this point would cause image hashes to change.
+                # Maybe let's remove it later when/if client requirements change.
+            ]
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             force_build=self.force_build or force_build,
             secrets=secrets,
@@ -761,15 +684,15 @@
         If not provided as argument the path to the lockfile is inferred. However, the
         file has to exist, unless `ignore_lockfile` is set to `True`.
 
         Note that the root project of the poetry project is not installed,
         only the dependencies. For including local packages see `modal.Mount.from_local_python_packages`
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             context_files = {"/.pyproject.toml": os.path.expanduser(poetry_pyproject_toml)}
 
             commands = ["FROM base", "RUN python -m pip install poetry~=1.7"]
 
             if old_installer:
                 commands += ["RUN poetry config experimental.new-installer false"]
 
@@ -781,18 +704,16 @@
                         raise NotFoundError(
                             f"poetry.lock not found at inferred location: {p.absolute()}. If a lockfile is not needed, `ignore_lockfile=True` can be used."
                         )
                     poetry_lockfile = p.as_posix()
                 context_files["/.poetry.lock"] = poetry_lockfile
                 commands += ["COPY /.poetry.lock /tmp/poetry/poetry.lock"]
 
-            install_cmd = "poetry install --no-root"
-            if version == "2023.12":
-                # Backwards compatability for previous string, which started with whitespace
-                install_cmd = "  " + install_cmd
+            # Indentation for back-compat TODO: fix when we update image_builder_version
+            install_cmd = "  poetry install --no-root"
 
             if with_:
                 install_cmd += f" --with {','.join(with_)}"
 
             if without:
                 install_cmd += f" --without {','.join(without)}"
 
@@ -827,15 +748,15 @@
         force_build: bool = False,
     ) -> "_Image":
         """Extend an image with arbitrary Dockerfile-like commands."""
         cmds = _flatten_str_args("dockerfile_commands", "dockerfile_commands", dockerfile_commands)
         if not cmds:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base", *cmds], context_files=context_files)
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu, raise_on_true=False),
@@ -851,46 +772,42 @@
         force_build: bool = False,
     ) -> "_Image":
         """Extend an image with a list of shell commands to run."""
         cmds = _flatten_str_args("run_commands", "commands", commands)
         if not cmds:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             return DockerfileSpec(commands=["FROM base"] + [f"RUN {cmd}" for cmd in cmds], context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu, raise_on_true=False),
             force_build=self.force_build or force_build,
         )
 
     @staticmethod
-    def conda(python_version: Optional[str] = None, force_build: bool = False) -> "_Image":
+    def conda(python_version: str = "3.9", force_build: bool = False) -> "_Image":
         """
         A Conda base image, using miniconda3 and derived from the official Docker Hub image.
         In most cases, using [`Image.micromamba()`](/docs/reference/modal.Image#micromamba) with [`micromamba_install`](/docs/reference/modal.Image#micromamba_install) is recommended over `Image.conda()`, as it leads to significantly faster image build times.
         """
+        _validate_python_version(python_version)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            nonlocal python_version
-            if version == "2023.12" and python_version is None:
-                python_version = "3.9"  # Backcompat for old hardcoded default param
-            validated_python_version = _validate_python_version(python_version)
-            debian_codename = _dockerhub_debian_codename(version)
-            requirements_path = _get_modal_requirements_path(version, python_version)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: requirements_path}
+        def build_dockerfile() -> DockerfileSpec:
+            requirements_path = _get_client_requirements_path(python_version)
+            context_files = {"/modal_requirements.txt": requirements_path}
 
             # Doesn't use the official continuumio/miniconda3 image as a base. That image has maintenance
             # issues (https://github.com/ContinuumIO/docker-images/issues) and building our own is more flexible.
             conda_install_script = "https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
             commands = [
-                f"FROM debian:{debian_codename}",  # the -slim images lack files required by Conda.
+                "FROM debian:bullseye",  # the -slim images lack files required by Conda.
                 # Temporarily add utility packages for conda installation.
                 "RUN apt-get --quiet update && apt-get --quiet --yes install curl bzip2 \\",
                 f"&& curl --silent --show-error --location {conda_install_script} --output /tmp/miniconda.sh \\",
                 # Install miniconda to a filesystem location on the $PATH of Modal container tasks.
                 # -b = install in batch mode w/o manual intervention.
                 # -f = allow install prefix to already exist.
                 # -p = the install prefix location.
@@ -898,41 +815,39 @@
                 "&& rm -rf /tmp/miniconda.sh",
                 # Biggest and most stable community-led Conda channel.
                 "RUN conda config --add channels conda-forge \\ ",
                 # softlinking can put conda in a broken state, surfacing error on uninstall like:
                 # `No such device or address: '/usr/local/lib/libz.so' -> '/usr/local/lib/libz.so.c~'`
                 "&& conda config --set allow_softlinks false \\ ",
                 # Install requested Python version from conda-forge channel; base debian image has only 3.7.
-                f"&& conda install --yes --channel conda-forge python={validated_python_version} \\ ",
+                f"&& conda install --yes --channel conda-forge python={python_version} \\ ",
                 "&& conda update conda \\ ",
                 # Remove now unneeded packages and files.
                 "&& apt-get --quiet --yes remove curl bzip2 \\ ",
                 "&& apt-get --quiet --yes autoremove \\ ",
                 "&& apt-get autoclean \\ ",
                 "&& rm -rf /var/lib/apt/lists/* /var/log/dpkg.log \\ ",
                 "&& conda clean --all --yes",
                 # Setup .bashrc for conda.
                 "RUN conda init bash --verbose",
-                f"COPY {CONTAINER_REQUIREMENTS_PATH} {CONTAINER_REQUIREMENTS_PATH}",
+                "COPY /modal_requirements.txt /modal_requirements.txt",
                 # .bashrc is explicitly sourced because RUN is a non-login shell and doesn't run bash.
                 "RUN . /root/.bashrc && conda activate base \\ ",
-                # Ensure that packaging tools are up to date and install client dependenices
-                f"&& python -m pip install --upgrade {'pip' if version == '2023.12' else 'pip wheel'} \\ ",
-                f"&& python -m {_get_modal_requirements_command(version)}",
+                "&& python -m pip install --upgrade pip \\ ",
+                "&& python -m pip install -r /modal_requirements.txt",
             ]
-            if version > "2023.12":
-                commands.append(f"RUN rm {CONTAINER_REQUIREMENTS_PATH}")
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         base = _Image._from_args(
             dockerfile_function=build_dockerfile,
             force_build=force_build,
             _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
         )
 
+        # TODO include these in the base image once we version the build?
         return base.dockerfile_commands(
             [
                 "ENV CONDA_EXE=/usr/local/bin/conda",
                 "ENV CONDA_PREFIX=/usr/local",
                 "ENV CONDA_PROMPT_MODIFIER=(base)",
                 "ENV CONDA_SHLVL=1",
                 "ENV CONDA_PYTHON_EXE=/usr/local/bin/python",
@@ -951,15 +866,15 @@
         """Install a list of additional packages using Conda. Note that in most cases, using [`Image.micromamba()`](/docs/reference/modal.Image#micromamba) with [`micromamba_install`](/docs/reference/modal.Image#micromamba_install)
         is recommended over `conda_install`, as it leads to significantly faster image build times."""
 
         pkgs = _flatten_str_args("conda_install", "packages", packages)
         if not pkgs:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             package_args = " ".join(shlex.quote(pkg) for pkg in pkgs)
             channel_args = "".join(f" -c {channel}" for channel in channels)
 
             commands = [
                 "FROM base",
                 f"RUN conda install {package_args}{channel_args} --yes \\ ",
                 "&& conda clean --yes --index-cache --tarballs --tempfiles --logfiles",
@@ -980,15 +895,15 @@
         force_build: bool = False,
         *,
         secrets: Sequence[_Secret] = [],
         gpu: GPU_T = None,
     ) -> "_Image":
         """Update a Conda environment using dependencies from a given environment.yml file."""
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             context_files = {"/environment.yml": os.path.expanduser(environment_yml)}
 
             commands = [
                 "FROM base",
                 "COPY /environment.yml /environment.yml",
                 "RUN conda env update --name base -f /environment.yml \\ ",
                 "&& conda clean --yes --index-cache --tarballs --tempfiles --logfiles",
@@ -1001,37 +916,32 @@
             force_build=self.force_build or force_build,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu),
         )
 
     @staticmethod
     def micromamba(
-        python_version: Optional[str] = None,
+        python_version: str = "3.9",
         force_build: bool = False,
     ) -> "_Image":
         """
         A Micromamba base image. Micromamba allows for fast building of small Conda-based containers.
         In most cases it will be faster than using [`Image.conda()`](/docs/reference/modal.Image#conda).
         """
+        _validate_python_version(python_version)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            nonlocal python_version
-            if version == "2023.12" and python_version is None:
-                python_version = "3.9"  # Backcompat for old hardcoded default param
-            validated_python_version = _validate_python_version(python_version)
-            micromamba_version = {"2023.12": "1.3.1", "2024.04": "1.5.8"}[version]
-            debian_codename = _dockerhub_debian_codename(version)
-            tag = f"mambaorg/micromamba:{micromamba_version}-{debian_codename}-slim"
+        def build_dockerfile() -> DockerfileSpec:
+            tag = "mambaorg/micromamba:1.3.1-bullseye-slim"
             setup_commands = [
                 'SHELL ["/usr/local/bin/_dockerfile_shell.sh"]',
                 "ENV MAMBA_DOCKERFILE_ACTIVATE=1",
-                f"RUN micromamba install -n base -y python={validated_python_version} pip -c conda-forge",
+                f"RUN micromamba install -n base -y python={python_version} pip -c conda-forge",
             ]
-            commands = _Image._registry_setup_commands(tag, version, setup_commands)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: _get_modal_requirements_path(version, python_version)}
+            commands = _Image._registry_setup_commands(tag, setup_commands, add_python=None)
+            context_files = {"/modal_requirements.txt": _get_client_requirements_path(python_version)}
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             dockerfile_function=build_dockerfile,
             force_build=force_build,
             _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
         )
@@ -1048,15 +958,15 @@
     ) -> "_Image":
         """Install a list of additional packages using micromamba."""
 
         pkgs = _flatten_str_args("micromamba_install", "packages", packages)
         if not pkgs:
             return self
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             package_args = " ".join(shlex.quote(pkg) for pkg in pkgs)
             channel_args = "".join(f" -c {channel}" for channel in channels)
 
             commands = [
                 "FROM base",
                 f"RUN micromamba install {package_args}{channel_args} --yes",
             ]
@@ -1067,42 +977,33 @@
             dockerfile_function=build_dockerfile,
             force_build=self.force_build or force_build,
             secrets=secrets,
             gpu_config=parse_gpu_config(gpu),
         )
 
     @staticmethod
-    def _registry_setup_commands(
-        tag: str,
-        builder_version: ImageBuilderVersion,
-        setup_commands: List[str],
-        add_python: Optional[str] = None,
-    ) -> List[str]:
+    def _registry_setup_commands(tag: str, setup_commands: List[str], add_python: Optional[str]) -> List[str]:
         add_python_commands: List[str] = []
         if add_python:
-            _validate_python_version(add_python, allow_micro_granularity=False)
             add_python_commands = [
                 "COPY /python/. /usr/local",
                 "RUN ln -s /usr/local/bin/python3 /usr/local/bin/python",
                 "ENV TERMINFO_DIRS=/etc/terminfo:/lib/terminfo:/usr/share/terminfo:/usr/lib/terminfo",
             ]
-
-        modal_requirements_commands = [
-            f"COPY {CONTAINER_REQUIREMENTS_PATH} {CONTAINER_REQUIREMENTS_PATH}",
-            f"RUN python -m pip install --upgrade {'pip' if builder_version == '2023.12' else 'pip wheel'}",
-            f"RUN python -m {_get_modal_requirements_command(builder_version)}",
-        ]
-        if builder_version > "2023.12":
-            modal_requirements_commands.append(f"RUN rm {CONTAINER_REQUIREMENTS_PATH}")
-
         return [
             f"FROM {tag}",
             *add_python_commands,
             *setup_commands,
-            *modal_requirements_commands,
+            "COPY /modal_requirements.txt /modal_requirements.txt",
+            "RUN python -m pip install --upgrade pip",
+            "RUN python -m pip install -r /modal_requirements.txt",
+            # TODO: We should add this next line at some point to clean up the image, but it would
+            # trigger a hash change, so batch it with the next rebuild-triggering change.
+            #
+            # "RUN rm /modal_requirements.txt",
         ]
 
     @staticmethod
     def from_registry(
         tag: str,
         *,
         secret: Optional[_Secret] = None,
@@ -1144,17 +1045,17 @@
                 python_standalone_mount_name(add_python),
                 namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
             )
 
         if "image_registry_config" not in kwargs and secret is not None:
             kwargs["image_registry_config"] = _ImageRegistryConfig(api_pb2.REGISTRY_AUTH_TYPE_STATIC_CREDS, secret)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            commands = _Image._registry_setup_commands(tag, version, setup_dockerfile_commands, add_python)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: _get_modal_requirements_path(version, add_python)}
+        def build_dockerfile() -> DockerfileSpec:
+            commands = _Image._registry_setup_commands(tag, setup_dockerfile_commands, add_python)
+            context_files = {"/modal_requirements.txt": _get_client_requirements_path(add_python)}
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             dockerfile_function=build_dockerfile,
             context_mount=context_mount,
             force_build=force_build,
             **kwargs,
@@ -1270,22 +1171,22 @@
         ```python
         image = modal.Image.from_dockerfile("./Dockerfile", add_python="3.12")
         ```
         """
 
         # --- Build the base dockerfile
 
-        def build_dockerfile_base(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_base_dockerfile() -> DockerfileSpec:
             with open(os.path.expanduser(path)) as f:
                 commands = f.read().split("\n")
             return DockerfileSpec(commands=commands, context_files={})
 
         gpu_config = parse_gpu_config(gpu)
         base_image = _Image._from_args(
-            dockerfile_function=build_dockerfile_base,
+            dockerfile_function=build_base_dockerfile,
             context_mount=context_mount,
             gpu_config=gpu_config,
             secrets=secrets,
         )
 
         # --- Now add in the modal dependencies, and, optionally a Python distribution
         # This happening in two steps is probably a vestigial consequence of previous limitations,
@@ -1295,50 +1196,67 @@
             context_mount = _Mount.from_name(
                 python_standalone_mount_name(add_python),
                 namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
             )
         else:
             context_mount = None
 
-        def build_dockerfile_python(version: ImageBuilderVersion) -> DockerfileSpec:
-            commands = _Image._registry_setup_commands("base", version, [], add_python)
-            requirements_path = _get_modal_requirements_path(version, add_python)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: requirements_path}
+        def enhance_dockerfile() -> DockerfileSpec:
+            requirements_path = _get_client_requirements_path(add_python)
+
+            add_python_commands = []
+            if add_python:
+                add_python_commands = [
+                    "COPY /python/. /usr/local",
+                    "RUN ln -s /usr/local/bin/python3 /usr/local/bin/python",
+                    "ENV TERMINFO_DIRS=/etc/terminfo:/lib/terminfo:/usr/share/terminfo:/usr/lib/terminfo",
+                ]
+
+            commands = [
+                "FROM base",
+                *add_python_commands,
+                "COPY /modal_requirements.txt /modal_requirements.txt",
+                "RUN python -m pip install --upgrade pip",
+                "RUN python -m pip install -r /modal_requirements.txt",
+            ]
+
+            context_files = {"/modal_requirements.txt": requirements_path}
+
             return DockerfileSpec(commands=commands, context_files=context_files)
 
         return _Image._from_args(
             base_images={"base": base_image},
-            dockerfile_function=build_dockerfile_python,
+            dockerfile_function=enhance_dockerfile,
             context_mount=context_mount,
             force_build=force_build,
         )
 
     @staticmethod
     def debian_slim(python_version: Optional[str] = None, force_build: bool = False) -> "_Image":
-        """Default image, based on the official `python` Docker images."""
+        """Default image, based on the official `python:X.Y.Z-slim-bullseye` Docker images."""
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            requirements_path = _get_modal_requirements_path(version, python_version)
-            context_files = {CONTAINER_REQUIREMENTS_PATH: requirements_path}
-            full_python_version = _dockerhub_python_version(version, python_version)
-            debian_codename = _dockerhub_debian_codename(version)
+        def build_dockerfile() -> DockerfileSpec:
+            full_python_version = _dockerhub_python_version(python_version)
 
+            requirements_path = _get_client_requirements_path(full_python_version)
             commands = [
-                f"FROM python:{full_python_version}-slim-{debian_codename}",
-                f"COPY {CONTAINER_REQUIREMENTS_PATH} {CONTAINER_REQUIREMENTS_PATH}",
+                f"FROM python:{full_python_version}-slim-bullseye",
+                "COPY /modal_requirements.txt /modal_requirements.txt",
                 "RUN apt-get update",
                 "RUN apt-get install -y gcc gfortran build-essential",
-                f"RUN pip install --upgrade {'pip' if version == '2023.12' else 'pip wheel'}",
-                f"RUN {_get_modal_requirements_command(version)}",
-                # Set debian front-end to non-interactive to avoid users getting stuck with input prompts.
+                "RUN pip install --upgrade pip",
+                "RUN pip install -r /modal_requirements.txt",
+                # Set debian front-end to non-interactive to avoid users getting stuck with input
+                # prompts.
                 "RUN echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections",
             ]
-            if version > "2023.12":
-                commands.append(f"RUN rm {CONTAINER_REQUIREMENTS_PATH}")
-            return DockerfileSpec(commands=commands, context_files=context_files)
+            return DockerfileSpec(
+                commands=commands,
+                context_files={"/modal_requirements.txt": requirements_path},
+            )
 
         return _Image._from_args(
             dockerfile_function=build_dockerfile,
             force_build=force_build,
             _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
         )
 
@@ -1359,15 +1277,15 @@
         """
         pkgs = _flatten_str_args("apt_install", "packages", packages)
         if not pkgs:
             return self
 
         package_args = " ".join(shlex.quote(pkg) for pkg in pkgs)
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = [
                 "FROM base",
                 "RUN apt-get update",
                 f"RUN apt-get install -y {package_args}",
             ]
             return DockerfileSpec(commands=commands, context_files={})
 
@@ -1472,15 +1390,15 @@
                 .env({"CONDA_OVERRIDE_CUDA": "11.2"})
                 .conda_install("jax", "cuda-nvcc", channels=["conda-forge", "nvidia"])
                 .pip_install("dm-haiku", "optax")
         )
         ```
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
+        def build_dockerfile() -> DockerfileSpec:
             commands = ["FROM base"] + [f"ENV {key}={shlex.quote(val)}" for (key, val) in vars.items()]
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
         )
@@ -1496,16 +1414,16 @@
             .run_commands("git clone https://xyz app")
             .workdir("/app")
             .run_commands("yarn install")
         )
         ```
         """
 
-        def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
-            commands = ["FROM base", f"WORKDIR {shlex.quote(path)}"]
+        def build_dockerfile() -> DockerfileSpec:
+            commands = ["FROM base"] + [f"WORKDIR {shlex.quote(path)}"]
             return DockerfileSpec(commands=commands, context_files={})
 
         return _Image._from_args(
             base_images={"base": self},
             dockerfile_function=build_dockerfile,
         )
 
@@ -1545,11 +1463,12 @@
         **Usage:**
 
         ```python notest
         with image.imports():
             import torch
         ```
         """
-        deprecation_error((2023, 12, 15), Image.run_inside.__doc__)
+        deprecation_warning((2023, 12, 15), Image.run_inside.__doc__)
+        return self.imports()
 
 
 Image = synchronize_api(_Image)
```

## modal/image.pyi

```diff
@@ -5,48 +5,34 @@
 import modal.network_file_system
 import modal.object
 import modal.secret
 import modal_proto.api_pb2
 import pathlib
 import typing
 
-ImageBuilderVersion = typing.Literal['2023.12', '2024.04']
-
-def _validate_python_version(version: typing.Union[str, None], allow_micro_granularity: bool = True) -> str:
-    ...
-
-
-def _dockerhub_python_version(builder_version: typing.Literal['2023.12', '2024.04'], python_version: typing.Union[str, None] = None) -> str:
-    ...
-
-
-def _dockerhub_debian_codename(builder_version: typing.Literal['2023.12', '2024.04']) -> str:
+def _validate_python_version(version: str) -> None:
     ...
 
 
-def _get_modal_requirements_path(builder_version: typing.Literal['2023.12', '2024.04'], python_version: typing.Union[str, None] = None) -> str:
+def _dockerhub_python_version(python_version=None):
     ...
 
 
-def _get_modal_requirements_command(version: typing.Literal['2023.12', '2024.04']) -> str:
+def _get_client_requirements_path(python_version: typing.Union[str, None] = None) -> str:
     ...
 
 
 def _flatten_str_args(function_name: str, arg_name: str, args: typing.Tuple[typing.Union[str, typing.List[str]], ...]) -> typing.List[str]:
     ...
 
 
 def _make_pip_install_args(find_links: typing.Union[str, None] = None, index_url: typing.Union[str, None] = None, extra_index_url: typing.Union[str, None] = None, pre: bool = False) -> str:
     ...
 
 
-def _get_image_builder_version(client_version: str) -> typing.Literal['2023.12', '2024.04']:
-    ...
-
-
 class _ImageRegistryConfig:
     def __init__(self, registry_auth_type: int = 0, secret: typing.Union[modal.secret._Secret, None] = None):
         ...
 
     def get_proto(self) -> modal_proto.api_pb2.ImageRegistryConfig:
         ...
 
@@ -72,15 +58,15 @@
     def _initialize_from_empty(self):
         ...
 
     def _hydrate_metadata(self, message: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     @staticmethod
-    def _from_args(*, base_images: typing.Union[typing.Dict[str, _Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[typing.Literal['2023.12', '2024.04']], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret._Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions._Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False, _namespace: int = 1):
+    def _from_args(*, base_images: typing.Union[typing.Dict[str, _Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret._Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions._Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False, _namespace: int = 1):
         ...
 
     def extend(self, *, secrets: typing.Union[typing.Sequence[modal.secret._Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions._Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False, _namespace: int = 1) -> _Image:
         ...
 
     def copy_mount(self, mount: modal.mount._Mount, remote_path: typing.Union[str, pathlib.Path] = '.') -> _Image:
         ...
@@ -109,32 +95,32 @@
     def dockerfile_commands(self, *dockerfile_commands: typing.Union[str, typing.List[str]], context_files: typing.Dict[str, str] = {}, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, context_mount: typing.Union[modal.mount._Mount, None] = None, force_build: bool = False) -> _Image:
         ...
 
     def run_commands(self, *commands: typing.Union[str, typing.List[str]], secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, force_build: bool = False) -> _Image:
         ...
 
     @staticmethod
-    def conda(python_version: typing.Union[str, None] = None, force_build: bool = False) -> _Image:
+    def conda(python_version: str = '3.9', force_build: bool = False) -> _Image:
         ...
 
     def conda_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> _Image:
         ...
 
     def conda_update_from_environment(self, environment_yml: str, force_build: bool = False, *, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> _Image:
         ...
 
     @staticmethod
-    def micromamba(python_version: typing.Union[str, None] = None, force_build: bool = False) -> _Image:
+    def micromamba(python_version: str = '3.9', force_build: bool = False) -> _Image:
         ...
 
     def micromamba_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret._Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> _Image:
         ...
 
     @staticmethod
-    def _registry_setup_commands(tag: str, builder_version: typing.Literal['2023.12', '2024.04'], setup_commands: typing.List[str], add_python: typing.Union[str, None] = None) -> typing.List[str]:
+    def _registry_setup_commands(tag: str, setup_commands: typing.List[str], add_python: typing.Union[str, None]) -> typing.List[str]:
         ...
 
     @staticmethod
     def from_registry(tag: str, *, secret: typing.Union[modal.secret._Secret, None] = None, setup_dockerfile_commands: typing.List[str] = [], force_build: bool = False, add_python: typing.Union[str, None] = None, **kwargs) -> _Image:
         ...
 
     @staticmethod
@@ -182,15 +168,15 @@
     def _initialize_from_empty(self):
         ...
 
     def _hydrate_metadata(self, message: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     @staticmethod
-    def _from_args(*, base_images: typing.Union[typing.Dict[str, Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[typing.Literal['2023.12', '2024.04']], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret.Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions.Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount.Mount, None] = None, force_build: bool = False, _namespace: int = 1):
+    def _from_args(*, base_images: typing.Union[typing.Dict[str, Image], None] = None, dockerfile_function: typing.Union[typing.Callable[[], DockerfileSpec], None] = None, secrets: typing.Union[typing.Sequence[modal.secret.Secret], None] = None, gpu_config: typing.Union[modal_proto.api_pb2.GPUConfig, None] = None, build_function: typing.Union[modal.functions.Function, None] = None, build_function_input: typing.Union[modal_proto.api_pb2.FunctionInput, None] = None, image_registry_config: typing.Union[_ImageRegistryConfig, None] = None, context_mount: typing.Union[modal.mount.Mount, None] = None, force_build: bool = False, _namespace: int = 1):
         ...
 
     def extend(self, **kwargs) -> Image:
         ...
 
     def copy_mount(self, mount: modal.mount.Mount, remote_path: typing.Union[str, pathlib.Path] = '.') -> Image:
         ...
@@ -219,32 +205,32 @@
     def dockerfile_commands(self, *dockerfile_commands: typing.Union[str, typing.List[str]], context_files: typing.Dict[str, str] = {}, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, context_mount: typing.Union[modal.mount.Mount, None] = None, force_build: bool = False) -> Image:
         ...
 
     def run_commands(self, *commands: typing.Union[str, typing.List[str]], secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, force_build: bool = False) -> Image:
         ...
 
     @staticmethod
-    def conda(python_version: typing.Union[str, None] = None, force_build: bool = False) -> Image:
+    def conda(python_version: str = '3.9', force_build: bool = False) -> Image:
         ...
 
     def conda_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> Image:
         ...
 
     def conda_update_from_environment(self, environment_yml: str, force_build: bool = False, *, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> Image:
         ...
 
     @staticmethod
-    def micromamba(python_version: typing.Union[str, None] = None, force_build: bool = False) -> Image:
+    def micromamba(python_version: str = '3.9', force_build: bool = False) -> Image:
         ...
 
     def micromamba_install(self, *packages: typing.Union[str, typing.List[str]], channels: typing.List[str] = [], force_build: bool = False, secrets: typing.Sequence[modal.secret.Secret] = [], gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None) -> Image:
         ...
 
     @staticmethod
-    def _registry_setup_commands(tag: str, builder_version: typing.Literal['2023.12', '2024.04'], setup_commands: typing.List[str], add_python: typing.Union[str, None] = None) -> typing.List[str]:
+    def _registry_setup_commands(tag: str, setup_commands: typing.List[str], add_python: typing.Union[str, None]) -> typing.List[str]:
         ...
 
     @staticmethod
     def from_registry(tag: str, *, secret: typing.Union[modal.secret.Secret, None] = None, setup_dockerfile_commands: typing.List[str] = [], force_build: bool = False, add_python: typing.Union[str, None] = None, **kwargs) -> Image:
         ...
 
     @staticmethod
@@ -276,10 +262,7 @@
         ...
 
     def imports(self):
         ...
 
     def run_inside(self):
         ...
-
-
-SUPPORTED_PYTHON_SERIES: typing.Set[str]
```

## modal/mount.py

```diff
@@ -50,15 +50,15 @@
     """Get the deployed name of the python-build-standalone mount."""
     if "-" in version:  # default to glibc
         version, libc = version.split("-")
     else:
         libc = "gnu"
     if version not in PYTHON_STANDALONE_VERSIONS:
         raise modal.exception.InvalidError(
-            f"Unsupported standalone python version: {version!r}, supported values are {list(PYTHON_STANDALONE_VERSIONS)}"
+            f"Unsupported standalone python version: {version}, supported values are {list(PYTHON_STANDALONE_VERSIONS.keys())}"
         )
     if libc != "gnu":
         raise modal.exception.InvalidError(f"Unsupported libc identifier: {libc}")
     release, full_version = PYTHON_STANDALONE_VERSIONS[version]
     return f"python-build-standalone.{release}.{full_version}-{libc}"
```

## modal/network_file_system.py

```diff
@@ -20,15 +20,14 @@
 from .object import (
     EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
     _get_environment_name,
     _Object,
     live_method,
     live_method_gen,
 )
-from .volume import FileEntry
 
 NETWORK_FILE_SYSTEM_PUT_FILE_CLIENT_TIMEOUT = (
     10 * 60
 )  # 10 min max for transferring files (does not include upload time to s3)
 
 
 def network_file_system_mount_protos(
@@ -139,15 +138,15 @@
                 namespace=namespace,
                 environment_name=_get_environment_name(environment_name, resolver),
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
             )
             response = await resolver.client.stub.SharedVolumeGetOrCreate(req)
             self._hydrate(response.shared_volume_id, resolver.client, None)
 
-        return _NetworkFileSystem._from_loader(_load, "NetworkFileSystem()", hydrate_lazily=True)
+        return _NetworkFileSystem._from_loader(_load, "NetworkFileSystem()")
 
     @classmethod
     @asynccontextmanager
     async def ephemeral(
         cls: Type["_NetworkFileSystem"],
         client: Optional[_Client] = None,
         environment_name: Optional[str] = None,
@@ -290,25 +289,25 @@
         if response.WhichOneof("data_oneof") == "data":
             yield response.data
         else:
             async for data in blob_iter(response.data_blob_id, self._client.stub):
                 yield data
 
     @live_method_gen
-    async def iterdir(self, path: str) -> AsyncIterator[FileEntry]:
+    async def iterdir(self, path: str) -> AsyncIterator[api_pb2.SharedVolumeListFilesEntry]:
         """Iterate over all files in a directory in the network file system.
 
         * Passing a directory path lists all files in the directory (names are relative to the directory)
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         req = api_pb2.SharedVolumeListFilesRequest(shared_volume_id=self.object_id, path=path)
         async for batch in unary_stream(self._client.stub.SharedVolumeListFilesStream, req):
             for entry in batch.entries:
-                yield FileEntry._from_proto(entry)
+                yield entry
 
     @live_method
     async def add_local_file(
         self, local_path: Union[Path, str], remote_path: Optional[Union[str, PurePosixPath, None]] = None
     ):
         local_path = Path(local_path)
         if remote_path is None:
@@ -339,15 +338,15 @@
                     continue
                 relpath_str = subpath.relative_to(_local_path).as_posix()
                 yield self.add_local_file(subpath, PurePosixPath(remote_path, relpath_str))
 
         await ConcurrencyPool(20).run_coros(gen_transfers(), return_exceptions=True)
 
     @live_method
-    async def listdir(self, path: str) -> List[FileEntry]:
+    async def listdir(self, path: str) -> List[api_pb2.SharedVolumeListFilesEntry]:
         """List all files in a directory in the network file system.
 
         * Passing a directory path lists all files in the directory (names are relative to the directory)
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         return [entry async for entry in self.iterdir(path)]
```

## modal/network_file_system.pyi

```diff
@@ -1,10 +1,9 @@
 import modal.client
 import modal.object
-import modal.volume
 import modal_proto.api_pb2
 import pathlib
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
 def network_file_system_mount_protos(validated_network_file_systems: typing.List[typing.Tuple[str, _NetworkFileSystem]], allow_cross_region_volumes: bool) -> typing.List[modal_proto.api_pb2.SharedVolumeMount]:
@@ -41,24 +40,24 @@
 
     async def write_file(self, remote_path: str, fp: typing.BinaryIO) -> int:
         ...
 
     def read_file(self, path: str) -> typing.AsyncIterator[bytes]:
         ...
 
-    def iterdir(self, path: str) -> typing.AsyncIterator[modal.volume.FileEntry]:
+    def iterdir(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
         ...
 
     async def add_local_file(self, local_path: typing.Union[pathlib.Path, str], remote_path: typing.Union[str, pathlib.PurePosixPath, None] = None):
         ...
 
     async def add_local_dir(self, local_path: typing.Union[pathlib.Path, str], remote_path: typing.Union[str, pathlib.PurePosixPath, None] = None):
         ...
 
-    async def listdir(self, path: str) -> typing.List[modal.volume.FileEntry]:
+    async def listdir(self, path: str) -> typing.List[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
         ...
 
     async def remove_file(self, path: str, recursive=False):
         ...
 
 
 class NetworkFileSystem(modal.object.Object):
@@ -117,18 +116,18 @@
 
         def aio(self, path: str) -> typing.AsyncIterator[bytes]:
             ...
 
     read_file: __read_file_spec
 
     class __iterdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.Iterator[modal.volume.FileEntry]:
+        def __call__(self, path: str) -> typing.Iterator[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
-        def aio(self, path: str) -> typing.AsyncIterator[modal.volume.FileEntry]:
+        def aio(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
     iterdir: __iterdir_spec
 
     class __add_local_file_spec(typing_extensions.Protocol):
         def __call__(self, local_path: typing.Union[pathlib.Path, str], remote_path: typing.Union[str, pathlib.PurePosixPath, None] = None):
             ...
@@ -144,18 +143,18 @@
 
         async def aio(self, *args, **kwargs):
             ...
 
     add_local_dir: __add_local_dir_spec
 
     class __listdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.List[modal.volume.FileEntry]:
+        def __call__(self, path: str) -> typing.List[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
-        async def aio(self, *args, **kwargs) -> typing.List[modal.volume.FileEntry]:
+        async def aio(self, *args, **kwargs) -> typing.List[modal_proto.api_pb2.SharedVolumeListFilesEntry]:
             ...
 
     listdir: __listdir_spec
 
     class __remove_file_spec(typing_extensions.Protocol):
         def __call__(self, path: str, recursive=False):
             ...
```

## modal/object.py

```diff
@@ -207,16 +207,15 @@
             raise ExecutionError(
                 "Object has not been hydrated and doesn't support lazy hydration."
                 " This might happen if an object is defined on a different stub,"
                 " or if it's on the same stub but it didn't get created because it"
                 " wasn't defined in global scope."
             )
         else:
-            # TODO: this client and/or resolver can't be changed by a caller to X.from_name()
-            resolver = Resolver(await _Client.from_env())
+            resolver = Resolver()  # TODO: this resolver has no attached Client!
             await resolver.load(self)
 
 
 Object = synchronize_api(_Object, target_module=__name__)
 
 
 def live_method(method):
```

## modal/partial_function.pyi

```diff
@@ -1,18 +1,24 @@
 import enum
 import modal.functions
 import modal_proto.api_pb2
 import typing
 
 class _PartialFunctionFlags(enum.IntFlag):
-    FUNCTION: int = 1
-    BUILD: int = 2
-    ENTER_PRE_CHECKPOINT: int = 4
-    ENTER_POST_CHECKPOINT: int = 8
-    EXIT: int = 16
+    FUNCTION: int
+    BUILD: int
+    ENTER_PRE_CHECKPOINT: int
+    ENTER_POST_CHECKPOINT: int
+    EXIT: int
+
+    def _generate_next_value_(name, start, count, last_values):
+        ...
+
+    def __new__(cls, value):
+        ...
 
 
 class _PartialFunction:
     raw_f: typing.Callable[..., typing.Any]
     flags: _PartialFunctionFlags
     webhook_config: typing.Union[modal_proto.api_pb2.WebhookConfig, None]
     is_generator: typing.Union[bool, None]
```

## modal/queue.py

```diff
@@ -1,57 +1,37 @@
 # Copyright Modal Labs 2022
 import queue  # The system library
 import time
 import warnings
-from typing import Any, AsyncGenerator, AsyncIterator, List, Optional, Type
+from typing import Any, AsyncIterator, List, Optional, Type
 
 from grpclib import GRPCError, Status
 from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_pb2
 
 from ._resolver import Resolver
 from ._serialization import deserialize, serialize
-from ._utils.async_utils import TaskContext, synchronize_api, warn_if_generator_is_not_consumed
+from ._utils.async_utils import TaskContext, synchronize_api
 from ._utils.grpc_utils import retry_transient_errors
 from .client import _Client
-from .exception import InvalidError, deprecation_warning
-from .object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
+from .exception import deprecation_warning
+from .object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method
 
 
 class _Queue(_Object, type_prefix="qu"):
     """Distributed, FIFO queue for data flow in Modal apps.
 
     The queue can contain any object serializable by `cloudpickle`, including Modal objects.
 
-    By default, the `Queue` object acts as a single FIFO queue which supports puts and gets (blocking and non-blocking).
+    **Lifetime of a queue and its contents**
 
-    **Queue partitions (beta)**
-
-    Specifying partition keys gives access to other independent FIFO partitions within the same `Queue` object.
-    Across any two partitions, puts and gets are completely independent. For example, a put in one partition does not affect
-    a get in any other partition.
-
-    When no partition key is specified (by default), puts and gets will operate on a default partition. This default partition
-    is also isolated from all other partitions. Please see the Usage section below for an example using partitions.
-
-    **Lifetime of a queue and its partitions**
-
-    By default, each partition is cleared 24 hours after the last `put` operation. A lower TTL can be specified by the `partition_ttl`
-    argument in the `put` or `put_many` methods. Each partition's expiry is handled independently.
-
-    As such, `Queue`s are best used for communication between active functions and not relied on for persistent storage.
-
-    On app completion or after stopping an app any associated `Queue` objects are cleaned up. All its partitions will be cleared.
-
-    **Limits**
-
-    A single `Queue` can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 256 KiB.
-
-    Partition keys must be non-empty and must not exceed 64 bytes.
+    A `Queue`'s lifetime matches the lifetime of the app it's attached to, but the contents expire after 30 days.
+    Because of this, `Queues`s are best used for communication between active functions and not relied on for
+    persistent storage. On app completion or after stopping an app any associated `Queue` objects are cleaned up.
 
     **Usage**
 
     ```python
     from modal import Queue, Stub
 
     stub = Stub()
@@ -60,29 +40,14 @@
     @stub.local_entrypoint()
     def main():
         my_queue.put("some value")
         my_queue.put(123)
 
         assert my_queue.get() == "some value"
         assert my_queue.get() == 123
-
-        my_queue.put(0)
-        my_queue.put(1, partition_key="foo")
-        my_queue.put(2, partition_key="bar")
-
-        # Default and "foo" partition are ignored by the get operation.
-        assert my_queue.get(partition_key="bar") == 2
-
-        # Set custom 10s expiration time on "foo" partition.
-        my_queue.put(3, partition_key="foo", partition_ttl=10)
-
-        # (beta feature) Iterate through items in place (read immutably)
-        my_queue.put(1)
-        assert [v for v in my_queue.iterate()] == [0, 1]
-
     ```
 
     For more examples, see the [guide](/docs/guide/dicts-and-queues#modal-queues).
     """
 
     @staticmethod
     def new():
@@ -99,25 +64,14 @@
 
         return _Queue._from_loader(_load, "Queue()")
 
     def __init__(self):
         """mdmd:hidden"""
         raise RuntimeError("Queue() is not allowed. Please use `Queue.from_name(...)` or `Queue.ephemeral()` instead.")
 
-    @staticmethod
-    def validate_partition_key(partition: Optional[str]) -> bytes:
-        if partition is not None:
-            partition_key = partition.encode("utf-8")
-            if len(partition_key) == 0 or len(partition_key) > 64:
-                raise InvalidError("Queue partition key must be between 1 and 64 characters.")
-        else:
-            partition_key = b""
-
-        return partition_key
-
     @classmethod
     @asynccontextmanager
     async def ephemeral(
         cls: Type["_Queue"],
         client: Optional[_Client] = None,
         environment_name: Optional[str] = None,
         _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
@@ -171,15 +125,15 @@
                 namespace=namespace,
                 environment_name=_get_environment_name(environment_name, resolver),
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
             )
             response = await resolver.client.stub.QueueGetOrCreate(req)
             self._hydrate(response.queue_id, resolver.client, None)
 
-        return _Queue._from_loader(_load, "Queue()", is_another_app=True, hydrate_lazily=True)
+        return _Queue._from_loader(_load, "Queue()", is_another_app=True)
 
     @staticmethod
     def persisted(
         label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None
     ) -> "_Queue":
         """Deprecated! Use `Queue.from_name(name, create_if_missing=True)`."""
         deprecation_warning((2024, 3, 1), _Queue.persisted.__doc__)
@@ -205,43 +159,41 @@
         )
         if client is None:
             client = await _Client.from_env()
         resolver = Resolver(client=client)
         await resolver.load(obj)
         return obj
 
-    async def _get_nonblocking(self, partition: Optional[str], n_values: int) -> List[Any]:
+    async def _get_nonblocking(self, n_values: int) -> List[Any]:
         request = api_pb2.QueueGetRequest(
             queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
             timeout=0,
             n_values=n_values,
         )
 
         response = await retry_transient_errors(self._client.stub.QueueGet, request)
         if response.values:
             return [deserialize(value, self._client) for value in response.values]
         else:
             return []
 
-    async def _get_blocking(self, partition: Optional[str], timeout: Optional[float], n_values: int) -> List[Any]:
+    async def _get_blocking(self, timeout: Optional[float], n_values: int) -> List[Any]:
         if timeout is not None:
             deadline = time.time() + timeout
         else:
             deadline = None
 
         while True:
             request_timeout = 50.0  # We prevent longer ones in order to keep the connection alive
 
             if deadline is not None:
                 request_timeout = min(request_timeout, deadline - time.time())
 
             request = api_pb2.QueueGetRequest(
                 queue_id=self.object_id,
-                partition_key=self.validate_partition_key(partition),
                 timeout=request_timeout,
                 n_values=n_values,
             )
 
             response = await retry_transient_errors(self._client.stub.QueueGet, request)
 
             if response.values:
@@ -249,183 +201,119 @@
 
             if deadline is not None and time.time() > deadline:
                 break
 
         raise queue.Empty()
 
     @live_method
-    async def get(
-        self, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
-    ) -> Optional[Any]:
+    async def get(self, block: bool = True, timeout: Optional[float] = None) -> Optional[Any]:
         """Remove and return the next object in the queue.
 
         If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
         an object, or until `timeout` if specified. Raises a native `queue.Empty` exception
         if the `timeout` is reached.
 
         If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
         ignored in this case.
         """
 
         if block:
-            values = await self._get_blocking(partition, timeout, 1)
+            values = await self._get_blocking(timeout, 1)
         else:
             if timeout is not None:
                 warnings.warn("Timeout is ignored for non-blocking get.")
-            values = await self._get_nonblocking(partition, 1)
+            values = await self._get_nonblocking(1)
 
         if values:
             return values[0]
         else:
             return None
 
     @live_method
-    async def get_many(
-        self, n_values: int, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
-    ) -> List[Any]:
+    async def get_many(self, n_values: int, block: bool = True, timeout: Optional[float] = None) -> List[Any]:
         """Remove and return up to `n_values` objects from the queue.
 
         If there are fewer than `n_values` items in the queue, return all of them.
 
         If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
         at least 1 object to be present, or until `timeout` if specified. Raises the stdlib's `queue.Empty`
         exception if the `timeout` is reached.
 
         If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
         ignored in this case.
         """
 
         if block:
-            return await self._get_blocking(partition, timeout, n_values)
+            return await self._get_blocking(timeout, n_values)
         else:
             if timeout is not None:
                 warnings.warn("Timeout is ignored for non-blocking get.")
-            return await self._get_nonblocking(partition, n_values)
+            return await self._get_nonblocking(n_values)
 
     @live_method
-    async def put(
-        self,
-        v: Any,
-        block: bool = True,
-        timeout: Optional[float] = None,
-        *,
-        partition: Optional[str] = None,
-        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
-    ) -> None:
+    async def put(self, v: Any, block: bool = True, timeout: Optional[float] = None) -> None:
         """Add an object to the end of the queue.
 
         If `block` is `True` and the queue is full, this method will retry indefinitely or
         until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
         If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.
 
         If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
         ignored in this case."""
-        await self.put_many([v], block, timeout, partition=partition, partition_ttl=partition_ttl)
+        await self.put_many([v], block, timeout)
 
     @live_method
-    async def put_many(
-        self,
-        vs: List[Any],
-        block: bool = True,
-        timeout: Optional[float] = None,
-        *,
-        partition: Optional[str] = None,
-        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
-    ) -> None:
+    async def put_many(self, vs: List[Any], block: bool = True, timeout: Optional[float] = None) -> None:
         """Add several objects to the end of the queue.
 
         If `block` is `True` and the queue is full, this method will retry indefinitely or
         until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
         If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.
 
         If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
-        ignored in this case.
-        """
+        ignored in this case."""
         if block:
-            await self._put_many_blocking(partition, partition_ttl, vs, timeout)
+            await self._put_many_blocking(vs, timeout)
         else:
             if timeout is not None:
                 warnings.warn("`timeout` argument is ignored for non-blocking put.")
-            await self._put_many_nonblocking(partition, partition_ttl, vs)
+            await self._put_many_nonblocking(vs)
 
-    async def _put_many_blocking(
-        self, partition: Optional[str], partition_ttl: int, vs: List[Any], timeout: Optional[float] = None
-    ):
+    async def _put_many_blocking(self, vs: List[Any], timeout: Optional[float] = None):
         vs_encoded = [serialize(v) for v in vs]
 
         request = api_pb2.QueuePutRequest(
             queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
             values=vs_encoded,
-            partition_ttl_seconds=partition_ttl,
         )
         try:
             await retry_transient_errors(
                 self._client.stub.QueuePut,
                 request,
                 # A full queue will return this status.
                 additional_status_codes=[Status.RESOURCE_EXHAUSTED],
                 max_delay=30.0,
                 total_timeout=timeout,
             )
         except GRPCError as exc:
             raise queue.Full(str(exc)) if exc.status == Status.RESOURCE_EXHAUSTED else exc
 
-    async def _put_many_nonblocking(self, partition: Optional[str], partition_ttl: int, vs: List[Any]):
+    async def _put_many_nonblocking(self, vs: List[Any]):
         vs_encoded = [serialize(v) for v in vs]
         request = api_pb2.QueuePutRequest(
             queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
             values=vs_encoded,
-            partition_ttl_seconds=partition_ttl,
         )
         try:
             await retry_transient_errors(self._client.stub.QueuePut, request)
         except GRPCError as exc:
             raise queue.Full(exc.message) if exc.status == Status.RESOURCE_EXHAUSTED else exc
 
     @live_method
-    async def len(self, *, partition: Optional[str] = None) -> int:
-        """Return the number of objects in the queue partition."""
-        request = api_pb2.QueueLenRequest(
-            queue_id=self.object_id,
-            partition_key=self.validate_partition_key(partition),
-        )
+    async def len(self) -> int:
+        """Return the number of objects in the queue."""
+        request = api_pb2.QueueLenRequest(queue_id=self.object_id)
         response = await retry_transient_errors(self._client.stub.QueueLen, request)
         return response.len
 
-    @warn_if_generator_is_not_consumed()
-    @live_method_gen
-    async def iterate(
-        self, *, partition: Optional[str] = None, item_poll_timeout: float = 0.0
-    ) -> AsyncGenerator[Any, None]:
-        """(Beta feature) Iterate through items in the queue without mutation.
-
-        Specify `item_poll_timeout` to control how long the iterator should wait for the next time before giving up.
-        """
-        last_entry_id: Optional[str] = None
-        validated_partition_key = self.validate_partition_key(partition)
-        fetch_deadline = time.time() + item_poll_timeout
-
-        MAX_POLL_DURATION = 30.0
-        while True:
-            poll_duration = max(0.0, min(MAX_POLL_DURATION, fetch_deadline - time.time()))
-            request = api_pb2.QueueNextItemsRequest(
-                queue_id=self.object_id,
-                partition_key=validated_partition_key,
-                last_entry_id=last_entry_id,
-                item_poll_timeout=poll_duration,
-            )
-
-            response: api_pb2.QueueNextItemsResponse = await retry_transient_errors(
-                self._client.stub.QueueNextItems, request
-            )
-            if response.items:
-                for item in response.items:
-                    yield deserialize(item.value, self._client)
-                    last_entry_id = item.entry_id
-                fetch_deadline = time.time() + item_poll_timeout
-            elif time.time() > fetch_deadline:
-                break
-
 
 Queue = synchronize_api(_Queue)
```

## modal/queue.pyi

```diff
@@ -8,18 +8,14 @@
     @staticmethod
     def new():
         ...
 
     def __init__(self):
         ...
 
-    @staticmethod
-    def validate_partition_key(partition: typing.Union[str, None]) -> bytes:
-        ...
-
     @classmethod
     def ephemeral(cls: typing.Type[_Queue], client: typing.Union[modal.client._Client, None] = None, environment_name: typing.Union[str, None] = None, _heartbeat_sleep: float = 300) -> typing.AsyncContextManager[_Queue]:
         ...
 
     @staticmethod
     def from_name(label: str, namespace=1, environment_name: typing.Union[str, None] = None, create_if_missing: bool = False) -> _Queue:
         ...
@@ -28,57 +24,50 @@
     def persisted(label: str, namespace=1, environment_name: typing.Union[str, None] = None) -> _Queue:
         ...
 
     @staticmethod
     async def lookup(label: str, namespace=1, client: typing.Union[modal.client._Client, None] = None, environment_name: typing.Union[str, None] = None, create_if_missing: bool = False) -> _Queue:
         ...
 
-    async def _get_nonblocking(self, partition: typing.Union[str, None], n_values: int) -> typing.List[typing.Any]:
+    async def _get_nonblocking(self, n_values: int) -> typing.List[typing.Any]:
         ...
 
-    async def _get_blocking(self, partition: typing.Union[str, None], timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
+    async def _get_blocking(self, timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
         ...
 
-    async def get(self, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.Union[typing.Any, None]:
+    async def get(self, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.Union[typing.Any, None]:
         ...
 
-    async def get_many(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.List[typing.Any]:
+    async def get_many(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.List[typing.Any]:
         ...
 
-    async def put(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+    async def put(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None) -> None:
         ...
 
-    async def put_many(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+    async def put_many(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None) -> None:
         ...
 
-    async def _put_many_blocking(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
+    async def _put_many_blocking(self, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
         ...
 
-    async def _put_many_nonblocking(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any]):
+    async def _put_many_nonblocking(self, vs: typing.List[typing.Any]):
         ...
 
-    async def len(self, *, partition: typing.Union[str, None] = None) -> int:
-        ...
-
-    def iterate(self, *, partition: typing.Union[str, None] = None, item_poll_timeout: float = 0.0) -> typing.AsyncGenerator[typing.Any, None]:
+    async def len(self) -> int:
         ...
 
 
 class Queue(modal.object.Object):
     def __init__(self):
         ...
 
     @staticmethod
     def new():
         ...
 
-    @staticmethod
-    def validate_partition_key(partition: typing.Union[str, None]) -> bytes:
-        ...
-
     @classmethod
     def ephemeral(cls: typing.Type[Queue], client: typing.Union[modal.client.Client, None] = None, environment_name: typing.Union[str, None] = None, _heartbeat_sleep: float = 300) -> synchronicity.combined_types.AsyncAndBlockingContextManager[Queue]:
         ...
 
     @staticmethod
     def from_name(label: str, namespace=1, environment_name: typing.Union[str, None] = None, create_if_missing: bool = False) -> Queue:
         ...
@@ -93,95 +82,86 @@
 
         async def aio(self, *args, **kwargs) -> Queue:
             ...
 
     lookup: __lookup_spec
 
     class ___get_nonblocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], n_values: int) -> typing.List[typing.Any]:
+        def __call__(self, n_values: int) -> typing.List[typing.Any]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.List[typing.Any]:
             ...
 
     _get_nonblocking: ___get_nonblocking_spec
 
     class ___get_blocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
+        def __call__(self, timeout: typing.Union[float, None], n_values: int) -> typing.List[typing.Any]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.List[typing.Any]:
             ...
 
     _get_blocking: ___get_blocking_spec
 
     class __get_spec(typing_extensions.Protocol):
-        def __call__(self, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.Union[typing.Any, None]:
+        def __call__(self, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.Union[typing.Any, None]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.Union[typing.Any, None]:
             ...
 
     get: __get_spec
 
     class __get_many_spec(typing_extensions.Protocol):
-        def __call__(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None) -> typing.List[typing.Any]:
+        def __call__(self, n_values: int, block: bool = True, timeout: typing.Union[float, None] = None) -> typing.List[typing.Any]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.List[typing.Any]:
             ...
 
     get_many: __get_many_spec
 
     class __put_spec(typing_extensions.Protocol):
-        def __call__(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+        def __call__(self, v: typing.Any, block: bool = True, timeout: typing.Union[float, None] = None) -> None:
             ...
 
         async def aio(self, *args, **kwargs) -> None:
             ...
 
     put: __put_spec
 
     class __put_many_spec(typing_extensions.Protocol):
-        def __call__(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None, *, partition: typing.Union[str, None] = None, partition_ttl: int = 86400) -> None:
+        def __call__(self, vs: typing.List[typing.Any], block: bool = True, timeout: typing.Union[float, None] = None) -> None:
             ...
 
         async def aio(self, *args, **kwargs) -> None:
             ...
 
     put_many: __put_many_spec
 
     class ___put_many_blocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
+        def __call__(self, vs: typing.List[typing.Any], timeout: typing.Union[float, None] = None):
             ...
 
         async def aio(self, *args, **kwargs):
             ...
 
     _put_many_blocking: ___put_many_blocking_spec
 
     class ___put_many_nonblocking_spec(typing_extensions.Protocol):
-        def __call__(self, partition: typing.Union[str, None], partition_ttl: int, vs: typing.List[typing.Any]):
+        def __call__(self, vs: typing.List[typing.Any]):
             ...
 
         async def aio(self, *args, **kwargs):
             ...
 
     _put_many_nonblocking: ___put_many_nonblocking_spec
 
     class __len_spec(typing_extensions.Protocol):
-        def __call__(self, *, partition: typing.Union[str, None] = None) -> int:
+        def __call__(self) -> int:
             ...
 
         async def aio(self, *args, **kwargs) -> int:
             ...
 
     len: __len_spec
-
-    class __iterate_spec(typing_extensions.Protocol):
-        def __call__(self, *, partition: typing.Union[str, None] = None, item_poll_timeout: float = 0.0) -> typing.Generator[typing.Any, None, None]:
-            ...
-
-        def aio(self, *, partition: typing.Union[str, None] = None, item_poll_timeout: float = 0.0) -> typing.AsyncGenerator[typing.Any, None]:
-            ...
-
-    iterate: __iterate_spec
```

## modal/runner.py

```diff
@@ -1,207 +1,67 @@
 # Copyright Modal Labs 2022
 import asyncio
 import dataclasses
 import os
 from multiprocessing.synchronize import Event
-from typing import TYPE_CHECKING, AsyncGenerator, Dict, List, Optional, TypeVar
+from typing import TYPE_CHECKING, AsyncGenerator, List, Optional, TypeVar
 
-from grpclib import GRPCError, Status
 from rich.console import Console
 from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_pb2
 
-from ._container_io_manager import is_local
 from ._output import OutputManager, get_app_logs_loop, step_completed, step_progress
 from ._pty import get_pty_info
-from ._resolver import Resolver
 from ._sandbox_shell import connect_to_sandbox
 from ._utils.app_utils import is_valid_app_name
 from ._utils.async_utils import TaskContext, synchronize_api
 from ._utils.grpc_utils import retry_transient_errors
+from .app import _LocalApp, is_local
 from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
-from .config import config, logger
-from .exception import ExecutionError, InteractiveTimeoutError, InvalidError, _CliUserExecutionError
-from .object import _Object
-from .running_app import RunningApp
+from .config import config
+from .exception import InteractiveTimeoutError, InvalidError, _CliUserExecutionError
 
 if TYPE_CHECKING:
-    from .stub import _App
+    from .stub import _Stub
 else:
-    _App = TypeVar("_App")
+    _Stub = TypeVar("_Stub")
 
 
 async def _heartbeat(client, app_id):
     request = api_pb2.AppHeartbeatRequest(app_id=app_id)
     # TODO(erikbern): we should capture exceptions here
     # * if request fails: destroy the client
     # * if server says the app is gone: print a helpful warning about detaching
     await retry_transient_errors(client.stub.AppHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT)
 
 
-async def _init_local_app_existing(client: _Client, existing_app_id: str) -> "RunningApp":
-    # Get all the objects first
-    obj_req = api_pb2.AppGetObjectsRequest(app_id=existing_app_id)
-    obj_resp = await retry_transient_errors(client.stub.AppGetObjects, obj_req)
-    app_page_url = f"https://modal.com/apps/{existing_app_id}"  # TODO (elias): this should come from the backend
-    object_ids = {item.tag: item.object.object_id for item in obj_resp.items}
-    return RunningApp(existing_app_id, app_page_url=app_page_url, tag_to_object_id=object_ids)
-
-
-async def _init_local_app_new(
-    client: _Client,
-    description: str,
-    app_state: int,
-    environment_name: str = "",
-    interactive=False,
-) -> "RunningApp":
-    app_req = api_pb2.AppCreateRequest(
-        description=description,
-        environment_name=environment_name,
-        app_state=app_state,
-    )
-    app_resp = await retry_transient_errors(client.stub.AppCreate, app_req)
-    app_page_url = app_resp.app_logs_url
-    logger.debug(f"Created new app with id {app_resp.app_id}")
-    return RunningApp(
-        app_resp.app_id, app_page_url=app_page_url, environment_name=environment_name, interactive=interactive
-    )
-
-
-async def _init_local_app_from_name(
-    client: _Client,
-    name: str,
-    namespace,
-    environment_name: str = "",
-):
-    # Look up any existing deployment
-    app_req = api_pb2.AppGetByDeploymentNameRequest(
-        name=name,
-        namespace=namespace,
-        environment_name=environment_name,
-    )
-    app_resp = await retry_transient_errors(client.stub.AppGetByDeploymentName, app_req)
-    existing_app_id = app_resp.app_id or None
-
-    # Grab the app
-    if existing_app_id is not None:
-        return await _init_local_app_existing(client, existing_app_id)
-    else:
-        return await _init_local_app_new(
-            client, name, api_pb2.APP_STATE_INITIALIZING, environment_name=environment_name
-        )
-
-
-async def _create_all_objects(
-    client: _Client,
-    app: RunningApp,
-    indexed_objects: Dict[str, _Object],
-    new_app_state: int,
-    environment_name: str,
-    output_mgr: Optional[OutputManager] = None,
-):  # api_pb2.AppState.V
-    """Create objects that have been defined but not created on the server."""
-    if not client.authenticated:
-        raise ExecutionError("Objects cannot be created with an unauthenticated client")
-
-    resolver = Resolver(
-        client,
-        output_mgr=output_mgr,
-        environment_name=environment_name,
-        app_id=app.app_id,
-    )
-    with resolver.display():
-        # Get current objects, and reset all objects
-        tag_to_object_id = app.tag_to_object_id
-        app.tag_to_object_id = {}
-
-        # Assign all objects
-        for tag, obj in indexed_objects.items():
-            # Reset object_id in case the app runs twice
-            # TODO(erikbern): clean up the interface
-            obj._unhydrate()
-
-        # Preload all functions to make sure they have ids assigned before they are loaded.
-        # This is important to make sure any enclosed function handle references in serialized
-        # functions have ids assigned to them when the function is serialized.
-        # Note: when handles/objs are merged, all objects will need to get ids pre-assigned
-        # like this in order to be referrable within serialized functions
-        for tag, obj in indexed_objects.items():
-            existing_object_id = tag_to_object_id.get(tag)
-            # Note: preload only currently implemented for Functions, returns None otherwise
-            # this is to ensure that directly referenced functions from the global scope has
-            # ids associated with them when they are serialized into other functions
-            await resolver.preload(obj, existing_object_id)
-            if obj.object_id is not None:
-                tag_to_object_id[tag] = obj.object_id
-
-        for tag, obj in indexed_objects.items():
-            existing_object_id = tag_to_object_id.get(tag)
-            await resolver.load(obj, existing_object_id)
-            app.tag_to_object_id[tag] = obj.object_id
-
-    # Create the app (and send a list of all tagged obs)
-    # TODO(erikbern): we should delete objects from a previous version that are no longer needed
-    # We just delete them from the app, but the actual objects will stay around
-    indexed_object_ids = app.tag_to_object_id
-    assert indexed_object_ids == app.tag_to_object_id
-    all_objects = resolver.objects()
-
-    unindexed_object_ids = list(set(obj.object_id for obj in all_objects) - set(app.tag_to_object_id.values()))
-    req_set = api_pb2.AppSetObjectsRequest(
-        app_id=app.app_id,
-        indexed_object_ids=indexed_object_ids,
-        unindexed_object_ids=unindexed_object_ids,
-        new_app_state=new_app_state,  # type: ignore
-    )
-    await retry_transient_errors(client.stub.AppSetObjects, req_set)
-
-
-async def _disconnect(
-    client: _Client,
-    app_id: str,
-    reason: "Optional[api_pb2.AppDisconnectReason.ValueType]" = None,
-    exc_str: Optional[str] = None,
-):
-    """Tell the server the client has disconnected for this app. Terminates all running tasks
-    for ephemeral apps."""
-
-    if exc_str:
-        exc_str = exc_str[:1000]  # Truncate to 1000 chars
-
-    logger.debug("Sending app disconnect/stop request")
-    req_disconnect = api_pb2.AppClientDisconnectRequest(app_id=app_id, reason=reason, exception=exc_str)
-    await retry_transient_errors(client.stub.AppClientDisconnect, req_disconnect)
-    logger.debug("App disconnected")
-
-
 @asynccontextmanager
-async def _run_app(
-    stub: _App,
+async def _run_stub(
+    stub: _Stub,
     client: Optional[_Client] = None,
     stdout=None,
     show_progress: bool = True,
     detach: bool = False,
     output_mgr: Optional[OutputManager] = None,
     environment_name: Optional[str] = None,
     shell=False,
     interactive=False,
-) -> AsyncGenerator[_App, None]:
+) -> AsyncGenerator[_Stub, None]:
     """mdmd:hidden"""
     if environment_name is None:
         environment_name = config.get("environment")
 
     if not is_local():
         raise InvalidError(
             "Can not run an app from within a container."
             " Are you calling stub.run() directly?"
             " Consider using the `modal run` shell command."
         )
-    if stub._running_app:
+    if stub._local_app:
         raise InvalidError(
             "App is already running and can't be started again.\n"
             "You should not use `stub.run` or `run_stub` within a Modal `local_entrypoint`"
         )
 
     if stub.description is None:
         import __main__
@@ -216,40 +76,38 @@
     if client is None:
         client = await _Client.from_env()
     if output_mgr is None:
         output_mgr = OutputManager(stdout, show_progress, "Running app...")
     if shell:
         output_mgr._visible_progress = False
     app_state = api_pb2.APP_STATE_DETACHED if detach else api_pb2.APP_STATE_EPHEMERAL
-    app = await _init_local_app_new(
+    app = await _LocalApp._init_new(
         client,
         stub.description,
         environment_name=environment_name,
         app_state=app_state,
         interactive=interactive,
     )
-    async with stub._set_local_app(client, app), TaskContext(grace=config["logs_timeout"]) as tc:
+    async with stub._set_local_app(app), TaskContext(grace=config["logs_timeout"]) as tc:
         # Start heartbeats loop to keep the client alive
         tc.infinite_loop(lambda: _heartbeat(client, app.app_id), sleep=HEARTBEAT_INTERVAL)
 
         with output_mgr.ctx_if_visible(output_mgr.make_live(step_progress("Initializing..."))):
-            initialized_msg = f"Initialized. [grey70]View run at [underline]{app.app_page_url}[/underline][/grey70]"
+            initialized_msg = f"Initialized. [grey70]View run at [underline]{app.log_url()}[/underline][/grey70]"
             output_mgr.print_if_visible(step_completed(initialized_msg))
-            output_mgr.update_app_page_url(app.app_page_url)
+            output_mgr.update_app_page_url(app.log_url())
 
         # Start logs loop
         if not shell:
             logs_loop = tc.create_task(get_app_logs_loop(app.app_id, client, output_mgr))
 
         exc_info: Optional[BaseException] = None
         try:
             # Create all members
-            await _create_all_objects(
-                client, app, stub._indexed_objects, app_state, environment_name, output_mgr=output_mgr
-            )
+            await app._create_all_objects(stub._indexed_objects, app_state, environment_name, output_mgr=output_mgr)
 
             # Update all functions client-side to have the output mgr
             for obj in stub.registered_functions.values():
                 obj._set_output_mgr(output_mgr)
 
             # Update all the classes client-side to propagate output manager to their methods.
             for obj in stub.registered_classes.values():
@@ -270,23 +128,21 @@
             # mute cancellation errors on all function handles to prevent exception spam
             for obj in stub.registered_functions.values():
                 obj._set_mute_cancellation(True)
 
             if detach:
                 output_mgr.print_if_visible(step_completed("Shutting down Modal client."))
                 output_mgr.print_if_visible(
-                    f"""The detached app keeps running. You can track its progress at: [magenta]{app.app_page_url}[/magenta]"""
+                    f"""The detached app keeps running. You can track its progress at: [magenta]{app.log_url()}[/magenta]"""
                 )
                 if not shell:
                     logs_loop.cancel()
             else:
                 output_mgr.print_if_visible(
-                    step_completed(
-                        f"App aborted. [grey70]View run at [underline]{app.app_page_url}[/underline][/grey70]"
-                    )
+                    step_completed(f"App aborted. [grey70]View run at [underline]{app.log_url()}[/underline][/grey70]")
                 )
                 output_mgr.print_if_visible(
                     "Disconnecting from Modal - This will terminate your Modal app in a few seconds.\n"
                 )
         except BaseException as e:
             exc_info = e
             raise e
@@ -301,38 +157,38 @@
             if isinstance(exc_info, _CliUserExecutionError):
                 exc_str = repr(exc_info.__cause__)
             elif exc_info:
                 exc_str = repr(exc_info)
             else:
                 exc_str = ""
 
-            await _disconnect(client, app.app_id, reason, exc_str)
+            await app.disconnect(reason, exc_str)
             stub._uncreate_all_objects()
 
     output_mgr.print_if_visible(
-        step_completed(f"App completed. [grey70]View run at [underline]{app.app_page_url}[/underline][/grey70]")
+        step_completed(f"App completed. [grey70]View run at [underline]{app.log_url()}[/underline][/grey70]")
     )
 
 
 async def _serve_update(
     stub,
     existing_app_id: str,
     is_ready: Event,
     environment_name: str,
 ) -> None:
     """mdmd:hidden"""
     # Used by child process to reinitialize a served app
     client = await _Client.from_env()
     try:
-        app = await _init_local_app_existing(client, existing_app_id)
+        app = await _LocalApp._init_existing(client, existing_app_id)
 
         # Create objects
         output_mgr = OutputManager(None, True)
-        await _create_all_objects(
-            client, app, stub._indexed_objects, api_pb2.APP_STATE_UNSPECIFIED, environment_name, output_mgr=output_mgr
+        await app._create_all_objects(
+            stub._indexed_objects, api_pb2.APP_STATE_UNSPECIFIED, environment_name, output_mgr=output_mgr
         )
 
         # Communicate to the parent process
         is_ready.set()
     except asyncio.exceptions.CancelledError:
         # Stopped by parent process
         pass
@@ -341,16 +197,16 @@
 @dataclasses.dataclass(frozen=True)
 class DeployResult:
     """Dataclass representing the result of deploying an app."""
 
     app_id: str
 
 
-async def _deploy_app(
-    stub: _App,
+async def _deploy_stub(
+    stub: _Stub,
     name: str = None,
     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
     client=None,
     stdout=None,
     show_progress=True,
     environment_name: Optional[str] = None,
     public: bool = False,
@@ -397,65 +253,43 @@
         )
 
     if client is None:
         client = await _Client.from_env()
 
     output_mgr = OutputManager(stdout, show_progress)
 
-    app = await _init_local_app_from_name(client, name, namespace, environment_name=environment_name)
+    app = await _LocalApp._init_from_name(client, name, namespace, environment_name=environment_name)
 
     async with TaskContext(0) as tc:
         # Start heartbeats loop to keep the client alive
         tc.infinite_loop(lambda: _heartbeat(client, app.app_id), sleep=HEARTBEAT_INTERVAL)
 
         # Don't change the app state - deploy state is set by AppDeploy
         post_init_state = api_pb2.APP_STATE_UNSPECIFIED
 
         try:
             # Create all members
-            await _create_all_objects(
-                client,
-                app,
-                stub._indexed_objects,
-                post_init_state,
-                environment_name=environment_name,
-                output_mgr=output_mgr,
+            await app._create_all_objects(
+                stub._indexed_objects, post_init_state, environment_name=environment_name, output_mgr=output_mgr
             )
 
             # Deploy app
             # TODO(erikbern): not needed if the app already existed
-            deploy_req = api_pb2.AppDeployRequest(
-                app_id=app.app_id,
-                name=name,
-                namespace=namespace,
-                object_entity="ap",
-                visibility=(
-                    api_pb2.APP_DEPLOY_VISIBILITY_PUBLIC if public else api_pb2.APP_DEPLOY_VISIBILITY_WORKSPACE
-                ),
-            )
-            try:
-                deploy_response = await retry_transient_errors(client.stub.AppDeploy, deploy_req)
-            except GRPCError as exc:
-                if exc.status == Status.INVALID_ARGUMENT:
-                    raise InvalidError(exc.message)
-                if exc.status == Status.FAILED_PRECONDITION:
-                    raise InvalidError(exc.message)
-                raise
-            url = deploy_response.url
+            url = await app.deploy(name, namespace, public)
         except Exception as e:
             # Note that AppClientDisconnect only stops the app if it's still initializing, and is a no-op otherwise.
-            await _disconnect(client, app.app_id, reason=api_pb2.APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION)
+            await app.disconnect(reason=api_pb2.APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION)
             raise e
 
     output_mgr.print_if_visible(step_completed("App deployed! "))
     output_mgr.print_if_visible(f"\nView Deployment: [magenta]{url}[/magenta]")
     return DeployResult(app_id=app.app_id)
 
 
-async def _interactive_shell(_stub: _App, cmd: List[str], environment_name: str = "", **kwargs):
+async def _interactive_shell(_stub: _Stub, cmd: List[str], environment_name: str = "", **kwargs):
     """Run an interactive shell (like `bash`) within the image for this app.
 
     This is useful for online debugging and interactive exploration of the
     contents of this image. If `cmd` is optionally provided, it will be run
     instead of the default shell inside this image.
 
     **Example**
@@ -492,17 +326,11 @@
             loading_status.stop()
             raise InteractiveTimeoutError("Timed out while waiting for sandbox to start")
 
         loading_status.stop()
         await connect_to_sandbox(sb)
 
 
-run_app = synchronize_api(_run_app)
+run_stub = synchronize_api(_run_stub)
 serve_update = synchronize_api(_serve_update)
-deploy_app = synchronize_api(_deploy_app)
+deploy_stub = synchronize_api(_deploy_stub)
 interactive_shell = synchronize_api(_interactive_shell)
-
-# Soon-to-be-deprecated ones, add warning soon
-_run_stub = _run_app
-run_stub = run_app
-_deploy_stub = _deploy_app
-deploy_stub = deploy_app
```

## modal/runner.pyi

```diff
@@ -1,43 +1,21 @@
 import modal._output
 import modal.client
-import modal.object
-import modal.running_app
 import multiprocessing.synchronize
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
-_App = typing.TypeVar("_App")
+_Stub = typing.TypeVar("_Stub")
 
 async def _heartbeat(client, app_id):
     ...
 
 
-async def _init_local_app_existing(client: modal.client._Client, existing_app_id: str) -> modal.running_app.RunningApp:
-    ...
-
-
-async def _init_local_app_new(client: modal.client._Client, description: str, app_state: int, environment_name: str = '', interactive=False) -> modal.running_app.RunningApp:
-    ...
-
-
-async def _init_local_app_from_name(client: modal.client._Client, name: str, namespace, environment_name: str = ''):
-    ...
-
-
-async def _create_all_objects(client: modal.client._Client, app: modal.running_app.RunningApp, indexed_objects: typing.Dict[str, modal.object._Object], new_app_state: int, environment_name: str, output_mgr: typing.Union[modal._output.OutputManager, None] = None):
-    ...
-
-
-async def _disconnect(client: modal.client._Client, app_id: str, reason: typing.Union[int, None] = None, exc_str: typing.Union[str, None] = None):
-    ...
-
-
-def _run_app(stub: _App, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
+def _run_stub(stub: _Stub, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_Stub]:
     ...
 
 
 async def _serve_update(stub, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str) -> None:
     ...
 
 
@@ -59,81 +37,53 @@
     def __delattr__(self, name):
         ...
 
     def __hash__(self):
         ...
 
 
-async def _deploy_app(stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
+async def _deploy_stub(stub: _Stub, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
     ...
 
 
-async def _interactive_shell(_stub: _App, cmd: typing.List[str], environment_name: str = '', **kwargs):
+async def _interactive_shell(_stub: _Stub, cmd: typing.List[str], environment_name: str = '', **kwargs):
     ...
 
 
-class __run_app_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
+class __run_stub_spec(typing_extensions.Protocol):
+    def __call__(self, stub: _Stub, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_Stub]:
         ...
 
-    def aio(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
+    def aio(self, stub: _Stub, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_Stub]:
         ...
 
-run_app: __run_app_spec
+run_stub: __run_stub_spec
 
 
 class __serve_update_spec(typing_extensions.Protocol):
     def __call__(self, stub, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str) -> None:
         ...
 
     async def aio(self, *args, **kwargs) -> None:
         ...
 
 serve_update: __serve_update_spec
 
 
-class __deploy_app_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
+class __deploy_stub_spec(typing_extensions.Protocol):
+    def __call__(self, stub: _Stub, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
         ...
 
     async def aio(self, *args, **kwargs) -> DeployResult:
         ...
 
-deploy_app: __deploy_app_spec
+deploy_stub: __deploy_stub_spec
 
 
 class __interactive_shell_spec(typing_extensions.Protocol):
-    def __call__(self, _stub: _App, cmd: typing.List[str], environment_name: str = '', **kwargs):
+    def __call__(self, _stub: _Stub, cmd: typing.List[str], environment_name: str = '', **kwargs):
         ...
 
     async def aio(self, *args, **kwargs):
         ...
 
 interactive_shell: __interactive_shell_spec
-
-
-def _run_stub(stub: _App, client: typing.Union[modal.client._Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
-    ...
-
-
-class __run_stub_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
-        ...
-
-    def aio(self, stub: _App, client: typing.Union[modal.client.Client, None] = None, stdout=None, show_progress: bool = True, detach: bool = False, output_mgr: typing.Union[modal._output.OutputManager, None] = None, environment_name: typing.Union[str, None] = None, shell=False, interactive=False) -> typing.AsyncContextManager[_App]:
-        ...
-
-run_stub: __run_stub_spec
-
-
-async def _deploy_stub(stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
-    ...
-
-
-class __deploy_stub_spec(typing_extensions.Protocol):
-    def __call__(self, stub: _App, name: str = None, namespace=1, client=None, stdout=None, show_progress=True, environment_name: typing.Union[str, None] = None, public: bool = False) -> DeployResult:
-        ...
-
-    async def aio(self, *args, **kwargs) -> DeployResult:
-        ...
-
-deploy_stub: __deploy_stub_spec
```

## modal/sandbox.py

```diff
@@ -1,29 +1,28 @@
 # Copyright Modal Labs 2022
 import asyncio
 import os
-from typing import AsyncIterator, Dict, List, Optional, Sequence, Tuple, Union
+from typing import AsyncIterator, Dict, List, Optional, Sequence, Union
 
 from google.protobuf.message import Message
 from grpclib.exceptions import GRPCError, StreamTerminatedError
 
 from modal.cloud_bucket_mount import _CloudBucketMount, cloud_bucket_mounts_to_proto
 from modal.exception import InvalidError, SandboxTerminatedError, SandboxTimeoutError
 from modal.volume import _Volume
 from modal_proto import api_pb2
 
 from ._location import parse_cloud_provider
 from ._resolver import Resolver
-from ._resources import convert_fn_config_to_resources_config
 from ._utils.async_utils import synchronize_api
 from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors, unary_stream
 from ._utils.mount_utils import validate_mount_points, validate_volumes
 from .client import _Client
 from .config import config
-from .gpu import GPU_T
+from .gpu import GPU_T, parse_gpu_config
 from .image import _Image
 from .mount import _Mount
 from .network_file_system import _NetworkFileSystem, network_file_system_mount_protos
 from .object import _Object
 from .secret import _Secret
 
 
@@ -238,15 +237,15 @@
         mounts: Sequence[_Mount],
         secrets: Sequence[_Secret],
         timeout: Optional[int] = None,
         workdir: Optional[str] = None,
         gpu: GPU_T = None,
         cloud: Optional[str] = None,
         cpu: Optional[float] = None,
-        memory: Optional[Union[int, Tuple[int, int]]] = None,
+        memory: Optional[int] = None,
         network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
         block_network: bool = False,
         volumes: Dict[Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]] = {},
         allow_background_volume_commits: bool = False,
         pty_info: Optional[api_pb2.PTYInfo] = None,
     ) -> "_Sandbox":
         """mdmd:hidden"""
@@ -271,14 +270,22 @@
                 deps.append(vol)
             for _, cloud_bucket_mount in cloud_bucket_mounts:
                 if cloud_bucket_mount.secret:
                     deps.append(cloud_bucket_mount.secret)
             return deps
 
         async def _load(self: _Sandbox, resolver: Resolver, _existing_object_id: Optional[str]):
+            gpu_config = parse_gpu_config(gpu)
+
+            cloud_provider = parse_cloud_provider(cloud) if cloud else None
+
+            if cpu is not None and cpu < 0.25:
+                raise InvalidError(f"Invalid fractional CPU value {cpu}. Cannot have less than 0.25 CPU resources.")
+            milli_cpu = int(1000 * cpu) if cpu is not None else None
+
             # Relies on dicts being ordered (true as of Python 3.6).
             volume_mounts = [
                 api_pb2.VolumeMount(
                     mount_path=path,
                     volume_id=volume.object_id,
                     allow_background_commits=allow_background_volume_commits,
                 )
@@ -288,16 +295,16 @@
             definition = api_pb2.Sandbox(
                 entrypoint_args=entrypoint_args,
                 image_id=image.object_id,
                 mount_ids=[mount.object_id for mount in mounts],
                 secret_ids=[secret.object_id for secret in secrets],
                 timeout_secs=timeout,
                 workdir=workdir,
-                resources=convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu),
-                cloud_provider=parse_cloud_provider(cloud) if cloud else None,
+                resources=api_pb2.Resources(gpu_config=gpu_config, milli_cpu=milli_cpu, memory_mb=memory),
+                cloud_provider=cloud_provider,
                 nfs_mounts=network_file_system_mount_protos(validated_network_file_systems, False),
                 runtime_debug=config.get("function_runtime_debug"),
                 block_network=block_network,
                 cloud_bucket_mounts=cloud_bucket_mounts_to_proto(cloud_bucket_mounts),
                 volume_mounts=volume_mounts,
                 pty_info=pty_info,
             )
```

## modal/sandbox.pyi

```diff
@@ -108,15 +108,15 @@
 class _Sandbox(modal.object._Object):
     _result: typing.Union[modal_proto.api_pb2.GenericResult, None]
     _stdout: _LogsReader
     _stderr: _LogsReader
     _stdin: _StreamWriter
 
     @staticmethod
-    def _new(entrypoint_args: typing.Sequence[str], image: modal.image._Image, mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system._NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> _Sandbox:
+    def _new(entrypoint_args: typing.Sequence[str], image: modal.image._Image, mounts: typing.Sequence[modal.mount._Mount], secrets: typing.Sequence[modal.secret._Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system._NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume._Volume, modal.cloud_bucket_mount._CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> _Sandbox:
         ...
 
     def _hydrate_metadata(self, handle_metadata: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     @staticmethod
     async def from_id(sandbox_id: str, client: typing.Union[modal.client._Client, None] = None) -> _Sandbox:
@@ -154,15 +154,15 @@
     _stderr: LogsReader
     _stdin: StreamWriter
 
     def __init__(self, *args, **kwargs):
         ...
 
     @staticmethod
-    def _new(entrypoint_args: typing.Sequence[str], image: modal.image.Image, mounts: typing.Sequence[modal.mount.Mount], secrets: typing.Sequence[modal.secret.Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, typing.Tuple[int, int], None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system.NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> Sandbox:
+    def _new(entrypoint_args: typing.Sequence[str], image: modal.image.Image, mounts: typing.Sequence[modal.mount.Mount], secrets: typing.Sequence[modal.secret.Secret], timeout: typing.Union[int, None] = None, workdir: typing.Union[str, None] = None, gpu: typing.Union[None, bool, str, modal.gpu._GPUConfig] = None, cloud: typing.Union[str, None] = None, cpu: typing.Union[float, None] = None, memory: typing.Union[int, None] = None, network_file_systems: typing.Dict[typing.Union[str, os.PathLike], modal.network_file_system.NetworkFileSystem] = {}, block_network: bool = False, volumes: typing.Dict[typing.Union[str, os.PathLike], typing.Union[modal.volume.Volume, modal.cloud_bucket_mount.CloudBucketMount]] = {}, allow_background_volume_commits: bool = False, pty_info: typing.Union[modal_proto.api_pb2.PTYInfo, None] = None) -> Sandbox:
         ...
 
     def _hydrate_metadata(self, handle_metadata: typing.Union[google.protobuf.message.Message, None]):
         ...
 
     class __from_id_spec(typing_extensions.Protocol):
         def __call__(self, sandbox_id: str, client: typing.Union[modal.client.Client, None] = None) -> Sandbox:
```

## modal/secret.py

```diff
@@ -2,18 +2,18 @@
 import os
 from typing import Dict, List, Optional, Union
 
 from grpclib import GRPCError, Status
 
 from modal_proto import api_pb2
 
-from ._container_io_manager import is_local
 from ._resolver import Resolver
 from ._utils.async_utils import synchronize_api
 from ._utils.grpc_utils import retry_transient_errors
+from .app import is_local
 from .client import _Client
 from .exception import InvalidError, NotFoundError
 from .object import _get_environment_name, _Object
 
 ENV_DICT_WRONG_TYPE_ERR = "the env_dict argument to Secret has to be a dict[str, Union[str, None]]"
```

## modal/serving.py

```diff
@@ -10,38 +10,38 @@
 from synchronicity import Interface
 from synchronicity.async_wrap import asynccontextmanager
 
 from ._output import OutputManager
 from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
 from ._utils.logger import logger
 from ._watcher import watch
-from .cli.import_refs import import_app
+from .cli.import_refs import import_stub
 from .client import _Client
 from .config import config
-from .runner import _disconnect, _run_app, serve_update
+from .runner import _run_stub, serve_update
 
 if TYPE_CHECKING:
-    from .stub import _App
+    from .stub import _Stub
 else:
-    _App = TypeVar("_App")
+    _Stub = TypeVar("_Stub")
 
 
-def _run_serve(app_ref: str, existing_app_id: str, is_ready: Event, environment_name: str):
+def _run_serve(stub_ref: str, existing_app_id: str, is_ready: Event, environment_name: str):
     # subprocess entrypoint
-    _app = import_app(app_ref)
-    blocking_app = synchronizer._translate_out(_app, Interface.BLOCKING)
-    serve_update(blocking_app, existing_app_id, is_ready, environment_name)
+    _stub = import_stub(stub_ref)
+    blocking_stub = synchronizer._translate_out(_stub, Interface.BLOCKING)
+    serve_update(blocking_stub, existing_app_id, is_ready, environment_name)
 
 
 async def _restart_serve(
-    app_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0
+    stub_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0
 ) -> SpawnProcess:
     ctx = multiprocessing.get_context("spawn")  # Needed to reload the interpreter
     is_ready = ctx.Event()
-    p = ctx.Process(target=_run_serve, args=(app_ref, existing_app_id, is_ready, environment_name))
+    p = ctx.Process(target=_run_serve, args=(stub_ref, existing_app_id, is_ready, environment_name))
     p.start()
     await asyncify(is_ready.wait)(timeout)
     # TODO(erikbern): we don't fail if the above times out, but that's somewhat intentional, since
     # the child process might build a huge image or similar
     return p
 
 
@@ -59,15 +59,15 @@
             )
             proc.kill()
     except ProcessLookupError:
         pass  # Child process already finished
 
 
 async def _run_watch_loop(
-    app_ref: str,
+    stub_ref: str,
     app_id: str,
     output_mgr: OutputManager,
     watcher: AsyncGenerator[Set[str], None],
     environment_name: str,
 ):
     unsupported_msg = None
     if platform.system() == "Windows":
@@ -79,56 +79,51 @@
             output_mgr.print_if_visible(unsupported_msg)
     else:
         curr_proc = None
         try:
             async for trigger_files in watcher:
                 logger.debug(f"The following files triggered an app update: {', '.join(trigger_files)}")
                 await _terminate(curr_proc, output_mgr)
-                curr_proc = await _restart_serve(app_ref, existing_app_id=app_id, environment_name=environment_name)
+                curr_proc = await _restart_serve(stub_ref, existing_app_id=app_id, environment_name=environment_name)
         finally:
             await _terminate(curr_proc, output_mgr)
 
 
-def _get_clean_app_description(app_ref: str) -> str:
+def _get_clean_stub_description(stub_ref: str) -> str:
     # If possible, consider the 'ref' argument the start of the app's args. Everything
     # before it Modal CLI cruft (eg. `modal serve --timeout 1.0`).
     try:
-        func_ref_arg_idx = sys.argv.index(app_ref)
+        func_ref_arg_idx = sys.argv.index(stub_ref)
         return " ".join(sys.argv[func_ref_arg_idx:])
     except ValueError:
         return " ".join(sys.argv)
 
 
 @asynccontextmanager
-async def _serve_app(
-    app: "_App",
-    app_ref: str,
+async def _serve_stub(
+    stub: "_Stub",
+    stub_ref: str,
     stdout: Optional[io.TextIOWrapper] = None,
     show_progress: bool = True,
     _watcher: Optional[AsyncGenerator[Set[str], None]] = None,  # for testing
     environment_name: Optional[str] = None,
-) -> AsyncGenerator["_App", None]:
+) -> AsyncGenerator["_Stub", None]:
     if environment_name is None:
         environment_name = config.get("environment")
 
     client = await _Client.from_env()
 
     output_mgr = OutputManager(stdout, show_progress, "Running app...")
     if _watcher is not None:
         watcher = _watcher  # Only used by tests
     else:
-        mounts_to_watch = app._get_watch_mounts()
+        mounts_to_watch = stub._get_watch_mounts()
         watcher = watch(mounts_to_watch, output_mgr)
 
-    async with _run_app(app, client=client, output_mgr=output_mgr, environment_name=environment_name):
-        app_id: str = app.app_id
-        client.set_pre_stop(lambda: _disconnect(client, app_id))
+    async with _run_stub(stub, client=client, output_mgr=output_mgr, environment_name=environment_name):
+        client.set_pre_stop(stub._local_app.disconnect)
         async with TaskContext(grace=0.1) as tc:
-            tc.create_task(_run_watch_loop(app_ref, app.app_id, output_mgr, watcher, environment_name))
-            yield app
+            tc.create_task(_run_watch_loop(stub_ref, stub.app_id, output_mgr, watcher, environment_name))
+            yield stub
 
 
-serve_app = synchronize_api(_serve_app)
-
-# Soon-to-be-deprecated ones, add warning soon
-_serve_stub = _serve_app
-serve_stub = serve_app
+serve_stub = synchronize_api(_serve_stub)
```

## modal/serving.pyi

```diff
@@ -2,55 +2,41 @@
 import modal._output
 import multiprocessing.context
 import multiprocessing.synchronize
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
-_App = typing.TypeVar("_App")
+_Stub = typing.TypeVar("_Stub")
 
-def _run_serve(app_ref: str, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str):
+def _run_serve(stub_ref: str, existing_app_id: str, is_ready: multiprocessing.synchronize.Event, environment_name: str):
     ...
 
 
-async def _restart_serve(app_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0) -> multiprocessing.context.SpawnProcess:
+async def _restart_serve(stub_ref: str, existing_app_id: str, environment_name: str, timeout: float = 5.0) -> multiprocessing.context.SpawnProcess:
     ...
 
 
 async def _terminate(proc: typing.Union[multiprocessing.context.SpawnProcess, None], output_mgr: modal._output.OutputManager, timeout: float = 5.0):
     ...
 
 
-async def _run_watch_loop(app_ref: str, app_id: str, output_mgr: modal._output.OutputManager, watcher: typing.AsyncGenerator[typing.Set[str], None], environment_name: str):
+async def _run_watch_loop(stub_ref: str, app_id: str, output_mgr: modal._output.OutputManager, watcher: typing.AsyncGenerator[typing.Set[str], None], environment_name: str):
     ...
 
 
-def _get_clean_app_description(app_ref: str) -> str:
+def _get_clean_stub_description(stub_ref: str) -> str:
     ...
 
 
-def _serve_app(app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
-    ...
-
-
-class __serve_app_spec(typing_extensions.Protocol):
-    def __call__(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.Generator[typing.Set[str], None, None], None] = None, environment_name: typing.Union[str, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
-        ...
-
-    def aio(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
-        ...
-
-serve_app: __serve_app_spec
-
-
-def _serve_stub(app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
+def _serve_stub(stub: _Stub, stub_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_Stub]:
     ...
 
 
 class __serve_stub_spec(typing_extensions.Protocol):
-    def __call__(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.Generator[typing.Set[str], None, None], None] = None, environment_name: typing.Union[str, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_App]:
+    def __call__(self, stub: _Stub, stub_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.Generator[typing.Set[str], None, None], None] = None, environment_name: typing.Union[str, None] = None) -> synchronicity.combined_types.AsyncAndBlockingContextManager[_Stub]:
         ...
 
-    def aio(self, app: _App, app_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_App]:
+    def aio(self, stub: _Stub, stub_ref: str, stdout: typing.Union[_io.TextIOWrapper, None] = None, show_progress: bool = True, _watcher: typing.Union[typing.AsyncGenerator[typing.Set[str], None], None] = None, environment_name: typing.Union[str, None] = None) -> typing.AsyncContextManager[_Stub]:
         ...
 
 serve_stub: __serve_stub_spec
```

## modal/token_flow.py

```diff
@@ -74,15 +74,16 @@
     next_url: Optional[str] = None,
 ):
     server_url = config.get("server_url", profile=profile)
 
     console = Console()
 
     result: Optional[api_pb2.TokenFlowWaitResponse] = None
-    async with _Client.anonymous(server_url) as client:
+    client = await _Client.unauthenticated_client(server_url)
+    async with client:
         token_flow = _TokenFlow(client)
 
         async with token_flow.start(source, next_url) as (_, web_url, code):
             with console.status("Waiting for authentication in the web browser", spinner="dots"):
                 # Open the web url in the browser
                 if _open_url(web_url):
                     console.print(
```

## modal/volume.py

```diff
@@ -1,17 +1,12 @@
 # Copyright Modal Labs 2023
 import asyncio
 import concurrent.futures
-import enum
-import os
-import platform
-import re
 import time
 from contextlib import nullcontext
-from dataclasses import dataclass
 from pathlib import Path, PurePosixPath
 from typing import (
     IO,
     AsyncGenerator,
     AsyncIterator,
     BinaryIO,
     Callable,
@@ -38,56 +33,21 @@
     blob_upload_file,
     get_file_upload_spec_from_fileobj,
     get_file_upload_spec_from_path,
 )
 from ._utils.grpc_utils import retry_transient_errors, unary_stream
 from .client import _Client
 from .config import logger
-from .exception import deprecation_error
 from .object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
 
 # Max duration for uploading to volumes files
 # As a guide, files >40GiB will take >10 minutes to upload.
 VOLUME_PUT_FILE_CLIENT_TIMEOUT = 30 * 60
 
 
-class FileEntryType(enum.IntEnum):
-    """Type of a file entry listed from a Modal volume."""
-
-    UNSPECIFIED = 0
-    FILE = 1
-    DIRECTORY = 2
-    SYMLINK = 3
-
-
-@dataclass(frozen=True)
-class FileEntry:
-    """A file or directory entry listed from a Modal volume."""
-
-    path: str
-    type: FileEntryType
-    mtime: int
-    size: int
-
-    @classmethod
-    def _from_proto(cls, proto: api_pb2.FileEntry) -> "FileEntry":
-        return cls(
-            path=proto.path,
-            type=FileEntryType(proto.type),
-            mtime=proto.mtime,
-            size=proto.size,
-        )
-
-    def __getattr__(self, name: str):
-        deprecation_error(
-            (2024, 4, 15),
-            f"The FileEntry dataclass was introduced to replace a private Protobuf message. This dataclass does not have the {name} attribute.",
-        )
-
-
 class _Volume(_Object, type_prefix="vo"):
     """A writeable volume that can be used to share files between one or more Modal functions.
 
     The contents of a volume is exposed as a filesystem. You can use it to share data between different functions, or
     to persist durable state across several instances of the same function.
 
     Unlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was mounted.
@@ -189,15 +149,15 @@
                 namespace=namespace,
                 environment_name=_get_environment_name(environment_name, resolver),
                 object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
             )
             response = await resolver.client.stub.VolumeGetOrCreate(req)
             self._hydrate(response.volume_id, resolver.client, None)
 
-        return _Volume._from_loader(_load, "Volume()", hydrate_lazily=True)
+        return _Volume._from_loader(_load, "Volume()")
 
     @classmethod
     @asynccontextmanager
     async def ephemeral(
         cls: Type["_Volume"],
         client: Optional[_Client] = None,
         environment_name: Optional[str] = None,
@@ -312,44 +272,31 @@
         reloading.
 
         Reloading will fail if there are open files for the volume.
         """
         try:
             await self._do_reload()
         except GRPCError as exc:
-            # TODO(staffan): This is brittle and janky, as it relies on specific paths and error messages which can
-            #  change server-side at any time. Consider returning the open files directly in the error emitted from the
-            #  server.
-            if exc.message == "there are open files preventing the operation":
-                # Attempt to identify what open files are problematic and include information about the first (to avoid
-                # really verbose errors) open file in the error message to help troubleshooting.
-                # This is best-effort and not necessarily bulletproof, as the view of open files inside the container
-                # might differ from that outside - but it will at least catch common errors.
-                vol_path = f"/__modal/volumes/{self.object_id}"
-                annotation = _open_files_error_annotation(vol_path)
-                if annotation:
-                    raise RuntimeError(f"{exc.message}: {annotation}")
-
             raise RuntimeError(exc.message) if exc.status in (Status.FAILED_PRECONDITION, Status.NOT_FOUND) else exc
 
     @live_method_gen
-    async def iterdir(self, path: str) -> AsyncIterator[FileEntry]:
+    async def iterdir(self, path: str) -> AsyncIterator[api_pb2.VolumeListFilesEntry]:
         """Iterate over all files in a directory in the volume.
 
         * Passing a directory path lists all files in the directory (names are relative to the directory)
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         req = api_pb2.VolumeListFilesRequest(volume_id=self.object_id, path=path)
         async for batch in unary_stream(self._client.stub.VolumeListFiles, req):
             for entry in batch.entries:
-                yield FileEntry._from_proto(entry)
+                yield entry
 
     @live_method
-    async def listdir(self, path: str) -> List[FileEntry]:
+    async def listdir(self, path: str) -> List[api_pb2.VolumeListFilesEntry]:
         """List all files under a path prefix in the modal.Volume.
 
         * Passing a directory path lists all files in the directory
         * Passing a file path returns a list containing only that file's listing description
         * Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths)
         """
         return [entry async for entry in self.iterdir(path)]
@@ -643,62 +590,7 @@
             sha256_hex=file_spec.sha256_hex,
             mode=file_spec.mode,
         )
 
 
 Volume = synchronize_api(_Volume)
 VolumeUploadContextManager = synchronize_api(_VolumeUploadContextManager)
-
-
-def _open_files_error_annotation(mount_path: str) -> Optional[str]:
-    if platform.system() != "Linux":
-        return None
-
-    self_pid = os.readlink("/proc/self")
-
-    def find_open_file_for_pid(pid: str) -> Optional[str]:
-        # /proc/{pid}/cmdline is null separated
-        with open(f"/proc/{pid}/cmdline", "rb") as f:
-            raw = f.read()
-            parts = raw.split(b"\0")
-            cmdline = " ".join([part.decode() for part in parts]).rstrip(" ")
-
-        cwd = PurePosixPath(os.readlink(f"/proc/{pid}/cwd"))
-        # NOTE(staffan): Python 3.8 doesn't have is_relative_to(), so we're stuck with catching ValueError until
-        # we drop Python 3.8 support.
-        try:
-            _rel_cwd = cwd.relative_to(mount_path)
-            if pid == self_pid:
-                return "cwd is inside volume"
-            else:
-                return f"cwd of '{cmdline}' is inside volume"
-        except ValueError:
-            pass
-
-        for fd in os.listdir(f"/proc/{pid}/fd"):
-            try:
-                path = PurePosixPath(os.readlink(f"/proc/{pid}/fd/{fd}"))
-                try:
-                    rel_path = path.relative_to(mount_path)
-                    if pid == self_pid:
-                        return f"path {rel_path} is open"
-                    else:
-                        return f"path {rel_path} is open from '{cmdline}'"
-                except ValueError:
-                    pass
-
-            except FileNotFoundError:
-                # File was closed
-                pass
-        return None
-
-    pid_re = re.compile("^[1-9][0-9]*$")
-    for dirent in os.listdir("/proc/"):
-        if pid_re.match(dirent):
-            try:
-                annotation = find_open_file_for_pid(dirent)
-                if annotation:
-                    return annotation
-            except (FileNotFoundError, PermissionError):
-                pass
-
-    return None
```

## modal/volume.pyi

```diff
@@ -1,59 +1,17 @@
 import asyncio.locks
-import enum
 import modal._utils.blob_utils
 import modal.client
 import modal.object
 import modal_proto.api_pb2
 import pathlib
 import synchronicity.combined_types
 import typing
 import typing_extensions
 
-class FileEntryType(enum.IntEnum):
-    """Type of a file entry listed from a Modal volume."""
-
-    UNSPECIFIED = 0
-    FILE = 1
-    DIRECTORY = 2
-    SYMLINK = 3
-
-
-class FileEntry:
-    path: str
-    type: FileEntryType
-    mtime: int
-    size: int
-
-    @classmethod
-    def _from_proto(cls, proto: modal_proto.api_pb2.FileEntry) -> FileEntry:
-        ...
-
-    def __getattr__(self, name: str):
-        ...
-
-    def __init__(self, path: str, type: FileEntryType, mtime: int, size: int) -> None:
-        ...
-
-    def __repr__(self):
-        ...
-
-    def __eq__(self, other):
-        ...
-
-    def __setattr__(self, name, value):
-        ...
-
-    def __delattr__(self, name):
-        ...
-
-    def __hash__(self):
-        ...
-
-
 class _Volume(modal.object._Object):
     _lock: asyncio.locks.Lock
 
     def _initialize_from_empty(self):
         ...
 
     @staticmethod
@@ -85,18 +43,18 @@
 
     async def commit(self):
         ...
 
     async def reload(self):
         ...
 
-    def iterdir(self, path: str) -> typing.AsyncIterator[FileEntry]:
+    def iterdir(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.VolumeListFilesEntry]:
         ...
 
-    async def listdir(self, path: str) -> typing.List[FileEntry]:
+    async def listdir(self, path: str) -> typing.List[modal_proto.api_pb2.VolumeListFilesEntry]:
         ...
 
     def read_file(self, path: typing.Union[str, bytes]) -> typing.AsyncIterator[bytes]:
         ...
 
     async def read_file_into_fileobj(self, path: typing.Union[str, bytes], fileobj: typing.IO[bytes], progress: bool = False) -> int:
         ...
@@ -206,27 +164,27 @@
 
         async def aio(self, *args, **kwargs):
             ...
 
     reload: __reload_spec
 
     class __iterdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.Iterator[FileEntry]:
+        def __call__(self, path: str) -> typing.Iterator[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
-        def aio(self, path: str) -> typing.AsyncIterator[FileEntry]:
+        def aio(self, path: str) -> typing.AsyncIterator[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
     iterdir: __iterdir_spec
 
     class __listdir_spec(typing_extensions.Protocol):
-        def __call__(self, path: str) -> typing.List[FileEntry]:
+        def __call__(self, path: str) -> typing.List[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
-        async def aio(self, *args, **kwargs) -> typing.List[FileEntry]:
+        async def aio(self, *args, **kwargs) -> typing.List[modal_proto.api_pb2.VolumeListFilesEntry]:
             ...
 
     listdir: __listdir_spec
 
     class __read_file_spec(typing_extensions.Protocol):
         def __call__(self, path: typing.Union[str, bytes]) -> typing.Iterator[bytes]:
             ...
@@ -313,11 +271,7 @@
         def __call__(self, file_spec: modal._utils.blob_utils.FileUploadSpec) -> modal_proto.api_pb2.MountFile:
             ...
 
         async def aio(self, *args, **kwargs) -> modal_proto.api_pb2.MountFile:
             ...
 
     _upload_file: ___upload_file_spec
-
-
-def _open_files_error_annotation(mount_path: str) -> typing.Union[str, None]:
-    ...
```

## modal/_utils/async_utils.py

```diff
@@ -8,15 +8,14 @@
 import typing
 from contextlib import asynccontextmanager
 from typing import Any, AsyncGenerator, Callable, Iterator, List, Optional, Set, TypeVar, cast
 
 import synchronicity
 from typing_extensions import ParamSpec
 
-from ..exception import InvalidError
 from .logger import logger
 
 synchronizer = synchronicity.Synchronizer()
 # atexit.register(synchronizer.close)
 
 
 def synchronize_api(obj, target_module=None):
@@ -231,16 +230,14 @@
         fut = executor.submit(asyncio.run, coro)
         return fut.result()
 
 
 async def queue_batch_iterator(q: asyncio.Queue, max_batch_size=100, debounce_time=0.015):
     """
     Read from a queue but return lists of items when queue is large
-
-    Treats a None value as end of queue items
     """
     item_list: List[Any] = []
 
     while True:
         if q.empty() and len(item_list) > 0:
             yield item_list
             item_list = []
@@ -256,102 +253,52 @@
             if len(item_list) > 0:
                 yield item_list
             break
         item_list.append(res)
 
 
 class _WarnIfGeneratorIsNotConsumed:
-    def __init__(self, gen, function_name: str):
+    def __init__(self, gen, gen_f):
         self.gen = gen
-        self.function_name = function_name
+        self.gen_f = gen_f
         self.iterated = False
         self.warned = False
 
     def __aiter__(self):
         self.iterated = True
-        return self.gen.__aiter__()
+        return self.gen
 
     async def __anext__(self):
         self.iterated = True
         return await self.gen.__anext__()
 
-    async def asend(self, value):
-        self.iterated = True
-        return await self.gen.asend(value)
-
     def __repr__(self):
         return repr(self.gen)
 
     def __del__(self):
         if not self.iterated and not self.warned:
             self.warned = True
+            name = self.gen_f.__name__
             logger.warning(
-                f"Warning: the results of a call to {self.function_name} was not consumed, so the call will never be executed."
-                f" Consider a for-loop like `for x in {self.function_name}(...)` or unpacking the generator using `list(...)`"
+                f"Warning: the results of a call to {name} was not consumed, so the call will never be executed."
+                f" Consider a for-loop like `for x in {name}(...)` or unpacking the generator using `list(...)`"
             )
 
 
 synchronize_api(_WarnIfGeneratorIsNotConsumed)
 
 
-class _WarnIfNonWrappedGeneratorIsNotConsumed(_WarnIfGeneratorIsNotConsumed):
-    # used for non-synchronicity-wrapped generators and iterators
-    def __iter__(self):
-        self.iterated = True
-        return iter(self.gen)
-
-    def __next__(self):
-        self.iterated = True
-        return self.gen.__next__()
-
-    def send(self, value):
-        self.iterated = True
-        return self.gen.send(value)
-
-
-def warn_if_generator_is_not_consumed(function_name: Optional[str] = None):
+def warn_if_generator_is_not_consumed(gen_f):
     # https://gist.github.com/erikbern/01ae78d15f89edfa7f77e5c0a827a94d
-    def decorator(gen_f):
-        presented_func_name = function_name if function_name is not None else gen_f.__name__
-
-        @functools.wraps(gen_f)
-        def f_wrapped(*args, **kwargs):
-            gen = gen_f(*args, **kwargs)
-            if inspect.isasyncgen(gen):
-                return _WarnIfGeneratorIsNotConsumed(gen, presented_func_name)
-            else:
-                return _WarnIfNonWrappedGeneratorIsNotConsumed(gen, presented_func_name)
-
-        return f_wrapped
-
-    return decorator
-
-
-class AsyncOrSyncIteratable:
-    """Compatibility class for non-synchronicity wrapped async iterables to get
-    both async and sync interfaces in the same way that synchronicity does (but on the main thread)
-    so they can be "lazily" iterated using either `for _ in x` or `async for _ in x`
-
-    nested_async_message is raised as an InvalidError if the async variant is called
-    from an already async context, since that would otherwise deadlock the event loop
-    """
-
-    def __init__(self, async_iterable: typing.AsyncIterable[Any], nested_async_message):
-        self._async_iterable = async_iterable
-        self.nested_async_message = nested_async_message
+    @functools.wraps(gen_f)
+    def f_wrapped(*args, **kwargs):
+        gen = gen_f(*args, **kwargs)
+        return _WarnIfGeneratorIsNotConsumed(gen, gen_f)
 
-    def __aiter__(self):
-        return self._async_iterable
-
-    def __iter__(self):
-        try:
-            for output in run_generator_sync(self._async_iterable):  # type: ignore
-                yield output
-        except NestedAsyncCalls:
-            raise InvalidError(self.nested_async_message)
+    return f_wrapped
 
 
 _shutdown_tasks = []
 
 
 def on_shutdown(coro):
     # hook into event loop shutdown when all active tasks get cancelled
@@ -433,46 +380,7 @@
     Note that for Python 3.10+ you can use contextlib.nullcontext() instead.
 
     Usage:
     async with asyncnullcontext():
         pass
     """
     yield
-
-
-YIELD_TYPE = typing.TypeVar("YIELD_TYPE")
-SEND_TYPE = typing.TypeVar("SEND_TYPE")
-
-
-class NestedAsyncCalls(Exception):
-    pass
-
-
-def run_generator_sync(
-    gen: typing.AsyncGenerator[YIELD_TYPE, SEND_TYPE],
-) -> typing.Generator[YIELD_TYPE, SEND_TYPE, None]:
-    try:
-        asyncio.get_running_loop()
-    except RuntimeError:
-        pass  # no event loop - this is what we expect!
-    else:
-        raise NestedAsyncCalls()
-    loop = asyncio.new_event_loop()  # set up new event loop for the map so we can use async logic
-
-    # more or less copied from synchronicity's implementation:
-    next_send: typing.Union[SEND_TYPE, None] = None
-    next_yield: YIELD_TYPE
-    exc: Optional[BaseException] = None
-    while True:
-        try:
-            if exc:
-                next_yield = loop.run_until_complete(gen.athrow(exc))
-            else:
-                next_yield = loop.run_until_complete(gen.asend(next_send))  # type: ignore[arg-type]
-        except StopAsyncIteration:
-            break
-        try:
-            next_send = yield next_yield
-            exc = None
-        except BaseException as err:
-            exc = err
-    loop.close()
```

## modal/_utils/function_utils.py

```diff
@@ -1,32 +1,26 @@
 # Copyright Modal Labs 2022
-import asyncio
 import inspect
 import os
 import site
 import sys
 import sysconfig
 import typing
 from collections import deque
 from enum import Enum
 from pathlib import Path, PurePosixPath
-from typing import Any, AsyncIterator, Callable, List, Literal, Optional, Set, Type
-
-from grpclib import GRPCError
-from grpclib.exceptions import StreamTerminatedError
+from typing import Callable, List, Optional, Set, Type
 
 from modal_proto import api_pb2
 
-from .._serialization import deserialize_data_format, serialize
+from .._serialization import serialize
 from ..config import config, logger
 from ..exception import InvalidError, ModuleNotMountable
 from ..mount import ROOT_DIR, _Mount
 from ..object import Object
-from .blob_utils import blob_download
-from .grpc_utils import RETRYABLE_GRPC_STATUS_CODES, unary_stream
 
 SYS_PREFIXES = {
     Path(p)
     for p in (
         sys.prefix,
         sys.base_prefix,
         sys.exec_prefix,
@@ -335,46 +329,7 @@
     Used for deprecation of @exit() parameters.
     """
     num_params = len(inspect.signature(f).parameters)
     if hasattr(f, "__self__"):
         return num_params > 0
     else:
         return num_params > 1
-
-
-async def _stream_function_call_data(
-    client, function_call_id: str, variant: Literal["data_in", "data_out"]
-) -> AsyncIterator[Any]:
-    """Read from the `data_in` or `data_out` stream of a function call."""
-    last_index = 0
-    retries_remaining = 10
-
-    if variant == "data_in":
-        stub_fn = client.stub.FunctionCallGetDataIn
-    elif variant == "data_out":
-        stub_fn = client.stub.FunctionCallGetDataOut
-    else:
-        raise ValueError(f"Invalid variant {variant}")
-
-    while True:
-        req = api_pb2.FunctionCallGetDataRequest(function_call_id=function_call_id, last_index=last_index)
-        try:
-            async for chunk in unary_stream(stub_fn, req):
-                if chunk.index <= last_index:
-                    continue
-                last_index = chunk.index
-                if chunk.data_blob_id:
-                    message_bytes = await blob_download(chunk.data_blob_id, client.stub)
-                else:
-                    message_bytes = chunk.data
-                message = deserialize_data_format(message_bytes, chunk.data_format, client)
-                yield message
-        except (GRPCError, StreamTerminatedError) as exc:
-            if retries_remaining > 0:
-                retries_remaining -= 1
-                if isinstance(exc, GRPCError):
-                    if exc.status in RETRYABLE_GRPC_STATUS_CODES:
-                        await asyncio.sleep(1.0)
-                        continue
-                elif isinstance(exc, StreamTerminatedError):
-                    continue
-            raise
```

## modal/_utils/grpc_testing.py

```diff
@@ -171,18 +171,15 @@
         # dropping any preceding requests if there is a match
         # returns the payload of the request
         for i, (_method_name, msg) in enumerate(self.calls):
             if _method_name == method_name:
                 self.calls = self.calls[i + 1 :]
                 return msg
 
-        raise KeyError(f"No message of that type in call list: {self.calls}")
-
-    def get_requests(self, method_name: str) -> List[Any]:
-        return [msg for _method_name, msg in self.calls if _method_name == method_name]
+        raise Exception(f"No message of that type in call list: {self.calls}")
 
 
 class InterceptedStream:
     def __init__(self, interception_context, method_name, stream):
         self.interception_context = interception_context
         self.method_name = method_name
         self.stream = stream
```

## modal/_utils/grpc_utils.py

```diff
@@ -261,20 +261,15 @@
             n_retries += 1
 
             await asyncio.sleep(delay)
             delay = min(delay * delay_factor, max_delay)
 
 
 def find_free_port() -> int:
-    """
-    Find a free TCP port, useful for testing.
-
-    WARN: if a returned free port is not bound immediately by the caller, that same port
-    may be returned in subsequent calls to this function, potentially creating port collisions.
-    """
+    """Find a free TCP port, useful for testing."""
     with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
         s.bind(("", 0))
         s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
         return s.getsockname()[1]
 
 
 def get_proto_oneof(message: Message, oneof_group: str) -> Optional[Message]:
```

## modal/_utils/hash_utils.py

```diff
@@ -1,59 +1,58 @@
 # Copyright Modal Labs 2022
 import base64
 import dataclasses
 import hashlib
-from typing import BinaryIO, Callable, List, Union
+from typing import IO, Union
 
 HASH_CHUNK_SIZE = 4096
 
 
-def _update(hashers: List[Callable[[bytes], None]], data: Union[bytes, BinaryIO]) -> None:
+def _update(hashers, data: Union[bytes, IO[bytes]]):
     if isinstance(data, bytes):
         for hasher in hashers:
-            hasher(data)
+            hasher.update(data)
     else:
-        assert not isinstance(data, (bytearray, memoryview))  # https://github.com/microsoft/pyright/issues/5697
         pos = data.tell()
-        while True:
+        while 1:
             chunk = data.read(HASH_CHUNK_SIZE)
             if not isinstance(chunk, bytes):
                 raise ValueError(f"Only accepts bytes or byte buffer objects, not {type(chunk)} buffers")
             if not chunk:
                 break
             for hasher in hashers:
-                hasher(chunk)
+                hasher.update(chunk)
         data.seek(pos)
 
 
-def get_sha256_hex(data: Union[bytes, BinaryIO]) -> str:
+def get_sha256_hex(data: Union[bytes, IO[bytes]]) -> str:
     hasher = hashlib.sha256()
-    _update([hasher.update], data)
+    _update([hasher], data)
     return hasher.hexdigest()
 
 
-def get_sha256_base64(data: Union[bytes, BinaryIO]) -> str:
+def get_sha256_base64(data: Union[bytes, IO[bytes]]) -> str:
     hasher = hashlib.sha256()
-    _update([hasher.update], data)
+    _update([hasher], data)
     return base64.b64encode(hasher.digest()).decode("ascii")
 
 
-def get_md5_base64(data: Union[bytes, BinaryIO]) -> str:
+def get_md5_base64(data: Union[bytes, IO[bytes]]) -> str:
     hasher = hashlib.md5()
-    _update([hasher.update], data)
+    _update([hasher], data)
     return base64.b64encode(hasher.digest()).decode("utf-8")
 
 
 @dataclasses.dataclass
 class UploadHashes:
     md5_base64: str
     sha256_base64: str
 
 
-def get_upload_hashes(data: Union[bytes, BinaryIO]) -> UploadHashes:
+def get_upload_hashes(data: Union[bytes, IO[bytes]]) -> UploadHashes:
     md5 = hashlib.md5()
     sha256 = hashlib.sha256()
-    _update([md5.update, sha256.update], data)
+    _update([md5, sha256], data)
     return UploadHashes(
         md5_base64=base64.b64encode(md5.digest()).decode("ascii"),
         sha256_base64=base64.b64encode(sha256.digest()).decode("ascii"),
     )
```

## modal/_utils/mount_utils.py

```diff
@@ -1,12 +1,12 @@
 # Copyright Modal Labs 2022
 import posixpath
 import typing
 from pathlib import PurePath, PurePosixPath
-from typing import TYPE_CHECKING, Dict, List, Mapping, Sequence, Tuple, Union
+from typing import TYPE_CHECKING, Dict, List, Mapping, Tuple, Union
 
 from ..exception import InvalidError
 from ..volume import _Volume
 
 if TYPE_CHECKING:
     from ..cloud_bucket_mount import _CloudBucketMount
     from ..network_file_system import _NetworkFileSystem
@@ -36,15 +36,15 @@
             raise InvalidError(f"{display_name} {path} cannot be mounted at '/tmp'.")
         validated.append((path, vol))
     return validated
 
 
 def validate_volumes(
     volumes: Mapping[Union[str, PurePosixPath], Union["_Volume", "_CloudBucketMount"]],
-) -> Sequence[Tuple[str, Union["_Volume", "_NetworkFileSystem", "_CloudBucketMount"]]]:
+) -> List[Tuple[str, Union["_Volume", "_NetworkFileSystem", "_CloudBucketMount"]]]:
     if not isinstance(volumes, dict):
         raise InvalidError("volumes must be a dict[str, Volume] where the keys are paths")
 
     validated_volumes = validate_mount_points("Volume", volumes)
     # We don't support mounting a volume in more than one location
     volume_to_paths: Dict["_Volume", List[str]] = {}
     for path, volume in validated_volumes:
```

## modal/_utils/rand_pb_testing.py

```diff
@@ -54,18 +54,20 @@
                     element = msg_field.add()
                     _fill(element, field.message_type, rand)
             else:
                 _fill(msg_field, field.message_type, rand)
         else:
             if field.type == FieldDescriptor.TYPE_ENUM:
                 enum_values = [x.number for x in field.enum_type.values]
-                generator = lambda rand: rand.choice(enum_values)  # noqa: E731
+
+                def generator(rand):
+                    return rand.choice(enum_values)
 
             else:
-                generator = _FIELD_RANDOM_GENERATOR[field.type]
+                generator = _FIELD_RANDOM_GENERATOR.get(field.type)
             if is_repeated:
                 num = rand.randint(0, 2)
                 msg_field = getattr(msg, field.name)
                 for _ in range(num):
                     msg_field.append(generator(rand))
             else:
                 setattr(msg, field.name, generator(rand))
```

## modal/cli/_download.py

```diff
@@ -1,36 +1,62 @@
 # Copyright Modal Labs 2023
 import asyncio
 import os
 import shutil
 import sys
 from pathlib import Path
-from typing import Optional, Tuple, Union
+from typing import Callable, Optional, Tuple, Union, overload
 
 from click import UsageError
 
 from modal.network_file_system import _NetworkFileSystem
-from modal.volume import FileEntry, FileEntryType, _Volume
+from modal.volume import _Volume
+from modal_proto import api_pb2
+
+_Entry = Union[api_pb2.SharedVolumeListFilesEntry, api_pb2.VolumeListFilesEntry]
+
+
+@overload
+def _glob_download(
+    volume: _Volume,
+    is_file_fn: Callable[[api_pb2.VolumeListFilesEntry], bool],
+    remote_glob_path: str,
+    local_destination: Path,
+    overwrite: bool,
+):
+    ...
+
+
+@overload
+def _glob_download(
+    volume: _NetworkFileSystem,
+    is_file_fn: Callable[[api_pb2.SharedVolumeListFilesEntry], bool],
+    remote_glob_path: str,
+    local_destination: Path,
+    overwrite: bool,
+):
+    ...
 
 
 async def _glob_download(
-    volume: Union[_NetworkFileSystem, _Volume],
+    volume,
+    is_file_fn,
     remote_glob_path: str,
     local_destination: Path,
     overwrite: bool,
 ):
-    q: asyncio.Queue[Tuple[Optional[Path], Optional[FileEntry]]] = asyncio.Queue()
+    q: asyncio.Queue[Tuple[Optional[Path], Optional[_Entry]]] = asyncio.Queue()
     num_consumers = 10  # concurrency limit
 
     async def producer():
         async for entry in volume.iterdir(remote_glob_path):
             output_path = local_destination / entry.path
             if output_path.exists():
                 if overwrite:
-                    if output_path.is_file():
+                    if is_file_fn(entry):
                         os.remove(output_path)
                     else:
                         shutil.rmtree(output_path)
                 else:
                     raise UsageError(
                         f"Output path '{output_path}' already exists. Use --force to overwrite the output directory"
                     )
@@ -41,21 +67,20 @@
 
     async def consumer():
         while True:
             output_path, entry = await q.get()
             if output_path is None:
                 return
             try:
-                if entry.type == FileEntryType.FILE:
+                if is_file_fn(entry):
                     output_path.parent.mkdir(parents=True, exist_ok=True)
                     with output_path.open("wb") as fp:
                         b = 0
                         async for chunk in volume.read_file(entry.path):
                             b += fp.write(chunk)
+
                     print(f"Wrote {b} bytes to {output_path}", file=sys.stderr)
-                elif entry.type == FileEntryType.DIRECTORY:
-                    output_path.mkdir(parents=True, exist_ok=True)
             finally:
                 q.task_done()
 
     consumers = [consumer() for _ in range(num_consumers)]
     await asyncio.gather(producer(), *consumers)
```

## modal/cli/app.py

```diff
@@ -5,15 +5,15 @@
 import typer
 from click import UsageError
 from grpclib import GRPCError, Status
 from rich.text import Text
 
 from modal._output import OutputManager, get_app_logs_loop
 from modal._utils.async_utils import synchronizer
-from modal.app_utils import _list_apps
+from modal.app import _list_apps
 from modal.cli.utils import ENV_OPTION, display_table, timestamp_to_local
 from modal.client import _Client
 from modal.environments import ensure_env
 from modal_proto import api_pb2
 
 app_cli = typer.Typer(name="app", help="Manage deployed and running apps.", no_args_is_help=True)
 
@@ -28,19 +28,20 @@
 }
 
 
 @app_cli.command("list")
 @synchronizer.create_blocking
 async def list(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
     """List all running or recently running Modal apps for the current account"""
+    client = await _Client.from_env()
     env = ensure_env(env)
 
     column_names = ["App ID", "Name", "State", "Creation time", "Stop time"]
     rows: List[List[Union[Text, str]]] = []
-    apps: List[api_pb2.AppStats] = await _list_apps(env)
+    apps = await _list_apps(env=env, client=client)
     for app_stats in apps:
         state = APP_STATE_TO_MESSAGE.get(app_stats.state, Text("unknown", style="gray"))
 
         rows.append(
             [
                 app_stats.app_id,
                 app_stats.description,
```

## modal/cli/import_refs.py

```diff
@@ -15,17 +15,17 @@
 from typing import Any, Optional, Union
 
 import click
 from rich.console import Console
 from rich.markdown import Markdown
 
 import modal
-from modal.app import App, LocalEntrypoint
 from modal.exception import _CliUserExecutionError
 from modal.functions import Function
+from modal.stub import LocalEntrypoint, Stub
 
 
 @dataclasses.dataclass
 class ImportRef:
     file_or_module: str
     object_path: Optional[str]
 
@@ -37,16 +37,15 @@
         raise modal.exception.InvalidError(f"Invalid object reference: {object_ref}. Did you mean '::' instead of ':'?")
     else:
         file_or_module, object_path = object_ref, None
 
     return ImportRef(file_or_module, object_path)
 
 
-DEFAULT_APP_NAME = "stub"
-POSSIBLE_APP_NAMES = ["stub", "app"]
+DEFAULT_STUB_NAME = "stub"
 
 
 def import_file_or_module(file_or_module: str):
     if "" not in sys.path:
         # When running from a CLI like `modal run`
         # the current working directory isn't added to sys.path
         # so we add it in order to make module path specification possible
@@ -71,25 +70,25 @@
             module = importlib.import_module(file_or_module)
         except Exception as exc:
             raise _CliUserExecutionError(file_or_module) from exc
 
     return module
 
 
-def get_by_object_path(obj: Any, obj_path: Optional[str]) -> Optional[Any]:
+def get_by_object_path(obj: Any, obj_path: str) -> Optional[Any]:
     # Try to evaluate a `.`-delimited object path in a Modal context
     # With the caveat that some object names can actually have `.` in their name (lifecycled methods' tags)
 
     # Note: this is eager, so no backtracking is performed in case an
     # earlier match fails at some later point in the path expansion
     prefix = ""
     for segment in obj_path.split("."):
         attr = prefix + segment
         try:
-            if isinstance(obj, App):
+            if isinstance(obj, Stub):
                 if attr in obj.registered_entrypoints:
                     # local entrypoints are not on stub blueprint
                     obj = obj.registered_entrypoints[attr]
                     continue
             obj = getattr(obj, attr)
 
         except Exception:
@@ -99,116 +98,103 @@
 
     if prefix:
         return None
 
     return obj
 
 
-def get_by_object_path_try_possible_app_names(obj: Any, obj_path: Optional[str]) -> Optional[Any]:
-    """This just exists as a dumb workaround to support both "stub" and "app" """
-
-    if obj_path:
-        return get_by_object_path(obj, obj_path)
-    else:
-        for obj_path in POSSIBLE_APP_NAMES:
-            app = get_by_object_path(obj, obj_path)
-            if app is not None:
-                return app
-        else:
-            return None
-
-
 def _infer_function_or_help(
-    app: App, module, accept_local_entrypoint: bool, accept_webhook: bool
+    stub: Stub, module, accept_local_entrypoint: bool, accept_webhook: bool
 ) -> Union[Function, LocalEntrypoint]:
-    function_choices = set(app.registered_functions.keys())
+    function_choices = set(stub.registered_functions.keys())
     if not accept_webhook:
-        function_choices -= set(app.registered_web_endpoints)
+        function_choices -= set(stub.registered_web_endpoints)
     if accept_local_entrypoint:
-        function_choices |= set(app.registered_entrypoints.keys())
+        function_choices |= set(stub.registered_entrypoints.keys())
 
     sorted_function_choices = sorted(function_choices)
     registered_functions_str = "\n".join(sorted_function_choices)
     filtered_local_entrypoints = [
         name
-        for name, entrypoint in app.registered_entrypoints.items()
+        for name, entrypoint in stub.registered_entrypoints.items()
         if entrypoint.info.module_name == module.__name__
     ]
 
     if accept_local_entrypoint and len(filtered_local_entrypoints) == 1:
         # If there is just a single local entrypoint in the target module, use
         # that regardless of other functions.
         function_name = list(filtered_local_entrypoints)[0]
-    elif accept_local_entrypoint and len(app.registered_entrypoints) == 1:
+    elif accept_local_entrypoint and len(stub.registered_entrypoints) == 1:
         # Otherwise, if there is just a single local entrypoint in the stub as a whole,
         # use that one.
-        function_name = list(app.registered_entrypoints.keys())[0]
+        function_name = list(stub.registered_entrypoints.keys())[0]
     elif len(function_choices) == 1:
         function_name = sorted_function_choices[0]
     elif len(function_choices) == 0:
-        if app.registered_web_endpoints:
+        if stub.registered_web_endpoints:
             err_msg = "Modal stub has only web endpoints. Use `modal serve` instead of `modal run`."
         else:
             err_msg = "Modal stub has no registered functions. Nothing to run."
         raise click.UsageError(err_msg)
     else:
         help_text = f"""You need to specify a Modal function or local entrypoint to run, e.g.
 
 modal run app.py::my_function [...args]
 
 Registered functions and local entrypoints on the selected stub are:
 {registered_functions_str}
 """
         raise click.UsageError(help_text)
 
-    if function_name in app.registered_entrypoints:
+    if function_name in stub.registered_entrypoints:
         # entrypoint is in entrypoint registry, for now
-        return app.registered_entrypoints[function_name]
+        return stub.registered_entrypoints[function_name]
 
-    function = app.indexed_objects[function_name]  # functions are in blueprint
+    function = stub.indexed_objects[function_name]  # functions are in blueprint
     assert isinstance(function, Function)
     return function
 
 
-def _show_no_auto_detectable_app(app_ref: ImportRef) -> None:
-    object_path = app_ref.object_path
-    import_path = app_ref.file_or_module
+def _show_no_auto_detectable_stub(stub_ref: ImportRef) -> None:
+    object_path = stub_ref.object_path
+    import_path = stub_ref.file_or_module
     error_console = Console(stderr=True)
     error_console.print(f"[bold red]Could not find Modal stub '{object_path}' in {import_path}.[/bold red]")
 
     if object_path is None:
         guidance_msg = (
-            f"Expected to find a stub variable named **`{DEFAULT_APP_NAME}`** (the default stub name). If your `modal.Stub` is named differently, "
+            f"Expected to find a stub variable named **`{DEFAULT_STUB_NAME}`** (the default stub name). If your `modal.Stub` is named differently, "
             "you must specify it in the stub ref argument. "
             f"For example a stub variable `app_stub = modal.Stub()` in `{import_path}` would "
             f"be specified as `{import_path}::app_stub`."
         )
         md = Markdown(guidance_msg)
         error_console.print(md)
 
 
-def import_app(app_ref: str) -> App:
-    import_ref = parse_import_ref(app_ref)
+def import_stub(stub_ref: str) -> Stub:
+    import_ref = parse_import_ref(stub_ref)
 
     module = import_file_or_module(import_ref.file_or_module)
-    app = get_by_object_path_try_possible_app_names(module, import_ref.object_path)
+    obj_path = import_ref.object_path or DEFAULT_STUB_NAME  # get variable named "stub" by default
+    stub = get_by_object_path(module, obj_path)
 
-    if app is None:
-        _show_no_auto_detectable_app(import_ref)
+    if stub is None:
+        _show_no_auto_detectable_stub(import_ref)
         sys.exit(1)
 
-    if not isinstance(app, App):
-        raise click.UsageError(f"{app} is not a Modal Stub")
+    if not isinstance(stub, Stub):
+        raise click.UsageError(f"{stub} is not a Modal Stub")
 
-    return app
+    return stub
 
 
-def _show_function_ref_help(app_ref: ImportRef, base_cmd: str) -> None:
-    object_path = app_ref.object_path
-    import_path = app_ref.file_or_module
+def _show_function_ref_help(stub_ref: ImportRef, base_cmd: str) -> None:
+    object_path = stub_ref.object_path
+    import_path = stub_ref.file_or_module
     error_console = Console(stderr=True)
     if object_path:
         error_console.print(
             f"[bold red]Could not find Modal function or local entrypoint '{object_path}' in '{import_path}'.[/bold red]"
         )
     else:
         error_console.print(
@@ -232,33 +218,29 @@
 
 def import_function(
     func_ref: str, base_cmd: str, accept_local_entrypoint=True, accept_webhook=False
 ) -> Union[Function, LocalEntrypoint]:
     import_ref = parse_import_ref(func_ref)
 
     module = import_file_or_module(import_ref.file_or_module)
-    app_or_function = get_by_object_path_try_possible_app_names(module, import_ref.object_path)
+    obj_path = import_ref.object_path or DEFAULT_STUB_NAME  # get variable named "stub" by default
+    stub_or_function = get_by_object_path(module, obj_path)
 
-    if app_or_function is None:
+    if stub_or_function is None:
         _show_function_ref_help(import_ref, base_cmd)
         sys.exit(1)
 
-    if isinstance(app_or_function, App):
+    if isinstance(stub_or_function, Stub):
         # infer function or display help for how to select one
-        app = app_or_function
-        function_handle = _infer_function_or_help(app, module, accept_local_entrypoint, accept_webhook)
+        stub = stub_or_function
+        function_handle = _infer_function_or_help(stub, module, accept_local_entrypoint, accept_webhook)
         return function_handle
-    elif isinstance(app_or_function, Function):
-        return app_or_function
-    elif isinstance(app_or_function, LocalEntrypoint):
+    elif isinstance(stub_or_function, Function):
+        return stub_or_function
+    elif isinstance(stub_or_function, LocalEntrypoint):
         if not accept_local_entrypoint:
             raise click.UsageError(
                 f"{func_ref} is not a Modal Function (a Modal local_entrypoint can't be used in this context)"
             )
-        return app_or_function
+        return stub_or_function
     else:
-        raise click.UsageError(f"{app_or_function} is not a Modal entity (should be a App or Function)")
-
-
-# For backwards compatibility - delete soon
-# We use it in our internal intergration tests
-import_stub = import_app
+        raise click.UsageError(f"{stub_or_function} is not a Modal entity (should be a Stub or Function)")
```

## modal/cli/launch.py

```diff
@@ -1,52 +1,36 @@
 # Copyright Modal Labs 2023
-import asyncio
-import inspect
-import json
-import os
+import subprocess
+import tempfile
 from pathlib import Path
-from typing import Any, Dict, Optional
+from typing import Optional
 
 from typer import Typer
 
-from ..app import App
-from ..exception import _CliUserExecutionError
-from ..runner import run_app
-from .import_refs import import_function
-
 launch_cli = Typer(
     name="launch",
     no_args_is_help=True,
     help="""
     [Preview] Open a serverless app instance on Modal.
 
     This command is in preview and may change in the future.
     """,
 )
 
 
-def _launch_program(name: str, filename: str, args: Dict[str, Any]) -> None:
-    os.environ["MODAL_LAUNCH_LOCAL_ARGS"] = json.dumps(args)
-
-    program_path = str(Path(__file__).parent / "programs" / filename)
-    entrypoint = import_function(program_path, "modal launch")
-    app: App = entrypoint.stub
-    app.set_description(f"modal launch {name}")
-
-    # `launch/` scripts must have a `local_entrypoint()` with no args, for simplicity here.
-    func = entrypoint.info.raw_f
-    isasync = inspect.iscoroutinefunction(func)
-    with run_app(app):
-        try:
-            if isasync:
-                asyncio.run(func())
-            else:
-                func()
-        except Exception as exc:
-            raise _CliUserExecutionError(inspect.getsourcefile(func)) from exc
+def _launch_program(name: str, args) -> None:
+    contents = (Path(__file__).parent / "programs" / name).read_text()
+    contents = contents.replace("args: Dict[str, Any] = {}", f"args: Any = {repr(args)}")
+
+    # TODO: This is a big hack and can break for unexpected $PATH reasons. Make an actual code path
+    # for correctly setting up and running a program in the CLI.
+    with tempfile.TemporaryDirectory() as tmpdir:
+        f = Path(tmpdir) / name
+        f.write_text(contents)
+        subprocess.run(["modal", "run", f])
 
 
 @launch_cli.command(name="jupyter", help="Start Jupyter Lab on Modal.")
 def jupyter(
     cpu: int = 8,
     memory: int = 32768,
     gpu: Optional[str] = None,
@@ -58,15 +42,15 @@
         "cpu": cpu,
         "memory": memory,
         "gpu": gpu,
         "timeout": timeout,
         "image": image,
         "add_python": add_python,
     }
-    _launch_program("jupyter", "run_jupyter.py", args)
+    _launch_program("run_jupyter.py", args)
 
 
 @launch_cli.command(name="vscode", help="Start Visual Studio Code on Modal.")
 def vscode(
     cpu: int = 8,
     memory: int = 32768,
     gpu: Optional[str] = None,
@@ -74,8 +58,8 @@
 ):
     args = {
         "cpu": cpu,
         "memory": memory,
         "gpu": gpu,
         "timeout": timeout,
     }
-    _launch_program("vscode", "vscode.py", args)
+    _launch_program("vscode.py", args)
```

## modal/cli/network_file_system.py

```diff
@@ -25,14 +25,17 @@
 from modal.cli._download import _glob_download
 from modal.cli.utils import ENV_OPTION, display_table
 from modal.client import _Client
 from modal.environments import ensure_env
 from modal.network_file_system import _NetworkFileSystem
 from modal_proto import api_pb2
 
+FileType = api_pb2.SharedVolumeListFilesEntry.FileType
+
+
 nfs_cli = Typer(name="nfs", help="Read and edit `modal.NetworkFileSystem` file systems.", no_args_is_help=True)
 
 
 @nfs_cli.command(name="list", help="List the names of all network file systems.")
 @synchronizer.create_blocking
 async def list(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
     env = ensure_env(env)
@@ -107,15 +110,15 @@
         console.print(f"Directory listing of '{path}' in '{volume_name}'")
         table = Table()
 
         table.add_column("filename")
         table.add_column("type")
 
         for entry in entries:
-            filetype = "dir" if entry.type == api_pb2.FileEntry.FileType.DIRECTORY else "file"
+            filetype = "dir" if entry.type == FileType.DIRECTORY else "file"
             table.add_row(entry.path, filetype)
         console.print(table)
     else:
         for entry in entries:
             print(entry.path)
 
 
@@ -188,16 +191,25 @@
 
     Use "-" (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).
     """
     ensure_env(env)
     destination = Path(local_destination)
     volume = await _volume_from_name(volume_name)
 
+    def is_file_fn(entry):
+        return entry.type == FileType.FILE
+
     if "*" in remote_path:
-        await _glob_download(volume, remote_path, destination, force)
+        await _glob_download(
+            volume,
+            is_file_fn,
+            remote_path,
+            destination,
+            force,
+        )
         return
 
     if destination != PIPE_PATH:
         if destination.is_dir():
             destination = destination / remote_path.rsplit("/")[-1]
 
         if destination.exists() and not force:
```

## modal/cli/run.py

```diff
@@ -9,23 +9,23 @@
 from typing import Any, Callable, Dict, Optional, get_type_hints
 
 import click
 import typer
 from rich.console import Console
 from typing_extensions import TypedDict
 
-from ..app import App, LocalEntrypoint
 from ..config import config
 from ..environments import ensure_env
 from ..exception import ExecutionError, InvalidError, _CliUserExecutionError
-from ..functions import Function, _FunctionSpec
+from ..functions import Function, FunctionEnv
 from ..image import Image
-from ..runner import deploy_app, interactive_shell, run_app
-from ..serving import serve_app
-from .import_refs import import_app, import_function
+from ..runner import deploy_stub, interactive_shell, run_stub
+from ..serving import serve_stub
+from ..stub import LocalEntrypoint, Stub
+from .import_refs import import_function, import_stub
 from .utils import ENV_OPTION, ENV_OPTION_HELP
 
 
 class ParameterMetadata(TypedDict):
     name: str
     default: Any
     annotation: Any
@@ -114,26 +114,26 @@
         else:
             kwargs["required"] = True
 
         click.option(cli_name, **kwargs)(func)
     return func
 
 
-def _get_clean_app_description(func_ref: str) -> str:
+def _get_clean_stub_description(func_ref: str) -> str:
     # If possible, consider the 'ref' argument the start of the app's args. Everything
     # before it Modal CLI cruft (eg. `modal run --detach`).
     try:
         func_ref_arg_idx = sys.argv.index(func_ref)
         return " ".join(sys.argv[func_ref_arg_idx:])
     except ValueError:
         return " ".join(sys.argv)
 
 
-def _get_click_command_for_function(app: App, function_tag):
-    function = app.indexed_objects[function_tag]
+def _get_click_command_for_function(stub: Stub, function_tag):
+    function = stub.indexed_objects[function_tag]
     assert isinstance(function, Function)
 
     if function.is_generator:
         raise InvalidError("`modal run` is not supported for generator functions")
 
     signature: Dict[str, ParameterMetadata]
     if function.info.cls is not None:
@@ -142,16 +142,16 @@
         signature = dict(**cls_signature, **fun_signature)  # Pool all arguments
         # TODO(erikbern): assert there's no overlap?
     else:
         signature = _get_signature(function.info.raw_f)
 
     @click.pass_context
     def f(ctx, **kwargs):
-        with run_app(
-            app,
+        with run_stub(
+            stub,
             detach=ctx.obj["detach"],
             show_progress=ctx.obj["show_progress"],
             environment_name=ctx.obj["env"],
             interactive=ctx.obj["interactive"],
         ):
             if function.info.cls is None:
                 function.remote(**kwargs)
@@ -163,27 +163,27 @@
                 method = function.from_parametrized(None, False, None, tuple(), cls_kwargs)
                 method.remote(**fun_kwargs)
 
     with_click_options = _add_click_options(f, signature)
     return click.command(with_click_options)
 
 
-def _get_click_command_for_local_entrypoint(app: App, entrypoint: LocalEntrypoint):
+def _get_click_command_for_local_entrypoint(stub: Stub, entrypoint: LocalEntrypoint):
     func = entrypoint.info.raw_f
     isasync = inspect.iscoroutinefunction(func)
 
     @click.pass_context
     def f(ctx, *args, **kwargs):
         if ctx.obj["detach"]:
             print(
                 "Note that running a local entrypoint in detached mode only keeps the last triggered Modal function alive after the parent process has been killed or disconnected."
             )
 
-        with run_app(
-            app,
+        with run_stub(
+            stub,
             detach=ctx.obj["detach"],
             show_progress=ctx.obj["show_progress"],
             environment_name=ctx.obj["env"],
             interactive=ctx.obj["interactive"],
         ):
             try:
                 if isasync:
@@ -201,22 +201,22 @@
     def get_command(self, ctx, func_ref):
         # note: get_command here is run before the "group logic" in the `run` logic below
         # so to ensure that `env` has been globally populated before user code is loaded, it
         # needs to be handled here, and not in the `run` logic below
         ctx.ensure_object(dict)
         ctx.obj["env"] = ensure_env(ctx.params["env"])
         function_or_entrypoint = import_function(func_ref, accept_local_entrypoint=True, base_cmd="modal run")
-        app: App = function_or_entrypoint.stub
-        if app.description is None:
-            app.set_description(_get_clean_app_description(func_ref))
+        stub: Stub = function_or_entrypoint.stub
+        if stub.description is None:
+            stub.set_description(_get_clean_stub_description(func_ref))
         if isinstance(function_or_entrypoint, LocalEntrypoint):
-            click_command = _get_click_command_for_local_entrypoint(app, function_or_entrypoint)
+            click_command = _get_click_command_for_local_entrypoint(stub, function_or_entrypoint)
         else:
             tag = function_or_entrypoint.info.get_tag()
-            click_command = _get_click_command_for_function(app, tag)
+            click_command = _get_click_command_for_function(stub, tag)
 
         return click_command
 
 
 @click.group(
     cls=RunGroup,
     subcommand_metavar="FUNC_REF",
@@ -259,61 +259,61 @@
     ctx.ensure_object(dict)
     ctx.obj["detach"] = detach  # if subcommand would be a click command...
     ctx.obj["show_progress"] = False if quiet else True
     ctx.obj["interactive"] = interactive
 
 
 def deploy(
-    app_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
+    stub_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
     name: str = typer.Option(None, help="Name of the deployment."),
     env: str = ENV_OPTION,
     public: bool = typer.Option(
         False, help="[beta] Publicize the deployment so other workspaces can lookup the function."
     ),
     skip_confirm: bool = typer.Option(False, help="Skip public app confirmation dialog."),
 ):
     # this ensures that `modal.lookup()` without environment specification uses the same env as specified
     env = ensure_env(env)
 
-    app = import_app(app_ref)
+    stub = import_stub(stub_ref)
 
     if name is None:
-        name = app.name
+        name = stub.name
 
     if public and not skip_confirm:
         if not click.confirm(
             " Public apps are a beta feature. \n"
             "Making an app public will allow any user (including from outside your workspace) to look up and use your functions.\n"
             "Are you sure you want your app to be public?"
         ):
             return
 
-    deploy_app(app, name=name, environment_name=env, public=public)
+    deploy_stub(stub, name=name, environment_name=env, public=public)
 
 
 def serve(
-    app_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
+    stub_ref: str = typer.Argument(..., help="Path to a Python file with a stub."),
     timeout: Optional[float] = None,
     env: str = ENV_OPTION,
 ):
     """Run a web endpoint(s) associated with a Modal stub and hot-reload code.
 
     **Examples:**
 
     ```bash
     modal serve hello_world.py
     ```
     """
     env = ensure_env(env)
 
-    app = import_app(app_ref)
-    if app.description is None:
-        app.set_description(_get_clean_app_description(app_ref))
+    stub = import_stub(stub_ref)
+    if stub.description is None:
+        stub.set_description(_get_clean_stub_description(stub_ref))
 
-    with serve_app(app, app_ref, environment_name=env):
+    with serve_stub(stub, stub_ref, environment_name=env):
         if timeout is None:
             timeout = config["serve_timeout"]
         if timeout is None:
             timeout = float("inf")
         while timeout > 0:
             t = min(timeout, 3600)
             time.sleep(t)
@@ -371,31 +371,31 @@
     """
     env = ensure_env(env)
 
     console = Console()
     if not console.is_terminal:
         raise click.UsageError("`modal shell` can only be run from a terminal.")
 
-    app = App("modal shell")
+    stub = Stub("modal shell")
 
     if func_ref is not None:
         function = import_function(func_ref, accept_local_entrypoint=False, accept_webhook=True, base_cmd="modal shell")
         assert isinstance(function, Function)
-        function_spec: _FunctionSpec = function.spec
+        function_env: FunctionEnv = function.env
         start_shell = partial(
             interactive_shell,
-            image=function_spec.image,
-            mounts=function_spec.mounts,
-            secrets=function_spec.secrets,
-            network_file_systems=function_spec.network_file_systems,
-            gpu=function_spec.gpu,
-            cloud=function_spec.cloud,
-            cpu=function_spec.cpu,
-            memory=function_spec.memory,
-            volumes=function_spec.volumes,
+            image=function_env.image,
+            mounts=function_env.mounts,
+            secrets=function_env.secrets,
+            network_file_systems=function_env.network_file_systems,
+            gpu=function_env.gpu,
+            cloud=function_env.cloud,
+            cpu=function_env.cpu,
+            memory=function_env.memory,
+            volumes=function_env.volumes,
             _allow_background_volume_commits=True,
         )
     else:
         modal_image = Image.from_registry(image, add_python=add_python) if image else None
         start_shell = partial(interactive_shell, image=modal_image, cpu=cpu, memory=memory, gpu=gpu, cloud=cloud)
 
-    start_shell(app, cmd=[cmd], environment_name=env, timeout=3600)
+    start_shell(stub, cmd=[cmd], environment_name=env, timeout=3600)
```

## modal/cli/volume.py

```diff
@@ -24,14 +24,15 @@
 from modal.cli._download import _glob_download
 from modal.cli.utils import ENV_OPTION, display_table
 from modal.client import _Client
 from modal.environments import ensure_env
 from modal.volume import _Volume, _VolumeUploadContextManager
 from modal_proto import api_pb2
 
+FileType = api_pb2.VolumeListFilesEntry.FileType
 PIPE_PATH = Path("-")
 
 volume_cli = Typer(
     name="volume",
     no_args_is_help=True,
     help="""
     Read and edit `modal.Volume` volumes.
@@ -99,16 +100,19 @@
 
     Use "-" (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).
     """
     ensure_env(env)
     destination = Path(local_destination)
     volume = await _Volume.lookup(volume_name, environment_name=env)
 
+    def is_file_fn(entry):
+        return entry.type == FileType.FILE
+
     if "*" in remote_path:
-        await _glob_download(volume, remote_path, destination, force)
+        await _glob_download(volume, is_file_fn, remote_path, destination, force)
         return
 
     if destination != PIPE_PATH:
         if destination.is_dir():
             destination = destination / remote_path.rsplit("/")[-1]
 
         if destination.exists() and not force:
@@ -178,17 +182,17 @@
         console.print(f"Directory listing of '{path}' in '{volume_name}'")
         table = Table()
         for name in ["filename", "type", "created/modified", "size"]:
             table.add_column(name)
 
         locale_tz = datetime.now().astimezone().tzinfo
         for entry in entries:
-            if entry.type == api_pb2.FileEntry.FileType.DIRECTORY:
+            if entry.type == FileType.DIRECTORY:
                 filetype = "dir"
-            elif entry.type == api_pb2.FileEntry.FileType.SYMLINK:
+            elif entry.type == FileType.SYMLINK:
                 filetype = "link"
             else:
                 filetype = "file"
             table.add_row(
                 entry.path,
                 filetype,
                 str(datetime.fromtimestamp(entry.mtime, tz=locale_tz)),
```

## modal/cli/programs/run_jupyter.py

```diff
@@ -1,38 +1,35 @@
 # Copyright Modal Labs 2023
 # type: ignore
-import json
 import os
 import secrets
 import socket
 import subprocess
 import threading
 import time
 import webbrowser
 from typing import Any, Dict
 
 from modal import Image, Queue, Stub, forward
 
-# Passed by `modal launch` locally via CLI, empty on remote runner.
-args: Dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_LOCAL_ARGS", "{}"))
-
+args: Dict[str, Any] = {}
 
 stub = Stub()
 stub.image = Image.from_registry(args.get("image"), add_python=args.get("add_python")).pip_install("jupyterlab")
 
 
 def wait_for_port(url: str, q: Queue):
     start_time = time.monotonic()
     while True:
         try:
-            with socket.create_connection(("localhost", 8888), timeout=30.0):
+            with socket.create_connection(("localhost", 8888), timeout=15.0):
                 break
         except OSError as exc:
             time.sleep(0.01)
-            if time.monotonic() - start_time >= 30.0:
+            if time.monotonic() - start_time >= 15.0:
                 raise TimeoutError("Waited too long for port 8888 to accept connections") from exc
     q.put(url)
 
 
 @stub.function(cpu=args.get("cpu"), memory=args.get("memory"), gpu=args.get("gpu"), timeout=args.get("timeout"))
 def run_jupyter(q: Queue):
     os.mkdir("/lab")
@@ -57,14 +54,14 @@
         )
     q.put("done")
 
 
 @stub.local_entrypoint()
 def main():
     with Queue.ephemeral() as q:
-        run_jupyter.spawn(q)
+        stub.run_jupyter.spawn(q)
         url = q.get()
         time.sleep(1)  # Give Jupyter a chance to start up
         print("\nJupyter on Modal, opening in browser...")
         print(f"   -> {url}\n")
         webbrowser.open(url)
         assert q.get() == "done"
```

## modal/cli/programs/vscode.py

```diff
@@ -1,38 +1,35 @@
 # Copyright Modal Labs 2023
 # type: ignore
-import json
 import os
 import secrets
 import socket
 import subprocess
 import threading
 import time
 import webbrowser
 from typing import Any, Dict, Tuple
 
 from modal import Image, Queue, Stub, forward
 
-# Passed by `modal launch` locally via CLI, empty on remote runner.
-args: Dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_LOCAL_ARGS", "{}"))
-
+args: Dict[str, Any] = {}
 
 stub = Stub()
 stub.image = Image.from_registry("codercom/code-server", add_python="3.11").dockerfile_commands("ENTRYPOINT []")
 
 
 def wait_for_port(data: Tuple[str, str], q: Queue):
     start_time = time.monotonic()
     while True:
         try:
-            with socket.create_connection(("localhost", 8080), timeout=30.0):
+            with socket.create_connection(("localhost", 8080), timeout=15.0):
                 break
         except OSError as exc:
             time.sleep(0.01)
-            if time.monotonic() - start_time >= 30.0:
+            if time.monotonic() - start_time >= 15.0:
                 raise TimeoutError("Waited too long for port 8080 to accept connections") from exc
     q.put(data)
 
 
 @stub.function(cpu=args.get("cpu"), memory=args.get("memory"), gpu=args.get("gpu"), timeout=args.get("timeout"))
 def run_vscode(q: Queue):
     os.chdir("/home/coder")
@@ -46,15 +43,15 @@
         )
     q.put("done")
 
 
 @stub.local_entrypoint()
 def main():
     with Queue.ephemeral() as q:
-        run_vscode.spawn(q)
+        stub.run_vscode.spawn(q)
         url, token = q.get()
-        time.sleep(1)  # Give VS Code a chance to start up
+        time.sleep(1)  # Give Jupyter a chance to start up
         print("\nVS Code on Modal, opening in browser...")
         print(f"   -> {url}")
         print(f"   -> password: {token}\n")
         webbrowser.open(url)
         assert q.get() == "done"
```

## modal_docs/mdmd/signatures.py

```diff
@@ -1,11 +1,10 @@
 # Copyright Modal Labs 2023
 import ast
 import inspect
-import re
 import textwrap
 import warnings
 from typing import Tuple
 
 from synchronicity.synchronizer import FunctionWithAio
 
 
@@ -67,11 +66,8 @@
     ):
         # hack to "reset" signature to a blocking one if the underlying source definition is async
         # but the wrapper function isn't (like when synchronicity wraps an async function as a blocking one)
         definition_source = definition_source.replace("async def", "def")
         definition_source = definition_source.replace("asynccontextmanager", "contextmanager")
         definition_source = definition_source.replace("AsyncIterator", "Iterator")
 
-    # remove any synchronicity-internal decorators
-    definition_source, _ = re.subn(r"^\s*@synchronizer\..*\n", "", definition_source)
-
     return definition_source
```

## modal_proto/api.proto

```diff
@@ -80,42 +80,38 @@
   CLIENT_TYPE_WORKER = 2 [deprecated=true];
   CLIENT_TYPE_CONTAINER = 3;
   CLIENT_TYPE_SERVER = 4 [deprecated=true];
   CLIENT_TYPE_WEB_SERVER = 5;
 }
 
 message CloudBucketMount {
-  enum BucketType {
-    UNSPECIFIED = 0;
-    S3 = 1;
-    R2 = 2;
-    GCP = 3;
-  }
-
   string bucket_name = 1;
   string mount_path = 2;
   string credentials_secret_id = 3;
   bool read_only = 4;
-  BucketType bucket_type = 5;
   bool requester_pays = 6;
-  optional string bucket_endpoint_url = 7;
+
+  enum BucketType {
+    UNSPECIFIED = 0;
+    S3 = 1;
+  }
+  BucketType bucket_type = 5;
 }
 
 enum CloudProvider {
   CLOUD_PROVIDER_UNSPECIFIED = 0;
   CLOUD_PROVIDER_AWS = 1;
   CLOUD_PROVIDER_GCP = 2;
   CLOUD_PROVIDER_AUTO = 3;
   CLOUD_PROVIDER_OCI = 4;
-  CLOUD_PROVIDER_LAMBDA_LABS = 5;
 }
 
 // Which data format a binary message is encoded with.
 enum DataFormat {
-  DATA_FORMAT_UNSPECIFIED = 0;
+  DATA_FORMAT_UNSPECIFIED = 0; // Equivalent to PICKLE in client version 0.52 and earlier.
   DATA_FORMAT_PICKLE = 1; // Cloudpickle
   DATA_FORMAT_ASGI = 2; // "Asgi" protobuf message
   DATA_FORMAT_GENERATOR_DONE = 3; // "GeneratorDone" protobuf message
 }
 
 enum DeploymentNamespace {
   DEPLOYMENT_NAMESPACE_UNSPECIFIED = 0;
@@ -132,28 +128,14 @@
 enum FileDescriptor {
   FILE_DESCRIPTOR_UNSPECIFIED = 0;
   FILE_DESCRIPTOR_STDOUT = 1;
   FILE_DESCRIPTOR_STDERR = 2;
   FILE_DESCRIPTOR_INFO = 3;
 }
 
-// A file entry when listing files in a volume or network file system.
-message FileEntry {
-  enum FileType {
-    UNSPECIFIED = 0;
-    FILE = 1;
-    DIRECTORY = 2;
-    SYMLINK = 3;
-  }
-  string path = 1;
-  FileType type = 2;
-  uint64 mtime = 3;
-  uint64 size = 4;
-}
-
 enum FunctionCallType {
   FUNCTION_CALL_TYPE_UNSPECIFIED = 0;
   FUNCTION_CALL_TYPE_UNARY = 1;
   FUNCTION_CALL_TYPE_MAP = 2;
 }
 
 enum GPUType {
@@ -697,22 +679,14 @@
   optional bytes value = 2;
 }
 
 message DictHeartbeatRequest {
   string dict_id = 1;
 }
 
-message DictContentsRequest {
-  string dict_id = 1;
-  // Setting these to True will populate the corresponding field in the response, otherwise it will be null
-  // This lets us support the keys/values/items SDK API through one RPC without unnecessary data transfer
-  bool keys = 2;
-  bool values = 3;
-}
-
 message DictLenRequest {
   string dict_id = 1;
 }
 
 message DictLenResponse {
   int32 len = 1;
 }
@@ -1250,14 +1224,18 @@
   string runtime = 19;
   // Not included in image definition checksum as debug features do not affect built image.
   bool runtime_debug = 20;
 
   BuildFunction build_function = 21;
 }
 
+message ImageBuilderVersionLookupResponse {
+  string version = 1;
+}
+
 message ImageContextFile {
   string filename = 1;
   bytes data = 2;
 }
 
 message ImageGetOrCreateRequest {
   Image image = 2;
@@ -1429,67 +1407,46 @@
   string queue_id = 1;
 }
 
 message QueueGetRequest {
   string queue_id = 1;
   float timeout = 3;
   int32 n_values = 4;
-  bytes partition_key = 5;
 }
 
 message QueueGetResponse {
   repeated bytes values = 2;
 }
 
 message QueueHeartbeatRequest {
   string queue_id = 1;
 }
 
 message QueuePutRequest {
   string queue_id = 1;
   repeated bytes values = 4;
-  bytes partition_key = 5;
-  int32 partition_ttl_seconds = 6;
 }
 
 message QueueLenRequest {
   string queue_id = 1;
-  bytes partition_key = 2;
 }
 
 message QueueLenResponse {
   int32 len = 1;
 }
 
-message QueueNextItemsRequest {
-  string queue_id = 1;
-  bytes partition_key = 2;
-  string last_entry_id = 3;
-  float item_poll_timeout = 4; // seconds
-}
-
-message QueueItem {
-  bytes value = 1;
-  string entry_id = 2;
-}
-
-message QueueNextItemsResponse {
-  repeated QueueItem items = 1;
-}
-
 message RateLimit {
   int32 limit = 1;
   RateLimitInterval interval = 2;
 }
 
 message Resources {
-  uint32 memory_mb = 2; // MiB
-  uint32 milli_cpu = 3; // milli CPU cores
+  uint32 memory_mb = 2;
+  uint32 milli_cpu = 3;
   GPUConfig gpu_config = 4;
-  uint32 memory_mb_max = 5; // MiB
 }
 
 message S3Mount {
   string bucket_name = 1;
   string mount_path = 2;
   string credentials_secret_id = 3;
   bool read_only = 4;
@@ -1727,16 +1684,26 @@
 
 message SharedVolumeRemoveFileRequest {
   string shared_volume_id = 1 [ (modal.options.audit_target_attr) = true ];
   string path = 2;
   bool recursive = 3;
 }
 
+message SharedVolumeListFilesEntry {
+  enum FileType {
+    UNSPECIFIED = 0;
+    FILE = 1;
+    DIRECTORY = 2;
+  }
+  string path = 1;
+  FileType type = 2;
+}
+
 message SharedVolumeListFilesResponse {
-  repeated FileEntry entries = 1;
+  repeated SharedVolumeListFilesEntry entries = 1;
 }
 
 message SharedVolumeMount {
   string mount_path = 1;
   string shared_volume_id = 2;
   CloudProvider cloud_provider = 3;
   bool allow_cross_region = 4;
@@ -1899,22 +1866,35 @@
     string data_blob_id = 2;
   }
   uint64 size = 3; // total file size
   uint64 start = 4; // file position of first byte returned
   uint64 len = 5; // number of bytes returned
 }
 
+message VolumeListFilesEntry {
+  enum FileType {
+    UNSPECIFIED = 0;
+    FILE = 1;
+    DIRECTORY = 2;
+    SYMLINK = 3;
+  }
+  string path = 1;
+  FileType type = 2;
+  uint64 mtime = 3;
+  uint64 size = 4;
+}
+
 message VolumeListFilesRequest {
   string volume_id = 1;
   string path = 2;
   optional int32 max_entries = 3;
 }
 
 message VolumeListFilesResponse {
-  repeated FileEntry entries = 1;
+  repeated VolumeListFilesEntry entries = 1;
 }
 
 message VolumeListItem {
   string label = 1;  // app name of object entity app
   string volume_id = 2;
   double created_at = 3;
 }
@@ -1975,15 +1955,15 @@
 message WebUrlInfo {
   bool truncated = 1;
   bool has_unique_hash = 2;
   bool label_stolen = 3;
 }
 
 message WorkspaceNameLookupResponse {
-  string workspace_name = 1 [deprecated=true];
+  string workspace_name = 1;
   string username = 2;
 }
 
 // Used for `modal container exec`
 message RuntimeOutputMessage {
   // only stdout / stderr is used
   FileDescriptor file_descriptor = 1;
@@ -2041,15 +2021,14 @@
   rpc ContainerCheckpoint(ContainerCheckpointRequest) returns (google.protobuf.Empty);
 
   // Dicts
   rpc DictClear(DictClearRequest) returns (google.protobuf.Empty);
   rpc DictCreate(DictCreateRequest) returns (DictCreateResponse);  // Will be superseded by DictGetOrCreate
   rpc DictGetOrCreate(DictGetOrCreateRequest) returns (DictGetOrCreateResponse);
   rpc DictHeartbeat(DictHeartbeatRequest) returns (google.protobuf.Empty);
-  rpc DictContents(DictContentsRequest) returns (stream DictEntry);
   rpc DictUpdate(DictUpdateRequest) returns (DictUpdateResponse);
   rpc DictGet(DictGetRequest) returns (DictGetResponse);
   rpc DictPop(DictPopRequest) returns (DictPopResponse);
   rpc DictContains(DictContainsRequest) returns (DictContainsResponse);
   rpc DictLen(DictLenRequest) returns (DictLenResponse);
 
   // Domains
@@ -2087,14 +2066,15 @@
 
   // Interactive functions
   rpc FunctionStartPtyShell(google.protobuf.Empty) returns (google.protobuf.Empty);
 
   // Images
   rpc ImageGetOrCreate(ImageGetOrCreateRequest) returns (ImageGetOrCreateResponse);
   rpc ImageJoinStreaming(ImageJoinStreamingRequest) returns (stream ImageJoinStreamingResponse);
+  rpc ImageBuilderVersionLookup(google.protobuf.Empty) returns (ImageBuilderVersionLookupResponse);
 
   // Mounts
   rpc MountPutFile(MountPutFileRequest) returns (MountPutFileResponse);
   rpc MountBuild(MountBuildRequest) returns (MountBuildResponse);
   rpc MountGetOrCreate(MountGetOrCreateRequest) returns (MountGetOrCreateResponse);
 
   // Proxies
@@ -2103,15 +2083,14 @@
   // Queues
   rpc QueueCreate(QueueCreateRequest) returns (QueueCreateResponse);
   rpc QueueGetOrCreate(QueueGetOrCreateRequest) returns (QueueGetOrCreateResponse);
   rpc QueueGet(QueueGetRequest) returns (QueueGetResponse);
   rpc QueueHeartbeat(QueueHeartbeatRequest) returns (google.protobuf.Empty);
   rpc QueuePut(QueuePutRequest) returns (google.protobuf.Empty);
   rpc QueueLen(QueueLenRequest) returns (QueueLenResponse);
-  rpc QueueNextItems(QueueNextItemsRequest) returns (QueueNextItemsResponse);
 
   // Sandboxes
   rpc SandboxCreate(SandboxCreateRequest) returns (SandboxCreateResponse);
   rpc SandboxGetTaskId(SandboxGetTaskIdRequest) returns (SandboxGetTaskIdResponse); // needed for modal container exec
   rpc SandboxGetLogs(SandboxGetLogsRequest) returns (stream TaskLogsBatch);
   rpc SandboxWait(SandboxWaitRequest) returns (SandboxWaitResponse);
   rpc SandboxList(SandboxListRequest) returns (SandboxListResponse);
```

## modal_proto/api_grpc.py

```diff
@@ -126,18 +126,14 @@
         pass
 
     @abc.abstractmethod
     async def DictHeartbeat(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictHeartbeatRequest, google.protobuf.empty_pb2.Empty]') -> None:
         pass
 
     @abc.abstractmethod
-    async def DictContents(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictContentsRequest, modal_proto.api_pb2.DictEntry]') -> None:
-        pass
-
-    @abc.abstractmethod
     async def DictUpdate(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictUpdateRequest, modal_proto.api_pb2.DictUpdateResponse]') -> None:
         pass
 
     @abc.abstractmethod
     async def DictGet(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.DictGetRequest, modal_proto.api_pb2.DictGetResponse]') -> None:
         pass
 
@@ -262,14 +258,18 @@
         pass
 
     @abc.abstractmethod
     async def ImageJoinStreaming(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.ImageJoinStreamingRequest, modal_proto.api_pb2.ImageJoinStreamingResponse]') -> None:
         pass
 
     @abc.abstractmethod
+    async def ImageBuilderVersionLookup(self, stream: 'grpclib.server.Stream[google.protobuf.empty_pb2.Empty, modal_proto.api_pb2.ImageBuilderVersionLookupResponse]') -> None:
+        pass
+
+    @abc.abstractmethod
     async def MountPutFile(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.MountPutFileRequest, modal_proto.api_pb2.MountPutFileResponse]') -> None:
         pass
 
     @abc.abstractmethod
     async def MountBuild(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.MountBuildRequest, modal_proto.api_pb2.MountBuildResponse]') -> None:
         pass
 
@@ -302,18 +302,14 @@
         pass
 
     @abc.abstractmethod
     async def QueueLen(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.QueueLenRequest, modal_proto.api_pb2.QueueLenResponse]') -> None:
         pass
 
     @abc.abstractmethod
-    async def QueueNextItems(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.QueueNextItemsRequest, modal_proto.api_pb2.QueueNextItemsResponse]') -> None:
-        pass
-
-    @abc.abstractmethod
     async def SandboxCreate(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.SandboxCreateRequest, modal_proto.api_pb2.SandboxCreateResponse]') -> None:
         pass
 
     @abc.abstractmethod
     async def SandboxGetTaskId(self, stream: 'grpclib.server.Stream[modal_proto.api_pb2.SandboxGetTaskIdRequest, modal_proto.api_pb2.SandboxGetTaskIdResponse]') -> None:
         pass
 
@@ -631,20 +627,14 @@
             ),
             '/modal.client.ModalClient/DictHeartbeat': grpclib.const.Handler(
                 self.DictHeartbeat,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.DictHeartbeatRequest,
                 google.protobuf.empty_pb2.Empty,
             ),
-            '/modal.client.ModalClient/DictContents': grpclib.const.Handler(
-                self.DictContents,
-                grpclib.const.Cardinality.UNARY_STREAM,
-                modal_proto.api_pb2.DictContentsRequest,
-                modal_proto.api_pb2.DictEntry,
-            ),
             '/modal.client.ModalClient/DictUpdate': grpclib.const.Handler(
                 self.DictUpdate,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.DictUpdateRequest,
                 modal_proto.api_pb2.DictUpdateResponse,
             ),
             '/modal.client.ModalClient/DictGet': grpclib.const.Handler(
@@ -835,14 +825,20 @@
             ),
             '/modal.client.ModalClient/ImageJoinStreaming': grpclib.const.Handler(
                 self.ImageJoinStreaming,
                 grpclib.const.Cardinality.UNARY_STREAM,
                 modal_proto.api_pb2.ImageJoinStreamingRequest,
                 modal_proto.api_pb2.ImageJoinStreamingResponse,
             ),
+            '/modal.client.ModalClient/ImageBuilderVersionLookup': grpclib.const.Handler(
+                self.ImageBuilderVersionLookup,
+                grpclib.const.Cardinality.UNARY_UNARY,
+                google.protobuf.empty_pb2.Empty,
+                modal_proto.api_pb2.ImageBuilderVersionLookupResponse,
+            ),
             '/modal.client.ModalClient/MountPutFile': grpclib.const.Handler(
                 self.MountPutFile,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.MountPutFileRequest,
                 modal_proto.api_pb2.MountPutFileResponse,
             ),
             '/modal.client.ModalClient/MountBuild': grpclib.const.Handler(
@@ -895,20 +891,14 @@
             ),
             '/modal.client.ModalClient/QueueLen': grpclib.const.Handler(
                 self.QueueLen,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.QueueLenRequest,
                 modal_proto.api_pb2.QueueLenResponse,
             ),
-            '/modal.client.ModalClient/QueueNextItems': grpclib.const.Handler(
-                self.QueueNextItems,
-                grpclib.const.Cardinality.UNARY_UNARY,
-                modal_proto.api_pb2.QueueNextItemsRequest,
-                modal_proto.api_pb2.QueueNextItemsResponse,
-            ),
             '/modal.client.ModalClient/SandboxCreate': grpclib.const.Handler(
                 self.SandboxCreate,
                 grpclib.const.Cardinality.UNARY_UNARY,
                 modal_proto.api_pb2.SandboxCreateRequest,
                 modal_proto.api_pb2.SandboxCreateResponse,
             ),
             '/modal.client.ModalClient/SandboxGetTaskId': grpclib.const.Handler(
@@ -1309,20 +1299,14 @@
         )
         self.DictHeartbeat = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/DictHeartbeat',
             modal_proto.api_pb2.DictHeartbeatRequest,
             google.protobuf.empty_pb2.Empty,
         )
-        self.DictContents = grpclib.client.UnaryStreamMethod(
-            channel,
-            '/modal.client.ModalClient/DictContents',
-            modal_proto.api_pb2.DictContentsRequest,
-            modal_proto.api_pb2.DictEntry,
-        )
         self.DictUpdate = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/DictUpdate',
             modal_proto.api_pb2.DictUpdateRequest,
             modal_proto.api_pb2.DictUpdateResponse,
         )
         self.DictGet = grpclib.client.UnaryUnaryMethod(
@@ -1513,14 +1497,20 @@
         )
         self.ImageJoinStreaming = grpclib.client.UnaryStreamMethod(
             channel,
             '/modal.client.ModalClient/ImageJoinStreaming',
             modal_proto.api_pb2.ImageJoinStreamingRequest,
             modal_proto.api_pb2.ImageJoinStreamingResponse,
         )
+        self.ImageBuilderVersionLookup = grpclib.client.UnaryUnaryMethod(
+            channel,
+            '/modal.client.ModalClient/ImageBuilderVersionLookup',
+            google.protobuf.empty_pb2.Empty,
+            modal_proto.api_pb2.ImageBuilderVersionLookupResponse,
+        )
         self.MountPutFile = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/MountPutFile',
             modal_proto.api_pb2.MountPutFileRequest,
             modal_proto.api_pb2.MountPutFileResponse,
         )
         self.MountBuild = grpclib.client.UnaryUnaryMethod(
@@ -1573,20 +1563,14 @@
         )
         self.QueueLen = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/QueueLen',
             modal_proto.api_pb2.QueueLenRequest,
             modal_proto.api_pb2.QueueLenResponse,
         )
-        self.QueueNextItems = grpclib.client.UnaryUnaryMethod(
-            channel,
-            '/modal.client.ModalClient/QueueNextItems',
-            modal_proto.api_pb2.QueueNextItemsRequest,
-            modal_proto.api_pb2.QueueNextItemsResponse,
-        )
         self.SandboxCreate = grpclib.client.UnaryUnaryMethod(
             channel,
             '/modal.client.ModalClient/SandboxCreate',
             modal_proto.api_pb2.SandboxCreateRequest,
             modal_proto.api_pb2.SandboxCreateResponse,
         )
         self.SandboxGetTaskId = grpclib.client.UnaryUnaryMethod(
```

## modal_proto/api_pb2.py

```diff
@@ -14,15 +14,15 @@
 
 
 from modal_proto import options_pb2 as modal__proto_dot_options__pb2
 from google.protobuf import empty_pb2 as google_dot_protobuf_dot_empty__pb2
 from google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15modal_proto/api.proto\x12\x0cmodal.client\x1a\x19modal_proto/options.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1egoogle/protobuf/wrappers.proto\"\xb7\x02\n\x10\x43loudBucketMount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\x12>\n\x0b\x62ucket_type\x18\x05 \x01(\x0e\x32).modal.client.CloudBucketMount.BucketType\x12\x16\n\x0erequester_pays\x18\x06 \x01(\x08\x12 \n\x13\x62ucket_endpoint_url\x18\x07 \x01(\tH\x00\x88\x01\x01\"6\n\nBucketType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x06\n\x02S3\x10\x01\x12\x06\n\x02R2\x10\x02\x12\x07\n\x03GCP\x10\x03\x42\x16\n\x14_bucket_endpoint_url\"\xa9\x01\n\tFileEntry\x12\x0c\n\x04path\x18\x01 \x01(\t\x12.\n\x04type\x18\x02 \x01(\x0e\x32 .modal.client.FileEntry.FileType\x12\r\n\x05mtime\x18\x03 \x01(\x04\x12\x0c\n\x04size\x18\x04 \x01(\x04\"A\n\x08\x46ileType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x08\n\x04\x46ILE\x10\x01\x12\r\n\tDIRECTORY\x10\x02\x12\x0b\n\x07SYMLINK\x10\x03\"r\n\x1a\x41ppClientDisconnectRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x31\n\x06reason\x18\x02 \x01(\x0e\x32!.modal.client.AppDisconnectReason\x12\x11\n\texception\x18\x03 \x01(\t\"\xb3\x01\n\x10\x41ppCreateRequest\x12\x17\n\tclient_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x13\n\x0b\x64\x65scription\x18\x02 \x01(\t\x12\x12\n\x06\x64\x65tach\x18\x03 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x0cinitializing\x18\x04 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x10\x65nvironment_name\x18\x05 \x01(\t\x12)\n\tapp_state\x18\x06 \x01(\x0e\x32\x16.modal.client.AppState\"9\n\x11\x41ppCreateResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x14\n\x0c\x61pp_logs_url\x18\x02 \x01(\t\"S\n\x0e\x41ppStopRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12+\n\x06source\x18\x02 \x01(\x0e\x32\x1b.modal.client.AppStopSource\"\xba\x01\n\x10\x41ppDeployRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x15\n\robject_entity\x18\x04 \x01(\t\x12\x35\n\nvisibility\x18\x05 \x01(\x0e\x32!.modal.client.AppDeployVisibility\" \n\x11\x41ppDeployResponse\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x8f\x01\n\x1c\x41ppDeploySingleObjectRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12\x11\n\tobject_id\x18\x04 \x01(\t\"/\n\x1d\x41ppDeploySingleObjectResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"}\n\x1d\x41ppGetByDeploymentNameRequest\x12\x34\n\tnamespace\x18\x01 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"0\n\x1e\x41ppGetByDeploymentNameResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"\xba\x01\n\x11\x41ppGetLogsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x05 \x01(\t\x12\x10\n\x08input_id\x18\x06 \x01(\t\x12\x0f\n\x07task_id\x18\x07 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\"A\n\x14\x41ppGetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11include_unindexed\x18\x02 \x01(\x08\"F\n\x11\x41ppGetObjectsItem\x12\x0b\n\x03tag\x18\x01 \x01(\t\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\"G\n\x15\x41ppGetObjectsResponse\x12.\n\x05items\x18\x02 \x03(\x0b\x32\x1f.modal.client.AppGetObjectsItem\"%\n\x13\x41ppHeartbeatRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"*\n\x0e\x41ppListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"7\n\x0f\x41ppListResponse\x12$\n\x04\x61pps\x18\x01 \x03(\x0b\x32\x16.modal.client.AppStats\"\xb8\x01\n\x16\x41ppLookupObjectRequest\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x10\n\x08\x61pp_name\x18\x03 \x01(\t\x12\x12\n\nobject_tag\x18\x04 \x01(\t\x12\x11\n\tobject_id\x18\x05 \x01(\t\x12\x15\n\robject_entity\x18\x06 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x07 \x01(\t\"O\n\x17\x41ppLookupObjectResponse\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\x12\x0e\n\x06\x61pp_id\x18\x07 \x01(\t\"\xaf\x02\n\x14\x41ppSetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12T\n\x12indexed_object_ids\x18\x02 \x03(\x0b\x32\x38.modal.client.AppSetObjectsRequest.IndexedObjectIdsEntry\x12\x11\n\tclient_id\x18\x03 \x01(\t\x12\x1c\n\x14unindexed_object_ids\x18\x04 \x03(\t\x12-\n\rnew_app_state\x18\x05 \x01(\x0e\x32\x16.modal.client.AppState\x12\x18\n\x10single_object_id\x18\x06 \x01(\t\x1a\x37\n\x15IndexedObjectIdsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\xd1\x01\n\x08\x41ppStats\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64\x65scription\x18\x03 \x01(\t\x12%\n\x05state\x18\x04 \x01(\x0e\x32\x16.modal.client.AppState\x12\x12\n\ncreated_at\x18\x05 \x01(\x01\x12\x12\n\nstopped_at\x18\x06 \x01(\x01\x12\x17\n\x0fn_running_tasks\x18\x08 \x01(\x05\x12\x15\n\robject_entity\x18\t \x01(\t\x12\x0c\n\x04name\x18\n \x01(\t\x12\x13\n\x0b\x64\x65ployed_at\x18\x0b \x01(\x01\"\xa3\x0e\n\x04\x41sgi\x12\'\n\x04http\x18\x01 \x01(\x0b\x32\x17.modal.client.Asgi.HttpH\x00\x12\x36\n\x0chttp_request\x18\x02 \x01(\x0b\x32\x1e.modal.client.Asgi.HttpRequestH\x00\x12\x43\n\x13http_response_start\x18\x03 \x01(\x0b\x32$.modal.client.Asgi.HttpResponseStartH\x00\x12\x41\n\x12http_response_body\x18\x04 \x01(\x0b\x32#.modal.client.Asgi.HttpResponseBodyH\x00\x12I\n\x16http_response_trailers\x18\x05 \x01(\x0b\x32\'.modal.client.Asgi.HttpResponseTrailersH\x00\x12<\n\x0fhttp_disconnect\x18\x06 \x01(\x0b\x32!.modal.client.Asgi.HttpDisconnectH\x00\x12\x31\n\twebsocket\x18\x07 \x01(\x0b\x32\x1c.modal.client.Asgi.WebsocketH\x00\x12@\n\x11websocket_connect\x18\x08 \x01(\x0b\x32#.modal.client.Asgi.WebsocketConnectH\x00\x12>\n\x10websocket_accept\x18\t \x01(\x0b\x32\".modal.client.Asgi.WebsocketAcceptH\x00\x12@\n\x11websocket_receive\x18\n \x01(\x0b\x32#.modal.client.Asgi.WebsocketReceiveH\x00\x12:\n\x0ewebsocket_send\x18\x0b \x01(\x0b\x32 .modal.client.Asgi.WebsocketSendH\x00\x12\x46\n\x14websocket_disconnect\x18\x0c \x01(\x0b\x32&.modal.client.Asgi.WebsocketDisconnectH\x00\x12<\n\x0fwebsocket_close\x18\r \x01(\x0b\x32!.modal.client.Asgi.WebsocketCloseH\x00\x1a\xc5\x01\n\x04Http\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x0e\n\x06scheme\x18\x03 \x01(\t\x12\x0c\n\x04path\x18\x04 \x01(\t\x12\x14\n\x0cquery_string\x18\x05 \x01(\x0c\x12\x0f\n\x07headers\x18\x06 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x07 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x08 \x01(\rH\x01\x88\x01\x01\x42\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a.\n\x0bHttpRequest\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a\x46\n\x11HttpResponseStart\x12\x0e\n\x06status\x18\x01 \x01(\r\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x12\x10\n\x08trailers\x18\x03 \x01(\x08\x1a\x33\n\x10HttpResponseBody\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a>\n\x14HttpResponseTrailers\x12\x0f\n\x07headers\x18\x01 \x03(\x0c\x12\x15\n\rmore_trailers\x18\x02 \x01(\x08\x1a\x10\n\x0eHttpDisconnect\x1a\xd0\x01\n\tWebsocket\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06scheme\x18\x02 \x01(\t\x12\x0c\n\x04path\x18\x03 \x01(\t\x12\x14\n\x0cquery_string\x18\x04 \x01(\x0c\x12\x0f\n\x07headers\x18\x05 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x06 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x07 \x01(\rH\x01\x88\x01\x01\x12\x14\n\x0csubprotocols\x18\x08 \x03(\tB\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a\x12\n\x10WebsocketConnect\x1aL\n\x0fWebsocketAccept\x12\x18\n\x0bsubprotocol\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x42\x0e\n\x0c_subprotocol\x1a>\n\x10WebsocketReceive\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a;\n\rWebsocketSend\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a\x31\n\x13WebsocketDisconnect\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x42\x07\n\x05_code\x1a<\n\x0eWebsocketClose\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x12\x0e\n\x06reason\x18\x02 \x01(\tB\x07\n\x05_codeB\x06\n\x04type\"7\n\tBaseImage\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x12\n\ndocker_tag\x18\x02 \x01(\tJ\x04\x08\x04\x10\x05\"_\n\x11\x42lobCreateRequest\x12\x13\n\x0b\x63ontent_md5\x18\x01 \x01(\t\x12\x1d\n\x15\x63ontent_sha256_base64\x18\x02 \x01(\t\x12\x16\n\x0e\x63ontent_length\x18\x03 \x01(\x03\"\x84\x01\n\x12\x42lobCreateResponse\x12\x0f\n\x07\x62lob_id\x18\x02 \x01(\t\x12\x14\n\nupload_url\x18\x01 \x01(\tH\x00\x12\x32\n\tmultipart\x18\x03 \x01(\x0b\x32\x1d.modal.client.MultiPartUploadH\x00\x42\x13\n\x11upload_type_oneof\"!\n\x0e\x42lobGetRequest\x12\x0f\n\x07\x62lob_id\x18\x01 \x01(\t\"\'\n\x0f\x42lobGetResponse\x12\x14\n\x0c\x64ownload_url\x18\x01 \x01(\t\"i\n\x0e\x43heckpointInfo\x12\x10\n\x08\x63hecksum\x18\x01 \x01(\t\x12.\n\x06status\x18\x02 \x01(\x0e\x32\x1e.modal.client.CheckpointStatus\x12\x15\n\rcheckpoint_id\x18\x03 \x01(\t\"q\n\x12\x43lassCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_class_id\x18\x02 \x01(\t\x12*\n\x07methods\x18\x03 \x03(\x0b\x32\x19.modal.client.ClassMethod\"c\n\x13\x43lassCreateResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"\xb9\x01\n\x0f\x43lassGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\x12\x18\n\x10lookup_published\x18\x08 \x01(\x08\x12\x16\n\x0eworkspace_name\x18\t \x01(\t\"`\n\x10\x43lassGetResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"A\n\x13\x43lassHandleMetadata\x12*\n\x07methods\x18\x01 \x03(\x0b\x32\x19.modal.client.ClassMethod\"\x81\x01\n\x0b\x43lassMethod\x12\x15\n\rfunction_name\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x46\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"[\n\x13\x43lientCreateRequest\x12\x33\n\x0b\x63lient_type\x18\x01 \x01(\x0e\x32\x18.modal.client.ClientTypeB\x04\x80\xb5\x18\x01\x12\x0f\n\x07version\x18\x02 \x01(\t\"U\n\x14\x43lientCreateResponse\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12\x1b\n\x13\x64\x65precation_warning\x18\x03 \x01(\t\"E\n\x13\x43lientHelloResponse\x12\x0f\n\x07warning\x18\x01 \x01(\t\x12\x1d\n\x15image_builder_version\x18\x02 \x01(\t\"g\n\x16\x43lientHeartbeatRequest\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\x18\n\x10\x63urrent_input_id\x18\x03 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x04 \x01(\x01\"\x9f\x03\n\x12\x43ontainerArguments\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x04 \x01(\t\x12,\n\x0c\x66unction_def\x18\x07 \x01(\x0b\x32\x16.modal.client.Function\x12+\n\nproxy_info\x18\x08 \x01(\x0b\x32\x17.modal.client.ProxyInfo\x12M\n\x0ftracing_context\x18\t \x03(\x0b\x32\x34.modal.client.ContainerArguments.TracingContextEntry\x12\x19\n\x11serialized_params\x18\n \x01(\x0c\x12\x0f\n\x07runtime\x18\x0b \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\r \x01(\t\x12\x1a\n\rcheckpoint_id\x18\x0e \x01(\tH\x00\x88\x01\x01\x1a\x35\n\x13TracingContextEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x42\x10\n\x0e_checkpoint_id\"%\n\x10\x43\x61ncelInputEvent\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"t\n\x1a\x43ontainerHeartbeatResponse\x12?\n\x12\x63\x61ncel_input_event\x18\x01 \x01(\x0b\x32\x1e.modal.client.CancelInputEventH\x00\x88\x01\x01\x42\x15\n\x13_cancel_input_event\"\x85\x01\n\x19\x43ontainerHeartbeatRequest\x12\x18\n\x10\x63urrent_input_id\x18\x01 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x02 \x01(\x01\x12,\n$supports_graceful_input_cancellation\x18\x03 \x01(\x08\"3\n\x1a\x43ontainerCheckpointRequest\x12\x15\n\rcheckpoint_id\x18\x01 \x01(\t\"\x9d\x01\n\x14\x43ontainerExecRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0f\n\x07\x63ommand\x18\x02 \x03(\t\x12\'\n\x08pty_info\x18\x03 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12#\n\x1bterminate_container_on_exit\x18\x04 \x01(\x08\x12\x15\n\rruntime_debug\x18\x05 \x01(\x08\"[\n\x1d\x43ontainerExecGetOutputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x18\n\x10last_batch_index\x18\x03 \x01(\x04\"a\n\x1c\x43ontainerExecPutInputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x30\n\x05input\x18\x02 \x01(\x0b\x32!.modal.client.RuntimeInputMessage\"(\n\x15\x43ontainerExecResponse\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\"\"\n\x12\x43ustomDomainConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\"\x1f\n\x10\x43ustomDomainInfo\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x7f\n\tDataChunk\x12-\n\x0b\x64\x61ta_format\x18\x01 \x01(\x0e\x32\x18.modal.client.DataFormat\x12\x0e\n\x04\x64\x61ta\x18\x02 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x03 \x01(\tH\x00\x12\r\n\x05index\x18\x04 \x01(\x04\x42\x0c\n\ndata_oneof\"#\n\x10\x44ictClearRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"3\n\x13\x44ictContainsRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\"%\n\x14\x44ictContainsResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\"j\n\x11\x44ictCreateRequest\x12%\n\x04\x64\x61ta\x18\x01 \x03(\x0b\x32\x17.modal.client.DictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x18\n\x10\x65xisting_dict_id\x18\x03 \x01(\t\"%\n\x12\x44ictCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\xe8\x01\n\x16\x44ictGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12%\n\x04\x64\x61ta\x18\x05 \x03(\x0b\x32\x17.modal.client.DictEntry\"*\n\x17\x44ictGetOrCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\'\n\tDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\x0c\x12\r\n\x05value\x18\x02 \x01(\x0c\".\n\x0e\x44ictGetRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictGetResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"\'\n\x14\x44ictHeartbeatRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"D\n\x13\x44ictContentsRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0c\n\x04keys\x18\x02 \x01(\x08\x12\x0e\n\x06values\x18\x03 \x01(\x08\"!\n\x0e\x44ictLenRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\x1e\n\x0f\x44ictLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\".\n\x0e\x44ictPopRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictPopResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"N\n\x11\x44ictUpdateRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12(\n\x07updates\x18\x02 \x03(\x0b\x32\x17.modal.client.DictEntry\"\x14\n\x12\x44ictUpdateResponse\"S\n\tDNSRecord\x12)\n\x04type\x18\x01 \x01(\x0e\x32\x1b.modal.client.DNSRecordType\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\r\n\x05value\x18\x03 \x01(\t\"0\n\x13\x44omainCreateRequest\x12\x19\n\x0b\x64omain_name\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"W\n\x14\x44omainCreateResponse\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12,\n\x0b\x64ns_records\x18\x02 \x03(\x0b\x32\x17.modal.client.DNSRecord\"\x13\n\x11\x44omainListRequest\"\xaf\x01\n\x06\x44omain\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64omain_name\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12;\n\x12\x63\x65rtificate_status\x18\x04 \x01(\x0e\x32\x1f.modal.client.CertificateStatus\x12,\n\x0b\x64ns_records\x18\x05 \x03(\x0b\x32\x17.modal.client.DNSRecord\";\n\x12\x44omainListResponse\x12%\n\x07\x64omains\x18\x01 \x03(\x0b\x32\x14.modal.client.Domain\"3\n\x1e\x44omainCertificateVerifyRequest\x12\x11\n\tdomain_id\x18\x01 \x01(\t\"G\n\x1f\x44omainCertificateVerifyResponse\x12$\n\x06\x64omain\x18\x01 \x01(\x0b\x32\x14.modal.client.Domain\"(\n\x18\x45nvironmentCreateRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"(\n\x18\x45nvironmentDeleteRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\";\n\x13\x45nvironmentListItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x16\n\x0ewebhook_suffix\x18\x02 \x01(\t\"K\n\x17\x45nvironmentListResponse\x12\x30\n\x05items\x18\x02 \x03(\x0b\x32!.modal.client.EnvironmentListItem\"\x8e\x01\n\x18\x45nvironmentUpdateRequest\x12\x14\n\x0c\x63urrent_name\x18\x01 \x01(\t\x12*\n\x04name\x18\x02 \x01(\x0b\x32\x1c.google.protobuf.StringValue\x12\x30\n\nweb_suffix\x18\x03 \x01(\x0b\x32\x1c.google.protobuf.StringValue\"d\n\x1a\x46unctionCallPutDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12,\n\x0b\x64\x61ta_chunks\x18\x02 \x03(\x0b\x32\x17.modal.client.DataChunk\"J\n\x1a\x46unctionCallGetDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nlast_index\x18\x02 \x01(\x04\"\xf4\x0e\n\x08\x46unction\x12\x13\n\x0bmodule_name\x18\x01 \x01(\t\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12\x11\n\tmount_ids\x18\x03 \x03(\t\x12\x10\n\x08image_id\x18\x04 \x01(\t\x12\x1b\n\x13\x66unction_serialized\x18\x06 \x01(\x0c\x12>\n\x0f\x64\x65\x66inition_type\x18\x07 \x01(\x0e\x32%.modal.client.Function.DefinitionType\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12*\n\tresources\x18\t \x01(\x0b\x32\x17.modal.client.Resources\x12\x12\n\nsecret_ids\x18\n \x03(\t\x12+\n\nrate_limit\x18\x0b \x01(\x0b\x32\x17.modal.client.RateLimit\x12\x33\n\x0ewebhook_config\x18\x0f \x01(\x0b\x32\x1b.modal.client.WebhookConfig\x12=\n\x14shared_volume_mounts\x18\x10 \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\x08proxy_id\x18\x11 \x01(\tH\x00\x88\x01\x01\x12\x37\n\x0cretry_policy\x18\x12 \x01(\x0b\x32!.modal.client.FunctionRetryPolicy\x12\x19\n\x11\x63oncurrency_limit\x18\x13 \x01(\r\x12\x11\n\tkeep_warm\x18\x14 \x01(\x08\x12\x14\n\x0ctimeout_secs\x18\x15 \x01(\r\x12\'\n\x08pty_info\x18\x16 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12\x18\n\x10\x63lass_serialized\x18\x17 \x01(\x0c\x12\x1e\n\x16task_idle_timeout_secs\x18\x19 \x01(\r\x12\x38\n\x0e\x63loud_provider\x18\x1a \x01(\x0e\x32\x1b.modal.client.CloudProviderH\x01\x88\x01\x01\x12\x16\n\x0ewarm_pool_size\x18\x1b \x01(\r\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12.\n\x0cweb_url_info\x18\x1d \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12\x0f\n\x07runtime\x18\x1e \x01(\t\x12\x11\n\tstub_name\x18\x1f \x01(\t\x12\x30\n\rvolume_mounts\x18! \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\x1f\n\x17\x61llow_concurrent_inputs\x18\" \x01(\r\x12:\n\x12\x63ustom_domain_info\x18# \x03(\x0b\x32\x1e.modal.client.CustomDomainInfo\x12\x11\n\tworker_id\x18$ \x01(\t\x12\x15\n\rruntime_debug\x18% \x01(\x08\x12\x1b\n\x13is_builder_function\x18  \x01(\x08\x12\x18\n\x10is_auto_snapshot\x18& \x01(\x08\x12\x11\n\tis_method\x18\' \x01(\x08\x12!\n\x19is_checkpointing_function\x18( \x01(\x08\x12\x1d\n\x15\x63heckpointing_enabled\x18) \x01(\x08\x12\x30\n\ncheckpoint\x18* \x01(\x0b\x32\x1c.modal.client.CheckpointInfo\x12;\n\x13object_dependencies\x18+ \x03(\x0b\x32\x1e.modal.client.ObjectDependency\x12\x15\n\rblock_network\x18, \x01(\x08\x12\x12\n\nmax_inputs\x18. \x01(\r\x12(\n\ts3_mounts\x18/ \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x33 \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x1b\n\x13_experimental_boost\x18\x30 \x01(\x08\x12\x1f\n\x17_experimental_scheduler\x18\x31 \x01(\x08\x12P\n!_experimental_scheduler_placement\x18\x32 \x01(\x0b\x32 .modal.client.SchedulerPlacementH\x02\x88\x01\x01\"k\n\x0e\x44\x65\x66initionType\x12\x1f\n\x1b\x44\x45\x46INITION_TYPE_UNSPECIFIED\x10\x00\x12\x1e\n\x1a\x44\x45\x46INITION_TYPE_SERIALIZED\x10\x01\x12\x18\n\x14\x44\x45\x46INITION_TYPE_FILE\x10\x02\"f\n\x0c\x46unctionType\x12\x1d\n\x19\x46UNCTION_TYPE_UNSPECIFIED\x10\x00\x12\x1b\n\x17\x46UNCTION_TYPE_GENERATOR\x10\x01\x12\x1a\n\x16\x46UNCTION_TYPE_FUNCTION\x10\x02\x42\x0b\n\t_proxy_idB\x11\n\x0f_cloud_providerB$\n\"X_experimental_scheduler_placement\"|\n\x12SchedulerPlacement\x12\x14\n\x07_region\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x12\n\x05_zone\x18\x02 \x01(\tH\x01\x88\x01\x01\x12\x17\n\n_lifecycle\x18\x03 \x01(\tH\x02\x88\x01\x01\x42\n\n\x08X_regionB\x08\n\x06X_zoneB\r\n\x0bX_lifecycle\"\x8f\x01\n\x16\x46unctionHandleMetadata\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12\x11\n\tis_method\x18\' \x01(\x08\"\x9f\x01\n\x15\x46unctionCreateRequest\x12(\n\x08\x66unction\x18\x01 \x01(\x0b\x32\x16.modal.client.Function\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12(\n\x08schedule\x18\x06 \x01(\x0b\x32\x16.modal.client.Schedule\x12\x1c\n\x14\x65xisting_function_id\x18\x07 \x01(\t\"\xc7\x04\n\x0f\x46unctionOptions\x12\x12\n\nsecret_ids\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12/\n\tresources\x18\x03 \x01(\x0b\x32\x17.modal.client.ResourcesH\x00\x88\x01\x01\x12<\n\x0cretry_policy\x18\x04 \x01(\x0b\x32!.modal.client.FunctionRetryPolicyH\x01\x88\x01\x01\x12\x1e\n\x11\x63oncurrency_limit\x18\x05 \x01(\rH\x02\x88\x01\x01\x12\x19\n\x0ctimeout_secs\x18\x06 \x01(\rH\x03\x88\x01\x01\x12#\n\x16task_idle_timeout_secs\x18\x07 \x01(\rH\x04\x88\x01\x01\x12\x1b\n\x0ewarm_pool_size\x18\x08 \x01(\rH\x05\x88\x01\x01\x12\x30\n\rvolume_mounts\x18\t \x03(\x0b\x32\x19.modal.client.VolumeMount\x12$\n\x17\x61llow_concurrent_inputs\x18\n \x01(\rH\x06\x88\x01\x01\x12\x1d\n\x15replace_volume_mounts\x18\x0b \x01(\x08\x12\x1a\n\x12replace_secret_ids\x18\x0c \x01(\x08\x42\x0c\n\n_resourcesB\x0f\n\r_retry_policyB\x14\n\x12_concurrency_limitB\x0f\n\r_timeout_secsB\x19\n\x17_task_idle_timeout_secsB\x11\n\x0f_warm_pool_sizeB\x1a\n\x18_allow_concurrent_inputs\"\xd6\x01\n\x18\x46unctionPrecreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x1b\n\rfunction_name\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x1c\n\x14\x65xisting_function_id\x18\x03 \x01(\t\x12:\n\rfunction_type\x18\x04 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x33\n\x0ewebhook_config\x18\x05 \x01(\x0b\x32\x1b.modal.client.WebhookConfig\"o\n\x19\x46unctionPrecreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x9e\x01\n\x19\x46unctionBindParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x19\n\x11serialized_params\x18\x02 \x01(\x0c\x12\x37\n\x10\x66unction_options\x18\x03 \x01(\x0b\x32\x1d.modal.client.FunctionOptions\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"v\n\x1a\x46unctionBindParamsResponse\x12\x19\n\x11\x62ound_function_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\xd7\x01\n\x16\x46unctionCreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12.\n\x0cweb_url_info\x18\x03 \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12(\n\x08\x66unction\x18\x04 \x01(\x0b\x32\x16.modal.client.Function\x12=\n\x0fhandle_metadata\x18\x05 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x8a\x01\n\x12\x46unctionGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"i\n\x13\x46unctionGetResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"]\n%FunctionUpdateSchedulingParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x1f\n\x17warm_pool_size_override\x18\x02 \x01(\r\"(\n&FunctionUpdateSchedulingParamsResponse\"\x8a\x01\n\x15\x46unctionGetInputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\x12\x13\n\x0bkill_switch\x18\x03 \x01(\x08\x12\x18\n\x10\x66unction_call_id\x18\x05 \x01(\tJ\x04\x08\x04\x10\x05\"y\n\x18\x46unctionGetInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x03 \x01(\x05\x12\x19\n\x11\x61verage_call_time\x18\x05 \x01(\x02\x12\x19\n\x11input_concurrency\x18\x06 \x01(\x05\"s\n\x19\x46unctionGetInputsResponse\x12\x33\n\x06inputs\x18\x03 \x03(\x0b\x32#.modal.client.FunctionGetInputsItem\x12!\n\x19rate_limit_sleep_duration\x18\x04 \x01(\x02\"\xa6\x01\n\x16\x46unctionGetOutputsItem\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x10\n\x08input_id\x18\x03 \x01(\t\x12\x11\n\tgen_index\x18\x04 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x05 \x01(\x0e\x32\x18.modal.client.DataFormat\"\x8b\x01\n\x19\x46unctionGetOutputsRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x02 \x01(\x05\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x06 \x01(\t\x12\x18\n\x10\x63lear_on_success\x18\x07 \x01(\x08\"x\n\x1a\x46unctionGetOutputsResponse\x12\x0c\n\x04idxs\x18\x03 \x03(\x05\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionGetOutputsItem\x12\x15\n\rlast_entry_id\x18\x05 \x01(\t\"3\n\x1c\x46unctionGetSerializedRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"V\n\x1d\x46unctionGetSerializedResponse\x12\x1b\n\x13\x66unction_serialized\x18\x01 \x01(\x0c\x12\x18\n\x10\x63lass_serialized\x18\x02 \x01(\x0c\"\x89\x01\n\rFunctionInput\x12\x0e\n\x04\x61rgs\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x61rgs_blob_id\x18\x07 \x01(\tH\x00\x12\x13\n\x0b\x66inal_input\x18\t \x01(\x08\x12-\n\x0b\x64\x61ta_format\x18\n \x01(\x0e\x32\x18.modal.client.DataFormatB\x0c\n\nargs_oneof\"\xd8\x01\n\x12\x46unctionMapRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x19\n\x11return_exceptions\x18\x03 \x01(\x08\x12:\n\x12\x66unction_call_type\x18\x04 \x01(\x0e\x32\x1e.modal.client.FunctionCallType\x12=\n\x10pipelined_inputs\x18\x05 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\"v\n\x13\x46unctionMapResponse\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x45\n\x10pipelined_inputs\x18\x02 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"P\n\x15\x46unctionPutInputsItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"~\n\x18\x46unctionPutInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x33\n\x06inputs\x18\x04 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\">\n\x1d\x46unctionPutInputsResponseItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12\x10\n\x08input_id\x18\x02 \x01(\t\"X\n\x19\x46unctionPutInputsResponse\x12;\n\x06inputs\x18\x01 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"\xce\x01\n\x16\x46unctionPutOutputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x18\n\x10input_started_at\x18\x03 \x01(\x01\x12\x19\n\x11output_created_at\x18\x04 \x01(\x01\x12\x11\n\tgen_index\x18\x06 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x07 \x01(\x0e\x32\x18.modal.client.DataFormat\"R\n\x19\x46unctionPutOutputsRequest\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionPutOutputsItem\"s\n\x13\x46unctionRetryPolicy\x12\x1b\n\x13\x62\x61\x63koff_coefficient\x18\x01 \x01(\x02\x12\x18\n\x10initial_delay_ms\x18\x02 \x01(\r\x12\x14\n\x0cmax_delay_ms\x18\x03 \x01(\r\x12\x0f\n\x07retries\x18\x12 \x01(\r\"7\n\x1b\x46unctionGetCallGraphRequest\x12\x18\n\x10\x66unction_call_id\x18\x02 \x01(\t\"\x8c\x01\n\x12InputCallGraphInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x39\n\x06status\x18\x02 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x0f\n\x07task_id\x18\x04 \x01(\t\"z\n\x19\x46unctionCallCallGraphInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x15\n\rfunction_name\x18\x03 \x01(\t\x12\x13\n\x0bmodule_name\x18\x04 \x01(\t\"\x91\x01\n\x1c\x46unctionGetCallGraphResponse\x12\x30\n\x06inputs\x18\x01 \x03(\x0b\x32 .modal.client.InputCallGraphInfo\x12?\n\x0e\x66unction_calls\x18\x02 \x03(\x0b\x32\'.modal.client.FunctionCallCallGraphInfo\"5\n\x19\x46unctionCallCancelRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\".\n\x17\x46unctionCallListRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"\xc3\x03\n\x10\x46unctionCallInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x12\n\ncreated_at\x18\x06 \x01(\x01\x12\x14\n\x0cscheduled_at\x18\x07 \x01(\x01\x12\x37\n\x0epending_inputs\x18\x0c \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x36\n\rfailed_inputs\x18\r \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10succeeded_inputs\x18\x0e \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x37\n\x0etimeout_inputs\x18\x0f \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10\x63\x61ncelled_inputs\x18\x10 \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x14\n\x0ctotal_inputs\x18\x11 \x01(\x05J\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0c\"R\n\x18\x46unctionCallListResponse\x12\x36\n\x0e\x66unction_calls\x18\x01 \x03(\x0b\x32\x1e.modal.client.FunctionCallInfo\"5\n\x1e\x46unctionGetCurrentStatsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"S\n\rFunctionStats\x12\x0f\n\x07\x62\x61\x63klog\x18\x01 \x01(\r\x12\x18\n\x10num_active_tasks\x18\x02 \x01(\r\x12\x17\n\x0fnum_total_tasks\x18\x03 \x01(\r\"$\n\rGeneratorDone\x12\x13\n\x0bitems_total\x18\x01 \x01(\x04\"\xfe\x04\n\rGenericResult\x12\x39\n\x06status\x18\x01 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x11\n\texception\x18\x02 \x01(\t\x12\x10\n\x08\x65xitcode\x18\x03 \x01(\x05\x12\x11\n\ttraceback\x18\x04 \x01(\t\x12\x15\n\rserialized_tb\x18\x0b \x01(\x0c\x12\x15\n\rtb_line_cache\x18\x0c \x01(\x0c\x12\x0e\n\x04\x64\x61ta\x18\x05 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\n \x01(\tH\x00\x12?\n\ngen_status\x18\x07 \x01(\x0e\x32+.modal.client.GenericResult.GeneratorStatus\x12\x1a\n\x12propagation_reason\x18\r \x01(\t\"\xc3\x01\n\rGenericStatus\x12\x1e\n\x1aGENERIC_STATUS_UNSPECIFIED\x10\x00\x12\x1a\n\x16GENERIC_STATUS_SUCCESS\x10\x01\x12\x1a\n\x16GENERIC_STATUS_FAILURE\x10\x02\x12\x1d\n\x19GENERIC_STATUS_TERMINATED\x10\x03\x12\x1a\n\x16GENERIC_STATUS_TIMEOUT\x10\x04\x12\x1f\n\x1bGENERIC_STATUS_INIT_FAILURE\x10\x05\"s\n\x0fGeneratorStatus\x12 \n\x1cGENERATOR_STATUS_UNSPECIFIED\x10\x00\x12\x1f\n\x1bGENERATOR_STATUS_INCOMPLETE\x10\x01\x12\x1d\n\x19GENERATOR_STATUS_COMPLETE\x10\x02\x42\x0c\n\ndata_oneof\"O\n\tGPUConfig\x12#\n\x04type\x18\x01 \x01(\x0e\x32\x15.modal.client.GPUType\x12\r\n\x05\x63ount\x18\x02 \x01(\r\x12\x0e\n\x06memory\x18\x03 \x01(\r\"`\n\rBuildFunction\x12\x12\n\ndefinition\x18\x01 \x01(\t\x12\x0f\n\x07globals\x18\x02 \x01(\x0c\x12*\n\x05input\x18\x03 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"\xdd\x03\n\x05Image\x12,\n\x0b\x62\x61se_images\x18\x05 \x03(\x0b\x32\x17.modal.client.BaseImage\x12\x1b\n\x13\x64ockerfile_commands\x18\x06 \x03(\t\x12\x35\n\rcontext_files\x18\x07 \x03(\x0b\x32\x1e.modal.client.ImageContextFile\x12\x0f\n\x07version\x18\x0b \x01(\t\x12\x12\n\nsecret_ids\x18\x0c \x03(\t\x12\x0b\n\x03gpu\x18\r \x01(\x08\x12\x18\n\x10\x63ontext_mount_id\x18\x0f \x01(\t\x12+\n\ngpu_config\x18\x10 \x01(\x0b\x32\x17.modal.client.GPUConfig\x12@\n\x15image_registry_config\x18\x11 \x01(\x0b\x32!.modal.client.ImageRegistryConfig\x12\x1a\n\x12\x62uild_function_def\x18\x0e \x01(\t\x12\x1e\n\x16\x62uild_function_globals\x18\x12 \x01(\x0c\x12\x0f\n\x07runtime\x18\x13 \x01(\t\x12\x15\n\rruntime_debug\x18\x14 \x01(\x08\x12\x33\n\x0e\x62uild_function\x18\x15 \x01(\x0b\x32\x1b.modal.client.BuildFunction\"2\n\x10ImageContextFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\xed\x01\n\x17ImageGetOrCreateRequest\x12\"\n\x05image\x18\x02 \x01(\x0b\x32\x13.modal.client.Image\x12\x14\n\x06\x61pp_id\x18\x04 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_image_id\x18\x05 \x01(\t\x12\x19\n\x11\x62uild_function_id\x18\x06 \x01(\t\x12\x13\n\x0b\x66orce_build\x18\x07 \x01(\x08\x12\x34\n\tnamespace\x18\x08 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x17\n\x0f\x62uilder_version\x18\t \x01(\t\",\n\x18ImageGetOrCreateResponse\x12\x10\n\x08image_id\x18\x01 \x01(\t\"U\n\x19ImageJoinStreamingRequest\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x03 \x01(\t\"\x93\x01\n\x1aImageJoinStreamingResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12)\n\ttask_logs\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x03 \x01(\t\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"d\n\x13ImageRegistryConfig\x12:\n\x12registry_auth_type\x18\x01 \x01(\x0e\x32\x1e.modal.client.RegistryAuthType\x12\x11\n\tsecret_id\x18\x02 \x01(\t\"\x99\x01\n\tInputInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x0f\n\x07task_id\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x05 \x01(\x01\x12\x19\n\x11task_startup_time\x18\x06 \x01(\x01\x12\x18\n\x10task_first_input\x18\x07 \x01(\x08\"K\n\x11InputCategoryInfo\x12\r\n\x05total\x18\x01 \x01(\x05\x12\'\n\x06latest\x18\x02 \x03(\x0b\x32\x17.modal.client.InputInfo\"l\n\x11MountBuildRequest\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_mount_id\x18\x03 \x01(\t\x12&\n\x05\x66iles\x18\x04 \x03(\x0b\x32\x17.modal.client.MountFile\"b\n\x12MountBuildResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\"\xfa\x01\n\x17MountGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12&\n\x05\x66iles\x18\x05 \x03(\x0b\x32\x17.modal.client.MountFile\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\"h\n\x18MountGetOrCreateResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\":\n\x13MountHandleMetadata\x12#\n\x1b\x63ontent_checksum_sha256_hex\x18\x01 \x01(\t\"i\n\tMountFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x11\n\x04size\x18\x04 \x01(\x04H\x00\x88\x01\x01\x12\x11\n\x04mode\x18\x05 \x01(\rH\x01\x88\x01\x01\x42\x07\n\x05_sizeB\x07\n\x05_mode\"_\n\x13MountPutFileRequest\x12\x12\n\nsha256_hex\x18\x02 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x03 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"&\n\x14MountPutFileResponse\x12\x0e\n\x06\x65xists\x18\x02 \x01(\x08\"S\n\x0fMultiPartUpload\x12\x13\n\x0bpart_length\x18\x01 \x01(\x03\x12\x13\n\x0bupload_urls\x18\x02 \x03(\t\x12\x16\n\x0e\x63ompletion_url\x18\x03 \x01(\t\"\xce\x02\n\x06Object\x12\x11\n\tobject_id\x18\x01 \x01(\t\x12H\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadataH\x00\x12\x42\n\x15mount_handle_metadata\x18\x04 \x01(\x0b\x32!.modal.client.MountHandleMetadataH\x00\x12\x42\n\x15\x63lass_handle_metadata\x18\x05 \x01(\x0b\x32!.modal.client.ClassHandleMetadataH\x00\x12\x46\n\x17sandbox_handle_metadata\x18\x06 \x01(\x0b\x32#.modal.client.SandboxHandleMetadataH\x00\x42\x17\n\x15handle_metadata_oneof\"%\n\x10ObjectDependency\x12\x11\n\tobject_id\x18\x01 \x01(\t\"\xc2\x01\n\x17ProxyGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18ProxyGetOrCreateResponse\x12\x10\n\x08proxy_id\x18\x01 \x01(\t\"\\\n\tProxyInfo\x12\x12\n\nelastic_ip\x18\x01 \x01(\t\x12\x11\n\tproxy_key\x18\x02 \x01(\t\x12\x13\n\x0bremote_addr\x18\x03 \x01(\t\x12\x13\n\x0bremote_port\x18\x04 \x01(\x05\"\x86\x02\n\x07PTYInfo\x12\x0f\n\x07\x65nabled\x18\x01 \x01(\x08\x12\x12\n\nwinsz_rows\x18\x02 \x01(\r\x12\x12\n\nwinsz_cols\x18\x03 \x01(\r\x12\x10\n\x08\x65nv_term\x18\x04 \x01(\t\x12\x15\n\renv_colorterm\x18\x05 \x01(\t\x12\x18\n\x10\x65nv_term_program\x18\x06 \x01(\t\x12/\n\x08pty_type\x18\x07 \x01(\x0e\x32\x1d.modal.client.PTYInfo.PTYType\"N\n\x07PTYType\x12\x18\n\x14PTY_TYPE_UNSPECIFIED\x10\x00\x12\x15\n\x11PTY_TYPE_FUNCTION\x10\x01\x12\x12\n\x0ePTY_TYPE_SHELL\x10\x02\"?\n\x12QueueCreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11\x65xisting_queue_id\x18\x02 \x01(\t\"\'\n\x13QueueCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"\xc2\x01\n\x17QueueGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18QueueGetOrCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"]\n\x0fQueueGetRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x10\n\x08n_values\x18\x04 \x01(\x05\x12\x15\n\rpartition_key\x18\x05 \x01(\x0c\"\"\n\x10QueueGetResponse\x12\x0e\n\x06values\x18\x02 \x03(\x0c\")\n\x15QueueHeartbeatRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"i\n\x0fQueuePutRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0e\n\x06values\x18\x04 \x03(\x0c\x12\x15\n\rpartition_key\x18\x05 \x01(\x0c\x12\x1d\n\x15partition_ttl_seconds\x18\x06 \x01(\x05\":\n\x0fQueueLenRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x15\n\rpartition_key\x18\x02 \x01(\x0c\"\x1f\n\x10QueueLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\"r\n\x15QueueNextItemsRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x15\n\rpartition_key\x18\x02 \x01(\x0c\x12\x15\n\rlast_entry_id\x18\x03 \x01(\t\x12\x19\n\x11item_poll_timeout\x18\x04 \x01(\x02\",\n\tQueueItem\x12\r\n\x05value\x18\x01 \x01(\x0c\x12\x10\n\x08\x65ntry_id\x18\x02 \x01(\t\"@\n\x16QueueNextItemsResponse\x12&\n\x05items\x18\x01 \x03(\x0b\x32\x17.modal.client.QueueItem\"M\n\tRateLimit\x12\r\n\x05limit\x18\x01 \x01(\x05\x12\x31\n\x08interval\x18\x02 \x01(\x0e\x32\x1f.modal.client.RateLimitInterval\"u\n\tResources\x12\x11\n\tmemory_mb\x18\x02 \x01(\r\x12\x11\n\tmilli_cpu\x18\x03 \x01(\r\x12+\n\ngpu_config\x18\x04 \x01(\x0b\x32\x17.modal.client.GPUConfig\x12\x15\n\rmemory_mb_max\x18\x05 \x01(\r\"d\n\x07S3Mount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\"\x99\x04\n\x07Sandbox\x12\x17\n\x0f\x65ntrypoint_args\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12\x10\n\x08image_id\x18\x03 \x01(\t\x12\x12\n\nsecret_ids\x18\x04 \x03(\t\x12*\n\tresources\x18\x05 \x01(\x0b\x32\x17.modal.client.Resources\x12\x33\n\x0e\x63loud_provider\x18\x06 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x14\n\x0ctimeout_secs\x18\x07 \x01(\r\x12\x14\n\x07workdir\x18\x08 \x01(\tH\x00\x88\x01\x01\x12\x33\n\nnfs_mounts\x18\t \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\rruntime_debug\x18\n \x01(\x08\x12\x15\n\rblock_network\x18\x0b \x01(\x08\x12(\n\ts3_mounts\x18\x0c \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x0e \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x30\n\rvolume_mounts\x18\r \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\'\n\x08pty_info\x18\x0f \x01(\x0b\x32\x15.modal.client.PTYInfoB\n\n\x08_workdir\"W\n\x14SandboxCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\"+\n\x15SandboxCreateResponse\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"-\n\x17SandboxGetTaskIdRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"+\n\x18SandboxGetTaskIdResponse\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"\x8a\x01\n\x15SandboxGetLogsRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x02 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\"Y\n\x18SandboxStdinWriteRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\r\n\x05input\x18\x02 \x01(\x0c\x12\r\n\x05index\x18\x03 \x01(\r\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"\x1b\n\x19SandboxStdinWriteResponse\"D\n\x15SandboxHandleMetadata\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x83\x01\n\x0bSandboxInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12)\n\ttask_info\x18\x04 \x01(\x0b\x32\x16.modal.client.TaskInfo\">\n\x12SandboxListRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x18\n\x10\x62\x65\x66ore_timestamp\x18\x02 \x01(\x01\"C\n\x13SandboxListResponse\x12,\n\tsandboxes\x18\x01 \x03(\x0b\x32\x19.modal.client.SandboxInfo\"-\n\x17SandboxTerminateRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"P\n\x18SandboxTerminateResponse\x12\x34\n\x0f\x65xisting_result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"9\n\x12SandboxWaitRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\"B\n\x13SandboxWaitResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x8e\x02\n\x08Schedule\x12+\n\x04\x63ron\x18\x01 \x01(\x0b\x32\x1b.modal.client.Schedule.CronH\x00\x12/\n\x06period\x18\x02 \x01(\x0b\x32\x1d.modal.client.Schedule.PeriodH\x00\x1a\x1b\n\x04\x43ron\x12\x13\n\x0b\x63ron_string\x18\x01 \x01(\t\x1au\n\x06Period\x12\r\n\x05years\x18\x01 \x01(\x05\x12\x0e\n\x06months\x18\x02 \x01(\x05\x12\r\n\x05weeks\x18\x03 \x01(\x05\x12\x0c\n\x04\x64\x61ys\x18\x04 \x01(\x05\x12\r\n\x05hours\x18\x05 \x01(\x05\x12\x0f\n\x07minutes\x18\x06 \x01(\x05\x12\x0f\n\x07seconds\x18\x07 \x01(\x02\x42\x10\n\x0eschedule_oneof\"\xd0\x01\n\x13SecretCreateRequest\x12@\n\x08\x65nv_dict\x18\x01 \x03(\x0b\x32..modal.client.SecretCreateRequest.EnvDictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x15\n\rtemplate_type\x18\x03 \x01(\t\x12\x1a\n\x12\x65xisting_secret_id\x18\x04 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\")\n\x14SecretCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"\xca\x02\n\x18SecretGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x45\n\x08\x65nv_dict\x18\x05 \x03(\x0b\x32\x33.modal.client.SecretGetOrCreateRequest.EnvDictEntry\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\".\n\x19SecretGetOrCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"c\n\x0eSecretListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x12\n\ncreated_at\x18\x02 \x01(\x01\x12\x14\n\x0clast_used_at\x18\x03 \x01(\x01\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"-\n\x11SecretListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12SecretListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.SecretListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"\xd9\x01\n\x1eSharedVolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\";\n\x1fSharedVolumeGetOrCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"8\n\x1cSharedVolumeHeartbeatRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"f\n\x19SharedVolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x33\n\x0e\x63loud_provider\x18\x02 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"6\n\x1aSharedVolumeCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"\x88\x01\n\x14SharedVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12\x33\n\x0e\x63loud_provider\x18\x04 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"3\n\x17SharedVolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"g\n\x18SharedVolumeListResponse\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.SharedVolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"F\n\x1cSharedVolumeListFilesRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"\xa7\x01\n\x1aSharedVolumePutFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x04 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x12\x11\n\tresumable\x18\x06 \x01(\x08\x42\x0c\n\ndata_oneof\"-\n\x1bSharedVolumePutFileResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"D\n\x1aSharedVolumeGetFileRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"S\n\x1bSharedVolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"`\n\x1dSharedVolumeRemoveFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x11\n\trecursive\x18\x03 \x01(\x08\"I\n\x1dSharedVolumeListFilesResponse\x12(\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\x17.modal.client.FileEntry\"\x92\x01\n\x11SharedVolumeMount\x12\x12\n\nmount_path\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x33\n\x0e\x63loud_provider\x18\x03 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x1a\n\x12\x61llow_cross_region\x18\x04 \x01(\x08\".\n\x19TaskCurrentInputsResponse\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"l\n\x08TaskInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nstarted_at\x18\x02 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x03 \x01(\x01\x12+\n\x06result\x18\x04 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\xee\x01\n\x08TaskLogs\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12+\n\ntask_state\x18\x06 \x01(\x0e\x32\x17.modal.client.TaskState\x12\x11\n\ttimestamp\x18\x07 \x01(\x01\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x31\n\rtask_progress\x18\t \x01(\x0b\x32\x1a.modal.client.TaskProgress\x12\x18\n\x10\x66unction_call_id\x18\n \x01(\t\x12\x10\n\x08input_id\x18\x0b \x01(\t\"\x11\n\x0fTaskListRequest\":\n\x10TaskListResponse\x12&\n\x05tasks\x18\x01 \x03(\x0b\x32\x17.modal.client.TaskStats\"\xc6\x01\n\rTaskLogsBatch\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12%\n\x05items\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x05 \x01(\t\x12\x10\n\x08\x61pp_done\x18\n \x01(\x08\x12\x13\n\x0b\x66unction_id\x18\x0b \x01(\t\x12\x10\n\x08input_id\x18\x0c \x01(\t\x12\x10\n\x08image_id\x18\r \x01(\t\x12\x0b\n\x03\x65of\x18\x0e \x01(\x08\x12\x13\n\x0bpty_exec_id\x18\x0f \x01(\t\"p\n\x0cTaskProgress\x12\x0b\n\x03len\x18\x01 \x01(\x04\x12\x0b\n\x03pos\x18\x02 \x01(\x04\x12\x31\n\rprogress_type\x18\x03 \x01(\x0e\x32\x1a.modal.client.ProgressType\x12\x13\n\x0b\x64\x65scription\x18\x04 \x01(\t\"@\n\x11TaskResultRequest\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\"Y\n\tTaskStats\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x02 \x01(\t\x12\x17\n\x0f\x61pp_description\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\"V\n\x16TokenFlowCreateRequest\x12\x12\n\nutm_source\x18\x03 \x01(\t\x12\x16\n\x0elocalhost_port\x18\x04 \x01(\x05\x12\x10\n\x08next_url\x18\x05 \x01(\t\"d\n\x17TokenFlowCreateResponse\x12\x15\n\rtoken_flow_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12\x0c\n\x04\x63ode\x18\x03 \x01(\t\x12\x13\n\x0bwait_secret\x18\x04 \x01(\t\"S\n\x14TokenFlowWaitRequest\x12\x0f\n\x07timeout\x18\x01 \x01(\x02\x12\x15\n\rtoken_flow_id\x18\x02 \x01(\t\x12\x13\n\x0bwait_secret\x18\x03 \x01(\t\"l\n\x15TokenFlowWaitResponse\x12\x10\n\x08token_id\x18\x01 \x01(\t\x12\x14\n\x0ctoken_secret\x18\x02 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x08\x12\x1a\n\x12workspace_username\x18\x04 \x01(\t\"7\n\x12TunnelStartRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\x12\x13\n\x0bunencrypted\x18\x02 \x01(\x08\"\x99\x01\n\x13TunnelStartResponse\x12\x0c\n\x04host\x18\x01 \x01(\t\x12\x0c\n\x04port\x18\x02 \x01(\r\x12\x1d\n\x10unencrypted_host\x18\x03 \x01(\tH\x00\x88\x01\x01\x12\x1d\n\x10unencrypted_port\x18\x04 \x01(\rH\x01\x88\x01\x01\x42\x13\n\x11_unencrypted_hostB\x13\n\x11_unencrypted_port\"!\n\x11TunnelStopRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\"$\n\x12TunnelStopResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"\xd3\x01\n\x18VolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\".\n\x19VolumeGetOrCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x16VolumeHeartbeatRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x13VolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\")\n\x14VolumeCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\".\n\x13VolumeCommitRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"+\n\x14VolumeCommitResponse\x12\x13\n\x0bskip_reload\x18\x01 \x01(\x08\"F\n\x13VolumeDeleteRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x1c\n\x10\x65nvironment_name\x18\x02 \x01(\tB\x02\x18\x01\"S\n\x14VolumeGetFileRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\r\n\x05start\x18\x03 \x01(\x04\x12\x0b\n\x03len\x18\x04 \x01(\x04\"w\n\x15VolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x12\x0c\n\x04size\x18\x03 \x01(\x04\x12\r\n\x05start\x18\x04 \x01(\x04\x12\x0b\n\x03len\x18\x05 \x01(\x04\x42\x0c\n\ndata_oneof\"c\n\x16VolumeListFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x18\n\x0bmax_entries\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0e\n\x0c_max_entries\"C\n\x17VolumeListFilesResponse\x12(\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\x17.modal.client.FileEntry\"F\n\x0eVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x11\n\tvolume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\"-\n\x11VolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12VolumeListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.VolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"(\n\x13VolumeReloadRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"}\n\x15VolumePutFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12&\n\x05\x66iles\x18\x02 \x03(\x0b\x32\x17.modal.client.MountFile\x12)\n!disallow_overwrite_existing_files\x18\x03 \x01(\x08\"S\n\x17VolumeRemoveFileRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\x11\n\trecursive\x18\x03 \x01(\x08\"c\n\x16VolumeCopyFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x11\n\tsrc_paths\x18\x02 \x03(\x0c\x12\x10\n\x08\x64st_path\x18\x03 \x01(\x0c\x12\x11\n\trecursive\x18\x04 \x01(\x08\"V\n\x0bVolumeMount\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12 \n\x18\x61llow_background_commits\x18\x03 \x01(\x08\"\x8d\x02\n\rWebhookConfig\x12\'\n\x04type\x18\x01 \x01(\x0e\x32\x19.modal.client.WebhookType\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x18\n\x10requested_suffix\x18\x04 \x01(\t\x12\x32\n\nasync_mode\x18\x05 \x01(\x0e\x32\x1e.modal.client.WebhookAsyncMode\x12\x38\n\x0e\x63ustom_domains\x18\x06 \x03(\x0b\x32 .modal.client.CustomDomainConfig\x12\x17\n\x0fweb_server_port\x18\x07 \x01(\r\x12\"\n\x1aweb_server_startup_timeout\x18\x08 \x01(\x02\"N\n\nWebUrlInfo\x12\x11\n\ttruncated\x18\x01 \x01(\x08\x12\x17\n\x0fhas_unique_hash\x18\x02 \x01(\x08\x12\x14\n\x0clabel_stolen\x18\x03 \x01(\x08\"K\n\x1bWorkspaceNameLookupResponse\x12\x1a\n\x0eworkspace_name\x18\x01 \x01(\tB\x02\x18\x01\x12\x10\n\x08username\x18\x02 \x01(\t\"^\n\x14RuntimeOutputMessage\x12\x35\n\x0f\x66ile_descriptor\x18\x01 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07message\x18\x02 \x01(\t\"\x82\x01\n\x12RuntimeOutputBatch\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.RuntimeOutputMessage\x12\x13\n\x0b\x62\x61tch_index\x18\x02 \x01(\x04\x12\x16\n\texit_code\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0c\n\n_exit_code\"=\n\x13RuntimeInputMessage\x12\x0f\n\x07message\x18\x01 \x01(\x0c\x12\x15\n\rmessage_index\x18\x02 \x01(\x04*\x83\x01\n\x13\x41ppDeployVisibility\x12%\n!APP_DEPLOY_VISIBILITY_UNSPECIFIED\x10\x00\x12#\n\x1f\x41PP_DEPLOY_VISIBILITY_WORKSPACE\x10\x01\x12 \n\x1c\x41PP_DEPLOY_VISIBILITY_PUBLIC\x10\x02*\xf5\x01\n\x13\x41ppDisconnectReason\x12%\n!APP_DISCONNECT_REASON_UNSPECIFIED\x10\x00\x12)\n%APP_DISCONNECT_REASON_LOCAL_EXCEPTION\x10\x01\x12,\n(APP_DISCONNECT_REASON_KEYBOARD_INTERRUPT\x10\x02\x12.\n*APP_DISCONNECT_REASON_ENTRYPOINT_COMPLETED\x10\x03\x12.\n*APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION\x10\x04*\x8d\x02\n\x08\x41ppState\x12\x19\n\x15\x41PP_STATE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STATE_EPHEMERAL\x10\x01\x12\x16\n\x12\x41PP_STATE_DETACHED\x10\x02\x12\x16\n\x12\x41PP_STATE_DEPLOYED\x10\x03\x12\x16\n\x12\x41PP_STATE_STOPPING\x10\x04\x12\x15\n\x11\x41PP_STATE_STOPPED\x10\x05\x12\x1a\n\x16\x41PP_STATE_INITIALIZING\x10\x06\x12\x16\n\x12\x41PP_STATE_DISABLED\x10\x07\x12#\n\x1f\x41PP_STATE_DETACHED_DISCONNECTED\x10\x08\x12\x15\n\x11\x41PP_STATE_DERIVED\x10\t*\x85\x01\n\rAppStopSource\x12\x1f\n\x1b\x41PP_STOP_SOURCE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STOP_SOURCE_CLI\x10\x01\x12!\n\x1d\x41PP_STOP_SOURCE_PYTHON_CLIENT\x10\x02\x12\x17\n\x13\x41PP_STOP_SOURCE_WEB\x10\x03*\x91\x01\n\x11\x43\x65rtificateStatus\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_PENDING\x10\x00\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_ISSUED\x10\x01\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_FAILED\x10\x02\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_REVOKED\x10\x03*\xb1\x01\n\x10\x43heckpointStatus\x12!\n\x1d\x43HECKPOINT_STATUS_UNSPECIFIED\x10\x00\x12\x1d\n\x19\x43HECKPOINT_STATUS_PENDING\x10\x01\x12 \n\x1c\x43HECKPOINT_STATUS_PROCESSING\x10\x02\x12\x1b\n\x17\x43HECKPOINT_STATUS_READY\x10\x03\x12\x1c\n\x18\x43HECKPOINT_STATUS_FAILED\x10\x04*\xb0\x01\n\nClientType\x12\x1b\n\x17\x43LIENT_TYPE_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LIENT_TYPE_CLIENT\x10\x01\x12\x1a\n\x12\x43LIENT_TYPE_WORKER\x10\x02\x1a\x02\x08\x01\x12\x19\n\x15\x43LIENT_TYPE_CONTAINER\x10\x03\x12\x1a\n\x12\x43LIENT_TYPE_SERVER\x10\x04\x1a\x02\x08\x01\x12\x1a\n\x16\x43LIENT_TYPE_WEB_SERVER\x10\x05*\xb0\x01\n\rCloudProvider\x12\x1e\n\x1a\x43LOUD_PROVIDER_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LOUD_PROVIDER_AWS\x10\x01\x12\x16\n\x12\x43LOUD_PROVIDER_GCP\x10\x02\x12\x17\n\x13\x43LOUD_PROVIDER_AUTO\x10\x03\x12\x16\n\x12\x43LOUD_PROVIDER_OCI\x10\x04\x12\x1e\n\x1a\x43LOUD_PROVIDER_LAMBDA_LABS\x10\x05*w\n\nDataFormat\x12\x1b\n\x17\x44\x41TA_FORMAT_UNSPECIFIED\x10\x00\x12\x16\n\x12\x44\x41TA_FORMAT_PICKLE\x10\x01\x12\x14\n\x10\x44\x41TA_FORMAT_ASGI\x10\x02\x12\x1e\n\x1a\x44\x41TA_FORMAT_GENERATOR_DONE\x10\x03*\x80\x01\n\x13\x44\x65ploymentNamespace\x12$\n DEPLOYMENT_NAMESPACE_UNSPECIFIED\x10\x00\x12\"\n\x1e\x44\x45PLOYMENT_NAMESPACE_WORKSPACE\x10\x01\x12\x1f\n\x1b\x44\x45PLOYMENT_NAMESPACE_GLOBAL\x10\x03*Z\n\rDNSRecordType\x12\x15\n\x11\x44NS_RECORD_TYPE_A\x10\x00\x12\x17\n\x13\x44NS_RECORD_TYPE_TXT\x10\x01\x12\x19\n\x15\x44NS_RECORD_TYPE_CNAME\x10\x02*\x83\x01\n\x0e\x46ileDescriptor\x12\x1f\n\x1b\x46ILE_DESCRIPTOR_UNSPECIFIED\x10\x00\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDOUT\x10\x01\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDERR\x10\x02\x12\x18\n\x14\x46ILE_DESCRIPTOR_INFO\x10\x03*p\n\x10\x46unctionCallType\x12\"\n\x1e\x46UNCTION_CALL_TYPE_UNSPECIFIED\x10\x00\x12\x1c\n\x18\x46UNCTION_CALL_TYPE_UNARY\x10\x01\x12\x1a\n\x16\x46UNCTION_CALL_TYPE_MAP\x10\x02*\x82\x02\n\x07GPUType\x12\x18\n\x14GPU_TYPE_UNSPECIFIED\x10\x00\x12\x0f\n\x0bGPU_TYPE_T4\x10\x01\x12\x11\n\rGPU_TYPE_A100\x10\x02\x12\x11\n\rGPU_TYPE_A10G\x10\x03\x12\x10\n\x0cGPU_TYPE_ANY\x10\x04\x12\x19\n\x11GPU_TYPE_A100_20G\x10\x05\x1a\x02\x08\x01\x12\x1f\n\x17GPU_TYPE_A100_40GB_MANY\x10\x06\x1a\x02\x08\x01\x12\x1c\n\x14GPU_TYPE_INFERENTIA2\x10\x07\x1a\x02\x08\x01\x12\x16\n\x12GPU_TYPE_A100_80GB\x10\x08\x12\x0f\n\x0bGPU_TYPE_L4\x10\t\x12\x11\n\rGPU_TYPE_H100\x10\n*\xa0\x02\n\x12ObjectCreationType\x12$\n OBJECT_CREATION_TYPE_UNSPECIFIED\x10\x00\x12*\n&OBJECT_CREATION_TYPE_CREATE_IF_MISSING\x10\x01\x12.\n*OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS\x10\x02\x12\x33\n/OBJECT_CREATION_TYPE_CREATE_OVERWRITE_IF_EXISTS\x10\x03\x12/\n+OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP\x10\x04\x12\"\n\x1eOBJECT_CREATION_TYPE_EPHEMERAL\x10\x05*>\n\x0cProgressType\x12\x19\n\x15IMAGE_SNAPSHOT_UPLOAD\x10\x00\x12\x13\n\x0f\x46UNCTION_QUEUED\x10\x01*x\n\x11RateLimitInterval\x12#\n\x1fRATE_LIMIT_INTERVAL_UNSPECIFIED\x10\x00\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_SECOND\x10\x01\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_MINUTE\x10\x02*\xb2\x01\n\x10RegistryAuthType\x12\"\n\x1eREGISTRY_AUTH_TYPE_UNSPECIFIED\x10\x00\x12\x1a\n\x16REGISTRY_AUTH_TYPE_AWS\x10\x01\x12\x1a\n\x16REGISTRY_AUTH_TYPE_GCP\x10\x02\x12\x1d\n\x19REGISTRY_AUTH_TYPE_PUBLIC\x10\x03\x12#\n\x1fREGISTRY_AUTH_TYPE_STATIC_CREDS\x10\x04*\xdc\x02\n\tTaskState\x12\x1a\n\x16TASK_STATE_UNSPECIFIED\x10\x00\x12\x16\n\x12TASK_STATE_CREATED\x10\x06\x12\x15\n\x11TASK_STATE_QUEUED\x10\x01\x12\x1e\n\x1aTASK_STATE_WORKER_ASSIGNED\x10\x02\x12\x1c\n\x18TASK_STATE_LOADING_IMAGE\x10\x03\x12\x15\n\x11TASK_STATE_ACTIVE\x10\x04\x12\x18\n\x14TASK_STATE_COMPLETED\x10\x05\x12!\n\x1dTASK_STATE_CREATING_CONTAINER\x10\x07\x12\x13\n\x0fTASK_STATE_IDLE\x10\x08\x12\x1a\n\x16TASK_STATE_PREEMPTIBLE\x10\t\x12\x18\n\x14TASK_STATE_PREEMPTED\x10\n\x12\'\n#TASK_STATE_LOADING_CHECKPOINT_IMAGE\x10\x0b*\x99\x01\n\x0bWebhookType\x12\x1c\n\x18WEBHOOK_TYPE_UNSPECIFIED\x10\x00\x12\x19\n\x15WEBHOOK_TYPE_ASGI_APP\x10\x01\x12\x19\n\x15WEBHOOK_TYPE_FUNCTION\x10\x02\x12\x19\n\x15WEBHOOK_TYPE_WSGI_APP\x10\x03\x12\x1b\n\x17WEBHOOK_TYPE_WEB_SERVER\x10\x04*\x9a\x01\n\x10WebhookAsyncMode\x12\"\n\x1eWEBHOOK_ASYNC_MODE_UNSPECIFIED\x10\x00\x12\x1f\n\x1bWEBHOOK_ASYNC_MODE_DISABLED\x10\x02\x12\x1e\n\x1aWEBHOOK_ASYNC_MODE_TRIGGER\x10\x03\x12\x1b\n\x17WEBHOOK_ASYNC_MODE_AUTO\x10\x04\"\x04\x08\x01\x10\x01\x32\xe8N\n\x0bModalClient\x12L\n\tAppCreate\x12\x1e.modal.client.AppCreateRequest\x1a\x1f.modal.client.AppCreateResponse\x12W\n\x13\x41ppClientDisconnect\x12(.modal.client.AppClientDisconnectRequest\x1a\x16.google.protobuf.Empty\x12L\n\nAppGetLogs\x12\x1f.modal.client.AppGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12K\n\rAppSetObjects\x12\".modal.client.AppSetObjectsRequest\x1a\x16.google.protobuf.Empty\x12X\n\rAppGetObjects\x12\".modal.client.AppGetObjectsRequest\x1a#.modal.client.AppGetObjectsResponse\x12\x46\n\x07\x41ppList\x12\x1c.modal.client.AppListRequest\x1a\x1d.modal.client.AppListResponse\x12^\n\x0f\x41ppLookupObject\x12$.modal.client.AppLookupObjectRequest\x1a%.modal.client.AppLookupObjectResponse\x12L\n\tAppDeploy\x12\x1e.modal.client.AppDeployRequest\x1a\x1f.modal.client.AppDeployResponse\x12p\n\x15\x41ppDeploySingleObject\x12*.modal.client.AppDeploySingleObjectRequest\x1a+.modal.client.AppDeploySingleObjectResponse\x12s\n\x16\x41ppGetByDeploymentName\x12+.modal.client.AppGetByDeploymentNameRequest\x1a,.modal.client.AppGetByDeploymentNameResponse\x12?\n\x07\x41ppStop\x12\x1c.modal.client.AppStopRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0c\x41ppHeartbeat\x12!.modal.client.AppHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12O\n\nBlobCreate\x12\x1f.modal.client.BlobCreateRequest\x1a .modal.client.BlobCreateResponse\x12\x46\n\x07\x42lobGet\x12\x1c.modal.client.BlobGetRequest\x1a\x1d.modal.client.BlobGetResponse\x12R\n\x0b\x43lassCreate\x12 .modal.client.ClassCreateRequest\x1a!.modal.client.ClassCreateResponse\x12I\n\x08\x43lassGet\x12\x1d.modal.client.ClassGetRequest\x1a\x1e.modal.client.ClassGetResponse\x12U\n\x0c\x43lientCreate\x12!.modal.client.ClientCreateRequest\x1a\".modal.client.ClientCreateResponse\x12H\n\x0b\x43lientHello\x12\x16.google.protobuf.Empty\x1a!.modal.client.ClientHelloResponse\x12O\n\x0f\x43lientHeartbeat\x12$.modal.client.ClientHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12g\n\x12\x43ontainerHeartbeat\x12\'.modal.client.ContainerHeartbeatRequest\x1a(.modal.client.ContainerHeartbeatResponse\x12X\n\rContainerExec\x12\".modal.client.ContainerExecRequest\x1a#.modal.client.ContainerExecResponse\x12i\n\x16\x43ontainerExecGetOutput\x12+.modal.client.ContainerExecGetOutputRequest\x1a .modal.client.RuntimeOutputBatch0\x01\x12[\n\x15\x43ontainerExecPutInput\x12*.modal.client.ContainerExecPutInputRequest\x1a\x16.google.protobuf.Empty\x12W\n\x13\x43ontainerCheckpoint\x12(.modal.client.ContainerCheckpointRequest\x1a\x16.google.protobuf.Empty\x12\x43\n\tDictClear\x12\x1e.modal.client.DictClearRequest\x1a\x16.google.protobuf.Empty\x12O\n\nDictCreate\x12\x1f.modal.client.DictCreateRequest\x1a .modal.client.DictCreateResponse\x12^\n\x0f\x44ictGetOrCreate\x12$.modal.client.DictGetOrCreateRequest\x1a%.modal.client.DictGetOrCreateResponse\x12K\n\rDictHeartbeat\x12\".modal.client.DictHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12L\n\x0c\x44ictContents\x12!.modal.client.DictContentsRequest\x1a\x17.modal.client.DictEntry0\x01\x12O\n\nDictUpdate\x12\x1f.modal.client.DictUpdateRequest\x1a .modal.client.DictUpdateResponse\x12\x46\n\x07\x44ictGet\x12\x1c.modal.client.DictGetRequest\x1a\x1d.modal.client.DictGetResponse\x12\x46\n\x07\x44ictPop\x12\x1c.modal.client.DictPopRequest\x1a\x1d.modal.client.DictPopResponse\x12U\n\x0c\x44ictContains\x12!.modal.client.DictContainsRequest\x1a\".modal.client.DictContainsResponse\x12\x46\n\x07\x44ictLen\x12\x1c.modal.client.DictLenRequest\x1a\x1d.modal.client.DictLenResponse\x12U\n\x0c\x44omainCreate\x12!.modal.client.DomainCreateRequest\x1a\".modal.client.DomainCreateResponse\x12O\n\nDomainList\x12\x1f.modal.client.DomainListRequest\x1a .modal.client.DomainListResponse\x12v\n\x17\x44omainCertificateVerify\x12,.modal.client.DomainCertificateVerifyRequest\x1a-.modal.client.DomainCertificateVerifyResponse\x12S\n\x11\x45nvironmentCreate\x12&.modal.client.EnvironmentCreateRequest\x1a\x16.google.protobuf.Empty\x12P\n\x0f\x45nvironmentList\x12\x16.google.protobuf.Empty\x1a%.modal.client.EnvironmentListResponse\x12S\n\x11\x45nvironmentDelete\x12&.modal.client.EnvironmentDeleteRequest\x1a\x16.google.protobuf.Empty\x12^\n\x11\x45nvironmentUpdate\x12&.modal.client.EnvironmentUpdateRequest\x1a!.modal.client.EnvironmentListItem\x12g\n\x12\x46unctionBindParams\x12\'.modal.client.FunctionBindParamsRequest\x1a(.modal.client.FunctionBindParamsResponse\x12[\n\x0e\x46unctionCreate\x12#.modal.client.FunctionCreateRequest\x1a$.modal.client.FunctionCreateResponse\x12R\n\x0b\x46unctionGet\x12 .modal.client.FunctionGetRequest\x1a!.modal.client.FunctionGetResponse\x12m\n\x14\x46unctionGetCallGraph\x12).modal.client.FunctionGetCallGraphRequest\x1a*.modal.client.FunctionGetCallGraphResponse\x12\x64\n\x17\x46unctionGetCurrentStats\x12,.modal.client.FunctionGetCurrentStatsRequest\x1a\x1b.modal.client.FunctionStats\x12\x64\n\x11\x46unctionGetInputs\x12&.modal.client.FunctionGetInputsRequest\x1a\'.modal.client.FunctionGetInputsResponse\x12g\n\x12\x46unctionGetOutputs\x12\'.modal.client.FunctionGetOutputsRequest\x1a(.modal.client.FunctionGetOutputsResponse\x12p\n\x15\x46unctionGetSerialized\x12*.modal.client.FunctionGetSerializedRequest\x1a+.modal.client.FunctionGetSerializedResponse\x12R\n\x0b\x46unctionMap\x12 .modal.client.FunctionMapRequest\x1a!.modal.client.FunctionMapResponse\x12\x64\n\x11\x46unctionPrecreate\x12&.modal.client.FunctionPrecreateRequest\x1a\'.modal.client.FunctionPrecreateResponse\x12\x64\n\x11\x46unctionPutInputs\x12&.modal.client.FunctionPutInputsRequest\x1a\'.modal.client.FunctionPutInputsResponse\x12U\n\x12\x46unctionPutOutputs\x12\'.modal.client.FunctionPutOutputsRequest\x1a\x16.google.protobuf.Empty\x12\x8b\x01\n\x1e\x46unctionUpdateSchedulingParams\x12\x33.modal.client.FunctionUpdateSchedulingParamsRequest\x1a\x34.modal.client.FunctionUpdateSchedulingParamsResponse\x12U\n\x12\x46unctionCallCancel\x12\'.modal.client.FunctionCallCancelRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10\x46unctionCallList\x12%.modal.client.FunctionCallListRequest\x1a&.modal.client.FunctionCallListResponse\x12\\\n\x15\x46unctionCallGetDataIn\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12]\n\x16\x46unctionCallGetDataOut\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12Z\n\x16\x46unctionCallPutDataOut\x12(.modal.client.FunctionCallPutDataRequest\x1a\x16.google.protobuf.Empty\x12G\n\x15\x46unctionStartPtyShell\x12\x16.google.protobuf.Empty\x1a\x16.google.protobuf.Empty\x12\x61\n\x10ImageGetOrCreate\x12%.modal.client.ImageGetOrCreateRequest\x1a&.modal.client.ImageGetOrCreateResponse\x12i\n\x12ImageJoinStreaming\x12\'.modal.client.ImageJoinStreamingRequest\x1a(.modal.client.ImageJoinStreamingResponse0\x01\x12U\n\x0cMountPutFile\x12!.modal.client.MountPutFileRequest\x1a\".modal.client.MountPutFileResponse\x12O\n\nMountBuild\x12\x1f.modal.client.MountBuildRequest\x1a .modal.client.MountBuildResponse\x12\x61\n\x10MountGetOrCreate\x12%.modal.client.MountGetOrCreateRequest\x1a&.modal.client.MountGetOrCreateResponse\x12\x61\n\x10ProxyGetOrCreate\x12%.modal.client.ProxyGetOrCreateRequest\x1a&.modal.client.ProxyGetOrCreateResponse\x12R\n\x0bQueueCreate\x12 .modal.client.QueueCreateRequest\x1a!.modal.client.QueueCreateResponse\x12\x61\n\x10QueueGetOrCreate\x12%.modal.client.QueueGetOrCreateRequest\x1a&.modal.client.QueueGetOrCreateResponse\x12I\n\x08QueueGet\x12\x1d.modal.client.QueueGetRequest\x1a\x1e.modal.client.QueueGetResponse\x12M\n\x0eQueueHeartbeat\x12#.modal.client.QueueHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x41\n\x08QueuePut\x12\x1d.modal.client.QueuePutRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08QueueLen\x12\x1d.modal.client.QueueLenRequest\x1a\x1e.modal.client.QueueLenResponse\x12[\n\x0eQueueNextItems\x12#.modal.client.QueueNextItemsRequest\x1a$.modal.client.QueueNextItemsResponse\x12X\n\rSandboxCreate\x12\".modal.client.SandboxCreateRequest\x1a#.modal.client.SandboxCreateResponse\x12\x61\n\x10SandboxGetTaskId\x12%.modal.client.SandboxGetTaskIdRequest\x1a&.modal.client.SandboxGetTaskIdResponse\x12T\n\x0eSandboxGetLogs\x12#.modal.client.SandboxGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12R\n\x0bSandboxWait\x12 .modal.client.SandboxWaitRequest\x1a!.modal.client.SandboxWaitResponse\x12R\n\x0bSandboxList\x12 .modal.client.SandboxListRequest\x1a!.modal.client.SandboxListResponse\x12\x61\n\x10SandboxTerminate\x12%.modal.client.SandboxTerminateRequest\x1a&.modal.client.SandboxTerminateResponse\x12\x64\n\x11SandboxStdinWrite\x12&.modal.client.SandboxStdinWriteRequest\x1a\'.modal.client.SandboxStdinWriteResponse\x12U\n\x0cSecretCreate\x12!.modal.client.SecretCreateRequest\x1a\".modal.client.SecretCreateResponse\x12\x64\n\x11SecretGetOrCreate\x12&.modal.client.SecretGetOrCreateRequest\x1a\'.modal.client.SecretGetOrCreateResponse\x12O\n\nSecretList\x12\x1f.modal.client.SecretListRequest\x1a .modal.client.SecretListResponse\x12v\n\x17SharedVolumeGetOrCreate\x12,.modal.client.SharedVolumeGetOrCreateRequest\x1a-.modal.client.SharedVolumeGetOrCreateResponse\x12g\n\x12SharedVolumeCreate\x12\'.modal.client.SharedVolumeCreateRequest\x1a(.modal.client.SharedVolumeCreateResponse\x12[\n\x15SharedVolumeHeartbeat\x12*.modal.client.SharedVolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10SharedVolumeList\x12%.modal.client.SharedVolumeListRequest\x1a&.modal.client.SharedVolumeListResponse\x12p\n\x15SharedVolumeListFiles\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse\x12x\n\x1bSharedVolumeListFilesStream\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse0\x01\x12j\n\x13SharedVolumePutFile\x12(.modal.client.SharedVolumePutFileRequest\x1a).modal.client.SharedVolumePutFileResponse\x12j\n\x13SharedVolumeGetFile\x12(.modal.client.SharedVolumeGetFileRequest\x1a).modal.client.SharedVolumeGetFileResponse\x12]\n\x16SharedVolumeRemoveFile\x12+.modal.client.SharedVolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12\x45\n\nTaskResult\x12\x1f.modal.client.TaskResultRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08TaskList\x12\x1d.modal.client.TaskListRequest\x1a\x1e.modal.client.TaskListResponse\x12T\n\x11TaskCurrentInputs\x12\x16.google.protobuf.Empty\x1a\'.modal.client.TaskCurrentInputsResponse\x12^\n\x0fTokenFlowCreate\x12$.modal.client.TokenFlowCreateRequest\x1a%.modal.client.TokenFlowCreateResponse\x12X\n\rTokenFlowWait\x12\".modal.client.TokenFlowWaitRequest\x1a#.modal.client.TokenFlowWaitResponse\x12R\n\x0bTunnelStart\x12 .modal.client.TunnelStartRequest\x1a!.modal.client.TunnelStartResponse\x12O\n\nTunnelStop\x12\x1f.modal.client.TunnelStopRequest\x1a .modal.client.TunnelStopResponse\x12\x64\n\x11VolumeGetOrCreate\x12&.modal.client.VolumeGetOrCreateRequest\x1a\'.modal.client.VolumeGetOrCreateResponse\x12U\n\x0cVolumeCreate\x12!.modal.client.VolumeCreateRequest\x1a\".modal.client.VolumeCreateResponse\x12O\n\x0fVolumeHeartbeat\x12$.modal.client.VolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12U\n\x0cVolumeCommit\x12!.modal.client.VolumeCommitRequest\x1a\".modal.client.VolumeCommitResponse\x12I\n\x0cVolumeDelete\x12!.modal.client.VolumeDeleteRequest\x1a\x16.google.protobuf.Empty\x12X\n\rVolumeGetFile\x12\".modal.client.VolumeGetFileRequest\x1a#.modal.client.VolumeGetFileResponse\x12O\n\nVolumeList\x12\x1f.modal.client.VolumeListRequest\x1a .modal.client.VolumeListResponse\x12`\n\x0fVolumeListFiles\x12$.modal.client.VolumeListFilesRequest\x1a%.modal.client.VolumeListFilesResponse0\x01\x12M\n\x0eVolumePutFiles\x12#.modal.client.VolumePutFilesRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0cVolumeReload\x12!.modal.client.VolumeReloadRequest\x1a\x16.google.protobuf.Empty\x12Q\n\x10VolumeRemoveFile\x12%.modal.client.VolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12O\n\x0fVolumeCopyFiles\x12$.modal.client.VolumeCopyFilesRequest\x1a\x16.google.protobuf.Empty\x12X\n\x13WorkspaceNameLookup\x12\x16.google.protobuf.Empty\x1a).modal.client.WorkspaceNameLookupResponseb\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15modal_proto/api.proto\x12\x0cmodal.client\x1a\x19modal_proto/options.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1egoogle/protobuf/wrappers.proto\"\xec\x01\n\x10\x43loudBucketMount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\x12\x16\n\x0erequester_pays\x18\x06 \x01(\x08\x12>\n\x0b\x62ucket_type\x18\x05 \x01(\x0e\x32).modal.client.CloudBucketMount.BucketType\"%\n\nBucketType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x06\n\x02S3\x10\x01\"r\n\x1a\x41ppClientDisconnectRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x31\n\x06reason\x18\x02 \x01(\x0e\x32!.modal.client.AppDisconnectReason\x12\x11\n\texception\x18\x03 \x01(\t\"\xb3\x01\n\x10\x41ppCreateRequest\x12\x17\n\tclient_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x13\n\x0b\x64\x65scription\x18\x02 \x01(\t\x12\x12\n\x06\x64\x65tach\x18\x03 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x0cinitializing\x18\x04 \x01(\x08\x42\x02\x18\x01\x12\x18\n\x10\x65nvironment_name\x18\x05 \x01(\t\x12)\n\tapp_state\x18\x06 \x01(\x0e\x32\x16.modal.client.AppState\"9\n\x11\x41ppCreateResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x14\n\x0c\x61pp_logs_url\x18\x02 \x01(\t\"S\n\x0e\x41ppStopRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12+\n\x06source\x18\x02 \x01(\x0e\x32\x1b.modal.client.AppStopSource\"\xba\x01\n\x10\x41ppDeployRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x15\n\robject_entity\x18\x04 \x01(\t\x12\x35\n\nvisibility\x18\x05 \x01(\x0e\x32!.modal.client.AppDeployVisibility\" \n\x11\x41ppDeployResponse\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x8f\x01\n\x1c\x41ppDeploySingleObjectRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12\x11\n\tobject_id\x18\x04 \x01(\t\"/\n\x1d\x41ppDeploySingleObjectResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"}\n\x1d\x41ppGetByDeploymentNameRequest\x12\x34\n\tnamespace\x18\x01 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"0\n\x1e\x41ppGetByDeploymentNameResponse\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"\xba\x01\n\x11\x41ppGetLogsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x05 \x01(\t\x12\x10\n\x08input_id\x18\x06 \x01(\t\x12\x0f\n\x07task_id\x18\x07 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\"A\n\x14\x41ppGetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11include_unindexed\x18\x02 \x01(\x08\"F\n\x11\x41ppGetObjectsItem\x12\x0b\n\x03tag\x18\x01 \x01(\t\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\"G\n\x15\x41ppGetObjectsResponse\x12.\n\x05items\x18\x02 \x03(\x0b\x32\x1f.modal.client.AppGetObjectsItem\"%\n\x13\x41ppHeartbeatRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\"*\n\x0e\x41ppListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"7\n\x0f\x41ppListResponse\x12$\n\x04\x61pps\x18\x01 \x03(\x0b\x32\x16.modal.client.AppStats\"\xb8\x01\n\x16\x41ppLookupObjectRequest\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x10\n\x08\x61pp_name\x18\x03 \x01(\t\x12\x12\n\nobject_tag\x18\x04 \x01(\t\x12\x11\n\tobject_id\x18\x05 \x01(\t\x12\x15\n\robject_entity\x18\x06 \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\x07 \x01(\t\"O\n\x17\x41ppLookupObjectResponse\x12$\n\x06object\x18\x06 \x01(\x0b\x32\x14.modal.client.Object\x12\x0e\n\x06\x61pp_id\x18\x07 \x01(\t\"\xaf\x02\n\x14\x41ppSetObjectsRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12T\n\x12indexed_object_ids\x18\x02 \x03(\x0b\x32\x38.modal.client.AppSetObjectsRequest.IndexedObjectIdsEntry\x12\x11\n\tclient_id\x18\x03 \x01(\t\x12\x1c\n\x14unindexed_object_ids\x18\x04 \x03(\t\x12-\n\rnew_app_state\x18\x05 \x01(\x0e\x32\x16.modal.client.AppState\x12\x18\n\x10single_object_id\x18\x06 \x01(\t\x1a\x37\n\x15IndexedObjectIdsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\xd1\x01\n\x08\x41ppStats\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64\x65scription\x18\x03 \x01(\t\x12%\n\x05state\x18\x04 \x01(\x0e\x32\x16.modal.client.AppState\x12\x12\n\ncreated_at\x18\x05 \x01(\x01\x12\x12\n\nstopped_at\x18\x06 \x01(\x01\x12\x17\n\x0fn_running_tasks\x18\x08 \x01(\x05\x12\x15\n\robject_entity\x18\t \x01(\t\x12\x0c\n\x04name\x18\n \x01(\t\x12\x13\n\x0b\x64\x65ployed_at\x18\x0b \x01(\x01\"\xa3\x0e\n\x04\x41sgi\x12\'\n\x04http\x18\x01 \x01(\x0b\x32\x17.modal.client.Asgi.HttpH\x00\x12\x36\n\x0chttp_request\x18\x02 \x01(\x0b\x32\x1e.modal.client.Asgi.HttpRequestH\x00\x12\x43\n\x13http_response_start\x18\x03 \x01(\x0b\x32$.modal.client.Asgi.HttpResponseStartH\x00\x12\x41\n\x12http_response_body\x18\x04 \x01(\x0b\x32#.modal.client.Asgi.HttpResponseBodyH\x00\x12I\n\x16http_response_trailers\x18\x05 \x01(\x0b\x32\'.modal.client.Asgi.HttpResponseTrailersH\x00\x12<\n\x0fhttp_disconnect\x18\x06 \x01(\x0b\x32!.modal.client.Asgi.HttpDisconnectH\x00\x12\x31\n\twebsocket\x18\x07 \x01(\x0b\x32\x1c.modal.client.Asgi.WebsocketH\x00\x12@\n\x11websocket_connect\x18\x08 \x01(\x0b\x32#.modal.client.Asgi.WebsocketConnectH\x00\x12>\n\x10websocket_accept\x18\t \x01(\x0b\x32\".modal.client.Asgi.WebsocketAcceptH\x00\x12@\n\x11websocket_receive\x18\n \x01(\x0b\x32#.modal.client.Asgi.WebsocketReceiveH\x00\x12:\n\x0ewebsocket_send\x18\x0b \x01(\x0b\x32 .modal.client.Asgi.WebsocketSendH\x00\x12\x46\n\x14websocket_disconnect\x18\x0c \x01(\x0b\x32&.modal.client.Asgi.WebsocketDisconnectH\x00\x12<\n\x0fwebsocket_close\x18\r \x01(\x0b\x32!.modal.client.Asgi.WebsocketCloseH\x00\x1a\xc5\x01\n\x04Http\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x0e\n\x06scheme\x18\x03 \x01(\t\x12\x0c\n\x04path\x18\x04 \x01(\t\x12\x14\n\x0cquery_string\x18\x05 \x01(\x0c\x12\x0f\n\x07headers\x18\x06 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x07 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x08 \x01(\rH\x01\x88\x01\x01\x42\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a.\n\x0bHttpRequest\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a\x46\n\x11HttpResponseStart\x12\x0e\n\x06status\x18\x01 \x01(\r\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x12\x10\n\x08trailers\x18\x03 \x01(\x08\x1a\x33\n\x10HttpResponseBody\x12\x0c\n\x04\x62ody\x18\x01 \x01(\x0c\x12\x11\n\tmore_body\x18\x02 \x01(\x08\x1a>\n\x14HttpResponseTrailers\x12\x0f\n\x07headers\x18\x01 \x03(\x0c\x12\x15\n\rmore_trailers\x18\x02 \x01(\x08\x1a\x10\n\x0eHttpDisconnect\x1a\xd0\x01\n\tWebsocket\x12\x14\n\x0chttp_version\x18\x01 \x01(\t\x12\x0e\n\x06scheme\x18\x02 \x01(\t\x12\x0c\n\x04path\x18\x03 \x01(\t\x12\x14\n\x0cquery_string\x18\x04 \x01(\x0c\x12\x0f\n\x07headers\x18\x05 \x03(\x0c\x12\x18\n\x0b\x63lient_host\x18\x06 \x01(\tH\x00\x88\x01\x01\x12\x18\n\x0b\x63lient_port\x18\x07 \x01(\rH\x01\x88\x01\x01\x12\x14\n\x0csubprotocols\x18\x08 \x03(\tB\x0e\n\x0c_client_hostB\x0e\n\x0c_client_port\x1a\x12\n\x10WebsocketConnect\x1aL\n\x0fWebsocketAccept\x12\x18\n\x0bsubprotocol\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x0f\n\x07headers\x18\x02 \x03(\x0c\x42\x0e\n\x0c_subprotocol\x1a>\n\x10WebsocketReceive\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a;\n\rWebsocketSend\x12\x0f\n\x05\x62ytes\x18\x01 \x01(\x0cH\x00\x12\x0e\n\x04text\x18\x02 \x01(\tH\x00\x42\t\n\x07\x63ontent\x1a\x31\n\x13WebsocketDisconnect\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x42\x07\n\x05_code\x1a<\n\x0eWebsocketClose\x12\x11\n\x04\x63ode\x18\x01 \x01(\rH\x00\x88\x01\x01\x12\x0e\n\x06reason\x18\x02 \x01(\tB\x07\n\x05_codeB\x06\n\x04type\"7\n\tBaseImage\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x12\n\ndocker_tag\x18\x02 \x01(\tJ\x04\x08\x04\x10\x05\"_\n\x11\x42lobCreateRequest\x12\x13\n\x0b\x63ontent_md5\x18\x01 \x01(\t\x12\x1d\n\x15\x63ontent_sha256_base64\x18\x02 \x01(\t\x12\x16\n\x0e\x63ontent_length\x18\x03 \x01(\x03\"\x84\x01\n\x12\x42lobCreateResponse\x12\x0f\n\x07\x62lob_id\x18\x02 \x01(\t\x12\x14\n\nupload_url\x18\x01 \x01(\tH\x00\x12\x32\n\tmultipart\x18\x03 \x01(\x0b\x32\x1d.modal.client.MultiPartUploadH\x00\x42\x13\n\x11upload_type_oneof\"!\n\x0e\x42lobGetRequest\x12\x0f\n\x07\x62lob_id\x18\x01 \x01(\t\"\'\n\x0f\x42lobGetResponse\x12\x14\n\x0c\x64ownload_url\x18\x01 \x01(\t\"i\n\x0e\x43heckpointInfo\x12\x10\n\x08\x63hecksum\x18\x01 \x01(\t\x12.\n\x06status\x18\x02 \x01(\x0e\x32\x1e.modal.client.CheckpointStatus\x12\x15\n\rcheckpoint_id\x18\x03 \x01(\t\"q\n\x12\x43lassCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_class_id\x18\x02 \x01(\t\x12*\n\x07methods\x18\x03 \x03(\x0b\x32\x19.modal.client.ClassMethod\"c\n\x13\x43lassCreateResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"\xb9\x01\n\x0f\x43lassGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\x12\x18\n\x10lookup_published\x18\x08 \x01(\x08\x12\x16\n\x0eworkspace_name\x18\t \x01(\t\"`\n\x10\x43lassGetResponse\x12\x10\n\x08\x63lass_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.ClassHandleMetadata\"A\n\x13\x43lassHandleMetadata\x12*\n\x07methods\x18\x01 \x03(\x0b\x32\x19.modal.client.ClassMethod\"\x81\x01\n\x0b\x43lassMethod\x12\x15\n\rfunction_name\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x46\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"[\n\x13\x43lientCreateRequest\x12\x33\n\x0b\x63lient_type\x18\x01 \x01(\x0e\x32\x18.modal.client.ClientTypeB\x04\x80\xb5\x18\x01\x12\x0f\n\x07version\x18\x02 \x01(\t\"U\n\x14\x43lientCreateResponse\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12\x1b\n\x13\x64\x65precation_warning\x18\x03 \x01(\t\"E\n\x13\x43lientHelloResponse\x12\x0f\n\x07warning\x18\x01 \x01(\t\x12\x1d\n\x15image_builder_version\x18\x02 \x01(\t\"g\n\x16\x43lientHeartbeatRequest\x12\x11\n\tclient_id\x18\x01 \x01(\t\x12\x18\n\x10\x63urrent_input_id\x18\x03 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x04 \x01(\x01\"\x9f\x03\n\x12\x43ontainerArguments\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x13\n\x0b\x66unction_id\x18\x02 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x04 \x01(\t\x12,\n\x0c\x66unction_def\x18\x07 \x01(\x0b\x32\x16.modal.client.Function\x12+\n\nproxy_info\x18\x08 \x01(\x0b\x32\x17.modal.client.ProxyInfo\x12M\n\x0ftracing_context\x18\t \x03(\x0b\x32\x34.modal.client.ContainerArguments.TracingContextEntry\x12\x19\n\x11serialized_params\x18\n \x01(\x0c\x12\x0f\n\x07runtime\x18\x0b \x01(\t\x12\x18\n\x10\x65nvironment_name\x18\r \x01(\t\x12\x1a\n\rcheckpoint_id\x18\x0e \x01(\tH\x00\x88\x01\x01\x1a\x35\n\x13TracingContextEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x42\x10\n\x0e_checkpoint_id\"%\n\x10\x43\x61ncelInputEvent\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"t\n\x1a\x43ontainerHeartbeatResponse\x12?\n\x12\x63\x61ncel_input_event\x18\x01 \x01(\x0b\x32\x1e.modal.client.CancelInputEventH\x00\x88\x01\x01\x42\x15\n\x13_cancel_input_event\"\x85\x01\n\x19\x43ontainerHeartbeatRequest\x12\x18\n\x10\x63urrent_input_id\x18\x01 \x01(\t\x12 \n\x18\x63urrent_input_started_at\x18\x02 \x01(\x01\x12,\n$supports_graceful_input_cancellation\x18\x03 \x01(\x08\"3\n\x1a\x43ontainerCheckpointRequest\x12\x15\n\rcheckpoint_id\x18\x01 \x01(\t\"\x9d\x01\n\x14\x43ontainerExecRequest\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0f\n\x07\x63ommand\x18\x02 \x03(\t\x12\'\n\x08pty_info\x18\x03 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12#\n\x1bterminate_container_on_exit\x18\x04 \x01(\x08\x12\x15\n\rruntime_debug\x18\x05 \x01(\x08\"[\n\x1d\x43ontainerExecGetOutputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x18\n\x10last_batch_index\x18\x03 \x01(\x04\"a\n\x1c\x43ontainerExecPutInputRequest\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\x12\x30\n\x05input\x18\x02 \x01(\x0b\x32!.modal.client.RuntimeInputMessage\"(\n\x15\x43ontainerExecResponse\x12\x0f\n\x07\x65xec_id\x18\x01 \x01(\t\"\"\n\x12\x43ustomDomainConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\"\x1f\n\x10\x43ustomDomainInfo\x12\x0b\n\x03url\x18\x01 \x01(\t\"\x7f\n\tDataChunk\x12-\n\x0b\x64\x61ta_format\x18\x01 \x01(\x0e\x32\x18.modal.client.DataFormat\x12\x0e\n\x04\x64\x61ta\x18\x02 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x03 \x01(\tH\x00\x12\r\n\x05index\x18\x04 \x01(\x04\x42\x0c\n\ndata_oneof\"#\n\x10\x44ictClearRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"3\n\x13\x44ictContainsRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\"%\n\x14\x44ictContainsResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\"j\n\x11\x44ictCreateRequest\x12%\n\x04\x64\x61ta\x18\x01 \x03(\x0b\x32\x17.modal.client.DictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x18\n\x10\x65xisting_dict_id\x18\x03 \x01(\t\"%\n\x12\x44ictCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\xe8\x01\n\x16\x44ictGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12%\n\x04\x64\x61ta\x18\x05 \x03(\x0b\x32\x17.modal.client.DictEntry\"*\n\x17\x44ictGetOrCreateResponse\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\'\n\tDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\x0c\x12\r\n\x05value\x18\x02 \x01(\x0c\".\n\x0e\x44ictGetRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictGetResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"\'\n\x14\x44ictHeartbeatRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"!\n\x0e\x44ictLenRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\"\x1e\n\x0f\x44ictLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\".\n\x0e\x44ictPopRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\x0c\">\n\x0f\x44ictPopResponse\x12\r\n\x05\x66ound\x18\x01 \x01(\x08\x12\x12\n\x05value\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x08\n\x06_value\"N\n\x11\x44ictUpdateRequest\x12\x0f\n\x07\x64ict_id\x18\x01 \x01(\t\x12(\n\x07updates\x18\x02 \x03(\x0b\x32\x17.modal.client.DictEntry\"\x14\n\x12\x44ictUpdateResponse\"S\n\tDNSRecord\x12)\n\x04type\x18\x01 \x01(\x0e\x32\x1b.modal.client.DNSRecordType\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\r\n\x05value\x18\x03 \x01(\t\"0\n\x13\x44omainCreateRequest\x12\x19\n\x0b\x64omain_name\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"W\n\x14\x44omainCreateResponse\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12,\n\x0b\x64ns_records\x18\x02 \x03(\x0b\x32\x17.modal.client.DNSRecord\"\x13\n\x11\x44omainListRequest\"\xaf\x01\n\x06\x44omain\x12\x11\n\tdomain_id\x18\x01 \x01(\t\x12\x13\n\x0b\x64omain_name\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12;\n\x12\x63\x65rtificate_status\x18\x04 \x01(\x0e\x32\x1f.modal.client.CertificateStatus\x12,\n\x0b\x64ns_records\x18\x05 \x03(\x0b\x32\x17.modal.client.DNSRecord\";\n\x12\x44omainListResponse\x12%\n\x07\x64omains\x18\x01 \x03(\x0b\x32\x14.modal.client.Domain\"3\n\x1e\x44omainCertificateVerifyRequest\x12\x11\n\tdomain_id\x18\x01 \x01(\t\"G\n\x1f\x44omainCertificateVerifyResponse\x12$\n\x06\x64omain\x18\x01 \x01(\x0b\x32\x14.modal.client.Domain\"(\n\x18\x45nvironmentCreateRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"(\n\x18\x45nvironmentDeleteRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\";\n\x13\x45nvironmentListItem\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x16\n\x0ewebhook_suffix\x18\x02 \x01(\t\"K\n\x17\x45nvironmentListResponse\x12\x30\n\x05items\x18\x02 \x03(\x0b\x32!.modal.client.EnvironmentListItem\"\x8e\x01\n\x18\x45nvironmentUpdateRequest\x12\x14\n\x0c\x63urrent_name\x18\x01 \x01(\t\x12*\n\x04name\x18\x02 \x01(\x0b\x32\x1c.google.protobuf.StringValue\x12\x30\n\nweb_suffix\x18\x03 \x01(\x0b\x32\x1c.google.protobuf.StringValue\"d\n\x1a\x46unctionCallPutDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12,\n\x0b\x64\x61ta_chunks\x18\x02 \x03(\x0b\x32\x17.modal.client.DataChunk\"J\n\x1a\x46unctionCallGetDataRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nlast_index\x18\x02 \x01(\x04\"\xf4\x0e\n\x08\x46unction\x12\x13\n\x0bmodule_name\x18\x01 \x01(\t\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12\x11\n\tmount_ids\x18\x03 \x03(\t\x12\x10\n\x08image_id\x18\x04 \x01(\t\x12\x1b\n\x13\x66unction_serialized\x18\x06 \x01(\x0c\x12>\n\x0f\x64\x65\x66inition_type\x18\x07 \x01(\x0e\x32%.modal.client.Function.DefinitionType\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12*\n\tresources\x18\t \x01(\x0b\x32\x17.modal.client.Resources\x12\x12\n\nsecret_ids\x18\n \x03(\t\x12+\n\nrate_limit\x18\x0b \x01(\x0b\x32\x17.modal.client.RateLimit\x12\x33\n\x0ewebhook_config\x18\x0f \x01(\x0b\x32\x1b.modal.client.WebhookConfig\x12=\n\x14shared_volume_mounts\x18\x10 \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\x08proxy_id\x18\x11 \x01(\tH\x00\x88\x01\x01\x12\x37\n\x0cretry_policy\x18\x12 \x01(\x0b\x32!.modal.client.FunctionRetryPolicy\x12\x19\n\x11\x63oncurrency_limit\x18\x13 \x01(\r\x12\x11\n\tkeep_warm\x18\x14 \x01(\x08\x12\x14\n\x0ctimeout_secs\x18\x15 \x01(\r\x12\'\n\x08pty_info\x18\x16 \x01(\x0b\x32\x15.modal.client.PTYInfo\x12\x18\n\x10\x63lass_serialized\x18\x17 \x01(\x0c\x12\x1e\n\x16task_idle_timeout_secs\x18\x19 \x01(\r\x12\x38\n\x0e\x63loud_provider\x18\x1a \x01(\x0e\x32\x1b.modal.client.CloudProviderH\x01\x88\x01\x01\x12\x16\n\x0ewarm_pool_size\x18\x1b \x01(\r\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12.\n\x0cweb_url_info\x18\x1d \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12\x0f\n\x07runtime\x18\x1e \x01(\t\x12\x11\n\tstub_name\x18\x1f \x01(\t\x12\x30\n\rvolume_mounts\x18! \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\x1f\n\x17\x61llow_concurrent_inputs\x18\" \x01(\r\x12:\n\x12\x63ustom_domain_info\x18# \x03(\x0b\x32\x1e.modal.client.CustomDomainInfo\x12\x11\n\tworker_id\x18$ \x01(\t\x12\x15\n\rruntime_debug\x18% \x01(\x08\x12\x1b\n\x13is_builder_function\x18  \x01(\x08\x12\x18\n\x10is_auto_snapshot\x18& \x01(\x08\x12\x11\n\tis_method\x18\' \x01(\x08\x12!\n\x19is_checkpointing_function\x18( \x01(\x08\x12\x1d\n\x15\x63heckpointing_enabled\x18) \x01(\x08\x12\x30\n\ncheckpoint\x18* \x01(\x0b\x32\x1c.modal.client.CheckpointInfo\x12;\n\x13object_dependencies\x18+ \x03(\x0b\x32\x1e.modal.client.ObjectDependency\x12\x15\n\rblock_network\x18, \x01(\x08\x12\x12\n\nmax_inputs\x18. \x01(\r\x12(\n\ts3_mounts\x18/ \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x33 \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x1b\n\x13_experimental_boost\x18\x30 \x01(\x08\x12\x1f\n\x17_experimental_scheduler\x18\x31 \x01(\x08\x12P\n!_experimental_scheduler_placement\x18\x32 \x01(\x0b\x32 .modal.client.SchedulerPlacementH\x02\x88\x01\x01\"k\n\x0e\x44\x65\x66initionType\x12\x1f\n\x1b\x44\x45\x46INITION_TYPE_UNSPECIFIED\x10\x00\x12\x1e\n\x1a\x44\x45\x46INITION_TYPE_SERIALIZED\x10\x01\x12\x18\n\x14\x44\x45\x46INITION_TYPE_FILE\x10\x02\"f\n\x0c\x46unctionType\x12\x1d\n\x19\x46UNCTION_TYPE_UNSPECIFIED\x10\x00\x12\x1b\n\x17\x46UNCTION_TYPE_GENERATOR\x10\x01\x12\x1a\n\x16\x46UNCTION_TYPE_FUNCTION\x10\x02\x42\x0b\n\t_proxy_idB\x11\n\x0f_cloud_providerB$\n\"X_experimental_scheduler_placement\"|\n\x12SchedulerPlacement\x12\x14\n\x07_region\x18\x01 \x01(\tH\x00\x88\x01\x01\x12\x12\n\x05_zone\x18\x02 \x01(\tH\x01\x88\x01\x01\x12\x17\n\n_lifecycle\x18\x03 \x01(\tH\x02\x88\x01\x01\x42\n\n\x08X_regionB\x08\n\x06X_zoneB\r\n\x0bX_lifecycle\"\x8f\x01\n\x16\x46unctionHandleMetadata\x12\x15\n\rfunction_name\x18\x02 \x01(\t\x12:\n\rfunction_type\x18\x08 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x0f\n\x07web_url\x18\x1c \x01(\t\x12\x11\n\tis_method\x18\' \x01(\x08\"\x9f\x01\n\x15\x46unctionCreateRequest\x12(\n\x08\x66unction\x18\x01 \x01(\x0b\x32\x16.modal.client.Function\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12(\n\x08schedule\x18\x06 \x01(\x0b\x32\x16.modal.client.Schedule\x12\x1c\n\x14\x65xisting_function_id\x18\x07 \x01(\t\"\xc7\x04\n\x0f\x46unctionOptions\x12\x12\n\nsecret_ids\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12/\n\tresources\x18\x03 \x01(\x0b\x32\x17.modal.client.ResourcesH\x00\x88\x01\x01\x12<\n\x0cretry_policy\x18\x04 \x01(\x0b\x32!.modal.client.FunctionRetryPolicyH\x01\x88\x01\x01\x12\x1e\n\x11\x63oncurrency_limit\x18\x05 \x01(\rH\x02\x88\x01\x01\x12\x19\n\x0ctimeout_secs\x18\x06 \x01(\rH\x03\x88\x01\x01\x12#\n\x16task_idle_timeout_secs\x18\x07 \x01(\rH\x04\x88\x01\x01\x12\x1b\n\x0ewarm_pool_size\x18\x08 \x01(\rH\x05\x88\x01\x01\x12\x30\n\rvolume_mounts\x18\t \x03(\x0b\x32\x19.modal.client.VolumeMount\x12$\n\x17\x61llow_concurrent_inputs\x18\n \x01(\rH\x06\x88\x01\x01\x12\x1d\n\x15replace_volume_mounts\x18\x0b \x01(\x08\x12\x1a\n\x12replace_secret_ids\x18\x0c \x01(\x08\x42\x0c\n\n_resourcesB\x0f\n\r_retry_policyB\x14\n\x12_concurrency_limitB\x0f\n\r_timeout_secsB\x19\n\x17_task_idle_timeout_secsB\x11\n\x0f_warm_pool_sizeB\x1a\n\x18_allow_concurrent_inputs\"\xd6\x01\n\x18\x46unctionPrecreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x1b\n\rfunction_name\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x1c\n\x14\x65xisting_function_id\x18\x03 \x01(\t\x12:\n\rfunction_type\x18\x04 \x01(\x0e\x32#.modal.client.Function.FunctionType\x12\x33\n\x0ewebhook_config\x18\x05 \x01(\x0b\x32\x1b.modal.client.WebhookConfig\"o\n\x19\x46unctionPrecreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x9e\x01\n\x19\x46unctionBindParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x19\n\x11serialized_params\x18\x02 \x01(\x0c\x12\x37\n\x10\x66unction_options\x18\x03 \x01(\x0b\x32\x1d.modal.client.FunctionOptions\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"v\n\x1a\x46unctionBindParamsResponse\x12\x19\n\x11\x62ound_function_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\xd7\x01\n\x16\x46unctionCreateResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12.\n\x0cweb_url_info\x18\x03 \x01(\x0b\x32\x18.modal.client.WebUrlInfo\x12(\n\x08\x66unction\x18\x04 \x01(\x0b\x32\x16.modal.client.Function\x12=\n\x0fhandle_metadata\x18\x05 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"\x8a\x01\n\x12\x46unctionGetRequest\x12\x10\n\x08\x61pp_name\x18\x01 \x01(\t\x12\x12\n\nobject_tag\x18\x02 \x01(\t\x12\x34\n\tnamespace\x18\x03 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"i\n\x13\x46unctionGetResponse\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12=\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32$.modal.client.FunctionHandleMetadata\"]\n%FunctionUpdateSchedulingParamsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x1f\n\x17warm_pool_size_override\x18\x02 \x01(\r\"(\n&FunctionUpdateSchedulingParamsResponse\"\x8a\x01\n\x15\x46unctionGetInputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\x12\x13\n\x0bkill_switch\x18\x03 \x01(\x08\x12\x18\n\x10\x66unction_call_id\x18\x05 \x01(\tJ\x04\x08\x04\x10\x05\"y\n\x18\x46unctionGetInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x03 \x01(\x05\x12\x19\n\x11\x61verage_call_time\x18\x05 \x01(\x02\x12\x19\n\x11input_concurrency\x18\x06 \x01(\x05\"s\n\x19\x46unctionGetInputsResponse\x12\x33\n\x06inputs\x18\x03 \x03(\x0b\x32#.modal.client.FunctionGetInputsItem\x12!\n\x19rate_limit_sleep_duration\x18\x04 \x01(\x02\"\xa6\x01\n\x16\x46unctionGetOutputsItem\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x10\n\x08input_id\x18\x03 \x01(\t\x12\x11\n\tgen_index\x18\x04 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x05 \x01(\x0e\x32\x18.modal.client.DataFormat\"\x8b\x01\n\x19\x46unctionGetOutputsRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x12\n\nmax_values\x18\x02 \x01(\x05\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x06 \x01(\t\x12\x18\n\x10\x63lear_on_success\x18\x07 \x01(\x08\"x\n\x1a\x46unctionGetOutputsResponse\x12\x0c\n\x04idxs\x18\x03 \x03(\x05\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionGetOutputsItem\x12\x15\n\rlast_entry_id\x18\x05 \x01(\t\"3\n\x1c\x46unctionGetSerializedRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"V\n\x1d\x46unctionGetSerializedResponse\x12\x1b\n\x13\x66unction_serialized\x18\x01 \x01(\x0c\x12\x18\n\x10\x63lass_serialized\x18\x02 \x01(\x0c\"\x89\x01\n\rFunctionInput\x12\x0e\n\x04\x61rgs\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x61rgs_blob_id\x18\x07 \x01(\tH\x00\x12\x13\n\x0b\x66inal_input\x18\t \x01(\x08\x12-\n\x0b\x64\x61ta_format\x18\n \x01(\x0e\x32\x18.modal.client.DataFormatB\x0c\n\nargs_oneof\"\xd8\x01\n\x12\x46unctionMapRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x19\n\x11return_exceptions\x18\x03 \x01(\x08\x12:\n\x12\x66unction_call_type\x18\x04 \x01(\x0e\x32\x1e.modal.client.FunctionCallType\x12=\n\x10pipelined_inputs\x18\x05 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\"v\n\x13\x46unctionMapResponse\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x45\n\x10pipelined_inputs\x18\x02 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"P\n\x15\x46unctionPutInputsItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12*\n\x05input\x18\x02 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"~\n\x18\x46unctionPutInputsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x33\n\x06inputs\x18\x04 \x03(\x0b\x32#.modal.client.FunctionPutInputsItem\">\n\x1d\x46unctionPutInputsResponseItem\x12\x0b\n\x03idx\x18\x01 \x01(\x05\x12\x10\n\x08input_id\x18\x02 \x01(\t\"X\n\x19\x46unctionPutInputsResponse\x12;\n\x06inputs\x18\x01 \x03(\x0b\x32+.modal.client.FunctionPutInputsResponseItem\"\xce\x01\n\x16\x46unctionPutOutputsItem\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12\x18\n\x10input_started_at\x18\x03 \x01(\x01\x12\x19\n\x11output_created_at\x18\x04 \x01(\x01\x12\x11\n\tgen_index\x18\x06 \x01(\x05\x12-\n\x0b\x64\x61ta_format\x18\x07 \x01(\x0e\x32\x18.modal.client.DataFormat\"R\n\x19\x46unctionPutOutputsRequest\x12\x35\n\x07outputs\x18\x04 \x03(\x0b\x32$.modal.client.FunctionPutOutputsItem\"s\n\x13\x46unctionRetryPolicy\x12\x1b\n\x13\x62\x61\x63koff_coefficient\x18\x01 \x01(\x02\x12\x18\n\x10initial_delay_ms\x18\x02 \x01(\r\x12\x14\n\x0cmax_delay_ms\x18\x03 \x01(\r\x12\x0f\n\x07retries\x18\x12 \x01(\r\"7\n\x1b\x46unctionGetCallGraphRequest\x12\x18\n\x10\x66unction_call_id\x18\x02 \x01(\t\"\x8c\x01\n\x12InputCallGraphInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x39\n\x06status\x18\x02 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x18\n\x10\x66unction_call_id\x18\x03 \x01(\t\x12\x0f\n\x07task_id\x18\x04 \x01(\t\"z\n\x19\x46unctionCallCallGraphInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x17\n\x0fparent_input_id\x18\x02 \x01(\t\x12\x15\n\rfunction_name\x18\x03 \x01(\t\x12\x13\n\x0bmodule_name\x18\x04 \x01(\t\"\x91\x01\n\x1c\x46unctionGetCallGraphResponse\x12\x30\n\x06inputs\x18\x01 \x03(\x0b\x32 .modal.client.InputCallGraphInfo\x12?\n\x0e\x66unction_calls\x18\x02 \x03(\x0b\x32\'.modal.client.FunctionCallCallGraphInfo\"5\n\x19\x46unctionCallCancelRequest\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\".\n\x17\x46unctionCallListRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"\xc3\x03\n\x10\x46unctionCallInfo\x12\x18\n\x10\x66unction_call_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x12\n\ncreated_at\x18\x06 \x01(\x01\x12\x14\n\x0cscheduled_at\x18\x07 \x01(\x01\x12\x37\n\x0epending_inputs\x18\x0c \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x36\n\rfailed_inputs\x18\r \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10succeeded_inputs\x18\x0e \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x37\n\x0etimeout_inputs\x18\x0f \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x39\n\x10\x63\x61ncelled_inputs\x18\x10 \x01(\x0b\x32\x1f.modal.client.InputCategoryInfo\x12\x14\n\x0ctotal_inputs\x18\x11 \x01(\x05J\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0c\"R\n\x18\x46unctionCallListResponse\x12\x36\n\x0e\x66unction_calls\x18\x01 \x03(\x0b\x32\x1e.modal.client.FunctionCallInfo\"5\n\x1e\x46unctionGetCurrentStatsRequest\x12\x13\n\x0b\x66unction_id\x18\x01 \x01(\t\"S\n\rFunctionStats\x12\x0f\n\x07\x62\x61\x63klog\x18\x01 \x01(\r\x12\x18\n\x10num_active_tasks\x18\x02 \x01(\r\x12\x17\n\x0fnum_total_tasks\x18\x03 \x01(\r\"$\n\rGeneratorDone\x12\x13\n\x0bitems_total\x18\x01 \x01(\x04\"\xfe\x04\n\rGenericResult\x12\x39\n\x06status\x18\x01 \x01(\x0e\x32).modal.client.GenericResult.GenericStatus\x12\x11\n\texception\x18\x02 \x01(\t\x12\x10\n\x08\x65xitcode\x18\x03 \x01(\x05\x12\x11\n\ttraceback\x18\x04 \x01(\t\x12\x15\n\rserialized_tb\x18\x0b \x01(\x0c\x12\x15\n\rtb_line_cache\x18\x0c \x01(\x0c\x12\x0e\n\x04\x64\x61ta\x18\x05 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\n \x01(\tH\x00\x12?\n\ngen_status\x18\x07 \x01(\x0e\x32+.modal.client.GenericResult.GeneratorStatus\x12\x1a\n\x12propagation_reason\x18\r \x01(\t\"\xc3\x01\n\rGenericStatus\x12\x1e\n\x1aGENERIC_STATUS_UNSPECIFIED\x10\x00\x12\x1a\n\x16GENERIC_STATUS_SUCCESS\x10\x01\x12\x1a\n\x16GENERIC_STATUS_FAILURE\x10\x02\x12\x1d\n\x19GENERIC_STATUS_TERMINATED\x10\x03\x12\x1a\n\x16GENERIC_STATUS_TIMEOUT\x10\x04\x12\x1f\n\x1bGENERIC_STATUS_INIT_FAILURE\x10\x05\"s\n\x0fGeneratorStatus\x12 \n\x1cGENERATOR_STATUS_UNSPECIFIED\x10\x00\x12\x1f\n\x1bGENERATOR_STATUS_INCOMPLETE\x10\x01\x12\x1d\n\x19GENERATOR_STATUS_COMPLETE\x10\x02\x42\x0c\n\ndata_oneof\"O\n\tGPUConfig\x12#\n\x04type\x18\x01 \x01(\x0e\x32\x15.modal.client.GPUType\x12\r\n\x05\x63ount\x18\x02 \x01(\r\x12\x0e\n\x06memory\x18\x03 \x01(\r\"`\n\rBuildFunction\x12\x12\n\ndefinition\x18\x01 \x01(\t\x12\x0f\n\x07globals\x18\x02 \x01(\x0c\x12*\n\x05input\x18\x03 \x01(\x0b\x32\x1b.modal.client.FunctionInput\"\xdd\x03\n\x05Image\x12,\n\x0b\x62\x61se_images\x18\x05 \x03(\x0b\x32\x17.modal.client.BaseImage\x12\x1b\n\x13\x64ockerfile_commands\x18\x06 \x03(\t\x12\x35\n\rcontext_files\x18\x07 \x03(\x0b\x32\x1e.modal.client.ImageContextFile\x12\x0f\n\x07version\x18\x0b \x01(\t\x12\x12\n\nsecret_ids\x18\x0c \x03(\t\x12\x0b\n\x03gpu\x18\r \x01(\x08\x12\x18\n\x10\x63ontext_mount_id\x18\x0f \x01(\t\x12+\n\ngpu_config\x18\x10 \x01(\x0b\x32\x17.modal.client.GPUConfig\x12@\n\x15image_registry_config\x18\x11 \x01(\x0b\x32!.modal.client.ImageRegistryConfig\x12\x1a\n\x12\x62uild_function_def\x18\x0e \x01(\t\x12\x1e\n\x16\x62uild_function_globals\x18\x12 \x01(\x0c\x12\x0f\n\x07runtime\x18\x13 \x01(\t\x12\x15\n\rruntime_debug\x18\x14 \x01(\x08\x12\x33\n\x0e\x62uild_function\x18\x15 \x01(\x0b\x32\x1b.modal.client.BuildFunction\"4\n!ImageBuilderVersionLookupResponse\x12\x0f\n\x07version\x18\x01 \x01(\t\"2\n\x10ImageContextFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\xed\x01\n\x17ImageGetOrCreateRequest\x12\"\n\x05image\x18\x02 \x01(\x0b\x32\x13.modal.client.Image\x12\x14\n\x06\x61pp_id\x18\x04 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_image_id\x18\x05 \x01(\t\x12\x19\n\x11\x62uild_function_id\x18\x06 \x01(\t\x12\x13\n\x0b\x66orce_build\x18\x07 \x01(\x08\x12\x34\n\tnamespace\x18\x08 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x17\n\x0f\x62uilder_version\x18\t \x01(\t\",\n\x18ImageGetOrCreateResponse\x12\x10\n\x08image_id\x18\x01 \x01(\t\"U\n\x19ImageJoinStreamingRequest\x12\x10\n\x08image_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x03 \x01(\t\"\x93\x01\n\x1aImageJoinStreamingResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\x12)\n\ttask_logs\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x03 \x01(\t\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"d\n\x13ImageRegistryConfig\x12:\n\x12registry_auth_type\x18\x01 \x01(\x0e\x32\x1e.modal.client.RegistryAuthType\x12\x11\n\tsecret_id\x18\x02 \x01(\t\"\x99\x01\n\tInputInfo\x12\x10\n\x08input_id\x18\x01 \x01(\t\x12\x0b\n\x03idx\x18\x02 \x01(\x05\x12\x0f\n\x07task_id\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x05 \x01(\x01\x12\x19\n\x11task_startup_time\x18\x06 \x01(\x01\x12\x18\n\x10task_first_input\x18\x07 \x01(\x08\"K\n\x11InputCategoryInfo\x12\r\n\x05total\x18\x01 \x01(\x05\x12\'\n\x06latest\x18\x02 \x03(\x0b\x32\x17.modal.client.InputInfo\"l\n\x11MountBuildRequest\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x19\n\x11\x65xisting_mount_id\x18\x03 \x01(\t\x12&\n\x05\x66iles\x18\x04 \x03(\x0b\x32\x17.modal.client.MountFile\"b\n\x12MountBuildResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\"\xfa\x01\n\x17MountGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12&\n\x05\x66iles\x18\x05 \x03(\x0b\x32\x17.modal.client.MountFile\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\"h\n\x18MountGetOrCreateResponse\x12\x10\n\x08mount_id\x18\x01 \x01(\t\x12:\n\x0fhandle_metadata\x18\x02 \x01(\x0b\x32!.modal.client.MountHandleMetadata\":\n\x13MountHandleMetadata\x12#\n\x1b\x63ontent_checksum_sha256_hex\x18\x01 \x01(\t\"i\n\tMountFile\x12\x10\n\x08\x66ilename\x18\x01 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x11\n\x04size\x18\x04 \x01(\x04H\x00\x88\x01\x01\x12\x11\n\x04mode\x18\x05 \x01(\rH\x01\x88\x01\x01\x42\x07\n\x05_sizeB\x07\n\x05_mode\"_\n\x13MountPutFileRequest\x12\x12\n\nsha256_hex\x18\x02 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x03 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"&\n\x14MountPutFileResponse\x12\x0e\n\x06\x65xists\x18\x02 \x01(\x08\"S\n\x0fMultiPartUpload\x12\x13\n\x0bpart_length\x18\x01 \x01(\x03\x12\x13\n\x0bupload_urls\x18\x02 \x03(\t\x12\x16\n\x0e\x63ompletion_url\x18\x03 \x01(\t\"\xce\x02\n\x06Object\x12\x11\n\tobject_id\x18\x01 \x01(\t\x12H\n\x18\x66unction_handle_metadata\x18\x03 \x01(\x0b\x32$.modal.client.FunctionHandleMetadataH\x00\x12\x42\n\x15mount_handle_metadata\x18\x04 \x01(\x0b\x32!.modal.client.MountHandleMetadataH\x00\x12\x42\n\x15\x63lass_handle_metadata\x18\x05 \x01(\x0b\x32!.modal.client.ClassHandleMetadataH\x00\x12\x46\n\x17sandbox_handle_metadata\x18\x06 \x01(\x0b\x32#.modal.client.SandboxHandleMetadataH\x00\x42\x17\n\x15handle_metadata_oneof\"%\n\x10ObjectDependency\x12\x11\n\tobject_id\x18\x01 \x01(\t\"\xc2\x01\n\x17ProxyGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18ProxyGetOrCreateResponse\x12\x10\n\x08proxy_id\x18\x01 \x01(\t\"\\\n\tProxyInfo\x12\x12\n\nelastic_ip\x18\x01 \x01(\t\x12\x11\n\tproxy_key\x18\x02 \x01(\t\x12\x13\n\x0bremote_addr\x18\x03 \x01(\t\x12\x13\n\x0bremote_port\x18\x04 \x01(\x05\"\x86\x02\n\x07PTYInfo\x12\x0f\n\x07\x65nabled\x18\x01 \x01(\x08\x12\x12\n\nwinsz_rows\x18\x02 \x01(\r\x12\x12\n\nwinsz_cols\x18\x03 \x01(\r\x12\x10\n\x08\x65nv_term\x18\x04 \x01(\t\x12\x15\n\renv_colorterm\x18\x05 \x01(\t\x12\x18\n\x10\x65nv_term_program\x18\x06 \x01(\t\x12/\n\x08pty_type\x18\x07 \x01(\x0e\x32\x1d.modal.client.PTYInfo.PTYType\"N\n\x07PTYType\x12\x18\n\x14PTY_TYPE_UNSPECIFIED\x10\x00\x12\x15\n\x11PTY_TYPE_FUNCTION\x10\x01\x12\x12\n\x0ePTY_TYPE_SHELL\x10\x02\"?\n\x12QueueCreateRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x19\n\x11\x65xisting_queue_id\x18\x02 \x01(\t\"\'\n\x13QueueCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"\xc2\x01\n\x17QueueGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\",\n\x18QueueGetOrCreateResponse\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"F\n\x0fQueueGetRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x10\n\x08n_values\x18\x04 \x01(\x05\"\"\n\x10QueueGetResponse\x12\x0e\n\x06values\x18\x02 \x03(\x0c\")\n\x15QueueHeartbeatRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"3\n\x0fQueuePutRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\x12\x0e\n\x06values\x18\x04 \x03(\x0c\"#\n\x0fQueueLenRequest\x12\x10\n\x08queue_id\x18\x01 \x01(\t\"\x1f\n\x10QueueLenResponse\x12\x0b\n\x03len\x18\x01 \x01(\x05\"M\n\tRateLimit\x12\r\n\x05limit\x18\x01 \x01(\x05\x12\x31\n\x08interval\x18\x02 \x01(\x0e\x32\x1f.modal.client.RateLimitInterval\"^\n\tResources\x12\x11\n\tmemory_mb\x18\x02 \x01(\r\x12\x11\n\tmilli_cpu\x18\x03 \x01(\r\x12+\n\ngpu_config\x18\x04 \x01(\x0b\x32\x17.modal.client.GPUConfig\"d\n\x07S3Mount\x12\x13\n\x0b\x62ucket_name\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12\x1d\n\x15\x63redentials_secret_id\x18\x03 \x01(\t\x12\x11\n\tread_only\x18\x04 \x01(\x08\"\x99\x04\n\x07Sandbox\x12\x17\n\x0f\x65ntrypoint_args\x18\x01 \x03(\t\x12\x11\n\tmount_ids\x18\x02 \x03(\t\x12\x10\n\x08image_id\x18\x03 \x01(\t\x12\x12\n\nsecret_ids\x18\x04 \x03(\t\x12*\n\tresources\x18\x05 \x01(\x0b\x32\x17.modal.client.Resources\x12\x33\n\x0e\x63loud_provider\x18\x06 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x14\n\x0ctimeout_secs\x18\x07 \x01(\r\x12\x14\n\x07workdir\x18\x08 \x01(\tH\x00\x88\x01\x01\x12\x33\n\nnfs_mounts\x18\t \x03(\x0b\x32\x1f.modal.client.SharedVolumeMount\x12\x15\n\rruntime_debug\x18\n \x01(\x08\x12\x15\n\rblock_network\x18\x0b \x01(\x08\x12(\n\ts3_mounts\x18\x0c \x03(\x0b\x32\x15.modal.client.S3Mount\x12;\n\x13\x63loud_bucket_mounts\x18\x0e \x03(\x0b\x32\x1e.modal.client.CloudBucketMount\x12\x30\n\rvolume_mounts\x18\r \x03(\x0b\x32\x19.modal.client.VolumeMount\x12\'\n\x08pty_info\x18\x0f \x01(\x0b\x32\x15.modal.client.PTYInfoB\n\n\x08_workdir\"W\n\x14SandboxCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\"+\n\x15SandboxCreateResponse\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"-\n\x17SandboxGetTaskIdRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"+\n\x18SandboxGetTaskIdResponse\x12\x0f\n\x07task_id\x18\x01 \x01(\t\"\x8a\x01\n\x15SandboxGetLogsRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x35\n\x0f\x66ile_descriptor\x18\x02 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07timeout\x18\x03 \x01(\x02\x12\x15\n\rlast_entry_id\x18\x04 \x01(\t\"Y\n\x18SandboxStdinWriteRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\r\n\x05input\x18\x02 \x01(\x0c\x12\r\n\x05index\x18\x03 \x01(\r\x12\x0b\n\x03\x65of\x18\x04 \x01(\x08\"\x1b\n\x19SandboxStdinWriteResponse\"D\n\x15SandboxHandleMetadata\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x83\x01\n\x0bSandboxInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12)\n\ndefinition\x18\x02 \x01(\x0b\x32\x15.modal.client.Sandbox\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12)\n\ttask_info\x18\x04 \x01(\x0b\x32\x16.modal.client.TaskInfo\">\n\x12SandboxListRequest\x12\x0e\n\x06\x61pp_id\x18\x01 \x01(\t\x12\x18\n\x10\x62\x65\x66ore_timestamp\x18\x02 \x01(\x01\"C\n\x13SandboxListResponse\x12,\n\tsandboxes\x18\x01 \x03(\x0b\x32\x19.modal.client.SandboxInfo\"-\n\x17SandboxTerminateRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\"P\n\x18SandboxTerminateResponse\x12\x34\n\x0f\x65xisting_result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"9\n\x12SandboxWaitRequest\x12\x12\n\nsandbox_id\x18\x01 \x01(\t\x12\x0f\n\x07timeout\x18\x02 \x01(\x02\"B\n\x13SandboxWaitResponse\x12+\n\x06result\x18\x01 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\x8e\x02\n\x08Schedule\x12+\n\x04\x63ron\x18\x01 \x01(\x0b\x32\x1b.modal.client.Schedule.CronH\x00\x12/\n\x06period\x18\x02 \x01(\x0b\x32\x1d.modal.client.Schedule.PeriodH\x00\x1a\x1b\n\x04\x43ron\x12\x13\n\x0b\x63ron_string\x18\x01 \x01(\t\x1au\n\x06Period\x12\r\n\x05years\x18\x01 \x01(\x05\x12\x0e\n\x06months\x18\x02 \x01(\x05\x12\r\n\x05weeks\x18\x03 \x01(\x05\x12\x0c\n\x04\x64\x61ys\x18\x04 \x01(\x05\x12\r\n\x05hours\x18\x05 \x01(\x05\x12\x0f\n\x07minutes\x18\x06 \x01(\x05\x12\x0f\n\x07seconds\x18\x07 \x01(\x02\x42\x10\n\x0eschedule_oneof\"\xd0\x01\n\x13SecretCreateRequest\x12@\n\x08\x65nv_dict\x18\x01 \x03(\x0b\x32..modal.client.SecretCreateRequest.EnvDictEntry\x12\x14\n\x06\x61pp_id\x18\x02 \x01(\tB\x04\x80\xb5\x18\x01\x12\x15\n\rtemplate_type\x18\x03 \x01(\t\x12\x1a\n\x12\x65xisting_secret_id\x18\x04 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\")\n\x14SecretCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"\xca\x02\n\x18SecretGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x45\n\x08\x65nv_dict\x18\x05 \x03(\x0b\x32\x33.modal.client.SecretGetOrCreateRequest.EnvDictEntry\x12\x0e\n\x06\x61pp_id\x18\x06 \x01(\t\x1a.\n\x0c\x45nvDictEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\".\n\x19SecretGetOrCreateResponse\x12\x11\n\tsecret_id\x18\x01 \x01(\t\"c\n\x0eSecretListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x12\n\ncreated_at\x18\x02 \x01(\x01\x12\x14\n\x0clast_used_at\x18\x03 \x01(\x01\x12\x18\n\x10\x65nvironment_name\x18\x04 \x01(\t\"-\n\x11SecretListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12SecretListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.SecretListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"\xd9\x01\n\x1eSharedVolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\";\n\x1fSharedVolumeGetOrCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"8\n\x1cSharedVolumeHeartbeatRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"f\n\x19SharedVolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x33\n\x0e\x63loud_provider\x18\x02 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"6\n\x1aSharedVolumeCreateResponse\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\"\x88\x01\n\x14SharedVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\x12\x33\n\x0e\x63loud_provider\x18\x04 \x01(\x0e\x32\x1b.modal.client.CloudProvider\"3\n\x17SharedVolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"g\n\x18SharedVolumeListResponse\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.SharedVolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"F\n\x1cSharedVolumeListFilesRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"\xa7\x01\n\x1aSharedVolumePutFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x12\n\nsha256_hex\x18\x03 \x01(\t\x12\x0e\n\x04\x64\x61ta\x18\x04 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x05 \x01(\tH\x00\x12\x11\n\tresumable\x18\x06 \x01(\x08\x42\x0c\n\ndata_oneof\"-\n\x1bSharedVolumePutFileResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"D\n\x1aSharedVolumeGetFileRequest\x12\x18\n\x10shared_volume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\"S\n\x1bSharedVolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x42\x0c\n\ndata_oneof\"`\n\x1dSharedVolumeRemoveFileRequest\x12\x1e\n\x10shared_volume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x11\n\trecursive\x18\x03 \x01(\x08\"\xa1\x01\n\x1aSharedVolumeListFilesEntry\x12\x0c\n\x04path\x18\x01 \x01(\t\x12?\n\x04type\x18\x02 \x01(\x0e\x32\x31.modal.client.SharedVolumeListFilesEntry.FileType\"4\n\x08\x46ileType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x08\n\x04\x46ILE\x10\x01\x12\r\n\tDIRECTORY\x10\x02\"Z\n\x1dSharedVolumeListFilesResponse\x12\x39\n\x07\x65ntries\x18\x01 \x03(\x0b\x32(.modal.client.SharedVolumeListFilesEntry\"\x92\x01\n\x11SharedVolumeMount\x12\x12\n\nmount_path\x18\x01 \x01(\t\x12\x18\n\x10shared_volume_id\x18\x02 \x01(\t\x12\x33\n\x0e\x63loud_provider\x18\x03 \x01(\x0e\x32\x1b.modal.client.CloudProvider\x12\x1a\n\x12\x61llow_cross_region\x18\x04 \x01(\x08\".\n\x19TaskCurrentInputsResponse\x12\x11\n\tinput_ids\x18\x01 \x03(\t\"l\n\x08TaskInfo\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nstarted_at\x18\x02 \x01(\x01\x12\x13\n\x0b\x66inished_at\x18\x03 \x01(\x01\x12+\n\x06result\x18\x04 \x01(\x0b\x32\x1b.modal.client.GenericResult\"\xee\x01\n\x08TaskLogs\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12+\n\ntask_state\x18\x06 \x01(\x0e\x32\x17.modal.client.TaskState\x12\x11\n\ttimestamp\x18\x07 \x01(\x01\x12\x35\n\x0f\x66ile_descriptor\x18\x08 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x31\n\rtask_progress\x18\t \x01(\x0b\x32\x1a.modal.client.TaskProgress\x12\x18\n\x10\x66unction_call_id\x18\n \x01(\t\x12\x10\n\x08input_id\x18\x0b \x01(\t\"\x11\n\x0fTaskListRequest\":\n\x10TaskListResponse\x12&\n\x05tasks\x18\x01 \x03(\x0b\x32\x17.modal.client.TaskStats\"\xc6\x01\n\rTaskLogsBatch\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12%\n\x05items\x18\x02 \x03(\x0b\x32\x16.modal.client.TaskLogs\x12\x10\n\x08\x65ntry_id\x18\x05 \x01(\t\x12\x10\n\x08\x61pp_done\x18\n \x01(\x08\x12\x13\n\x0b\x66unction_id\x18\x0b \x01(\t\x12\x10\n\x08input_id\x18\x0c \x01(\t\x12\x10\n\x08image_id\x18\r \x01(\t\x12\x0b\n\x03\x65of\x18\x0e \x01(\x08\x12\x13\n\x0bpty_exec_id\x18\x0f \x01(\t\"p\n\x0cTaskProgress\x12\x0b\n\x03len\x18\x01 \x01(\x04\x12\x0b\n\x03pos\x18\x02 \x01(\x04\x12\x31\n\rprogress_type\x18\x03 \x01(\x0e\x32\x1a.modal.client.ProgressType\x12\x13\n\x0b\x64\x65scription\x18\x04 \x01(\t\"@\n\x11TaskResultRequest\x12+\n\x06result\x18\x02 \x01(\x0b\x32\x1b.modal.client.GenericResult\"Y\n\tTaskStats\x12\x0f\n\x07task_id\x18\x01 \x01(\t\x12\x0e\n\x06\x61pp_id\x18\x02 \x01(\t\x12\x17\n\x0f\x61pp_description\x18\x03 \x01(\t\x12\x12\n\nstarted_at\x18\x04 \x01(\x01\"V\n\x16TokenFlowCreateRequest\x12\x12\n\nutm_source\x18\x03 \x01(\t\x12\x16\n\x0elocalhost_port\x18\x04 \x01(\x05\x12\x10\n\x08next_url\x18\x05 \x01(\t\"d\n\x17TokenFlowCreateResponse\x12\x15\n\rtoken_flow_id\x18\x01 \x01(\t\x12\x0f\n\x07web_url\x18\x02 \x01(\t\x12\x0c\n\x04\x63ode\x18\x03 \x01(\t\x12\x13\n\x0bwait_secret\x18\x04 \x01(\t\"S\n\x14TokenFlowWaitRequest\x12\x0f\n\x07timeout\x18\x01 \x01(\x02\x12\x15\n\rtoken_flow_id\x18\x02 \x01(\t\x12\x13\n\x0bwait_secret\x18\x03 \x01(\t\"l\n\x15TokenFlowWaitResponse\x12\x10\n\x08token_id\x18\x01 \x01(\t\x12\x14\n\x0ctoken_secret\x18\x02 \x01(\t\x12\x0f\n\x07timeout\x18\x03 \x01(\x08\x12\x1a\n\x12workspace_username\x18\x04 \x01(\t\"7\n\x12TunnelStartRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\x12\x13\n\x0bunencrypted\x18\x02 \x01(\x08\"\x99\x01\n\x13TunnelStartResponse\x12\x0c\n\x04host\x18\x01 \x01(\t\x12\x0c\n\x04port\x18\x02 \x01(\r\x12\x1d\n\x10unencrypted_host\x18\x03 \x01(\tH\x00\x88\x01\x01\x12\x1d\n\x10unencrypted_port\x18\x04 \x01(\rH\x01\x88\x01\x01\x42\x13\n\x11_unencrypted_hostB\x13\n\x11_unencrypted_port\"!\n\x11TunnelStopRequest\x12\x0c\n\x04port\x18\x01 \x01(\r\"$\n\x12TunnelStopResponse\x12\x0e\n\x06\x65xists\x18\x01 \x01(\x08\"\xd3\x01\n\x18VolumeGetOrCreateRequest\x12\x17\n\x0f\x64\x65ployment_name\x18\x01 \x01(\t\x12\x34\n\tnamespace\x18\x02 \x01(\x0e\x32!.modal.client.DeploymentNamespace\x12\x18\n\x10\x65nvironment_name\x18\x03 \x01(\t\x12>\n\x14object_creation_type\x18\x04 \x01(\x0e\x32 .modal.client.ObjectCreationType\x12\x0e\n\x06\x61pp_id\x18\x05 \x01(\t\".\n\x19VolumeGetOrCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x16VolumeHeartbeatRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"+\n\x13VolumeCreateRequest\x12\x14\n\x06\x61pp_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\")\n\x14VolumeCreateResponse\x12\x11\n\tvolume_id\x18\x01 \x01(\t\".\n\x13VolumeCommitRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\"+\n\x14VolumeCommitResponse\x12\x13\n\x0bskip_reload\x18\x01 \x01(\x08\"F\n\x13VolumeDeleteRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x1c\n\x10\x65nvironment_name\x18\x02 \x01(\tB\x02\x18\x01\"S\n\x14VolumeGetFileRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\r\n\x05start\x18\x03 \x01(\x04\x12\x0b\n\x03len\x18\x04 \x01(\x04\"w\n\x15VolumeGetFileResponse\x12\x0e\n\x04\x64\x61ta\x18\x01 \x01(\x0cH\x00\x12\x16\n\x0c\x64\x61ta_blob_id\x18\x02 \x01(\tH\x00\x12\x0c\n\x04size\x18\x03 \x01(\x04\x12\r\n\x05start\x18\x04 \x01(\x04\x12\x0b\n\x03len\x18\x05 \x01(\x04\x42\x0c\n\ndata_oneof\"\xbf\x01\n\x14VolumeListFilesEntry\x12\x0c\n\x04path\x18\x01 \x01(\t\x12\x39\n\x04type\x18\x02 \x01(\x0e\x32+.modal.client.VolumeListFilesEntry.FileType\x12\r\n\x05mtime\x18\x03 \x01(\x04\x12\x0c\n\x04size\x18\x04 \x01(\x04\"A\n\x08\x46ileType\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x08\n\x04\x46ILE\x10\x01\x12\r\n\tDIRECTORY\x10\x02\x12\x0b\n\x07SYMLINK\x10\x03\"c\n\x16VolumeListFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x0c\n\x04path\x18\x02 \x01(\t\x12\x18\n\x0bmax_entries\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0e\n\x0c_max_entries\"N\n\x17VolumeListFilesResponse\x12\x33\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\".modal.client.VolumeListFilesEntry\"F\n\x0eVolumeListItem\x12\r\n\x05label\x18\x01 \x01(\t\x12\x11\n\tvolume_id\x18\x02 \x01(\t\x12\x12\n\ncreated_at\x18\x03 \x01(\x01\"-\n\x11VolumeListRequest\x12\x18\n\x10\x65nvironment_name\x18\x01 \x01(\t\"[\n\x12VolumeListResponse\x12+\n\x05items\x18\x01 \x03(\x0b\x32\x1c.modal.client.VolumeListItem\x12\x18\n\x10\x65nvironment_name\x18\x02 \x01(\t\"(\n\x13VolumeReloadRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\"}\n\x15VolumePutFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12&\n\x05\x66iles\x18\x02 \x03(\x0b\x32\x17.modal.client.MountFile\x12)\n!disallow_overwrite_existing_files\x18\x03 \x01(\x08\"S\n\x17VolumeRemoveFileRequest\x12\x17\n\tvolume_id\x18\x01 \x01(\tB\x04\x80\xb5\x18\x01\x12\x0c\n\x04path\x18\x02 \x01(\x0c\x12\x11\n\trecursive\x18\x03 \x01(\x08\"c\n\x16VolumeCopyFilesRequest\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x11\n\tsrc_paths\x18\x02 \x03(\x0c\x12\x10\n\x08\x64st_path\x18\x03 \x01(\x0c\x12\x11\n\trecursive\x18\x04 \x01(\x08\"V\n\x0bVolumeMount\x12\x11\n\tvolume_id\x18\x01 \x01(\t\x12\x12\n\nmount_path\x18\x02 \x01(\t\x12 \n\x18\x61llow_background_commits\x18\x03 \x01(\x08\"\x8d\x02\n\rWebhookConfig\x12\'\n\x04type\x18\x01 \x01(\x0e\x32\x19.modal.client.WebhookType\x12\x0e\n\x06method\x18\x02 \x01(\t\x12\x18\n\x10requested_suffix\x18\x04 \x01(\t\x12\x32\n\nasync_mode\x18\x05 \x01(\x0e\x32\x1e.modal.client.WebhookAsyncMode\x12\x38\n\x0e\x63ustom_domains\x18\x06 \x03(\x0b\x32 .modal.client.CustomDomainConfig\x12\x17\n\x0fweb_server_port\x18\x07 \x01(\r\x12\"\n\x1aweb_server_startup_timeout\x18\x08 \x01(\x02\"N\n\nWebUrlInfo\x12\x11\n\ttruncated\x18\x01 \x01(\x08\x12\x17\n\x0fhas_unique_hash\x18\x02 \x01(\x08\x12\x14\n\x0clabel_stolen\x18\x03 \x01(\x08\"G\n\x1bWorkspaceNameLookupResponse\x12\x16\n\x0eworkspace_name\x18\x01 \x01(\t\x12\x10\n\x08username\x18\x02 \x01(\t\"^\n\x14RuntimeOutputMessage\x12\x35\n\x0f\x66ile_descriptor\x18\x01 \x01(\x0e\x32\x1c.modal.client.FileDescriptor\x12\x0f\n\x07message\x18\x02 \x01(\t\"\x82\x01\n\x12RuntimeOutputBatch\x12\x31\n\x05items\x18\x01 \x03(\x0b\x32\".modal.client.RuntimeOutputMessage\x12\x13\n\x0b\x62\x61tch_index\x18\x02 \x01(\x04\x12\x16\n\texit_code\x18\x03 \x01(\x05H\x00\x88\x01\x01\x42\x0c\n\n_exit_code\"=\n\x13RuntimeInputMessage\x12\x0f\n\x07message\x18\x01 \x01(\x0c\x12\x15\n\rmessage_index\x18\x02 \x01(\x04*\x83\x01\n\x13\x41ppDeployVisibility\x12%\n!APP_DEPLOY_VISIBILITY_UNSPECIFIED\x10\x00\x12#\n\x1f\x41PP_DEPLOY_VISIBILITY_WORKSPACE\x10\x01\x12 \n\x1c\x41PP_DEPLOY_VISIBILITY_PUBLIC\x10\x02*\xf5\x01\n\x13\x41ppDisconnectReason\x12%\n!APP_DISCONNECT_REASON_UNSPECIFIED\x10\x00\x12)\n%APP_DISCONNECT_REASON_LOCAL_EXCEPTION\x10\x01\x12,\n(APP_DISCONNECT_REASON_KEYBOARD_INTERRUPT\x10\x02\x12.\n*APP_DISCONNECT_REASON_ENTRYPOINT_COMPLETED\x10\x03\x12.\n*APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION\x10\x04*\x8d\x02\n\x08\x41ppState\x12\x19\n\x15\x41PP_STATE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STATE_EPHEMERAL\x10\x01\x12\x16\n\x12\x41PP_STATE_DETACHED\x10\x02\x12\x16\n\x12\x41PP_STATE_DEPLOYED\x10\x03\x12\x16\n\x12\x41PP_STATE_STOPPING\x10\x04\x12\x15\n\x11\x41PP_STATE_STOPPED\x10\x05\x12\x1a\n\x16\x41PP_STATE_INITIALIZING\x10\x06\x12\x16\n\x12\x41PP_STATE_DISABLED\x10\x07\x12#\n\x1f\x41PP_STATE_DETACHED_DISCONNECTED\x10\x08\x12\x15\n\x11\x41PP_STATE_DERIVED\x10\t*\x85\x01\n\rAppStopSource\x12\x1f\n\x1b\x41PP_STOP_SOURCE_UNSPECIFIED\x10\x00\x12\x17\n\x13\x41PP_STOP_SOURCE_CLI\x10\x01\x12!\n\x1d\x41PP_STOP_SOURCE_PYTHON_CLIENT\x10\x02\x12\x17\n\x13\x41PP_STOP_SOURCE_WEB\x10\x03*\x91\x01\n\x11\x43\x65rtificateStatus\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_PENDING\x10\x00\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_ISSUED\x10\x01\x12\x1d\n\x19\x43\x45RTIFICATE_STATUS_FAILED\x10\x02\x12\x1e\n\x1a\x43\x45RTIFICATE_STATUS_REVOKED\x10\x03*\xb1\x01\n\x10\x43heckpointStatus\x12!\n\x1d\x43HECKPOINT_STATUS_UNSPECIFIED\x10\x00\x12\x1d\n\x19\x43HECKPOINT_STATUS_PENDING\x10\x01\x12 \n\x1c\x43HECKPOINT_STATUS_PROCESSING\x10\x02\x12\x1b\n\x17\x43HECKPOINT_STATUS_READY\x10\x03\x12\x1c\n\x18\x43HECKPOINT_STATUS_FAILED\x10\x04*\xb0\x01\n\nClientType\x12\x1b\n\x17\x43LIENT_TYPE_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LIENT_TYPE_CLIENT\x10\x01\x12\x1a\n\x12\x43LIENT_TYPE_WORKER\x10\x02\x1a\x02\x08\x01\x12\x19\n\x15\x43LIENT_TYPE_CONTAINER\x10\x03\x12\x1a\n\x12\x43LIENT_TYPE_SERVER\x10\x04\x1a\x02\x08\x01\x12\x1a\n\x16\x43LIENT_TYPE_WEB_SERVER\x10\x05*\x90\x01\n\rCloudProvider\x12\x1e\n\x1a\x43LOUD_PROVIDER_UNSPECIFIED\x10\x00\x12\x16\n\x12\x43LOUD_PROVIDER_AWS\x10\x01\x12\x16\n\x12\x43LOUD_PROVIDER_GCP\x10\x02\x12\x17\n\x13\x43LOUD_PROVIDER_AUTO\x10\x03\x12\x16\n\x12\x43LOUD_PROVIDER_OCI\x10\x04*w\n\nDataFormat\x12\x1b\n\x17\x44\x41TA_FORMAT_UNSPECIFIED\x10\x00\x12\x16\n\x12\x44\x41TA_FORMAT_PICKLE\x10\x01\x12\x14\n\x10\x44\x41TA_FORMAT_ASGI\x10\x02\x12\x1e\n\x1a\x44\x41TA_FORMAT_GENERATOR_DONE\x10\x03*\x80\x01\n\x13\x44\x65ploymentNamespace\x12$\n DEPLOYMENT_NAMESPACE_UNSPECIFIED\x10\x00\x12\"\n\x1e\x44\x45PLOYMENT_NAMESPACE_WORKSPACE\x10\x01\x12\x1f\n\x1b\x44\x45PLOYMENT_NAMESPACE_GLOBAL\x10\x03*Z\n\rDNSRecordType\x12\x15\n\x11\x44NS_RECORD_TYPE_A\x10\x00\x12\x17\n\x13\x44NS_RECORD_TYPE_TXT\x10\x01\x12\x19\n\x15\x44NS_RECORD_TYPE_CNAME\x10\x02*\x83\x01\n\x0e\x46ileDescriptor\x12\x1f\n\x1b\x46ILE_DESCRIPTOR_UNSPECIFIED\x10\x00\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDOUT\x10\x01\x12\x1a\n\x16\x46ILE_DESCRIPTOR_STDERR\x10\x02\x12\x18\n\x14\x46ILE_DESCRIPTOR_INFO\x10\x03*p\n\x10\x46unctionCallType\x12\"\n\x1e\x46UNCTION_CALL_TYPE_UNSPECIFIED\x10\x00\x12\x1c\n\x18\x46UNCTION_CALL_TYPE_UNARY\x10\x01\x12\x1a\n\x16\x46UNCTION_CALL_TYPE_MAP\x10\x02*\x82\x02\n\x07GPUType\x12\x18\n\x14GPU_TYPE_UNSPECIFIED\x10\x00\x12\x0f\n\x0bGPU_TYPE_T4\x10\x01\x12\x11\n\rGPU_TYPE_A100\x10\x02\x12\x11\n\rGPU_TYPE_A10G\x10\x03\x12\x10\n\x0cGPU_TYPE_ANY\x10\x04\x12\x19\n\x11GPU_TYPE_A100_20G\x10\x05\x1a\x02\x08\x01\x12\x1f\n\x17GPU_TYPE_A100_40GB_MANY\x10\x06\x1a\x02\x08\x01\x12\x1c\n\x14GPU_TYPE_INFERENTIA2\x10\x07\x1a\x02\x08\x01\x12\x16\n\x12GPU_TYPE_A100_80GB\x10\x08\x12\x0f\n\x0bGPU_TYPE_L4\x10\t\x12\x11\n\rGPU_TYPE_H100\x10\n*\xa0\x02\n\x12ObjectCreationType\x12$\n OBJECT_CREATION_TYPE_UNSPECIFIED\x10\x00\x12*\n&OBJECT_CREATION_TYPE_CREATE_IF_MISSING\x10\x01\x12.\n*OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS\x10\x02\x12\x33\n/OBJECT_CREATION_TYPE_CREATE_OVERWRITE_IF_EXISTS\x10\x03\x12/\n+OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP\x10\x04\x12\"\n\x1eOBJECT_CREATION_TYPE_EPHEMERAL\x10\x05*>\n\x0cProgressType\x12\x19\n\x15IMAGE_SNAPSHOT_UPLOAD\x10\x00\x12\x13\n\x0f\x46UNCTION_QUEUED\x10\x01*x\n\x11RateLimitInterval\x12#\n\x1fRATE_LIMIT_INTERVAL_UNSPECIFIED\x10\x00\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_SECOND\x10\x01\x12\x1e\n\x1aRATE_LIMIT_INTERVAL_MINUTE\x10\x02*\xb2\x01\n\x10RegistryAuthType\x12\"\n\x1eREGISTRY_AUTH_TYPE_UNSPECIFIED\x10\x00\x12\x1a\n\x16REGISTRY_AUTH_TYPE_AWS\x10\x01\x12\x1a\n\x16REGISTRY_AUTH_TYPE_GCP\x10\x02\x12\x1d\n\x19REGISTRY_AUTH_TYPE_PUBLIC\x10\x03\x12#\n\x1fREGISTRY_AUTH_TYPE_STATIC_CREDS\x10\x04*\xdc\x02\n\tTaskState\x12\x1a\n\x16TASK_STATE_UNSPECIFIED\x10\x00\x12\x16\n\x12TASK_STATE_CREATED\x10\x06\x12\x15\n\x11TASK_STATE_QUEUED\x10\x01\x12\x1e\n\x1aTASK_STATE_WORKER_ASSIGNED\x10\x02\x12\x1c\n\x18TASK_STATE_LOADING_IMAGE\x10\x03\x12\x15\n\x11TASK_STATE_ACTIVE\x10\x04\x12\x18\n\x14TASK_STATE_COMPLETED\x10\x05\x12!\n\x1dTASK_STATE_CREATING_CONTAINER\x10\x07\x12\x13\n\x0fTASK_STATE_IDLE\x10\x08\x12\x1a\n\x16TASK_STATE_PREEMPTIBLE\x10\t\x12\x18\n\x14TASK_STATE_PREEMPTED\x10\n\x12\'\n#TASK_STATE_LOADING_CHECKPOINT_IMAGE\x10\x0b*\x99\x01\n\x0bWebhookType\x12\x1c\n\x18WEBHOOK_TYPE_UNSPECIFIED\x10\x00\x12\x19\n\x15WEBHOOK_TYPE_ASGI_APP\x10\x01\x12\x19\n\x15WEBHOOK_TYPE_FUNCTION\x10\x02\x12\x19\n\x15WEBHOOK_TYPE_WSGI_APP\x10\x03\x12\x1b\n\x17WEBHOOK_TYPE_WEB_SERVER\x10\x04*\x9a\x01\n\x10WebhookAsyncMode\x12\"\n\x1eWEBHOOK_ASYNC_MODE_UNSPECIFIED\x10\x00\x12\x1f\n\x1bWEBHOOK_ASYNC_MODE_DISABLED\x10\x02\x12\x1e\n\x1aWEBHOOK_ASYNC_MODE_TRIGGER\x10\x03\x12\x1b\n\x17WEBHOOK_ASYNC_MODE_AUTO\x10\x04\"\x04\x08\x01\x10\x01\x32\xa3N\n\x0bModalClient\x12L\n\tAppCreate\x12\x1e.modal.client.AppCreateRequest\x1a\x1f.modal.client.AppCreateResponse\x12W\n\x13\x41ppClientDisconnect\x12(.modal.client.AppClientDisconnectRequest\x1a\x16.google.protobuf.Empty\x12L\n\nAppGetLogs\x12\x1f.modal.client.AppGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12K\n\rAppSetObjects\x12\".modal.client.AppSetObjectsRequest\x1a\x16.google.protobuf.Empty\x12X\n\rAppGetObjects\x12\".modal.client.AppGetObjectsRequest\x1a#.modal.client.AppGetObjectsResponse\x12\x46\n\x07\x41ppList\x12\x1c.modal.client.AppListRequest\x1a\x1d.modal.client.AppListResponse\x12^\n\x0f\x41ppLookupObject\x12$.modal.client.AppLookupObjectRequest\x1a%.modal.client.AppLookupObjectResponse\x12L\n\tAppDeploy\x12\x1e.modal.client.AppDeployRequest\x1a\x1f.modal.client.AppDeployResponse\x12p\n\x15\x41ppDeploySingleObject\x12*.modal.client.AppDeploySingleObjectRequest\x1a+.modal.client.AppDeploySingleObjectResponse\x12s\n\x16\x41ppGetByDeploymentName\x12+.modal.client.AppGetByDeploymentNameRequest\x1a,.modal.client.AppGetByDeploymentNameResponse\x12?\n\x07\x41ppStop\x12\x1c.modal.client.AppStopRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0c\x41ppHeartbeat\x12!.modal.client.AppHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12O\n\nBlobCreate\x12\x1f.modal.client.BlobCreateRequest\x1a .modal.client.BlobCreateResponse\x12\x46\n\x07\x42lobGet\x12\x1c.modal.client.BlobGetRequest\x1a\x1d.modal.client.BlobGetResponse\x12R\n\x0b\x43lassCreate\x12 .modal.client.ClassCreateRequest\x1a!.modal.client.ClassCreateResponse\x12I\n\x08\x43lassGet\x12\x1d.modal.client.ClassGetRequest\x1a\x1e.modal.client.ClassGetResponse\x12U\n\x0c\x43lientCreate\x12!.modal.client.ClientCreateRequest\x1a\".modal.client.ClientCreateResponse\x12H\n\x0b\x43lientHello\x12\x16.google.protobuf.Empty\x1a!.modal.client.ClientHelloResponse\x12O\n\x0f\x43lientHeartbeat\x12$.modal.client.ClientHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12g\n\x12\x43ontainerHeartbeat\x12\'.modal.client.ContainerHeartbeatRequest\x1a(.modal.client.ContainerHeartbeatResponse\x12X\n\rContainerExec\x12\".modal.client.ContainerExecRequest\x1a#.modal.client.ContainerExecResponse\x12i\n\x16\x43ontainerExecGetOutput\x12+.modal.client.ContainerExecGetOutputRequest\x1a .modal.client.RuntimeOutputBatch0\x01\x12[\n\x15\x43ontainerExecPutInput\x12*.modal.client.ContainerExecPutInputRequest\x1a\x16.google.protobuf.Empty\x12W\n\x13\x43ontainerCheckpoint\x12(.modal.client.ContainerCheckpointRequest\x1a\x16.google.protobuf.Empty\x12\x43\n\tDictClear\x12\x1e.modal.client.DictClearRequest\x1a\x16.google.protobuf.Empty\x12O\n\nDictCreate\x12\x1f.modal.client.DictCreateRequest\x1a .modal.client.DictCreateResponse\x12^\n\x0f\x44ictGetOrCreate\x12$.modal.client.DictGetOrCreateRequest\x1a%.modal.client.DictGetOrCreateResponse\x12K\n\rDictHeartbeat\x12\".modal.client.DictHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12O\n\nDictUpdate\x12\x1f.modal.client.DictUpdateRequest\x1a .modal.client.DictUpdateResponse\x12\x46\n\x07\x44ictGet\x12\x1c.modal.client.DictGetRequest\x1a\x1d.modal.client.DictGetResponse\x12\x46\n\x07\x44ictPop\x12\x1c.modal.client.DictPopRequest\x1a\x1d.modal.client.DictPopResponse\x12U\n\x0c\x44ictContains\x12!.modal.client.DictContainsRequest\x1a\".modal.client.DictContainsResponse\x12\x46\n\x07\x44ictLen\x12\x1c.modal.client.DictLenRequest\x1a\x1d.modal.client.DictLenResponse\x12U\n\x0c\x44omainCreate\x12!.modal.client.DomainCreateRequest\x1a\".modal.client.DomainCreateResponse\x12O\n\nDomainList\x12\x1f.modal.client.DomainListRequest\x1a .modal.client.DomainListResponse\x12v\n\x17\x44omainCertificateVerify\x12,.modal.client.DomainCertificateVerifyRequest\x1a-.modal.client.DomainCertificateVerifyResponse\x12S\n\x11\x45nvironmentCreate\x12&.modal.client.EnvironmentCreateRequest\x1a\x16.google.protobuf.Empty\x12P\n\x0f\x45nvironmentList\x12\x16.google.protobuf.Empty\x1a%.modal.client.EnvironmentListResponse\x12S\n\x11\x45nvironmentDelete\x12&.modal.client.EnvironmentDeleteRequest\x1a\x16.google.protobuf.Empty\x12^\n\x11\x45nvironmentUpdate\x12&.modal.client.EnvironmentUpdateRequest\x1a!.modal.client.EnvironmentListItem\x12g\n\x12\x46unctionBindParams\x12\'.modal.client.FunctionBindParamsRequest\x1a(.modal.client.FunctionBindParamsResponse\x12[\n\x0e\x46unctionCreate\x12#.modal.client.FunctionCreateRequest\x1a$.modal.client.FunctionCreateResponse\x12R\n\x0b\x46unctionGet\x12 .modal.client.FunctionGetRequest\x1a!.modal.client.FunctionGetResponse\x12m\n\x14\x46unctionGetCallGraph\x12).modal.client.FunctionGetCallGraphRequest\x1a*.modal.client.FunctionGetCallGraphResponse\x12\x64\n\x17\x46unctionGetCurrentStats\x12,.modal.client.FunctionGetCurrentStatsRequest\x1a\x1b.modal.client.FunctionStats\x12\x64\n\x11\x46unctionGetInputs\x12&.modal.client.FunctionGetInputsRequest\x1a\'.modal.client.FunctionGetInputsResponse\x12g\n\x12\x46unctionGetOutputs\x12\'.modal.client.FunctionGetOutputsRequest\x1a(.modal.client.FunctionGetOutputsResponse\x12p\n\x15\x46unctionGetSerialized\x12*.modal.client.FunctionGetSerializedRequest\x1a+.modal.client.FunctionGetSerializedResponse\x12R\n\x0b\x46unctionMap\x12 .modal.client.FunctionMapRequest\x1a!.modal.client.FunctionMapResponse\x12\x64\n\x11\x46unctionPrecreate\x12&.modal.client.FunctionPrecreateRequest\x1a\'.modal.client.FunctionPrecreateResponse\x12\x64\n\x11\x46unctionPutInputs\x12&.modal.client.FunctionPutInputsRequest\x1a\'.modal.client.FunctionPutInputsResponse\x12U\n\x12\x46unctionPutOutputs\x12\'.modal.client.FunctionPutOutputsRequest\x1a\x16.google.protobuf.Empty\x12\x8b\x01\n\x1e\x46unctionUpdateSchedulingParams\x12\x33.modal.client.FunctionUpdateSchedulingParamsRequest\x1a\x34.modal.client.FunctionUpdateSchedulingParamsResponse\x12U\n\x12\x46unctionCallCancel\x12\'.modal.client.FunctionCallCancelRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10\x46unctionCallList\x12%.modal.client.FunctionCallListRequest\x1a&.modal.client.FunctionCallListResponse\x12\\\n\x15\x46unctionCallGetDataIn\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12]\n\x16\x46unctionCallGetDataOut\x12(.modal.client.FunctionCallGetDataRequest\x1a\x17.modal.client.DataChunk0\x01\x12Z\n\x16\x46unctionCallPutDataOut\x12(.modal.client.FunctionCallPutDataRequest\x1a\x16.google.protobuf.Empty\x12G\n\x15\x46unctionStartPtyShell\x12\x16.google.protobuf.Empty\x1a\x16.google.protobuf.Empty\x12\x61\n\x10ImageGetOrCreate\x12%.modal.client.ImageGetOrCreateRequest\x1a&.modal.client.ImageGetOrCreateResponse\x12i\n\x12ImageJoinStreaming\x12\'.modal.client.ImageJoinStreamingRequest\x1a(.modal.client.ImageJoinStreamingResponse0\x01\x12\x64\n\x19ImageBuilderVersionLookup\x12\x16.google.protobuf.Empty\x1a/.modal.client.ImageBuilderVersionLookupResponse\x12U\n\x0cMountPutFile\x12!.modal.client.MountPutFileRequest\x1a\".modal.client.MountPutFileResponse\x12O\n\nMountBuild\x12\x1f.modal.client.MountBuildRequest\x1a .modal.client.MountBuildResponse\x12\x61\n\x10MountGetOrCreate\x12%.modal.client.MountGetOrCreateRequest\x1a&.modal.client.MountGetOrCreateResponse\x12\x61\n\x10ProxyGetOrCreate\x12%.modal.client.ProxyGetOrCreateRequest\x1a&.modal.client.ProxyGetOrCreateResponse\x12R\n\x0bQueueCreate\x12 .modal.client.QueueCreateRequest\x1a!.modal.client.QueueCreateResponse\x12\x61\n\x10QueueGetOrCreate\x12%.modal.client.QueueGetOrCreateRequest\x1a&.modal.client.QueueGetOrCreateResponse\x12I\n\x08QueueGet\x12\x1d.modal.client.QueueGetRequest\x1a\x1e.modal.client.QueueGetResponse\x12M\n\x0eQueueHeartbeat\x12#.modal.client.QueueHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x41\n\x08QueuePut\x12\x1d.modal.client.QueuePutRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08QueueLen\x12\x1d.modal.client.QueueLenRequest\x1a\x1e.modal.client.QueueLenResponse\x12X\n\rSandboxCreate\x12\".modal.client.SandboxCreateRequest\x1a#.modal.client.SandboxCreateResponse\x12\x61\n\x10SandboxGetTaskId\x12%.modal.client.SandboxGetTaskIdRequest\x1a&.modal.client.SandboxGetTaskIdResponse\x12T\n\x0eSandboxGetLogs\x12#.modal.client.SandboxGetLogsRequest\x1a\x1b.modal.client.TaskLogsBatch0\x01\x12R\n\x0bSandboxWait\x12 .modal.client.SandboxWaitRequest\x1a!.modal.client.SandboxWaitResponse\x12R\n\x0bSandboxList\x12 .modal.client.SandboxListRequest\x1a!.modal.client.SandboxListResponse\x12\x61\n\x10SandboxTerminate\x12%.modal.client.SandboxTerminateRequest\x1a&.modal.client.SandboxTerminateResponse\x12\x64\n\x11SandboxStdinWrite\x12&.modal.client.SandboxStdinWriteRequest\x1a\'.modal.client.SandboxStdinWriteResponse\x12U\n\x0cSecretCreate\x12!.modal.client.SecretCreateRequest\x1a\".modal.client.SecretCreateResponse\x12\x64\n\x11SecretGetOrCreate\x12&.modal.client.SecretGetOrCreateRequest\x1a\'.modal.client.SecretGetOrCreateResponse\x12O\n\nSecretList\x12\x1f.modal.client.SecretListRequest\x1a .modal.client.SecretListResponse\x12v\n\x17SharedVolumeGetOrCreate\x12,.modal.client.SharedVolumeGetOrCreateRequest\x1a-.modal.client.SharedVolumeGetOrCreateResponse\x12g\n\x12SharedVolumeCreate\x12\'.modal.client.SharedVolumeCreateRequest\x1a(.modal.client.SharedVolumeCreateResponse\x12[\n\x15SharedVolumeHeartbeat\x12*.modal.client.SharedVolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12\x61\n\x10SharedVolumeList\x12%.modal.client.SharedVolumeListRequest\x1a&.modal.client.SharedVolumeListResponse\x12p\n\x15SharedVolumeListFiles\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse\x12x\n\x1bSharedVolumeListFilesStream\x12*.modal.client.SharedVolumeListFilesRequest\x1a+.modal.client.SharedVolumeListFilesResponse0\x01\x12j\n\x13SharedVolumePutFile\x12(.modal.client.SharedVolumePutFileRequest\x1a).modal.client.SharedVolumePutFileResponse\x12j\n\x13SharedVolumeGetFile\x12(.modal.client.SharedVolumeGetFileRequest\x1a).modal.client.SharedVolumeGetFileResponse\x12]\n\x16SharedVolumeRemoveFile\x12+.modal.client.SharedVolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12\x45\n\nTaskResult\x12\x1f.modal.client.TaskResultRequest\x1a\x16.google.protobuf.Empty\x12I\n\x08TaskList\x12\x1d.modal.client.TaskListRequest\x1a\x1e.modal.client.TaskListResponse\x12T\n\x11TaskCurrentInputs\x12\x16.google.protobuf.Empty\x1a\'.modal.client.TaskCurrentInputsResponse\x12^\n\x0fTokenFlowCreate\x12$.modal.client.TokenFlowCreateRequest\x1a%.modal.client.TokenFlowCreateResponse\x12X\n\rTokenFlowWait\x12\".modal.client.TokenFlowWaitRequest\x1a#.modal.client.TokenFlowWaitResponse\x12R\n\x0bTunnelStart\x12 .modal.client.TunnelStartRequest\x1a!.modal.client.TunnelStartResponse\x12O\n\nTunnelStop\x12\x1f.modal.client.TunnelStopRequest\x1a .modal.client.TunnelStopResponse\x12\x64\n\x11VolumeGetOrCreate\x12&.modal.client.VolumeGetOrCreateRequest\x1a\'.modal.client.VolumeGetOrCreateResponse\x12U\n\x0cVolumeCreate\x12!.modal.client.VolumeCreateRequest\x1a\".modal.client.VolumeCreateResponse\x12O\n\x0fVolumeHeartbeat\x12$.modal.client.VolumeHeartbeatRequest\x1a\x16.google.protobuf.Empty\x12U\n\x0cVolumeCommit\x12!.modal.client.VolumeCommitRequest\x1a\".modal.client.VolumeCommitResponse\x12I\n\x0cVolumeDelete\x12!.modal.client.VolumeDeleteRequest\x1a\x16.google.protobuf.Empty\x12X\n\rVolumeGetFile\x12\".modal.client.VolumeGetFileRequest\x1a#.modal.client.VolumeGetFileResponse\x12O\n\nVolumeList\x12\x1f.modal.client.VolumeListRequest\x1a .modal.client.VolumeListResponse\x12`\n\x0fVolumeListFiles\x12$.modal.client.VolumeListFilesRequest\x1a%.modal.client.VolumeListFilesResponse0\x01\x12M\n\x0eVolumePutFiles\x12#.modal.client.VolumePutFilesRequest\x1a\x16.google.protobuf.Empty\x12I\n\x0cVolumeReload\x12!.modal.client.VolumeReloadRequest\x1a\x16.google.protobuf.Empty\x12Q\n\x10VolumeRemoveFile\x12%.modal.client.VolumeRemoveFileRequest\x1a\x16.google.protobuf.Empty\x12O\n\x0fVolumeCopyFiles\x12$.modal.client.VolumeCopyFilesRequest\x1a\x16.google.protobuf.Empty\x12X\n\x13WorkspaceNameLookup\x12\x16.google.protobuf.Empty\x1a).modal.client.WorkspaceNameLookupResponseb\x06proto3')
 
 _APPDEPLOYVISIBILITY = DESCRIPTOR.enum_types_by_name['AppDeployVisibility']
 AppDeployVisibility = enum_type_wrapper.EnumTypeWrapper(_APPDEPLOYVISIBILITY)
 _APPDISCONNECTREASON = DESCRIPTOR.enum_types_by_name['AppDisconnectReason']
 AppDisconnectReason = enum_type_wrapper.EnumTypeWrapper(_APPDISCONNECTREASON)
 _APPSTATE = DESCRIPTOR.enum_types_by_name['AppState']
 AppState = enum_type_wrapper.EnumTypeWrapper(_APPSTATE)
@@ -100,15 +100,14 @@
 CLIENT_TYPE_SERVER = 4
 CLIENT_TYPE_WEB_SERVER = 5
 CLOUD_PROVIDER_UNSPECIFIED = 0
 CLOUD_PROVIDER_AWS = 1
 CLOUD_PROVIDER_GCP = 2
 CLOUD_PROVIDER_AUTO = 3
 CLOUD_PROVIDER_OCI = 4
-CLOUD_PROVIDER_LAMBDA_LABS = 5
 DATA_FORMAT_UNSPECIFIED = 0
 DATA_FORMAT_PICKLE = 1
 DATA_FORMAT_ASGI = 2
 DATA_FORMAT_GENERATOR_DONE = 3
 DEPLOYMENT_NAMESPACE_UNSPECIFIED = 0
 DEPLOYMENT_NAMESPACE_WORKSPACE = 1
 DEPLOYMENT_NAMESPACE_GLOBAL = 3
@@ -169,15 +168,14 @@
 WEBHOOK_ASYNC_MODE_UNSPECIFIED = 0
 WEBHOOK_ASYNC_MODE_DISABLED = 2
 WEBHOOK_ASYNC_MODE_TRIGGER = 3
 WEBHOOK_ASYNC_MODE_AUTO = 4
 
 
 _CLOUDBUCKETMOUNT = DESCRIPTOR.message_types_by_name['CloudBucketMount']
-_FILEENTRY = DESCRIPTOR.message_types_by_name['FileEntry']
 _APPCLIENTDISCONNECTREQUEST = DESCRIPTOR.message_types_by_name['AppClientDisconnectRequest']
 _APPCREATEREQUEST = DESCRIPTOR.message_types_by_name['AppCreateRequest']
 _APPCREATERESPONSE = DESCRIPTOR.message_types_by_name['AppCreateResponse']
 _APPSTOPREQUEST = DESCRIPTOR.message_types_by_name['AppStopRequest']
 _APPDEPLOYREQUEST = DESCRIPTOR.message_types_by_name['AppDeployRequest']
 _APPDEPLOYRESPONSE = DESCRIPTOR.message_types_by_name['AppDeployResponse']
 _APPDEPLOYSINGLEOBJECTREQUEST = DESCRIPTOR.message_types_by_name['AppDeploySingleObjectRequest']
@@ -246,15 +244,14 @@
 _DICTCREATERESPONSE = DESCRIPTOR.message_types_by_name['DictCreateResponse']
 _DICTGETORCREATEREQUEST = DESCRIPTOR.message_types_by_name['DictGetOrCreateRequest']
 _DICTGETORCREATERESPONSE = DESCRIPTOR.message_types_by_name['DictGetOrCreateResponse']
 _DICTENTRY = DESCRIPTOR.message_types_by_name['DictEntry']
 _DICTGETREQUEST = DESCRIPTOR.message_types_by_name['DictGetRequest']
 _DICTGETRESPONSE = DESCRIPTOR.message_types_by_name['DictGetResponse']
 _DICTHEARTBEATREQUEST = DESCRIPTOR.message_types_by_name['DictHeartbeatRequest']
-_DICTCONTENTSREQUEST = DESCRIPTOR.message_types_by_name['DictContentsRequest']
 _DICTLENREQUEST = DESCRIPTOR.message_types_by_name['DictLenRequest']
 _DICTLENRESPONSE = DESCRIPTOR.message_types_by_name['DictLenResponse']
 _DICTPOPREQUEST = DESCRIPTOR.message_types_by_name['DictPopRequest']
 _DICTPOPRESPONSE = DESCRIPTOR.message_types_by_name['DictPopResponse']
 _DICTUPDATEREQUEST = DESCRIPTOR.message_types_by_name['DictUpdateRequest']
 _DICTUPDATERESPONSE = DESCRIPTOR.message_types_by_name['DictUpdateResponse']
 _DNSRECORD = DESCRIPTOR.message_types_by_name['DNSRecord']
@@ -315,14 +312,15 @@
 _FUNCTIONGETCURRENTSTATSREQUEST = DESCRIPTOR.message_types_by_name['FunctionGetCurrentStatsRequest']
 _FUNCTIONSTATS = DESCRIPTOR.message_types_by_name['FunctionStats']
 _GENERATORDONE = DESCRIPTOR.message_types_by_name['GeneratorDone']
 _GENERICRESULT = DESCRIPTOR.message_types_by_name['GenericResult']
 _GPUCONFIG = DESCRIPTOR.message_types_by_name['GPUConfig']
 _BUILDFUNCTION = DESCRIPTOR.message_types_by_name['BuildFunction']
 _IMAGE = DESCRIPTOR.message_types_by_name['Image']
+_IMAGEBUILDERVERSIONLOOKUPRESPONSE = DESCRIPTOR.message_types_by_name['ImageBuilderVersionLookupResponse']
 _IMAGECONTEXTFILE = DESCRIPTOR.message_types_by_name['ImageContextFile']
 _IMAGEGETORCREATEREQUEST = DESCRIPTOR.message_types_by_name['ImageGetOrCreateRequest']
 _IMAGEGETORCREATERESPONSE = DESCRIPTOR.message_types_by_name['ImageGetOrCreateResponse']
 _IMAGEJOINSTREAMINGREQUEST = DESCRIPTOR.message_types_by_name['ImageJoinStreamingRequest']
 _IMAGEJOINSTREAMINGRESPONSE = DESCRIPTOR.message_types_by_name['ImageJoinStreamingResponse']
 _IMAGEREGISTRYCONFIG = DESCRIPTOR.message_types_by_name['ImageRegistryConfig']
 _INPUTINFO = DESCRIPTOR.message_types_by_name['InputInfo']
@@ -348,17 +346,14 @@
 _QUEUEGETORCREATERESPONSE = DESCRIPTOR.message_types_by_name['QueueGetOrCreateResponse']
 _QUEUEGETREQUEST = DESCRIPTOR.message_types_by_name['QueueGetRequest']
 _QUEUEGETRESPONSE = DESCRIPTOR.message_types_by_name['QueueGetResponse']
 _QUEUEHEARTBEATREQUEST = DESCRIPTOR.message_types_by_name['QueueHeartbeatRequest']
 _QUEUEPUTREQUEST = DESCRIPTOR.message_types_by_name['QueuePutRequest']
 _QUEUELENREQUEST = DESCRIPTOR.message_types_by_name['QueueLenRequest']
 _QUEUELENRESPONSE = DESCRIPTOR.message_types_by_name['QueueLenResponse']
-_QUEUENEXTITEMSREQUEST = DESCRIPTOR.message_types_by_name['QueueNextItemsRequest']
-_QUEUEITEM = DESCRIPTOR.message_types_by_name['QueueItem']
-_QUEUENEXTITEMSRESPONSE = DESCRIPTOR.message_types_by_name['QueueNextItemsResponse']
 _RATELIMIT = DESCRIPTOR.message_types_by_name['RateLimit']
 _RESOURCES = DESCRIPTOR.message_types_by_name['Resources']
 _S3MOUNT = DESCRIPTOR.message_types_by_name['S3Mount']
 _SANDBOX = DESCRIPTOR.message_types_by_name['Sandbox']
 _SANDBOXCREATEREQUEST = DESCRIPTOR.message_types_by_name['SandboxCreateRequest']
 _SANDBOXCREATERESPONSE = DESCRIPTOR.message_types_by_name['SandboxCreateResponse']
 _SANDBOXGETTASKIDREQUEST = DESCRIPTOR.message_types_by_name['SandboxGetTaskIdRequest']
@@ -396,14 +391,15 @@
 _SHAREDVOLUMELISTRESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumeListResponse']
 _SHAREDVOLUMELISTFILESREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumeListFilesRequest']
 _SHAREDVOLUMEPUTFILEREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumePutFileRequest']
 _SHAREDVOLUMEPUTFILERESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumePutFileResponse']
 _SHAREDVOLUMEGETFILEREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumeGetFileRequest']
 _SHAREDVOLUMEGETFILERESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumeGetFileResponse']
 _SHAREDVOLUMEREMOVEFILEREQUEST = DESCRIPTOR.message_types_by_name['SharedVolumeRemoveFileRequest']
+_SHAREDVOLUMELISTFILESENTRY = DESCRIPTOR.message_types_by_name['SharedVolumeListFilesEntry']
 _SHAREDVOLUMELISTFILESRESPONSE = DESCRIPTOR.message_types_by_name['SharedVolumeListFilesResponse']
 _SHAREDVOLUMEMOUNT = DESCRIPTOR.message_types_by_name['SharedVolumeMount']
 _TASKCURRENTINPUTSRESPONSE = DESCRIPTOR.message_types_by_name['TaskCurrentInputsResponse']
 _TASKINFO = DESCRIPTOR.message_types_by_name['TaskInfo']
 _TASKLOGS = DESCRIPTOR.message_types_by_name['TaskLogs']
 _TASKLISTREQUEST = DESCRIPTOR.message_types_by_name['TaskListRequest']
 _TASKLISTRESPONSE = DESCRIPTOR.message_types_by_name['TaskListResponse']
@@ -425,14 +421,15 @@
 _VOLUMECREATEREQUEST = DESCRIPTOR.message_types_by_name['VolumeCreateRequest']
 _VOLUMECREATERESPONSE = DESCRIPTOR.message_types_by_name['VolumeCreateResponse']
 _VOLUMECOMMITREQUEST = DESCRIPTOR.message_types_by_name['VolumeCommitRequest']
 _VOLUMECOMMITRESPONSE = DESCRIPTOR.message_types_by_name['VolumeCommitResponse']
 _VOLUMEDELETEREQUEST = DESCRIPTOR.message_types_by_name['VolumeDeleteRequest']
 _VOLUMEGETFILEREQUEST = DESCRIPTOR.message_types_by_name['VolumeGetFileRequest']
 _VOLUMEGETFILERESPONSE = DESCRIPTOR.message_types_by_name['VolumeGetFileResponse']
+_VOLUMELISTFILESENTRY = DESCRIPTOR.message_types_by_name['VolumeListFilesEntry']
 _VOLUMELISTFILESREQUEST = DESCRIPTOR.message_types_by_name['VolumeListFilesRequest']
 _VOLUMELISTFILESRESPONSE = DESCRIPTOR.message_types_by_name['VolumeListFilesResponse']
 _VOLUMELISTITEM = DESCRIPTOR.message_types_by_name['VolumeListItem']
 _VOLUMELISTREQUEST = DESCRIPTOR.message_types_by_name['VolumeListRequest']
 _VOLUMELISTRESPONSE = DESCRIPTOR.message_types_by_name['VolumeListResponse']
 _VOLUMERELOADREQUEST = DESCRIPTOR.message_types_by_name['VolumeReloadRequest']
 _VOLUMEPUTFILESREQUEST = DESCRIPTOR.message_types_by_name['VolumePutFilesRequest']
@@ -442,34 +439,28 @@
 _WEBHOOKCONFIG = DESCRIPTOR.message_types_by_name['WebhookConfig']
 _WEBURLINFO = DESCRIPTOR.message_types_by_name['WebUrlInfo']
 _WORKSPACENAMELOOKUPRESPONSE = DESCRIPTOR.message_types_by_name['WorkspaceNameLookupResponse']
 _RUNTIMEOUTPUTMESSAGE = DESCRIPTOR.message_types_by_name['RuntimeOutputMessage']
 _RUNTIMEOUTPUTBATCH = DESCRIPTOR.message_types_by_name['RuntimeOutputBatch']
 _RUNTIMEINPUTMESSAGE = DESCRIPTOR.message_types_by_name['RuntimeInputMessage']
 _CLOUDBUCKETMOUNT_BUCKETTYPE = _CLOUDBUCKETMOUNT.enum_types_by_name['BucketType']
-_FILEENTRY_FILETYPE = _FILEENTRY.enum_types_by_name['FileType']
 _FUNCTION_DEFINITIONTYPE = _FUNCTION.enum_types_by_name['DefinitionType']
 _FUNCTION_FUNCTIONTYPE = _FUNCTION.enum_types_by_name['FunctionType']
 _GENERICRESULT_GENERICSTATUS = _GENERICRESULT.enum_types_by_name['GenericStatus']
 _GENERICRESULT_GENERATORSTATUS = _GENERICRESULT.enum_types_by_name['GeneratorStatus']
 _PTYINFO_PTYTYPE = _PTYINFO.enum_types_by_name['PTYType']
+_SHAREDVOLUMELISTFILESENTRY_FILETYPE = _SHAREDVOLUMELISTFILESENTRY.enum_types_by_name['FileType']
+_VOLUMELISTFILESENTRY_FILETYPE = _VOLUMELISTFILESENTRY.enum_types_by_name['FileType']
 CloudBucketMount = _reflection.GeneratedProtocolMessageType('CloudBucketMount', (_message.Message,), {
   'DESCRIPTOR' : _CLOUDBUCKETMOUNT,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.CloudBucketMount)
   })
 _sym_db.RegisterMessage(CloudBucketMount)
 
-FileEntry = _reflection.GeneratedProtocolMessageType('FileEntry', (_message.Message,), {
-  'DESCRIPTOR' : _FILEENTRY,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.FileEntry)
-  })
-_sym_db.RegisterMessage(FileEntry)
-
 AppClientDisconnectRequest = _reflection.GeneratedProtocolMessageType('AppClientDisconnectRequest', (_message.Message,), {
   'DESCRIPTOR' : _APPCLIENTDISCONNECTREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.AppClientDisconnectRequest)
   })
 _sym_db.RegisterMessage(AppClientDisconnectRequest)
 
@@ -1009,21 +1000,14 @@
 DictHeartbeatRequest = _reflection.GeneratedProtocolMessageType('DictHeartbeatRequest', (_message.Message,), {
   'DESCRIPTOR' : _DICTHEARTBEATREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.DictHeartbeatRequest)
   })
 _sym_db.RegisterMessage(DictHeartbeatRequest)
 
-DictContentsRequest = _reflection.GeneratedProtocolMessageType('DictContentsRequest', (_message.Message,), {
-  'DESCRIPTOR' : _DICTCONTENTSREQUEST,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.DictContentsRequest)
-  })
-_sym_db.RegisterMessage(DictContentsRequest)
-
 DictLenRequest = _reflection.GeneratedProtocolMessageType('DictLenRequest', (_message.Message,), {
   'DESCRIPTOR' : _DICTLENREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.DictLenRequest)
   })
 _sym_db.RegisterMessage(DictLenRequest)
 
@@ -1492,14 +1476,21 @@
 Image = _reflection.GeneratedProtocolMessageType('Image', (_message.Message,), {
   'DESCRIPTOR' : _IMAGE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.Image)
   })
 _sym_db.RegisterMessage(Image)
 
+ImageBuilderVersionLookupResponse = _reflection.GeneratedProtocolMessageType('ImageBuilderVersionLookupResponse', (_message.Message,), {
+  'DESCRIPTOR' : _IMAGEBUILDERVERSIONLOOKUPRESPONSE,
+  '__module__' : 'modal_proto.api_pb2'
+  # @@protoc_insertion_point(class_scope:modal.client.ImageBuilderVersionLookupResponse)
+  })
+_sym_db.RegisterMessage(ImageBuilderVersionLookupResponse)
+
 ImageContextFile = _reflection.GeneratedProtocolMessageType('ImageContextFile', (_message.Message,), {
   'DESCRIPTOR' : _IMAGECONTEXTFILE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.ImageContextFile)
   })
 _sym_db.RegisterMessage(ImageContextFile)
 
@@ -1723,35 +1714,14 @@
 QueueLenResponse = _reflection.GeneratedProtocolMessageType('QueueLenResponse', (_message.Message,), {
   'DESCRIPTOR' : _QUEUELENRESPONSE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.QueueLenResponse)
   })
 _sym_db.RegisterMessage(QueueLenResponse)
 
-QueueNextItemsRequest = _reflection.GeneratedProtocolMessageType('QueueNextItemsRequest', (_message.Message,), {
-  'DESCRIPTOR' : _QUEUENEXTITEMSREQUEST,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.QueueNextItemsRequest)
-  })
-_sym_db.RegisterMessage(QueueNextItemsRequest)
-
-QueueItem = _reflection.GeneratedProtocolMessageType('QueueItem', (_message.Message,), {
-  'DESCRIPTOR' : _QUEUEITEM,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.QueueItem)
-  })
-_sym_db.RegisterMessage(QueueItem)
-
-QueueNextItemsResponse = _reflection.GeneratedProtocolMessageType('QueueNextItemsResponse', (_message.Message,), {
-  'DESCRIPTOR' : _QUEUENEXTITEMSRESPONSE,
-  '__module__' : 'modal_proto.api_pb2'
-  # @@protoc_insertion_point(class_scope:modal.client.QueueNextItemsResponse)
-  })
-_sym_db.RegisterMessage(QueueNextItemsResponse)
-
 RateLimit = _reflection.GeneratedProtocolMessageType('RateLimit', (_message.Message,), {
   'DESCRIPTOR' : _RATELIMIT,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.RateLimit)
   })
 _sym_db.RegisterMessage(RateLimit)
 
@@ -2063,14 +2033,21 @@
 SharedVolumeRemoveFileRequest = _reflection.GeneratedProtocolMessageType('SharedVolumeRemoveFileRequest', (_message.Message,), {
   'DESCRIPTOR' : _SHAREDVOLUMEREMOVEFILEREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.SharedVolumeRemoveFileRequest)
   })
 _sym_db.RegisterMessage(SharedVolumeRemoveFileRequest)
 
+SharedVolumeListFilesEntry = _reflection.GeneratedProtocolMessageType('SharedVolumeListFilesEntry', (_message.Message,), {
+  'DESCRIPTOR' : _SHAREDVOLUMELISTFILESENTRY,
+  '__module__' : 'modal_proto.api_pb2'
+  # @@protoc_insertion_point(class_scope:modal.client.SharedVolumeListFilesEntry)
+  })
+_sym_db.RegisterMessage(SharedVolumeListFilesEntry)
+
 SharedVolumeListFilesResponse = _reflection.GeneratedProtocolMessageType('SharedVolumeListFilesResponse', (_message.Message,), {
   'DESCRIPTOR' : _SHAREDVOLUMELISTFILESRESPONSE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.SharedVolumeListFilesResponse)
   })
 _sym_db.RegisterMessage(SharedVolumeListFilesResponse)
 
@@ -2266,14 +2243,21 @@
 VolumeGetFileResponse = _reflection.GeneratedProtocolMessageType('VolumeGetFileResponse', (_message.Message,), {
   'DESCRIPTOR' : _VOLUMEGETFILERESPONSE,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.VolumeGetFileResponse)
   })
 _sym_db.RegisterMessage(VolumeGetFileResponse)
 
+VolumeListFilesEntry = _reflection.GeneratedProtocolMessageType('VolumeListFilesEntry', (_message.Message,), {
+  'DESCRIPTOR' : _VOLUMELISTFILESENTRY,
+  '__module__' : 'modal_proto.api_pb2'
+  # @@protoc_insertion_point(class_scope:modal.client.VolumeListFilesEntry)
+  })
+_sym_db.RegisterMessage(VolumeListFilesEntry)
+
 VolumeListFilesRequest = _reflection.GeneratedProtocolMessageType('VolumeListFilesRequest', (_message.Message,), {
   'DESCRIPTOR' : _VOLUMELISTFILESREQUEST,
   '__module__' : 'modal_proto.api_pb2'
   # @@protoc_insertion_point(class_scope:modal.client.VolumeListFilesRequest)
   })
 _sym_db.RegisterMessage(VolumeListFilesRequest)
 
@@ -2444,614 +2428,610 @@
   _VOLUMECREATEREQUEST.fields_by_name['app_id']._serialized_options = b'\200\265\030\001'
   _VOLUMECOMMITREQUEST.fields_by_name['volume_id']._options = None
   _VOLUMECOMMITREQUEST.fields_by_name['volume_id']._serialized_options = b'\200\265\030\001'
   _VOLUMEDELETEREQUEST.fields_by_name['environment_name']._options = None
   _VOLUMEDELETEREQUEST.fields_by_name['environment_name']._serialized_options = b'\030\001'
   _VOLUMEREMOVEFILEREQUEST.fields_by_name['volume_id']._options = None
   _VOLUMEREMOVEFILEREQUEST.fields_by_name['volume_id']._serialized_options = b'\200\265\030\001'
-  _WORKSPACENAMELOOKUPRESPONSE.fields_by_name['workspace_name']._options = None
-  _WORKSPACENAMELOOKUPRESPONSE.fields_by_name['workspace_name']._serialized_options = b'\030\001'
-  _APPDEPLOYVISIBILITY._serialized_start=31253
-  _APPDEPLOYVISIBILITY._serialized_end=31384
-  _APPDISCONNECTREASON._serialized_start=31387
-  _APPDISCONNECTREASON._serialized_end=31632
-  _APPSTATE._serialized_start=31635
-  _APPSTATE._serialized_end=31904
-  _APPSTOPSOURCE._serialized_start=31907
-  _APPSTOPSOURCE._serialized_end=32040
-  _CERTIFICATESTATUS._serialized_start=32043
-  _CERTIFICATESTATUS._serialized_end=32188
-  _CHECKPOINTSTATUS._serialized_start=32191
-  _CHECKPOINTSTATUS._serialized_end=32368
-  _CLIENTTYPE._serialized_start=32371
-  _CLIENTTYPE._serialized_end=32547
-  _CLOUDPROVIDER._serialized_start=32550
-  _CLOUDPROVIDER._serialized_end=32726
-  _DATAFORMAT._serialized_start=32728
-  _DATAFORMAT._serialized_end=32847
-  _DEPLOYMENTNAMESPACE._serialized_start=32850
-  _DEPLOYMENTNAMESPACE._serialized_end=32978
-  _DNSRECORDTYPE._serialized_start=32980
-  _DNSRECORDTYPE._serialized_end=33070
-  _FILEDESCRIPTOR._serialized_start=33073
-  _FILEDESCRIPTOR._serialized_end=33204
-  _FUNCTIONCALLTYPE._serialized_start=33206
-  _FUNCTIONCALLTYPE._serialized_end=33318
-  _GPUTYPE._serialized_start=33321
-  _GPUTYPE._serialized_end=33579
-  _OBJECTCREATIONTYPE._serialized_start=33582
-  _OBJECTCREATIONTYPE._serialized_end=33870
-  _PROGRESSTYPE._serialized_start=33872
-  _PROGRESSTYPE._serialized_end=33934
-  _RATELIMITINTERVAL._serialized_start=33936
-  _RATELIMITINTERVAL._serialized_end=34056
-  _REGISTRYAUTHTYPE._serialized_start=34059
-  _REGISTRYAUTHTYPE._serialized_end=34237
-  _TASKSTATE._serialized_start=34240
-  _TASKSTATE._serialized_end=34588
-  _WEBHOOKTYPE._serialized_start=34591
-  _WEBHOOKTYPE._serialized_end=34744
-  _WEBHOOKASYNCMODE._serialized_start=34747
-  _WEBHOOKASYNCMODE._serialized_end=34901
+  _APPDEPLOYVISIBILITY._serialized_start=31021
+  _APPDEPLOYVISIBILITY._serialized_end=31152
+  _APPDISCONNECTREASON._serialized_start=31155
+  _APPDISCONNECTREASON._serialized_end=31400
+  _APPSTATE._serialized_start=31403
+  _APPSTATE._serialized_end=31672
+  _APPSTOPSOURCE._serialized_start=31675
+  _APPSTOPSOURCE._serialized_end=31808
+  _CERTIFICATESTATUS._serialized_start=31811
+  _CERTIFICATESTATUS._serialized_end=31956
+  _CHECKPOINTSTATUS._serialized_start=31959
+  _CHECKPOINTSTATUS._serialized_end=32136
+  _CLIENTTYPE._serialized_start=32139
+  _CLIENTTYPE._serialized_end=32315
+  _CLOUDPROVIDER._serialized_start=32318
+  _CLOUDPROVIDER._serialized_end=32462
+  _DATAFORMAT._serialized_start=32464
+  _DATAFORMAT._serialized_end=32583
+  _DEPLOYMENTNAMESPACE._serialized_start=32586
+  _DEPLOYMENTNAMESPACE._serialized_end=32714
+  _DNSRECORDTYPE._serialized_start=32716
+  _DNSRECORDTYPE._serialized_end=32806
+  _FILEDESCRIPTOR._serialized_start=32809
+  _FILEDESCRIPTOR._serialized_end=32940
+  _FUNCTIONCALLTYPE._serialized_start=32942
+  _FUNCTIONCALLTYPE._serialized_end=33054
+  _GPUTYPE._serialized_start=33057
+  _GPUTYPE._serialized_end=33315
+  _OBJECTCREATIONTYPE._serialized_start=33318
+  _OBJECTCREATIONTYPE._serialized_end=33606
+  _PROGRESSTYPE._serialized_start=33608
+  _PROGRESSTYPE._serialized_end=33670
+  _RATELIMITINTERVAL._serialized_start=33672
+  _RATELIMITINTERVAL._serialized_end=33792
+  _REGISTRYAUTHTYPE._serialized_start=33795
+  _REGISTRYAUTHTYPE._serialized_end=33973
+  _TASKSTATE._serialized_start=33976
+  _TASKSTATE._serialized_end=34324
+  _WEBHOOKTYPE._serialized_start=34327
+  _WEBHOOKTYPE._serialized_end=34480
+  _WEBHOOKASYNCMODE._serialized_start=34483
+  _WEBHOOKASYNCMODE._serialized_end=34637
   _CLOUDBUCKETMOUNT._serialized_start=128
-  _CLOUDBUCKETMOUNT._serialized_end=439
-  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_start=361
-  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_end=415
-  _FILEENTRY._serialized_start=442
-  _FILEENTRY._serialized_end=611
-  _FILEENTRY_FILETYPE._serialized_start=546
-  _FILEENTRY_FILETYPE._serialized_end=611
-  _APPCLIENTDISCONNECTREQUEST._serialized_start=613
-  _APPCLIENTDISCONNECTREQUEST._serialized_end=727
-  _APPCREATEREQUEST._serialized_start=730
-  _APPCREATEREQUEST._serialized_end=909
-  _APPCREATERESPONSE._serialized_start=911
-  _APPCREATERESPONSE._serialized_end=968
-  _APPSTOPREQUEST._serialized_start=970
-  _APPSTOPREQUEST._serialized_end=1053
-  _APPDEPLOYREQUEST._serialized_start=1056
-  _APPDEPLOYREQUEST._serialized_end=1242
-  _APPDEPLOYRESPONSE._serialized_start=1244
-  _APPDEPLOYRESPONSE._serialized_end=1276
-  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_start=1279
-  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_end=1422
-  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_start=1424
-  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_end=1471
-  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_start=1473
-  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_end=1598
-  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_start=1600
-  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_end=1648
-  _APPGETLOGSREQUEST._serialized_start=1651
-  _APPGETLOGSREQUEST._serialized_end=1837
-  _APPGETOBJECTSREQUEST._serialized_start=1839
-  _APPGETOBJECTSREQUEST._serialized_end=1904
-  _APPGETOBJECTSITEM._serialized_start=1906
-  _APPGETOBJECTSITEM._serialized_end=1976
-  _APPGETOBJECTSRESPONSE._serialized_start=1978
-  _APPGETOBJECTSRESPONSE._serialized_end=2049
-  _APPHEARTBEATREQUEST._serialized_start=2051
-  _APPHEARTBEATREQUEST._serialized_end=2088
-  _APPLISTREQUEST._serialized_start=2090
-  _APPLISTREQUEST._serialized_end=2132
-  _APPLISTRESPONSE._serialized_start=2134
-  _APPLISTRESPONSE._serialized_end=2189
-  _APPLOOKUPOBJECTREQUEST._serialized_start=2192
-  _APPLOOKUPOBJECTREQUEST._serialized_end=2376
-  _APPLOOKUPOBJECTRESPONSE._serialized_start=2378
-  _APPLOOKUPOBJECTRESPONSE._serialized_end=2457
-  _APPSETOBJECTSREQUEST._serialized_start=2460
-  _APPSETOBJECTSREQUEST._serialized_end=2763
-  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_start=2708
-  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_end=2763
-  _APPSTATS._serialized_start=2766
-  _APPSTATS._serialized_end=2975
-  _ASGI._serialized_start=2978
-  _ASGI._serialized_end=4805
-  _ASGI_HTTP._serialized_start=3798
-  _ASGI_HTTP._serialized_end=3995
-  _ASGI_HTTPREQUEST._serialized_start=3997
-  _ASGI_HTTPREQUEST._serialized_end=4043
-  _ASGI_HTTPRESPONSESTART._serialized_start=4045
-  _ASGI_HTTPRESPONSESTART._serialized_end=4115
-  _ASGI_HTTPRESPONSEBODY._serialized_start=4117
-  _ASGI_HTTPRESPONSEBODY._serialized_end=4168
-  _ASGI_HTTPRESPONSETRAILERS._serialized_start=4170
-  _ASGI_HTTPRESPONSETRAILERS._serialized_end=4232
-  _ASGI_HTTPDISCONNECT._serialized_start=4234
-  _ASGI_HTTPDISCONNECT._serialized_end=4250
-  _ASGI_WEBSOCKET._serialized_start=4253
-  _ASGI_WEBSOCKET._serialized_end=4461
-  _ASGI_WEBSOCKETCONNECT._serialized_start=4463
-  _ASGI_WEBSOCKETCONNECT._serialized_end=4481
-  _ASGI_WEBSOCKETACCEPT._serialized_start=4483
-  _ASGI_WEBSOCKETACCEPT._serialized_end=4559
-  _ASGI_WEBSOCKETRECEIVE._serialized_start=4561
-  _ASGI_WEBSOCKETRECEIVE._serialized_end=4623
-  _ASGI_WEBSOCKETSEND._serialized_start=4625
-  _ASGI_WEBSOCKETSEND._serialized_end=4684
-  _ASGI_WEBSOCKETDISCONNECT._serialized_start=4686
-  _ASGI_WEBSOCKETDISCONNECT._serialized_end=4735
-  _ASGI_WEBSOCKETCLOSE._serialized_start=4737
-  _ASGI_WEBSOCKETCLOSE._serialized_end=4797
-  _BASEIMAGE._serialized_start=4807
-  _BASEIMAGE._serialized_end=4862
-  _BLOBCREATEREQUEST._serialized_start=4864
-  _BLOBCREATEREQUEST._serialized_end=4959
-  _BLOBCREATERESPONSE._serialized_start=4962
-  _BLOBCREATERESPONSE._serialized_end=5094
-  _BLOBGETREQUEST._serialized_start=5096
-  _BLOBGETREQUEST._serialized_end=5129
-  _BLOBGETRESPONSE._serialized_start=5131
-  _BLOBGETRESPONSE._serialized_end=5170
-  _CHECKPOINTINFO._serialized_start=5172
-  _CHECKPOINTINFO._serialized_end=5277
-  _CLASSCREATEREQUEST._serialized_start=5279
-  _CLASSCREATEREQUEST._serialized_end=5392
-  _CLASSCREATERESPONSE._serialized_start=5394
-  _CLASSCREATERESPONSE._serialized_end=5493
-  _CLASSGETREQUEST._serialized_start=5496
-  _CLASSGETREQUEST._serialized_end=5681
-  _CLASSGETRESPONSE._serialized_start=5683
-  _CLASSGETRESPONSE._serialized_end=5779
-  _CLASSHANDLEMETADATA._serialized_start=5781
-  _CLASSHANDLEMETADATA._serialized_end=5846
-  _CLASSMETHOD._serialized_start=5849
-  _CLASSMETHOD._serialized_end=5978
-  _CLIENTCREATEREQUEST._serialized_start=5980
-  _CLIENTCREATEREQUEST._serialized_end=6071
-  _CLIENTCREATERESPONSE._serialized_start=6073
-  _CLIENTCREATERESPONSE._serialized_end=6158
-  _CLIENTHELLORESPONSE._serialized_start=6160
-  _CLIENTHELLORESPONSE._serialized_end=6229
-  _CLIENTHEARTBEATREQUEST._serialized_start=6231
-  _CLIENTHEARTBEATREQUEST._serialized_end=6334
-  _CONTAINERARGUMENTS._serialized_start=6337
-  _CONTAINERARGUMENTS._serialized_end=6752
-  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_start=6681
-  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_end=6734
-  _CANCELINPUTEVENT._serialized_start=6754
-  _CANCELINPUTEVENT._serialized_end=6791
-  _CONTAINERHEARTBEATRESPONSE._serialized_start=6793
-  _CONTAINERHEARTBEATRESPONSE._serialized_end=6909
-  _CONTAINERHEARTBEATREQUEST._serialized_start=6912
-  _CONTAINERHEARTBEATREQUEST._serialized_end=7045
-  _CONTAINERCHECKPOINTREQUEST._serialized_start=7047
-  _CONTAINERCHECKPOINTREQUEST._serialized_end=7098
-  _CONTAINEREXECREQUEST._serialized_start=7101
-  _CONTAINEREXECREQUEST._serialized_end=7258
-  _CONTAINEREXECGETOUTPUTREQUEST._serialized_start=7260
-  _CONTAINEREXECGETOUTPUTREQUEST._serialized_end=7351
-  _CONTAINEREXECPUTINPUTREQUEST._serialized_start=7353
-  _CONTAINEREXECPUTINPUTREQUEST._serialized_end=7450
-  _CONTAINEREXECRESPONSE._serialized_start=7452
-  _CONTAINEREXECRESPONSE._serialized_end=7492
-  _CUSTOMDOMAINCONFIG._serialized_start=7494
-  _CUSTOMDOMAINCONFIG._serialized_end=7528
-  _CUSTOMDOMAININFO._serialized_start=7530
-  _CUSTOMDOMAININFO._serialized_end=7561
-  _DATACHUNK._serialized_start=7563
-  _DATACHUNK._serialized_end=7690
-  _DICTCLEARREQUEST._serialized_start=7692
-  _DICTCLEARREQUEST._serialized_end=7727
-  _DICTCONTAINSREQUEST._serialized_start=7729
-  _DICTCONTAINSREQUEST._serialized_end=7780
-  _DICTCONTAINSRESPONSE._serialized_start=7782
-  _DICTCONTAINSRESPONSE._serialized_end=7819
-  _DICTCREATEREQUEST._serialized_start=7821
-  _DICTCREATEREQUEST._serialized_end=7927
-  _DICTCREATERESPONSE._serialized_start=7929
-  _DICTCREATERESPONSE._serialized_end=7966
-  _DICTGETORCREATEREQUEST._serialized_start=7969
-  _DICTGETORCREATEREQUEST._serialized_end=8201
-  _DICTGETORCREATERESPONSE._serialized_start=8203
-  _DICTGETORCREATERESPONSE._serialized_end=8245
-  _DICTENTRY._serialized_start=8247
-  _DICTENTRY._serialized_end=8286
-  _DICTGETREQUEST._serialized_start=8288
-  _DICTGETREQUEST._serialized_end=8334
-  _DICTGETRESPONSE._serialized_start=8336
-  _DICTGETRESPONSE._serialized_end=8398
-  _DICTHEARTBEATREQUEST._serialized_start=8400
-  _DICTHEARTBEATREQUEST._serialized_end=8439
-  _DICTCONTENTSREQUEST._serialized_start=8441
-  _DICTCONTENTSREQUEST._serialized_end=8509
-  _DICTLENREQUEST._serialized_start=8511
-  _DICTLENREQUEST._serialized_end=8544
-  _DICTLENRESPONSE._serialized_start=8546
-  _DICTLENRESPONSE._serialized_end=8576
-  _DICTPOPREQUEST._serialized_start=8578
-  _DICTPOPREQUEST._serialized_end=8624
-  _DICTPOPRESPONSE._serialized_start=8626
-  _DICTPOPRESPONSE._serialized_end=8688
-  _DICTUPDATEREQUEST._serialized_start=8690
-  _DICTUPDATEREQUEST._serialized_end=8768
-  _DICTUPDATERESPONSE._serialized_start=8770
-  _DICTUPDATERESPONSE._serialized_end=8790
-  _DNSRECORD._serialized_start=8792
-  _DNSRECORD._serialized_end=8875
-  _DOMAINCREATEREQUEST._serialized_start=8877
-  _DOMAINCREATEREQUEST._serialized_end=8925
-  _DOMAINCREATERESPONSE._serialized_start=8927
-  _DOMAINCREATERESPONSE._serialized_end=9014
-  _DOMAINLISTREQUEST._serialized_start=9016
-  _DOMAINLISTREQUEST._serialized_end=9035
-  _DOMAIN._serialized_start=9038
-  _DOMAIN._serialized_end=9213
-  _DOMAINLISTRESPONSE._serialized_start=9215
-  _DOMAINLISTRESPONSE._serialized_end=9274
-  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_start=9276
-  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_end=9327
-  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_start=9329
-  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_end=9400
-  _ENVIRONMENTCREATEREQUEST._serialized_start=9402
-  _ENVIRONMENTCREATEREQUEST._serialized_end=9442
-  _ENVIRONMENTDELETEREQUEST._serialized_start=9444
-  _ENVIRONMENTDELETEREQUEST._serialized_end=9484
-  _ENVIRONMENTLISTITEM._serialized_start=9486
-  _ENVIRONMENTLISTITEM._serialized_end=9545
-  _ENVIRONMENTLISTRESPONSE._serialized_start=9547
-  _ENVIRONMENTLISTRESPONSE._serialized_end=9622
-  _ENVIRONMENTUPDATEREQUEST._serialized_start=9625
-  _ENVIRONMENTUPDATEREQUEST._serialized_end=9767
-  _FUNCTIONCALLPUTDATAREQUEST._serialized_start=9769
-  _FUNCTIONCALLPUTDATAREQUEST._serialized_end=9869
-  _FUNCTIONCALLGETDATAREQUEST._serialized_start=9871
-  _FUNCTIONCALLGETDATAREQUEST._serialized_end=9945
-  _FUNCTION._serialized_start=9948
-  _FUNCTION._serialized_end=11856
-  _FUNCTION_DEFINITIONTYPE._serialized_start=11575
-  _FUNCTION_DEFINITIONTYPE._serialized_end=11682
-  _FUNCTION_FUNCTIONTYPE._serialized_start=11684
-  _FUNCTION_FUNCTIONTYPE._serialized_end=11786
-  _SCHEDULERPLACEMENT._serialized_start=11858
-  _SCHEDULERPLACEMENT._serialized_end=11982
-  _FUNCTIONHANDLEMETADATA._serialized_start=11985
-  _FUNCTIONHANDLEMETADATA._serialized_end=12128
-  _FUNCTIONCREATEREQUEST._serialized_start=12131
-  _FUNCTIONCREATEREQUEST._serialized_end=12290
-  _FUNCTIONOPTIONS._serialized_start=12293
-  _FUNCTIONOPTIONS._serialized_end=12876
-  _FUNCTIONPRECREATEREQUEST._serialized_start=12879
-  _FUNCTIONPRECREATEREQUEST._serialized_end=13093
-  _FUNCTIONPRECREATERESPONSE._serialized_start=13095
-  _FUNCTIONPRECREATERESPONSE._serialized_end=13206
-  _FUNCTIONBINDPARAMSREQUEST._serialized_start=13209
-  _FUNCTIONBINDPARAMSREQUEST._serialized_end=13367
-  _FUNCTIONBINDPARAMSRESPONSE._serialized_start=13369
-  _FUNCTIONBINDPARAMSRESPONSE._serialized_end=13487
-  _FUNCTIONCREATERESPONSE._serialized_start=13490
-  _FUNCTIONCREATERESPONSE._serialized_end=13705
-  _FUNCTIONGETREQUEST._serialized_start=13708
-  _FUNCTIONGETREQUEST._serialized_end=13846
-  _FUNCTIONGETRESPONSE._serialized_start=13848
-  _FUNCTIONGETRESPONSE._serialized_end=13953
-  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_start=13955
-  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_end=14048
-  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_start=14050
-  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_end=14090
-  _FUNCTIONGETINPUTSITEM._serialized_start=14093
-  _FUNCTIONGETINPUTSITEM._serialized_end=14231
-  _FUNCTIONGETINPUTSREQUEST._serialized_start=14233
-  _FUNCTIONGETINPUTSREQUEST._serialized_end=14354
-  _FUNCTIONGETINPUTSRESPONSE._serialized_start=14356
-  _FUNCTIONGETINPUTSRESPONSE._serialized_end=14471
-  _FUNCTIONGETOUTPUTSITEM._serialized_start=14474
-  _FUNCTIONGETOUTPUTSITEM._serialized_end=14640
-  _FUNCTIONGETOUTPUTSREQUEST._serialized_start=14643
-  _FUNCTIONGETOUTPUTSREQUEST._serialized_end=14782
-  _FUNCTIONGETOUTPUTSRESPONSE._serialized_start=14784
-  _FUNCTIONGETOUTPUTSRESPONSE._serialized_end=14904
-  _FUNCTIONGETSERIALIZEDREQUEST._serialized_start=14906
-  _FUNCTIONGETSERIALIZEDREQUEST._serialized_end=14957
-  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_start=14959
-  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_end=15045
-  _FUNCTIONINPUT._serialized_start=15048
-  _FUNCTIONINPUT._serialized_end=15185
-  _FUNCTIONMAPREQUEST._serialized_start=15188
-  _FUNCTIONMAPREQUEST._serialized_end=15404
-  _FUNCTIONMAPRESPONSE._serialized_start=15406
-  _FUNCTIONMAPRESPONSE._serialized_end=15524
-  _FUNCTIONPUTINPUTSITEM._serialized_start=15526
-  _FUNCTIONPUTINPUTSITEM._serialized_end=15606
-  _FUNCTIONPUTINPUTSREQUEST._serialized_start=15608
-  _FUNCTIONPUTINPUTSREQUEST._serialized_end=15734
-  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_start=15736
-  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_end=15798
-  _FUNCTIONPUTINPUTSRESPONSE._serialized_start=15800
-  _FUNCTIONPUTINPUTSRESPONSE._serialized_end=15888
-  _FUNCTIONPUTOUTPUTSITEM._serialized_start=15891
-  _FUNCTIONPUTOUTPUTSITEM._serialized_end=16097
-  _FUNCTIONPUTOUTPUTSREQUEST._serialized_start=16099
-  _FUNCTIONPUTOUTPUTSREQUEST._serialized_end=16181
-  _FUNCTIONRETRYPOLICY._serialized_start=16183
-  _FUNCTIONRETRYPOLICY._serialized_end=16298
-  _FUNCTIONGETCALLGRAPHREQUEST._serialized_start=16300
-  _FUNCTIONGETCALLGRAPHREQUEST._serialized_end=16355
-  _INPUTCALLGRAPHINFO._serialized_start=16358
-  _INPUTCALLGRAPHINFO._serialized_end=16498
-  _FUNCTIONCALLCALLGRAPHINFO._serialized_start=16500
-  _FUNCTIONCALLCALLGRAPHINFO._serialized_end=16622
-  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_start=16625
-  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_end=16770
-  _FUNCTIONCALLCANCELREQUEST._serialized_start=16772
-  _FUNCTIONCALLCANCELREQUEST._serialized_end=16825
-  _FUNCTIONCALLLISTREQUEST._serialized_start=16827
-  _FUNCTIONCALLLISTREQUEST._serialized_end=16873
-  _FUNCTIONCALLINFO._serialized_start=16876
-  _FUNCTIONCALLINFO._serialized_end=17327
-  _FUNCTIONCALLLISTRESPONSE._serialized_start=17329
-  _FUNCTIONCALLLISTRESPONSE._serialized_end=17411
-  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_start=17413
-  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_end=17466
-  _FUNCTIONSTATS._serialized_start=17468
-  _FUNCTIONSTATS._serialized_end=17551
-  _GENERATORDONE._serialized_start=17553
-  _GENERATORDONE._serialized_end=17589
-  _GENERICRESULT._serialized_start=17592
-  _GENERICRESULT._serialized_end=18230
-  _GENERICRESULT_GENERICSTATUS._serialized_start=17904
-  _GENERICRESULT_GENERICSTATUS._serialized_end=18099
-  _GENERICRESULT_GENERATORSTATUS._serialized_start=18101
-  _GENERICRESULT_GENERATORSTATUS._serialized_end=18216
-  _GPUCONFIG._serialized_start=18232
-  _GPUCONFIG._serialized_end=18311
-  _BUILDFUNCTION._serialized_start=18313
-  _BUILDFUNCTION._serialized_end=18409
-  _IMAGE._serialized_start=18412
-  _IMAGE._serialized_end=18889
-  _IMAGECONTEXTFILE._serialized_start=18891
-  _IMAGECONTEXTFILE._serialized_end=18941
-  _IMAGEGETORCREATEREQUEST._serialized_start=18944
-  _IMAGEGETORCREATEREQUEST._serialized_end=19181
-  _IMAGEGETORCREATERESPONSE._serialized_start=19183
-  _IMAGEGETORCREATERESPONSE._serialized_end=19227
-  _IMAGEJOINSTREAMINGREQUEST._serialized_start=19229
-  _IMAGEJOINSTREAMINGREQUEST._serialized_end=19314
-  _IMAGEJOINSTREAMINGRESPONSE._serialized_start=19317
-  _IMAGEJOINSTREAMINGRESPONSE._serialized_end=19464
-  _IMAGEREGISTRYCONFIG._serialized_start=19466
-  _IMAGEREGISTRYCONFIG._serialized_end=19566
-  _INPUTINFO._serialized_start=19569
-  _INPUTINFO._serialized_end=19722
-  _INPUTCATEGORYINFO._serialized_start=19724
-  _INPUTCATEGORYINFO._serialized_end=19799
-  _MOUNTBUILDREQUEST._serialized_start=19801
-  _MOUNTBUILDREQUEST._serialized_end=19909
-  _MOUNTBUILDRESPONSE._serialized_start=19911
-  _MOUNTBUILDRESPONSE._serialized_end=20009
-  _MOUNTGETORCREATEREQUEST._serialized_start=20012
-  _MOUNTGETORCREATEREQUEST._serialized_end=20262
-  _MOUNTGETORCREATERESPONSE._serialized_start=20264
-  _MOUNTGETORCREATERESPONSE._serialized_end=20368
-  _MOUNTHANDLEMETADATA._serialized_start=20370
-  _MOUNTHANDLEMETADATA._serialized_end=20428
-  _MOUNTFILE._serialized_start=20430
-  _MOUNTFILE._serialized_end=20535
-  _MOUNTPUTFILEREQUEST._serialized_start=20537
-  _MOUNTPUTFILEREQUEST._serialized_end=20632
-  _MOUNTPUTFILERESPONSE._serialized_start=20634
-  _MOUNTPUTFILERESPONSE._serialized_end=20672
-  _MULTIPARTUPLOAD._serialized_start=20674
-  _MULTIPARTUPLOAD._serialized_end=20757
-  _OBJECT._serialized_start=20760
-  _OBJECT._serialized_end=21094
-  _OBJECTDEPENDENCY._serialized_start=21096
-  _OBJECTDEPENDENCY._serialized_end=21133
-  _PROXYGETORCREATEREQUEST._serialized_start=21136
-  _PROXYGETORCREATEREQUEST._serialized_end=21330
-  _PROXYGETORCREATERESPONSE._serialized_start=21332
-  _PROXYGETORCREATERESPONSE._serialized_end=21376
-  _PROXYINFO._serialized_start=21378
-  _PROXYINFO._serialized_end=21470
-  _PTYINFO._serialized_start=21473
-  _PTYINFO._serialized_end=21735
-  _PTYINFO_PTYTYPE._serialized_start=21657
-  _PTYINFO_PTYTYPE._serialized_end=21735
-  _QUEUECREATEREQUEST._serialized_start=21737
-  _QUEUECREATEREQUEST._serialized_end=21800
-  _QUEUECREATERESPONSE._serialized_start=21802
-  _QUEUECREATERESPONSE._serialized_end=21841
-  _QUEUEGETORCREATEREQUEST._serialized_start=21844
-  _QUEUEGETORCREATEREQUEST._serialized_end=22038
-  _QUEUEGETORCREATERESPONSE._serialized_start=22040
-  _QUEUEGETORCREATERESPONSE._serialized_end=22084
-  _QUEUEGETREQUEST._serialized_start=22086
-  _QUEUEGETREQUEST._serialized_end=22179
-  _QUEUEGETRESPONSE._serialized_start=22181
-  _QUEUEGETRESPONSE._serialized_end=22215
-  _QUEUEHEARTBEATREQUEST._serialized_start=22217
-  _QUEUEHEARTBEATREQUEST._serialized_end=22258
-  _QUEUEPUTREQUEST._serialized_start=22260
-  _QUEUEPUTREQUEST._serialized_end=22365
-  _QUEUELENREQUEST._serialized_start=22367
-  _QUEUELENREQUEST._serialized_end=22425
-  _QUEUELENRESPONSE._serialized_start=22427
-  _QUEUELENRESPONSE._serialized_end=22458
-  _QUEUENEXTITEMSREQUEST._serialized_start=22460
-  _QUEUENEXTITEMSREQUEST._serialized_end=22574
-  _QUEUEITEM._serialized_start=22576
-  _QUEUEITEM._serialized_end=22620
-  _QUEUENEXTITEMSRESPONSE._serialized_start=22622
-  _QUEUENEXTITEMSRESPONSE._serialized_end=22686
-  _RATELIMIT._serialized_start=22688
-  _RATELIMIT._serialized_end=22765
-  _RESOURCES._serialized_start=22767
-  _RESOURCES._serialized_end=22884
-  _S3MOUNT._serialized_start=22886
-  _S3MOUNT._serialized_end=22986
-  _SANDBOX._serialized_start=22989
-  _SANDBOX._serialized_end=23526
-  _SANDBOXCREATEREQUEST._serialized_start=23528
-  _SANDBOXCREATEREQUEST._serialized_end=23615
-  _SANDBOXCREATERESPONSE._serialized_start=23617
-  _SANDBOXCREATERESPONSE._serialized_end=23660
-  _SANDBOXGETTASKIDREQUEST._serialized_start=23662
-  _SANDBOXGETTASKIDREQUEST._serialized_end=23707
-  _SANDBOXGETTASKIDRESPONSE._serialized_start=23709
-  _SANDBOXGETTASKIDRESPONSE._serialized_end=23752
-  _SANDBOXGETLOGSREQUEST._serialized_start=23755
-  _SANDBOXGETLOGSREQUEST._serialized_end=23893
-  _SANDBOXSTDINWRITEREQUEST._serialized_start=23895
-  _SANDBOXSTDINWRITEREQUEST._serialized_end=23984
-  _SANDBOXSTDINWRITERESPONSE._serialized_start=23986
-  _SANDBOXSTDINWRITERESPONSE._serialized_end=24013
-  _SANDBOXHANDLEMETADATA._serialized_start=24015
-  _SANDBOXHANDLEMETADATA._serialized_end=24083
-  _SANDBOXINFO._serialized_start=24086
-  _SANDBOXINFO._serialized_end=24217
-  _SANDBOXLISTREQUEST._serialized_start=24219
-  _SANDBOXLISTREQUEST._serialized_end=24281
-  _SANDBOXLISTRESPONSE._serialized_start=24283
-  _SANDBOXLISTRESPONSE._serialized_end=24350
-  _SANDBOXTERMINATEREQUEST._serialized_start=24352
-  _SANDBOXTERMINATEREQUEST._serialized_end=24397
-  _SANDBOXTERMINATERESPONSE._serialized_start=24399
-  _SANDBOXTERMINATERESPONSE._serialized_end=24479
-  _SANDBOXWAITREQUEST._serialized_start=24481
-  _SANDBOXWAITREQUEST._serialized_end=24538
-  _SANDBOXWAITRESPONSE._serialized_start=24540
-  _SANDBOXWAITRESPONSE._serialized_end=24606
-  _SCHEDULE._serialized_start=24609
-  _SCHEDULE._serialized_end=24879
-  _SCHEDULE_CRON._serialized_start=24715
-  _SCHEDULE_CRON._serialized_end=24742
-  _SCHEDULE_PERIOD._serialized_start=24744
-  _SCHEDULE_PERIOD._serialized_end=24861
-  _SECRETCREATEREQUEST._serialized_start=24882
-  _SECRETCREATEREQUEST._serialized_end=25090
-  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_start=25044
-  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_end=25090
-  _SECRETCREATERESPONSE._serialized_start=25092
-  _SECRETCREATERESPONSE._serialized_end=25133
-  _SECRETGETORCREATEREQUEST._serialized_start=25136
-  _SECRETGETORCREATEREQUEST._serialized_end=25466
-  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_start=25044
-  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_end=25090
-  _SECRETGETORCREATERESPONSE._serialized_start=25468
-  _SECRETGETORCREATERESPONSE._serialized_end=25514
-  _SECRETLISTITEM._serialized_start=25516
-  _SECRETLISTITEM._serialized_end=25615
-  _SECRETLISTREQUEST._serialized_start=25617
-  _SECRETLISTREQUEST._serialized_end=25662
-  _SECRETLISTRESPONSE._serialized_start=25664
-  _SECRETLISTRESPONSE._serialized_end=25755
-  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_start=25758
-  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_end=25975
-  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_start=25977
-  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_end=26036
-  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_start=26038
-  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_end=26094
-  _SHAREDVOLUMECREATEREQUEST._serialized_start=26096
-  _SHAREDVOLUMECREATEREQUEST._serialized_end=26198
-  _SHAREDVOLUMECREATERESPONSE._serialized_start=26200
-  _SHAREDVOLUMECREATERESPONSE._serialized_end=26254
-  _SHAREDVOLUMELISTITEM._serialized_start=26257
-  _SHAREDVOLUMELISTITEM._serialized_end=26393
-  _SHAREDVOLUMELISTREQUEST._serialized_start=26395
-  _SHAREDVOLUMELISTREQUEST._serialized_end=26446
-  _SHAREDVOLUMELISTRESPONSE._serialized_start=26448
-  _SHAREDVOLUMELISTRESPONSE._serialized_end=26551
-  _SHAREDVOLUMELISTFILESREQUEST._serialized_start=26553
-  _SHAREDVOLUMELISTFILESREQUEST._serialized_end=26623
-  _SHAREDVOLUMEPUTFILEREQUEST._serialized_start=26626
-  _SHAREDVOLUMEPUTFILEREQUEST._serialized_end=26793
-  _SHAREDVOLUMEPUTFILERESPONSE._serialized_start=26795
-  _SHAREDVOLUMEPUTFILERESPONSE._serialized_end=26840
-  _SHAREDVOLUMEGETFILEREQUEST._serialized_start=26842
-  _SHAREDVOLUMEGETFILEREQUEST._serialized_end=26910
-  _SHAREDVOLUMEGETFILERESPONSE._serialized_start=26912
-  _SHAREDVOLUMEGETFILERESPONSE._serialized_end=26995
-  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_start=26997
-  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_end=27093
-  _SHAREDVOLUMELISTFILESRESPONSE._serialized_start=27095
-  _SHAREDVOLUMELISTFILESRESPONSE._serialized_end=27168
-  _SHAREDVOLUMEMOUNT._serialized_start=27171
-  _SHAREDVOLUMEMOUNT._serialized_end=27317
-  _TASKCURRENTINPUTSRESPONSE._serialized_start=27319
-  _TASKCURRENTINPUTSRESPONSE._serialized_end=27365
-  _TASKINFO._serialized_start=27367
-  _TASKINFO._serialized_end=27475
-  _TASKLOGS._serialized_start=27478
-  _TASKLOGS._serialized_end=27716
-  _TASKLISTREQUEST._serialized_start=27718
-  _TASKLISTREQUEST._serialized_end=27735
-  _TASKLISTRESPONSE._serialized_start=27737
-  _TASKLISTRESPONSE._serialized_end=27795
-  _TASKLOGSBATCH._serialized_start=27798
-  _TASKLOGSBATCH._serialized_end=27996
-  _TASKPROGRESS._serialized_start=27998
-  _TASKPROGRESS._serialized_end=28110
-  _TASKRESULTREQUEST._serialized_start=28112
-  _TASKRESULTREQUEST._serialized_end=28176
-  _TASKSTATS._serialized_start=28178
-  _TASKSTATS._serialized_end=28267
-  _TOKENFLOWCREATEREQUEST._serialized_start=28269
-  _TOKENFLOWCREATEREQUEST._serialized_end=28355
-  _TOKENFLOWCREATERESPONSE._serialized_start=28357
-  _TOKENFLOWCREATERESPONSE._serialized_end=28457
-  _TOKENFLOWWAITREQUEST._serialized_start=28459
-  _TOKENFLOWWAITREQUEST._serialized_end=28542
-  _TOKENFLOWWAITRESPONSE._serialized_start=28544
-  _TOKENFLOWWAITRESPONSE._serialized_end=28652
-  _TUNNELSTARTREQUEST._serialized_start=28654
-  _TUNNELSTARTREQUEST._serialized_end=28709
-  _TUNNELSTARTRESPONSE._serialized_start=28712
-  _TUNNELSTARTRESPONSE._serialized_end=28865
-  _TUNNELSTOPREQUEST._serialized_start=28867
-  _TUNNELSTOPREQUEST._serialized_end=28900
-  _TUNNELSTOPRESPONSE._serialized_start=28902
-  _TUNNELSTOPRESPONSE._serialized_end=28938
-  _VOLUMEGETORCREATEREQUEST._serialized_start=28941
-  _VOLUMEGETORCREATEREQUEST._serialized_end=29152
-  _VOLUMEGETORCREATERESPONSE._serialized_start=29154
-  _VOLUMEGETORCREATERESPONSE._serialized_end=29200
-  _VOLUMEHEARTBEATREQUEST._serialized_start=29202
-  _VOLUMEHEARTBEATREQUEST._serialized_end=29245
-  _VOLUMECREATEREQUEST._serialized_start=29247
-  _VOLUMECREATEREQUEST._serialized_end=29290
-  _VOLUMECREATERESPONSE._serialized_start=29292
-  _VOLUMECREATERESPONSE._serialized_end=29333
-  _VOLUMECOMMITREQUEST._serialized_start=29335
-  _VOLUMECOMMITREQUEST._serialized_end=29381
-  _VOLUMECOMMITRESPONSE._serialized_start=29383
-  _VOLUMECOMMITRESPONSE._serialized_end=29426
-  _VOLUMEDELETEREQUEST._serialized_start=29428
-  _VOLUMEDELETEREQUEST._serialized_end=29498
-  _VOLUMEGETFILEREQUEST._serialized_start=29500
-  _VOLUMEGETFILEREQUEST._serialized_end=29583
-  _VOLUMEGETFILERESPONSE._serialized_start=29585
-  _VOLUMEGETFILERESPONSE._serialized_end=29704
-  _VOLUMELISTFILESREQUEST._serialized_start=29706
-  _VOLUMELISTFILESREQUEST._serialized_end=29805
-  _VOLUMELISTFILESRESPONSE._serialized_start=29807
-  _VOLUMELISTFILESRESPONSE._serialized_end=29874
-  _VOLUMELISTITEM._serialized_start=29876
-  _VOLUMELISTITEM._serialized_end=29946
-  _VOLUMELISTREQUEST._serialized_start=29948
-  _VOLUMELISTREQUEST._serialized_end=29993
-  _VOLUMELISTRESPONSE._serialized_start=29995
-  _VOLUMELISTRESPONSE._serialized_end=30086
-  _VOLUMERELOADREQUEST._serialized_start=30088
-  _VOLUMERELOADREQUEST._serialized_end=30128
-  _VOLUMEPUTFILESREQUEST._serialized_start=30130
-  _VOLUMEPUTFILESREQUEST._serialized_end=30255
-  _VOLUMEREMOVEFILEREQUEST._serialized_start=30257
-  _VOLUMEREMOVEFILEREQUEST._serialized_end=30340
-  _VOLUMECOPYFILESREQUEST._serialized_start=30342
-  _VOLUMECOPYFILESREQUEST._serialized_end=30441
-  _VOLUMEMOUNT._serialized_start=30443
-  _VOLUMEMOUNT._serialized_end=30529
-  _WEBHOOKCONFIG._serialized_start=30532
-  _WEBHOOKCONFIG._serialized_end=30801
-  _WEBURLINFO._serialized_start=30803
-  _WEBURLINFO._serialized_end=30881
-  _WORKSPACENAMELOOKUPRESPONSE._serialized_start=30883
-  _WORKSPACENAMELOOKUPRESPONSE._serialized_end=30958
-  _RUNTIMEOUTPUTMESSAGE._serialized_start=30960
-  _RUNTIMEOUTPUTMESSAGE._serialized_end=31054
-  _RUNTIMEOUTPUTBATCH._serialized_start=31057
-  _RUNTIMEOUTPUTBATCH._serialized_end=31187
-  _RUNTIMEINPUTMESSAGE._serialized_start=31189
-  _RUNTIMEINPUTMESSAGE._serialized_end=31250
-  _MODALCLIENT._serialized_start=34904
-  _MODALCLIENT._serialized_end=44992
+  _CLOUDBUCKETMOUNT._serialized_end=364
+  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_start=327
+  _CLOUDBUCKETMOUNT_BUCKETTYPE._serialized_end=364
+  _APPCLIENTDISCONNECTREQUEST._serialized_start=366
+  _APPCLIENTDISCONNECTREQUEST._serialized_end=480
+  _APPCREATEREQUEST._serialized_start=483
+  _APPCREATEREQUEST._serialized_end=662
+  _APPCREATERESPONSE._serialized_start=664
+  _APPCREATERESPONSE._serialized_end=721
+  _APPSTOPREQUEST._serialized_start=723
+  _APPSTOPREQUEST._serialized_end=806
+  _APPDEPLOYREQUEST._serialized_start=809
+  _APPDEPLOYREQUEST._serialized_end=995
+  _APPDEPLOYRESPONSE._serialized_start=997
+  _APPDEPLOYRESPONSE._serialized_end=1029
+  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_start=1032
+  _APPDEPLOYSINGLEOBJECTREQUEST._serialized_end=1175
+  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_start=1177
+  _APPDEPLOYSINGLEOBJECTRESPONSE._serialized_end=1224
+  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_start=1226
+  _APPGETBYDEPLOYMENTNAMEREQUEST._serialized_end=1351
+  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_start=1353
+  _APPGETBYDEPLOYMENTNAMERESPONSE._serialized_end=1401
+  _APPGETLOGSREQUEST._serialized_start=1404
+  _APPGETLOGSREQUEST._serialized_end=1590
+  _APPGETOBJECTSREQUEST._serialized_start=1592
+  _APPGETOBJECTSREQUEST._serialized_end=1657
+  _APPGETOBJECTSITEM._serialized_start=1659
+  _APPGETOBJECTSITEM._serialized_end=1729
+  _APPGETOBJECTSRESPONSE._serialized_start=1731
+  _APPGETOBJECTSRESPONSE._serialized_end=1802
+  _APPHEARTBEATREQUEST._serialized_start=1804
+  _APPHEARTBEATREQUEST._serialized_end=1841
+  _APPLISTREQUEST._serialized_start=1843
+  _APPLISTREQUEST._serialized_end=1885
+  _APPLISTRESPONSE._serialized_start=1887
+  _APPLISTRESPONSE._serialized_end=1942
+  _APPLOOKUPOBJECTREQUEST._serialized_start=1945
+  _APPLOOKUPOBJECTREQUEST._serialized_end=2129
+  _APPLOOKUPOBJECTRESPONSE._serialized_start=2131
+  _APPLOOKUPOBJECTRESPONSE._serialized_end=2210
+  _APPSETOBJECTSREQUEST._serialized_start=2213
+  _APPSETOBJECTSREQUEST._serialized_end=2516
+  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_start=2461
+  _APPSETOBJECTSREQUEST_INDEXEDOBJECTIDSENTRY._serialized_end=2516
+  _APPSTATS._serialized_start=2519
+  _APPSTATS._serialized_end=2728
+  _ASGI._serialized_start=2731
+  _ASGI._serialized_end=4558
+  _ASGI_HTTP._serialized_start=3551
+  _ASGI_HTTP._serialized_end=3748
+  _ASGI_HTTPREQUEST._serialized_start=3750
+  _ASGI_HTTPREQUEST._serialized_end=3796
+  _ASGI_HTTPRESPONSESTART._serialized_start=3798
+  _ASGI_HTTPRESPONSESTART._serialized_end=3868
+  _ASGI_HTTPRESPONSEBODY._serialized_start=3870
+  _ASGI_HTTPRESPONSEBODY._serialized_end=3921
+  _ASGI_HTTPRESPONSETRAILERS._serialized_start=3923
+  _ASGI_HTTPRESPONSETRAILERS._serialized_end=3985
+  _ASGI_HTTPDISCONNECT._serialized_start=3987
+  _ASGI_HTTPDISCONNECT._serialized_end=4003
+  _ASGI_WEBSOCKET._serialized_start=4006
+  _ASGI_WEBSOCKET._serialized_end=4214
+  _ASGI_WEBSOCKETCONNECT._serialized_start=4216
+  _ASGI_WEBSOCKETCONNECT._serialized_end=4234
+  _ASGI_WEBSOCKETACCEPT._serialized_start=4236
+  _ASGI_WEBSOCKETACCEPT._serialized_end=4312
+  _ASGI_WEBSOCKETRECEIVE._serialized_start=4314
+  _ASGI_WEBSOCKETRECEIVE._serialized_end=4376
+  _ASGI_WEBSOCKETSEND._serialized_start=4378
+  _ASGI_WEBSOCKETSEND._serialized_end=4437
+  _ASGI_WEBSOCKETDISCONNECT._serialized_start=4439
+  _ASGI_WEBSOCKETDISCONNECT._serialized_end=4488
+  _ASGI_WEBSOCKETCLOSE._serialized_start=4490
+  _ASGI_WEBSOCKETCLOSE._serialized_end=4550
+  _BASEIMAGE._serialized_start=4560
+  _BASEIMAGE._serialized_end=4615
+  _BLOBCREATEREQUEST._serialized_start=4617
+  _BLOBCREATEREQUEST._serialized_end=4712
+  _BLOBCREATERESPONSE._serialized_start=4715
+  _BLOBCREATERESPONSE._serialized_end=4847
+  _BLOBGETREQUEST._serialized_start=4849
+  _BLOBGETREQUEST._serialized_end=4882
+  _BLOBGETRESPONSE._serialized_start=4884
+  _BLOBGETRESPONSE._serialized_end=4923
+  _CHECKPOINTINFO._serialized_start=4925
+  _CHECKPOINTINFO._serialized_end=5030
+  _CLASSCREATEREQUEST._serialized_start=5032
+  _CLASSCREATEREQUEST._serialized_end=5145
+  _CLASSCREATERESPONSE._serialized_start=5147
+  _CLASSCREATERESPONSE._serialized_end=5246
+  _CLASSGETREQUEST._serialized_start=5249
+  _CLASSGETREQUEST._serialized_end=5434
+  _CLASSGETRESPONSE._serialized_start=5436
+  _CLASSGETRESPONSE._serialized_end=5532
+  _CLASSHANDLEMETADATA._serialized_start=5534
+  _CLASSHANDLEMETADATA._serialized_end=5599
+  _CLASSMETHOD._serialized_start=5602
+  _CLASSMETHOD._serialized_end=5731
+  _CLIENTCREATEREQUEST._serialized_start=5733
+  _CLIENTCREATEREQUEST._serialized_end=5824
+  _CLIENTCREATERESPONSE._serialized_start=5826
+  _CLIENTCREATERESPONSE._serialized_end=5911
+  _CLIENTHELLORESPONSE._serialized_start=5913
+  _CLIENTHELLORESPONSE._serialized_end=5982
+  _CLIENTHEARTBEATREQUEST._serialized_start=5984
+  _CLIENTHEARTBEATREQUEST._serialized_end=6087
+  _CONTAINERARGUMENTS._serialized_start=6090
+  _CONTAINERARGUMENTS._serialized_end=6505
+  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_start=6434
+  _CONTAINERARGUMENTS_TRACINGCONTEXTENTRY._serialized_end=6487
+  _CANCELINPUTEVENT._serialized_start=6507
+  _CANCELINPUTEVENT._serialized_end=6544
+  _CONTAINERHEARTBEATRESPONSE._serialized_start=6546
+  _CONTAINERHEARTBEATRESPONSE._serialized_end=6662
+  _CONTAINERHEARTBEATREQUEST._serialized_start=6665
+  _CONTAINERHEARTBEATREQUEST._serialized_end=6798
+  _CONTAINERCHECKPOINTREQUEST._serialized_start=6800
+  _CONTAINERCHECKPOINTREQUEST._serialized_end=6851
+  _CONTAINEREXECREQUEST._serialized_start=6854
+  _CONTAINEREXECREQUEST._serialized_end=7011
+  _CONTAINEREXECGETOUTPUTREQUEST._serialized_start=7013
+  _CONTAINEREXECGETOUTPUTREQUEST._serialized_end=7104
+  _CONTAINEREXECPUTINPUTREQUEST._serialized_start=7106
+  _CONTAINEREXECPUTINPUTREQUEST._serialized_end=7203
+  _CONTAINEREXECRESPONSE._serialized_start=7205
+  _CONTAINEREXECRESPONSE._serialized_end=7245
+  _CUSTOMDOMAINCONFIG._serialized_start=7247
+  _CUSTOMDOMAINCONFIG._serialized_end=7281
+  _CUSTOMDOMAININFO._serialized_start=7283
+  _CUSTOMDOMAININFO._serialized_end=7314
+  _DATACHUNK._serialized_start=7316
+  _DATACHUNK._serialized_end=7443
+  _DICTCLEARREQUEST._serialized_start=7445
+  _DICTCLEARREQUEST._serialized_end=7480
+  _DICTCONTAINSREQUEST._serialized_start=7482
+  _DICTCONTAINSREQUEST._serialized_end=7533
+  _DICTCONTAINSRESPONSE._serialized_start=7535
+  _DICTCONTAINSRESPONSE._serialized_end=7572
+  _DICTCREATEREQUEST._serialized_start=7574
+  _DICTCREATEREQUEST._serialized_end=7680
+  _DICTCREATERESPONSE._serialized_start=7682
+  _DICTCREATERESPONSE._serialized_end=7719
+  _DICTGETORCREATEREQUEST._serialized_start=7722
+  _DICTGETORCREATEREQUEST._serialized_end=7954
+  _DICTGETORCREATERESPONSE._serialized_start=7956
+  _DICTGETORCREATERESPONSE._serialized_end=7998
+  _DICTENTRY._serialized_start=8000
+  _DICTENTRY._serialized_end=8039
+  _DICTGETREQUEST._serialized_start=8041
+  _DICTGETREQUEST._serialized_end=8087
+  _DICTGETRESPONSE._serialized_start=8089
+  _DICTGETRESPONSE._serialized_end=8151
+  _DICTHEARTBEATREQUEST._serialized_start=8153
+  _DICTHEARTBEATREQUEST._serialized_end=8192
+  _DICTLENREQUEST._serialized_start=8194
+  _DICTLENREQUEST._serialized_end=8227
+  _DICTLENRESPONSE._serialized_start=8229
+  _DICTLENRESPONSE._serialized_end=8259
+  _DICTPOPREQUEST._serialized_start=8261
+  _DICTPOPREQUEST._serialized_end=8307
+  _DICTPOPRESPONSE._serialized_start=8309
+  _DICTPOPRESPONSE._serialized_end=8371
+  _DICTUPDATEREQUEST._serialized_start=8373
+  _DICTUPDATEREQUEST._serialized_end=8451
+  _DICTUPDATERESPONSE._serialized_start=8453
+  _DICTUPDATERESPONSE._serialized_end=8473
+  _DNSRECORD._serialized_start=8475
+  _DNSRECORD._serialized_end=8558
+  _DOMAINCREATEREQUEST._serialized_start=8560
+  _DOMAINCREATEREQUEST._serialized_end=8608
+  _DOMAINCREATERESPONSE._serialized_start=8610
+  _DOMAINCREATERESPONSE._serialized_end=8697
+  _DOMAINLISTREQUEST._serialized_start=8699
+  _DOMAINLISTREQUEST._serialized_end=8718
+  _DOMAIN._serialized_start=8721
+  _DOMAIN._serialized_end=8896
+  _DOMAINLISTRESPONSE._serialized_start=8898
+  _DOMAINLISTRESPONSE._serialized_end=8957
+  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_start=8959
+  _DOMAINCERTIFICATEVERIFYREQUEST._serialized_end=9010
+  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_start=9012
+  _DOMAINCERTIFICATEVERIFYRESPONSE._serialized_end=9083
+  _ENVIRONMENTCREATEREQUEST._serialized_start=9085
+  _ENVIRONMENTCREATEREQUEST._serialized_end=9125
+  _ENVIRONMENTDELETEREQUEST._serialized_start=9127
+  _ENVIRONMENTDELETEREQUEST._serialized_end=9167
+  _ENVIRONMENTLISTITEM._serialized_start=9169
+  _ENVIRONMENTLISTITEM._serialized_end=9228
+  _ENVIRONMENTLISTRESPONSE._serialized_start=9230
+  _ENVIRONMENTLISTRESPONSE._serialized_end=9305
+  _ENVIRONMENTUPDATEREQUEST._serialized_start=9308
+  _ENVIRONMENTUPDATEREQUEST._serialized_end=9450
+  _FUNCTIONCALLPUTDATAREQUEST._serialized_start=9452
+  _FUNCTIONCALLPUTDATAREQUEST._serialized_end=9552
+  _FUNCTIONCALLGETDATAREQUEST._serialized_start=9554
+  _FUNCTIONCALLGETDATAREQUEST._serialized_end=9628
+  _FUNCTION._serialized_start=9631
+  _FUNCTION._serialized_end=11539
+  _FUNCTION_DEFINITIONTYPE._serialized_start=11258
+  _FUNCTION_DEFINITIONTYPE._serialized_end=11365
+  _FUNCTION_FUNCTIONTYPE._serialized_start=11367
+  _FUNCTION_FUNCTIONTYPE._serialized_end=11469
+  _SCHEDULERPLACEMENT._serialized_start=11541
+  _SCHEDULERPLACEMENT._serialized_end=11665
+  _FUNCTIONHANDLEMETADATA._serialized_start=11668
+  _FUNCTIONHANDLEMETADATA._serialized_end=11811
+  _FUNCTIONCREATEREQUEST._serialized_start=11814
+  _FUNCTIONCREATEREQUEST._serialized_end=11973
+  _FUNCTIONOPTIONS._serialized_start=11976
+  _FUNCTIONOPTIONS._serialized_end=12559
+  _FUNCTIONPRECREATEREQUEST._serialized_start=12562
+  _FUNCTIONPRECREATEREQUEST._serialized_end=12776
+  _FUNCTIONPRECREATERESPONSE._serialized_start=12778
+  _FUNCTIONPRECREATERESPONSE._serialized_end=12889
+  _FUNCTIONBINDPARAMSREQUEST._serialized_start=12892
+  _FUNCTIONBINDPARAMSREQUEST._serialized_end=13050
+  _FUNCTIONBINDPARAMSRESPONSE._serialized_start=13052
+  _FUNCTIONBINDPARAMSRESPONSE._serialized_end=13170
+  _FUNCTIONCREATERESPONSE._serialized_start=13173
+  _FUNCTIONCREATERESPONSE._serialized_end=13388
+  _FUNCTIONGETREQUEST._serialized_start=13391
+  _FUNCTIONGETREQUEST._serialized_end=13529
+  _FUNCTIONGETRESPONSE._serialized_start=13531
+  _FUNCTIONGETRESPONSE._serialized_end=13636
+  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_start=13638
+  _FUNCTIONUPDATESCHEDULINGPARAMSREQUEST._serialized_end=13731
+  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_start=13733
+  _FUNCTIONUPDATESCHEDULINGPARAMSRESPONSE._serialized_end=13773
+  _FUNCTIONGETINPUTSITEM._serialized_start=13776
+  _FUNCTIONGETINPUTSITEM._serialized_end=13914
+  _FUNCTIONGETINPUTSREQUEST._serialized_start=13916
+  _FUNCTIONGETINPUTSREQUEST._serialized_end=14037
+  _FUNCTIONGETINPUTSRESPONSE._serialized_start=14039
+  _FUNCTIONGETINPUTSRESPONSE._serialized_end=14154
+  _FUNCTIONGETOUTPUTSITEM._serialized_start=14157
+  _FUNCTIONGETOUTPUTSITEM._serialized_end=14323
+  _FUNCTIONGETOUTPUTSREQUEST._serialized_start=14326
+  _FUNCTIONGETOUTPUTSREQUEST._serialized_end=14465
+  _FUNCTIONGETOUTPUTSRESPONSE._serialized_start=14467
+  _FUNCTIONGETOUTPUTSRESPONSE._serialized_end=14587
+  _FUNCTIONGETSERIALIZEDREQUEST._serialized_start=14589
+  _FUNCTIONGETSERIALIZEDREQUEST._serialized_end=14640
+  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_start=14642
+  _FUNCTIONGETSERIALIZEDRESPONSE._serialized_end=14728
+  _FUNCTIONINPUT._serialized_start=14731
+  _FUNCTIONINPUT._serialized_end=14868
+  _FUNCTIONMAPREQUEST._serialized_start=14871
+  _FUNCTIONMAPREQUEST._serialized_end=15087
+  _FUNCTIONMAPRESPONSE._serialized_start=15089
+  _FUNCTIONMAPRESPONSE._serialized_end=15207
+  _FUNCTIONPUTINPUTSITEM._serialized_start=15209
+  _FUNCTIONPUTINPUTSITEM._serialized_end=15289
+  _FUNCTIONPUTINPUTSREQUEST._serialized_start=15291
+  _FUNCTIONPUTINPUTSREQUEST._serialized_end=15417
+  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_start=15419
+  _FUNCTIONPUTINPUTSRESPONSEITEM._serialized_end=15481
+  _FUNCTIONPUTINPUTSRESPONSE._serialized_start=15483
+  _FUNCTIONPUTINPUTSRESPONSE._serialized_end=15571
+  _FUNCTIONPUTOUTPUTSITEM._serialized_start=15574
+  _FUNCTIONPUTOUTPUTSITEM._serialized_end=15780
+  _FUNCTIONPUTOUTPUTSREQUEST._serialized_start=15782
+  _FUNCTIONPUTOUTPUTSREQUEST._serialized_end=15864
+  _FUNCTIONRETRYPOLICY._serialized_start=15866
+  _FUNCTIONRETRYPOLICY._serialized_end=15981
+  _FUNCTIONGETCALLGRAPHREQUEST._serialized_start=15983
+  _FUNCTIONGETCALLGRAPHREQUEST._serialized_end=16038
+  _INPUTCALLGRAPHINFO._serialized_start=16041
+  _INPUTCALLGRAPHINFO._serialized_end=16181
+  _FUNCTIONCALLCALLGRAPHINFO._serialized_start=16183
+  _FUNCTIONCALLCALLGRAPHINFO._serialized_end=16305
+  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_start=16308
+  _FUNCTIONGETCALLGRAPHRESPONSE._serialized_end=16453
+  _FUNCTIONCALLCANCELREQUEST._serialized_start=16455
+  _FUNCTIONCALLCANCELREQUEST._serialized_end=16508
+  _FUNCTIONCALLLISTREQUEST._serialized_start=16510
+  _FUNCTIONCALLLISTREQUEST._serialized_end=16556
+  _FUNCTIONCALLINFO._serialized_start=16559
+  _FUNCTIONCALLINFO._serialized_end=17010
+  _FUNCTIONCALLLISTRESPONSE._serialized_start=17012
+  _FUNCTIONCALLLISTRESPONSE._serialized_end=17094
+  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_start=17096
+  _FUNCTIONGETCURRENTSTATSREQUEST._serialized_end=17149
+  _FUNCTIONSTATS._serialized_start=17151
+  _FUNCTIONSTATS._serialized_end=17234
+  _GENERATORDONE._serialized_start=17236
+  _GENERATORDONE._serialized_end=17272
+  _GENERICRESULT._serialized_start=17275
+  _GENERICRESULT._serialized_end=17913
+  _GENERICRESULT_GENERICSTATUS._serialized_start=17587
+  _GENERICRESULT_GENERICSTATUS._serialized_end=17782
+  _GENERICRESULT_GENERATORSTATUS._serialized_start=17784
+  _GENERICRESULT_GENERATORSTATUS._serialized_end=17899
+  _GPUCONFIG._serialized_start=17915
+  _GPUCONFIG._serialized_end=17994
+  _BUILDFUNCTION._serialized_start=17996
+  _BUILDFUNCTION._serialized_end=18092
+  _IMAGE._serialized_start=18095
+  _IMAGE._serialized_end=18572
+  _IMAGEBUILDERVERSIONLOOKUPRESPONSE._serialized_start=18574
+  _IMAGEBUILDERVERSIONLOOKUPRESPONSE._serialized_end=18626
+  _IMAGECONTEXTFILE._serialized_start=18628
+  _IMAGECONTEXTFILE._serialized_end=18678
+  _IMAGEGETORCREATEREQUEST._serialized_start=18681
+  _IMAGEGETORCREATEREQUEST._serialized_end=18918
+  _IMAGEGETORCREATERESPONSE._serialized_start=18920
+  _IMAGEGETORCREATERESPONSE._serialized_end=18964
+  _IMAGEJOINSTREAMINGREQUEST._serialized_start=18966
+  _IMAGEJOINSTREAMINGREQUEST._serialized_end=19051
+  _IMAGEJOINSTREAMINGRESPONSE._serialized_start=19054
+  _IMAGEJOINSTREAMINGRESPONSE._serialized_end=19201
+  _IMAGEREGISTRYCONFIG._serialized_start=19203
+  _IMAGEREGISTRYCONFIG._serialized_end=19303
+  _INPUTINFO._serialized_start=19306
+  _INPUTINFO._serialized_end=19459
+  _INPUTCATEGORYINFO._serialized_start=19461
+  _INPUTCATEGORYINFO._serialized_end=19536
+  _MOUNTBUILDREQUEST._serialized_start=19538
+  _MOUNTBUILDREQUEST._serialized_end=19646
+  _MOUNTBUILDRESPONSE._serialized_start=19648
+  _MOUNTBUILDRESPONSE._serialized_end=19746
+  _MOUNTGETORCREATEREQUEST._serialized_start=19749
+  _MOUNTGETORCREATEREQUEST._serialized_end=19999
+  _MOUNTGETORCREATERESPONSE._serialized_start=20001
+  _MOUNTGETORCREATERESPONSE._serialized_end=20105
+  _MOUNTHANDLEMETADATA._serialized_start=20107
+  _MOUNTHANDLEMETADATA._serialized_end=20165
+  _MOUNTFILE._serialized_start=20167
+  _MOUNTFILE._serialized_end=20272
+  _MOUNTPUTFILEREQUEST._serialized_start=20274
+  _MOUNTPUTFILEREQUEST._serialized_end=20369
+  _MOUNTPUTFILERESPONSE._serialized_start=20371
+  _MOUNTPUTFILERESPONSE._serialized_end=20409
+  _MULTIPARTUPLOAD._serialized_start=20411
+  _MULTIPARTUPLOAD._serialized_end=20494
+  _OBJECT._serialized_start=20497
+  _OBJECT._serialized_end=20831
+  _OBJECTDEPENDENCY._serialized_start=20833
+  _OBJECTDEPENDENCY._serialized_end=20870
+  _PROXYGETORCREATEREQUEST._serialized_start=20873
+  _PROXYGETORCREATEREQUEST._serialized_end=21067
+  _PROXYGETORCREATERESPONSE._serialized_start=21069
+  _PROXYGETORCREATERESPONSE._serialized_end=21113
+  _PROXYINFO._serialized_start=21115
+  _PROXYINFO._serialized_end=21207
+  _PTYINFO._serialized_start=21210
+  _PTYINFO._serialized_end=21472
+  _PTYINFO_PTYTYPE._serialized_start=21394
+  _PTYINFO_PTYTYPE._serialized_end=21472
+  _QUEUECREATEREQUEST._serialized_start=21474
+  _QUEUECREATEREQUEST._serialized_end=21537
+  _QUEUECREATERESPONSE._serialized_start=21539
+  _QUEUECREATERESPONSE._serialized_end=21578
+  _QUEUEGETORCREATEREQUEST._serialized_start=21581
+  _QUEUEGETORCREATEREQUEST._serialized_end=21775
+  _QUEUEGETORCREATERESPONSE._serialized_start=21777
+  _QUEUEGETORCREATERESPONSE._serialized_end=21821
+  _QUEUEGETREQUEST._serialized_start=21823
+  _QUEUEGETREQUEST._serialized_end=21893
+  _QUEUEGETRESPONSE._serialized_start=21895
+  _QUEUEGETRESPONSE._serialized_end=21929
+  _QUEUEHEARTBEATREQUEST._serialized_start=21931
+  _QUEUEHEARTBEATREQUEST._serialized_end=21972
+  _QUEUEPUTREQUEST._serialized_start=21974
+  _QUEUEPUTREQUEST._serialized_end=22025
+  _QUEUELENREQUEST._serialized_start=22027
+  _QUEUELENREQUEST._serialized_end=22062
+  _QUEUELENRESPONSE._serialized_start=22064
+  _QUEUELENRESPONSE._serialized_end=22095
+  _RATELIMIT._serialized_start=22097
+  _RATELIMIT._serialized_end=22174
+  _RESOURCES._serialized_start=22176
+  _RESOURCES._serialized_end=22270
+  _S3MOUNT._serialized_start=22272
+  _S3MOUNT._serialized_end=22372
+  _SANDBOX._serialized_start=22375
+  _SANDBOX._serialized_end=22912
+  _SANDBOXCREATEREQUEST._serialized_start=22914
+  _SANDBOXCREATEREQUEST._serialized_end=23001
+  _SANDBOXCREATERESPONSE._serialized_start=23003
+  _SANDBOXCREATERESPONSE._serialized_end=23046
+  _SANDBOXGETTASKIDREQUEST._serialized_start=23048
+  _SANDBOXGETTASKIDREQUEST._serialized_end=23093
+  _SANDBOXGETTASKIDRESPONSE._serialized_start=23095
+  _SANDBOXGETTASKIDRESPONSE._serialized_end=23138
+  _SANDBOXGETLOGSREQUEST._serialized_start=23141
+  _SANDBOXGETLOGSREQUEST._serialized_end=23279
+  _SANDBOXSTDINWRITEREQUEST._serialized_start=23281
+  _SANDBOXSTDINWRITEREQUEST._serialized_end=23370
+  _SANDBOXSTDINWRITERESPONSE._serialized_start=23372
+  _SANDBOXSTDINWRITERESPONSE._serialized_end=23399
+  _SANDBOXHANDLEMETADATA._serialized_start=23401
+  _SANDBOXHANDLEMETADATA._serialized_end=23469
+  _SANDBOXINFO._serialized_start=23472
+  _SANDBOXINFO._serialized_end=23603
+  _SANDBOXLISTREQUEST._serialized_start=23605
+  _SANDBOXLISTREQUEST._serialized_end=23667
+  _SANDBOXLISTRESPONSE._serialized_start=23669
+  _SANDBOXLISTRESPONSE._serialized_end=23736
+  _SANDBOXTERMINATEREQUEST._serialized_start=23738
+  _SANDBOXTERMINATEREQUEST._serialized_end=23783
+  _SANDBOXTERMINATERESPONSE._serialized_start=23785
+  _SANDBOXTERMINATERESPONSE._serialized_end=23865
+  _SANDBOXWAITREQUEST._serialized_start=23867
+  _SANDBOXWAITREQUEST._serialized_end=23924
+  _SANDBOXWAITRESPONSE._serialized_start=23926
+  _SANDBOXWAITRESPONSE._serialized_end=23992
+  _SCHEDULE._serialized_start=23995
+  _SCHEDULE._serialized_end=24265
+  _SCHEDULE_CRON._serialized_start=24101
+  _SCHEDULE_CRON._serialized_end=24128
+  _SCHEDULE_PERIOD._serialized_start=24130
+  _SCHEDULE_PERIOD._serialized_end=24247
+  _SECRETCREATEREQUEST._serialized_start=24268
+  _SECRETCREATEREQUEST._serialized_end=24476
+  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_start=24430
+  _SECRETCREATEREQUEST_ENVDICTENTRY._serialized_end=24476
+  _SECRETCREATERESPONSE._serialized_start=24478
+  _SECRETCREATERESPONSE._serialized_end=24519
+  _SECRETGETORCREATEREQUEST._serialized_start=24522
+  _SECRETGETORCREATEREQUEST._serialized_end=24852
+  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_start=24430
+  _SECRETGETORCREATEREQUEST_ENVDICTENTRY._serialized_end=24476
+  _SECRETGETORCREATERESPONSE._serialized_start=24854
+  _SECRETGETORCREATERESPONSE._serialized_end=24900
+  _SECRETLISTITEM._serialized_start=24902
+  _SECRETLISTITEM._serialized_end=25001
+  _SECRETLISTREQUEST._serialized_start=25003
+  _SECRETLISTREQUEST._serialized_end=25048
+  _SECRETLISTRESPONSE._serialized_start=25050
+  _SECRETLISTRESPONSE._serialized_end=25141
+  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_start=25144
+  _SHAREDVOLUMEGETORCREATEREQUEST._serialized_end=25361
+  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_start=25363
+  _SHAREDVOLUMEGETORCREATERESPONSE._serialized_end=25422
+  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_start=25424
+  _SHAREDVOLUMEHEARTBEATREQUEST._serialized_end=25480
+  _SHAREDVOLUMECREATEREQUEST._serialized_start=25482
+  _SHAREDVOLUMECREATEREQUEST._serialized_end=25584
+  _SHAREDVOLUMECREATERESPONSE._serialized_start=25586
+  _SHAREDVOLUMECREATERESPONSE._serialized_end=25640
+  _SHAREDVOLUMELISTITEM._serialized_start=25643
+  _SHAREDVOLUMELISTITEM._serialized_end=25779
+  _SHAREDVOLUMELISTREQUEST._serialized_start=25781
+  _SHAREDVOLUMELISTREQUEST._serialized_end=25832
+  _SHAREDVOLUMELISTRESPONSE._serialized_start=25834
+  _SHAREDVOLUMELISTRESPONSE._serialized_end=25937
+  _SHAREDVOLUMELISTFILESREQUEST._serialized_start=25939
+  _SHAREDVOLUMELISTFILESREQUEST._serialized_end=26009
+  _SHAREDVOLUMEPUTFILEREQUEST._serialized_start=26012
+  _SHAREDVOLUMEPUTFILEREQUEST._serialized_end=26179
+  _SHAREDVOLUMEPUTFILERESPONSE._serialized_start=26181
+  _SHAREDVOLUMEPUTFILERESPONSE._serialized_end=26226
+  _SHAREDVOLUMEGETFILEREQUEST._serialized_start=26228
+  _SHAREDVOLUMEGETFILEREQUEST._serialized_end=26296
+  _SHAREDVOLUMEGETFILERESPONSE._serialized_start=26298
+  _SHAREDVOLUMEGETFILERESPONSE._serialized_end=26381
+  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_start=26383
+  _SHAREDVOLUMEREMOVEFILEREQUEST._serialized_end=26479
+  _SHAREDVOLUMELISTFILESENTRY._serialized_start=26482
+  _SHAREDVOLUMELISTFILESENTRY._serialized_end=26643
+  _SHAREDVOLUMELISTFILESENTRY_FILETYPE._serialized_start=26591
+  _SHAREDVOLUMELISTFILESENTRY_FILETYPE._serialized_end=26643
+  _SHAREDVOLUMELISTFILESRESPONSE._serialized_start=26645
+  _SHAREDVOLUMELISTFILESRESPONSE._serialized_end=26735
+  _SHAREDVOLUMEMOUNT._serialized_start=26738
+  _SHAREDVOLUMEMOUNT._serialized_end=26884
+  _TASKCURRENTINPUTSRESPONSE._serialized_start=26886
+  _TASKCURRENTINPUTSRESPONSE._serialized_end=26932
+  _TASKINFO._serialized_start=26934
+  _TASKINFO._serialized_end=27042
+  _TASKLOGS._serialized_start=27045
+  _TASKLOGS._serialized_end=27283
+  _TASKLISTREQUEST._serialized_start=27285
+  _TASKLISTREQUEST._serialized_end=27302
+  _TASKLISTRESPONSE._serialized_start=27304
+  _TASKLISTRESPONSE._serialized_end=27362
+  _TASKLOGSBATCH._serialized_start=27365
+  _TASKLOGSBATCH._serialized_end=27563
+  _TASKPROGRESS._serialized_start=27565
+  _TASKPROGRESS._serialized_end=27677
+  _TASKRESULTREQUEST._serialized_start=27679
+  _TASKRESULTREQUEST._serialized_end=27743
+  _TASKSTATS._serialized_start=27745
+  _TASKSTATS._serialized_end=27834
+  _TOKENFLOWCREATEREQUEST._serialized_start=27836
+  _TOKENFLOWCREATEREQUEST._serialized_end=27922
+  _TOKENFLOWCREATERESPONSE._serialized_start=27924
+  _TOKENFLOWCREATERESPONSE._serialized_end=28024
+  _TOKENFLOWWAITREQUEST._serialized_start=28026
+  _TOKENFLOWWAITREQUEST._serialized_end=28109
+  _TOKENFLOWWAITRESPONSE._serialized_start=28111
+  _TOKENFLOWWAITRESPONSE._serialized_end=28219
+  _TUNNELSTARTREQUEST._serialized_start=28221
+  _TUNNELSTARTREQUEST._serialized_end=28276
+  _TUNNELSTARTRESPONSE._serialized_start=28279
+  _TUNNELSTARTRESPONSE._serialized_end=28432
+  _TUNNELSTOPREQUEST._serialized_start=28434
+  _TUNNELSTOPREQUEST._serialized_end=28467
+  _TUNNELSTOPRESPONSE._serialized_start=28469
+  _TUNNELSTOPRESPONSE._serialized_end=28505
+  _VOLUMEGETORCREATEREQUEST._serialized_start=28508
+  _VOLUMEGETORCREATEREQUEST._serialized_end=28719
+  _VOLUMEGETORCREATERESPONSE._serialized_start=28721
+  _VOLUMEGETORCREATERESPONSE._serialized_end=28767
+  _VOLUMEHEARTBEATREQUEST._serialized_start=28769
+  _VOLUMEHEARTBEATREQUEST._serialized_end=28812
+  _VOLUMECREATEREQUEST._serialized_start=28814
+  _VOLUMECREATEREQUEST._serialized_end=28857
+  _VOLUMECREATERESPONSE._serialized_start=28859
+  _VOLUMECREATERESPONSE._serialized_end=28900
+  _VOLUMECOMMITREQUEST._serialized_start=28902
+  _VOLUMECOMMITREQUEST._serialized_end=28948
+  _VOLUMECOMMITRESPONSE._serialized_start=28950
+  _VOLUMECOMMITRESPONSE._serialized_end=28993
+  _VOLUMEDELETEREQUEST._serialized_start=28995
+  _VOLUMEDELETEREQUEST._serialized_end=29065
+  _VOLUMEGETFILEREQUEST._serialized_start=29067
+  _VOLUMEGETFILEREQUEST._serialized_end=29150
+  _VOLUMEGETFILERESPONSE._serialized_start=29152
+  _VOLUMEGETFILERESPONSE._serialized_end=29271
+  _VOLUMELISTFILESENTRY._serialized_start=29274
+  _VOLUMELISTFILESENTRY._serialized_end=29465
+  _VOLUMELISTFILESENTRY_FILETYPE._serialized_start=29400
+  _VOLUMELISTFILESENTRY_FILETYPE._serialized_end=29465
+  _VOLUMELISTFILESREQUEST._serialized_start=29467
+  _VOLUMELISTFILESREQUEST._serialized_end=29566
+  _VOLUMELISTFILESRESPONSE._serialized_start=29568
+  _VOLUMELISTFILESRESPONSE._serialized_end=29646
+  _VOLUMELISTITEM._serialized_start=29648
+  _VOLUMELISTITEM._serialized_end=29718
+  _VOLUMELISTREQUEST._serialized_start=29720
+  _VOLUMELISTREQUEST._serialized_end=29765
+  _VOLUMELISTRESPONSE._serialized_start=29767
+  _VOLUMELISTRESPONSE._serialized_end=29858
+  _VOLUMERELOADREQUEST._serialized_start=29860
+  _VOLUMERELOADREQUEST._serialized_end=29900
+  _VOLUMEPUTFILESREQUEST._serialized_start=29902
+  _VOLUMEPUTFILESREQUEST._serialized_end=30027
+  _VOLUMEREMOVEFILEREQUEST._serialized_start=30029
+  _VOLUMEREMOVEFILEREQUEST._serialized_end=30112
+  _VOLUMECOPYFILESREQUEST._serialized_start=30114
+  _VOLUMECOPYFILESREQUEST._serialized_end=30213
+  _VOLUMEMOUNT._serialized_start=30215
+  _VOLUMEMOUNT._serialized_end=30301
+  _WEBHOOKCONFIG._serialized_start=30304
+  _WEBHOOKCONFIG._serialized_end=30573
+  _WEBURLINFO._serialized_start=30575
+  _WEBURLINFO._serialized_end=30653
+  _WORKSPACENAMELOOKUPRESPONSE._serialized_start=30655
+  _WORKSPACENAMELOOKUPRESPONSE._serialized_end=30726
+  _RUNTIMEOUTPUTMESSAGE._serialized_start=30728
+  _RUNTIMEOUTPUTMESSAGE._serialized_end=30822
+  _RUNTIMEOUTPUTBATCH._serialized_start=30825
+  _RUNTIMEOUTPUTBATCH._serialized_end=30955
+  _RUNTIMEINPUTMESSAGE._serialized_start=30957
+  _RUNTIMEINPUTMESSAGE._serialized_end=31018
+  _MODALCLIENT._serialized_start=34640
+  _MODALCLIENT._serialized_end=44659
 # @@protoc_insertion_point(module_scope)
```

## modal_proto/api_pb2_grpc.py

```diff
@@ -151,19 +151,14 @@
                 response_deserializer=modal__proto_dot_api__pb2.DictGetOrCreateResponse.FromString,
                 )
         self.DictHeartbeat = channel.unary_unary(
                 '/modal.client.ModalClient/DictHeartbeat',
                 request_serializer=modal__proto_dot_api__pb2.DictHeartbeatRequest.SerializeToString,
                 response_deserializer=google_dot_protobuf_dot_empty__pb2.Empty.FromString,
                 )
-        self.DictContents = channel.unary_stream(
-                '/modal.client.ModalClient/DictContents',
-                request_serializer=modal__proto_dot_api__pb2.DictContentsRequest.SerializeToString,
-                response_deserializer=modal__proto_dot_api__pb2.DictEntry.FromString,
-                )
         self.DictUpdate = channel.unary_unary(
                 '/modal.client.ModalClient/DictUpdate',
                 request_serializer=modal__proto_dot_api__pb2.DictUpdateRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.DictUpdateResponse.FromString,
                 )
         self.DictGet = channel.unary_unary(
                 '/modal.client.ModalClient/DictGet',
@@ -321,14 +316,19 @@
                 response_deserializer=modal__proto_dot_api__pb2.ImageGetOrCreateResponse.FromString,
                 )
         self.ImageJoinStreaming = channel.unary_stream(
                 '/modal.client.ModalClient/ImageJoinStreaming',
                 request_serializer=modal__proto_dot_api__pb2.ImageJoinStreamingRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.ImageJoinStreamingResponse.FromString,
                 )
+        self.ImageBuilderVersionLookup = channel.unary_unary(
+                '/modal.client.ModalClient/ImageBuilderVersionLookup',
+                request_serializer=google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
+                response_deserializer=modal__proto_dot_api__pb2.ImageBuilderVersionLookupResponse.FromString,
+                )
         self.MountPutFile = channel.unary_unary(
                 '/modal.client.ModalClient/MountPutFile',
                 request_serializer=modal__proto_dot_api__pb2.MountPutFileRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.MountPutFileResponse.FromString,
                 )
         self.MountBuild = channel.unary_unary(
                 '/modal.client.ModalClient/MountBuild',
@@ -371,19 +371,14 @@
                 response_deserializer=google_dot_protobuf_dot_empty__pb2.Empty.FromString,
                 )
         self.QueueLen = channel.unary_unary(
                 '/modal.client.ModalClient/QueueLen',
                 request_serializer=modal__proto_dot_api__pb2.QueueLenRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.QueueLenResponse.FromString,
                 )
-        self.QueueNextItems = channel.unary_unary(
-                '/modal.client.ModalClient/QueueNextItems',
-                request_serializer=modal__proto_dot_api__pb2.QueueNextItemsRequest.SerializeToString,
-                response_deserializer=modal__proto_dot_api__pb2.QueueNextItemsResponse.FromString,
-                )
         self.SandboxCreate = channel.unary_unary(
                 '/modal.client.ModalClient/SandboxCreate',
                 request_serializer=modal__proto_dot_api__pb2.SandboxCreateRequest.SerializeToString,
                 response_deserializer=modal__proto_dot_api__pb2.SandboxCreateResponse.FromString,
                 )
         self.SandboxGetTaskId = channel.unary_unary(
                 '/modal.client.ModalClient/SandboxGetTaskId',
@@ -752,20 +747,14 @@
 
     def DictHeartbeat(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
-    def DictContents(self, request, context):
-        """Missing associated documentation comment in .proto file."""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details('Method not implemented!')
-        raise NotImplementedError('Method not implemented!')
-
     def DictUpdate(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
     def DictGet(self, request, context):
@@ -965,14 +954,20 @@
 
     def ImageJoinStreaming(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
+    def ImageBuilderVersionLookup(self, request, context):
+        """Missing associated documentation comment in .proto file."""
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
     def MountPutFile(self, request, context):
         """Mounts
         """
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
@@ -1028,20 +1023,14 @@
 
     def QueueLen(self, request, context):
         """Missing associated documentation comment in .proto file."""
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
-    def QueueNextItems(self, request, context):
-        """Missing associated documentation comment in .proto file."""
-        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-        context.set_details('Method not implemented!')
-        raise NotImplementedError('Method not implemented!')
-
     def SandboxCreate(self, request, context):
         """Sandboxes
         """
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
@@ -1422,19 +1411,14 @@
                     response_serializer=modal__proto_dot_api__pb2.DictGetOrCreateResponse.SerializeToString,
             ),
             'DictHeartbeat': grpc.unary_unary_rpc_method_handler(
                     servicer.DictHeartbeat,
                     request_deserializer=modal__proto_dot_api__pb2.DictHeartbeatRequest.FromString,
                     response_serializer=google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
             ),
-            'DictContents': grpc.unary_stream_rpc_method_handler(
-                    servicer.DictContents,
-                    request_deserializer=modal__proto_dot_api__pb2.DictContentsRequest.FromString,
-                    response_serializer=modal__proto_dot_api__pb2.DictEntry.SerializeToString,
-            ),
             'DictUpdate': grpc.unary_unary_rpc_method_handler(
                     servicer.DictUpdate,
                     request_deserializer=modal__proto_dot_api__pb2.DictUpdateRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.DictUpdateResponse.SerializeToString,
             ),
             'DictGet': grpc.unary_unary_rpc_method_handler(
                     servicer.DictGet,
@@ -1592,14 +1576,19 @@
                     response_serializer=modal__proto_dot_api__pb2.ImageGetOrCreateResponse.SerializeToString,
             ),
             'ImageJoinStreaming': grpc.unary_stream_rpc_method_handler(
                     servicer.ImageJoinStreaming,
                     request_deserializer=modal__proto_dot_api__pb2.ImageJoinStreamingRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.ImageJoinStreamingResponse.SerializeToString,
             ),
+            'ImageBuilderVersionLookup': grpc.unary_unary_rpc_method_handler(
+                    servicer.ImageBuilderVersionLookup,
+                    request_deserializer=google_dot_protobuf_dot_empty__pb2.Empty.FromString,
+                    response_serializer=modal__proto_dot_api__pb2.ImageBuilderVersionLookupResponse.SerializeToString,
+            ),
             'MountPutFile': grpc.unary_unary_rpc_method_handler(
                     servicer.MountPutFile,
                     request_deserializer=modal__proto_dot_api__pb2.MountPutFileRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.MountPutFileResponse.SerializeToString,
             ),
             'MountBuild': grpc.unary_unary_rpc_method_handler(
                     servicer.MountBuild,
@@ -1642,19 +1631,14 @@
                     response_serializer=google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
             ),
             'QueueLen': grpc.unary_unary_rpc_method_handler(
                     servicer.QueueLen,
                     request_deserializer=modal__proto_dot_api__pb2.QueueLenRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.QueueLenResponse.SerializeToString,
             ),
-            'QueueNextItems': grpc.unary_unary_rpc_method_handler(
-                    servicer.QueueNextItems,
-                    request_deserializer=modal__proto_dot_api__pb2.QueueNextItemsRequest.FromString,
-                    response_serializer=modal__proto_dot_api__pb2.QueueNextItemsResponse.SerializeToString,
-            ),
             'SandboxCreate': grpc.unary_unary_rpc_method_handler(
                     servicer.SandboxCreate,
                     request_deserializer=modal__proto_dot_api__pb2.SandboxCreateRequest.FromString,
                     response_serializer=modal__proto_dot_api__pb2.SandboxCreateResponse.SerializeToString,
             ),
             'SandboxGetTaskId': grpc.unary_unary_rpc_method_handler(
                     servicer.SandboxGetTaskId,
@@ -2329,31 +2313,14 @@
         return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/DictHeartbeat',
             modal__proto_dot_api__pb2.DictHeartbeatRequest.SerializeToString,
             google_dot_protobuf_dot_empty__pb2.Empty.FromString,
             options, channel_credentials,
             insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def DictContents(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_stream(request, target, '/modal.client.ModalClient/DictContents',
-            modal__proto_dot_api__pb2.DictContentsRequest.SerializeToString,
-            modal__proto_dot_api__pb2.DictEntry.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
-
-    @staticmethod
     def DictUpdate(request,
             target,
             options=(),
             channel_credentials=None,
             call_credentials=None,
             insecure=False,
             compression=None,
@@ -2907,14 +2874,31 @@
         return grpc.experimental.unary_stream(request, target, '/modal.client.ModalClient/ImageJoinStreaming',
             modal__proto_dot_api__pb2.ImageJoinStreamingRequest.SerializeToString,
             modal__proto_dot_api__pb2.ImageJoinStreamingResponse.FromString,
             options, channel_credentials,
             insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
+    def ImageBuilderVersionLookup(request,
+            target,
+            options=(),
+            channel_credentials=None,
+            call_credentials=None,
+            insecure=False,
+            compression=None,
+            wait_for_ready=None,
+            timeout=None,
+            metadata=None):
+        return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/ImageBuilderVersionLookup',
+            google_dot_protobuf_dot_empty__pb2.Empty.SerializeToString,
+            modal__proto_dot_api__pb2.ImageBuilderVersionLookupResponse.FromString,
+            options, channel_credentials,
+            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+
+    @staticmethod
     def MountPutFile(request,
             target,
             options=(),
             channel_credentials=None,
             call_credentials=None,
             insecure=False,
             compression=None,
@@ -3077,31 +3061,14 @@
         return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/QueueLen',
             modal__proto_dot_api__pb2.QueueLenRequest.SerializeToString,
             modal__proto_dot_api__pb2.QueueLenResponse.FromString,
             options, channel_credentials,
             insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
 
     @staticmethod
-    def QueueNextItems(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/modal.client.ModalClient/QueueNextItems',
-            modal__proto_dot_api__pb2.QueueNextItemsRequest.SerializeToString,
-            modal__proto_dot_api__pb2.QueueNextItemsResponse.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
-
-    @staticmethod
     def SandboxCreate(request,
             target,
             options=(),
             channel_credentials=None,
             call_credentials=None,
             insecure=False,
             compression=None,
```

## modal_version/_version_generated.py

```diff
@@ -1,4 +1,4 @@
 # Copyright Modal Labs 2024
 
 # Note: Reset this value to -1 whenever you make a minor `0.X` release of the client.
-build_number = 87  # git: 1e0a2e7
+build_number = 9  # git: 828a4bc
```

## test/aio_test.py

```diff
@@ -1,12 +1,12 @@
 # Copyright Modal Labs 2023
 import pytest
 
 
 @pytest.mark.asyncio
 async def test_new(servicer, client):
-    from modal import App
+    from modal import Stub
 
-    app = App()
+    stub = Stub()
 
-    async with app.run(client=client):
+    async with stub.run(client=client):
         pass
```

## test/async_utils_test.py

```diff
@@ -153,15 +153,15 @@
         await asyncio.sleep(DEBOUNCE_TIME + 0.001)
 
         assert len(drained_items) == 3
 
 
 @pytest.mark.asyncio
 async def test_warn_if_generator_is_not_consumed(caplog):
-    @warn_if_generator_is_not_consumed()
+    @warn_if_generator_is_not_consumed
     async def my_generator():
         yield 42
 
     with caplog.at_level(logging.WARNING):
         g = my_generator()
         assert "my_generator" in repr(g)
         del g  # Force destructor
@@ -169,33 +169,16 @@
     assert len(caplog.records) == 1
     assert "my_generator" in caplog.text
     assert "for" in caplog.text
     assert "list" in caplog.text
 
 
 @pytest.mark.asyncio
-def test_warn_if_generator_is_not_consumed_sync(caplog):
-    @warn_if_generator_is_not_consumed()
-    def my_generator():
-        yield 42
-
-    with caplog.at_level(logging.WARNING):
-        g = my_generator()
-        assert "my_generator" in repr(g)
-        del g  # Force destructor
-
-    assert len(caplog.records) == 1
-    assert "my_generator" in caplog.text
-    assert "for" in caplog.text
-    assert "list" in caplog.text
-
-
-@pytest.mark.asyncio
 async def test_no_warn_if_generator_is_consumed(caplog):
-    @warn_if_generator_is_not_consumed()
+    @warn_if_generator_is_not_consumed
     async def my_generator():
         yield 42
 
     with caplog.at_level(logging.WARNING):
         g = my_generator()
         async for _ in g:
             pass
```

## test/cli_imports_test.py

```diff
@@ -1,18 +1,18 @@
 # Copyright Modal Labs 2023
 import pytest
 
 from modal._utils.async_utils import synchronizer
-from modal.app import _App, _LocalEntrypoint
 from modal.cli.import_refs import (
-    DEFAULT_APP_NAME,
+    DEFAULT_STUB_NAME,
     get_by_object_path,
     import_file_or_module,
     parse_import_ref,
 )
+from modal.stub import _LocalEntrypoint, _Stub
 
 # Some helper vars for import_stub tests:
 local_entrypoint_src = """
 import modal
 
 stub = modal.Stub()
 @stub.local_entrypoint()
@@ -81,34 +81,34 @@
 }
 
 
 @pytest.mark.parametrize(
     ["dir_structure", "ref", "expected_object_type"],
     [
         # # file syntax
-        (empty_dir_with_python_file, "mod.py", _App),
-        (empty_dir_with_python_file, "mod.py::stub", _App),
-        (empty_dir_with_python_file, "mod.py::other_stub", _App),
-        (dir_containing_python_package, "pack/file.py", _App),
-        (dir_containing_python_package, "pack/sub/subfile.py", _App),
-        (dir_containing_python_package, "dir/sub/subfile.py", _App),
+        (empty_dir_with_python_file, "mod.py", _Stub),
+        (empty_dir_with_python_file, "mod.py::stub", _Stub),
+        (empty_dir_with_python_file, "mod.py::other_stub", _Stub),
+        (dir_containing_python_package, "pack/file.py", _Stub),
+        (dir_containing_python_package, "pack/sub/subfile.py", _Stub),
+        (dir_containing_python_package, "dir/sub/subfile.py", _Stub),
         # # python module syntax
-        (empty_dir_with_python_file, "mod", _App),
-        (empty_dir_with_python_file, "mod::stub", _App),
-        (empty_dir_with_python_file, "mod::other_stub", _App),
-        (dir_containing_python_package, "pack.mod", _App),
-        (dir_containing_python_package, "pack.mod::other_stub", _App),
+        (empty_dir_with_python_file, "mod", _Stub),
+        (empty_dir_with_python_file, "mod::stub", _Stub),
+        (empty_dir_with_python_file, "mod::other_stub", _Stub),
+        (dir_containing_python_package, "pack.mod", _Stub),
+        (dir_containing_python_package, "pack.mod::other_stub", _Stub),
         (dir_containing_python_package, "pack/local.py::stub.main", _LocalEntrypoint),
     ],
 )
 def test_import_object(dir_structure, ref, expected_object_type, mock_dir):
     with mock_dir(dir_structure):
         import_ref = parse_import_ref(ref)
         module = import_file_or_module(import_ref.file_or_module)
-        imported_object = get_by_object_path(module, import_ref.object_path or DEFAULT_APP_NAME)
+        imported_object = get_by_object_path(module, import_ref.object_path or DEFAULT_STUB_NAME)
         _translated_obj = synchronizer._translate_in(imported_object)
         assert isinstance(_translated_obj, expected_object_type)
 
 
 def test_import_package_and_module_names(monkeypatch, supports_dir):
     # We try to reproduce the package/module naming standard that the `python` command line tool uses,
     # i.e. when loading using a module path (-m flag w/ python) you get a fully qualified package/module name
```

## test/cli_test.py

```diff
@@ -10,16 +10,18 @@
 import tempfile
 import traceback
 from typing import List, Optional
 from unittest import mock
 
 import click
 import click.testing
+import pytest_asyncio
 import toml
 
+from modal import Client
 from modal.cli.entry_point import entrypoint_cli
 from modal_proto import api_pb2
 
 from .supports.skip import skip_windows
 
 dummy_app_file = """
 import modal
@@ -33,14 +35,23 @@
 mod = sys.modules[__name__]
 assert mod.stub == stub
 """
 
 dummy_other_module_file = "x = 42"
 
 
+@pytest_asyncio.fixture
+async def set_env_client(client):
+    try:
+        Client.set_env_client(client)
+        yield
+    finally:
+        Client.set_env_client(None)
+
+
 def _run(args: List[str], expected_exit_code: int = 0, expected_stderr: Optional[str] = ""):
     runner = click.testing.CliRunner(mix_stderr=False)
     with mock.patch.object(sys, "argv", args):
         res = runner.invoke(entrypoint_cli, args)
     if res.exit_code != expected_exit_code:
         print("stdout:", repr(res.stdout))
         print("stderr:", repr(res.stderr))
@@ -113,28 +124,21 @@
         _run(["setup", "--profile", "_test"])
         assert "_test" in toml.load(config_file_path)
 
 
 def test_run(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "default_stub.py"
     _run(["run", stub_file.as_posix()])
-    _run(["run", stub_file.as_posix() + "::app"])
+    _run(["run", stub_file.as_posix() + "::stub"])
     _run(["run", stub_file.as_posix() + "::foo"])
     _run(["run", stub_file.as_posix() + "::bar"], expected_exit_code=1, expected_stderr=None)
     file_with_entrypoint = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
     _run(["run", file_with_entrypoint.as_posix()])
     _run(["run", file_with_entrypoint.as_posix() + "::main"])
-    _run(["run", file_with_entrypoint.as_posix() + "::app.main"])
-
-
-def test_run_app(servicer, set_env_client, test_dir):
-    stub_file = test_dir / "supports" / "app_run_tests" / "stub_is_now_app.py"
-    _run(["run", stub_file.as_posix()])
-    _run(["run", stub_file.as_posix() + "::app"])
-    _run(["run", stub_file.as_posix() + "::foo"])
+    _run(["run", file_with_entrypoint.as_posix() + "::stub.main"])
 
 
 def test_run_async(servicer, set_env_client, test_dir):
     sync_fn = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
     res = _run(["run", sync_fn.as_posix()])
     assert "called locally" in res.stdout
 
@@ -192,32 +196,32 @@
     stub_file = test_dir / "supports" / "app_run_tests" / "default_stub.py"
     _run(["deploy", "--name=deployment_name", stub_file.as_posix()])
     assert servicer.app_state_history["ap-1"] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
 
 def test_run_custom_stub(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "custom_stub.py"
-    res = _run(["run", stub_file.as_posix() + "::app"], expected_exit_code=1, expected_stderr=None)
+    res = _run(["run", stub_file.as_posix() + "::stub"], expected_exit_code=1, expected_stderr=None)
     assert "Could not find" in res.stderr
-    res = _run(["run", stub_file.as_posix() + "::app.foo"], expected_exit_code=1, expected_stderr=None)
+    res = _run(["run", stub_file.as_posix() + "::stub.foo"], expected_exit_code=1, expected_stderr=None)
     assert "Could not find" in res.stderr
 
     _run(["run", stub_file.as_posix() + "::foo"])
 
 
 def test_run_aiofunc(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "async_stub.py"
     _run(["run", stub_file.as_posix()])
     assert len(servicer.client_calls) == 1
 
 
 def test_run_local_entrypoint(servicer, set_env_client, test_dir):
     stub_file = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
 
-    res = _run(["run", stub_file.as_posix() + "::app.main"])  # explicit name
+    res = _run(["run", stub_file.as_posix() + "::stub.main"])  # explicit name
     assert "called locally" in res.stdout
     assert len(servicer.client_calls) == 2
 
     res = _run(["run", stub_file.as_posix()])  # only one entry-point, no name needed
     assert "called locally" in res.stdout
     assert len(servicer.client_calls) == 4
 
@@ -236,15 +240,15 @@
     res = _run(["run", stub_file.as_posix()], expected_exit_code=2, expected_stderr=None)
     assert "You need to specify a Modal function or local entrypoint to run" in res.stderr
 
     valid_call_args = [
         (
             [
                 "run",
-                f"{stub_file.as_posix()}::app.dt_arg",
+                f"{stub_file.as_posix()}::stub.dt_arg",
                 "--dt",
                 "2022-10-31",
             ],
             "the day is 31",
         ),
         (["run", f"{stub_file.as_posix()}::dt_arg", "--dt=2022-10-31"], "the day is 31"),
         (["run", f"{stub_file.as_posix()}::int_arg", "--i=200"], "200 <class 'int'>"),
@@ -649,18 +653,7 @@
                 os.environ["MODAL_TOKEN_ID"] = orig_env_token_id
             else:
                 del os.environ["MODAL_TOKEN_ID"]
             if orig_env_token_secret:
                 os.environ["MODAL_TOKEN_SECRET"] = orig_env_token_secret
             else:
                 del os.environ["MODAL_TOKEN_SECRET"]
-
-
-def test_list_apps(servicer, mock_dir, set_env_client):
-    res = _run(["app", "list"])
-    assert "my_app_foo" not in res.stdout
-
-    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
-        _run(["deploy", "myapp.py", "--name", "my_app_foo"])
-
-    res = _run(["app", "list"])
-    assert "my_app_foo" in res.stdout
```

## test/cls_test.py

```diff
@@ -1,139 +1,132 @@
 # Copyright Modal Labs 2022
 import pytest
 import threading
 from typing import TYPE_CHECKING, Callable, Dict
 
 from typing_extensions import assert_type
 
-from modal import App, Cls, Function, Image, Queue, build, enter, exit, method
+from modal import Cls, Function, Image, Queue, Stub, build, enter, exit, method
 from modal._serialization import deserialize
+from modal.app import ContainerApp
 from modal.exception import DeprecationError, ExecutionError, InvalidError
 from modal.partial_function import (
     _find_callables_for_obj,
     _find_partial_methods_for_cls,
     _PartialFunction,
     _PartialFunctionFlags,
 )
-from modal.runner import deploy_app
-from modal.running_app import RunningApp
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
 from .supports.base_class import BaseCls2
 
-app = App("app")
+stub = Stub("stub")
 
 
-@pytest.fixture(autouse=True)
-def auto_use_set_env_client(set_env_client):
-    # TODO(elias): remove set_env_client fixture here if/when possible - this is required only since
-    #  Client.from_env happens to inject an unused client when loading the
-    #  parameterized function
-    return
-
-
-@app.cls(cpu=42)
+@stub.cls(cpu=42)
 class Foo:
     @method()
     def bar(self, x: int) -> float:
         return x**3
 
 
 def test_run_class(client, servicer):
     assert servicer.n_functions == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         function_id = Foo.bar.object_id
         assert isinstance(Foo, Cls)
         class_id = Foo.object_id
-        app_id = app.app_id
+        app_id = stub.app_id
 
     objects = servicer.app_objects[app_id]
     assert len(objects) == 2  # classes and functions
     assert objects["Foo.bar"] == function_id
     assert objects["Foo"] == class_id
 
 
 def test_call_class_sync(client, servicer):
-    with app.run(client=client):
+    with stub.run(client=client):
         foo: Foo = Foo()
         ret: float = foo.bar.remote(42)
         assert ret == 1764
 
 
-# Reusing the app runs into an issue with stale function handles.
-# TODO (akshat): have all the client tests use separate apps, and throw
-# an exception if the user tries to reuse a app.
-app_remote = App()
+# Reusing the stub runs into an issue with stale function handles.
+# TODO (akshat): have all the client tests use separate stubs, and throw
+# an exception if the user tries to reuse a stub.
+stub_remote = Stub()
 
 
-@app_remote.cls(cpu=42)
+@stub_remote.cls(cpu=42)
 class FooRemote:
     def __init__(self, x: int, y: str) -> None:
         self.x = x
         self.y = y
 
     @method()
     def bar(self, z: int):
         return z**3
 
 
 def test_call_cls_remote_sync(client):
-    with app_remote.run(client=client):
+    with stub_remote.run(client=client):
         foo_remote: FooRemote = FooRemote(3, "hello")
         ret: float = foo_remote.bar.remote(8)
         assert ret == 64  # Mock servicer just squares the argument
 
 
 def test_call_cls_remote_invalid_type(client):
-    with app_remote.run(client=client):
+    with stub_remote.run(client=client):
 
         def my_function():
             print("Hello, world!")
 
         with pytest.raises(ValueError) as excinfo:
             FooRemote(42, my_function)  # type: ignore
 
         exc = excinfo.value
         assert "function" in str(exc)
 
 
 def test_call_cls_remote_modal_type(client):
-    with app_remote.run(client=client):
+    with stub_remote.run(client=client):
         with Queue.ephemeral(client) as q:
             FooRemote(42, q)  # type: ignore
 
 
-app_2 = App()
+
+stub_2 = Stub()
 
 
-@app_2.cls(cpu=42)
+@stub_2.cls(cpu=42)
 class Bar:
     @method()
     def baz(self, x):
         return x**3
 
 
 @pytest.mark.asyncio
 async def test_call_class_async(client, servicer):
-    async with app_2.run(client=client):
+    async with stub_2.run(client=client):
         bar = Bar()
         assert await bar.baz.remote.aio(42) == 1764
 
 
 def test_run_class_serialized(client, servicer):
-    app_ser = App()
+    stub_ser = Stub()
 
-    @app_ser.cls(cpu=42, serialized=True)
+    @stub_ser.cls(cpu=42, serialized=True)
     class FooSer:
         @method()
         def bar(self, x):
             return x**3
 
     assert servicer.n_functions == 0
-    with app_ser.run(client=client):
+    with stub_ser.run(client=client):
         pass
 
     assert servicer.n_functions == 1
     (function_id,) = servicer.app_functions.keys()
     function = servicer.app_functions[function_id]
     assert function.function_name.endswith("FooSer.bar")  # because it's defined in a local scope
     assert function.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED
@@ -144,93 +137,93 @@
     obj = cls()
     meth = fun.__get__(obj, cls)
 
     # Make sure it's callable
     assert meth(100) == 1000000
 
 
-app_remote_2 = App()
+stub_remote_2 = Stub()
 
 
-@app_remote_2.cls(cpu=42)
+@stub_remote_2.cls(cpu=42)
 class BarRemote:
     def __init__(self, x: int, y: str) -> None:
         self.x = x
         self.y = y
 
     @method()
     def baz(self, z: int):
         return z**3
 
 
 @pytest.mark.asyncio
 async def test_call_cls_remote_async(client):
-    async with app_remote_2.run(client=client):
+    async with stub_remote_2.run(client=client):
         bar_remote = BarRemote(3, "hello")
         assert await bar_remote.baz.remote.aio(8) == 64  # Mock servicer just squares the argument
 
 
-app_local = App()
+stub_local = Stub()
 
 
-@app_local.cls(cpu=42)
+@stub_local.cls(cpu=42)
 class FooLocal:
     @method()
     def bar(self, x):
         return x**3
 
     @method()
     def baz(self, y):
         return self.bar.local(y + 1)
 
 
 def test_can_call_locally(client):
     foo = FooLocal()
     assert foo.bar.local(4) == 64
     assert foo.baz.local(4) == 125
-    with app_local.run(client=client):
+    with stub_local.run(client=client):
         assert foo.baz.local(2) == 27
 
 
 def test_can_call_remotely_from_local(client):
-    with app_local.run(client=client):
+    with stub_local.run(client=client):
         foo = FooLocal()
         # remote calls use the mockservicer func impl
         # which just squares the arguments
         assert foo.bar.remote(8) == 64
         assert foo.baz.remote(9) == 81
 
 
-app_remote_3 = App()
+stub_remote_3 = Stub()
 
 
-@app_remote_3.cls(cpu=42)
+@stub_remote_3.cls(cpu=42)
 class NoArgRemote:
     def __init__(self) -> None:
         pass
 
     @method()
     def baz(self, z: int):
         return z**3
 
 
 def test_call_cls_remote_no_args(client):
-    with app_remote_3.run(client=client):
+    with stub_remote_3.run(client=client):
         foo_remote = NoArgRemote()
         assert foo_remote.baz.remote(8) == 64  # Mock servicer just squares the argument
 
 
 if TYPE_CHECKING:
     # Check that type annotations carry through to the decorated classes
     assert_type(Foo(), Foo)
     assert_type(Foo().bar, Function)
 
 
 def test_lookup(client, servicer):
-    deploy_app(app, "my-cls-app", client=client)
+    deploy_stub(stub, "my-cls-app", client=client)
 
     cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)
 
     assert cls.object_id.startswith("cs-")
     assert cls.bar.object_id.startswith("fu-")
 
     # Check that function properties are preserved
@@ -246,55 +239,55 @@
     # Make sure local calls fail
     with pytest.raises(ExecutionError):
         assert obj.bar.local(1, 2)
 
 
 def test_lookup_lazy_remote(client, servicer):
     # See #972 (PR) and #985 (revert PR): adding unit test to catch regression
-    deploy_app(app, "my-cls-app", client=client)
+    deploy_stub(stub, "my-cls-app", client=client)
     cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)
     obj = cls("foo", 234)
     assert obj.bar.remote(42, 77) == 7693
 
 
 def test_lookup_lazy_spawn(client, servicer):
     # See #1071
-    deploy_app(app, "my-cls-app", client=client)
+    deploy_stub(stub, "my-cls-app", client=client)
     cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)
     obj = cls("foo", 234)
     function_call = obj.bar.spawn(42, 77)
     assert function_call.get() == 7693
 
 
-baz_app = App()
+baz_stub = Stub()
 
 
-@baz_app.cls()
+@baz_stub.cls()
 class Baz:
     def __init__(self, x):
         self.x = x
 
     def not_modal_method(self, y: int) -> int:
         return self.x * y
 
 
 def test_call_not_modal_method():
     baz: Baz = Baz(5)
     assert baz.x == 5
     assert baz.not_modal_method(7) == 35
 
 
-cls_with_enter_app = App()
+cls_with_enter_stub = Stub()
 
 
 def get_thread_id():
     return threading.current_thread().name
 
 
-@cls_with_enter_app.cls()
+@cls_with_enter_stub.cls()
 class ClsWithEnter:
     def __init__(self, thread_id):
         self.inited = True
         self.entered = False
         self.thread_id = thread_id
         assert get_thread_id() == self.thread_id
 
@@ -329,15 +322,15 @@
 def test_enter_on_local_modal_call():
     obj = ClsWithEnter(get_thread_id())
     assert obj.modal_method.local(7) == 49
     assert obj.inited
     assert obj.entered
 
 
-@cls_with_enter_app.cls()
+@cls_with_enter_stub.cls()
 class ClsWithAsyncEnter:
     def __init__(self):
         self.inited = True
         self.entered = False
 
     @enter()
     async def enter(self):
@@ -352,105 +345,106 @@
 async def test_async_enter_on_local_modal_call():
     obj = ClsWithAsyncEnter()
     assert await obj.modal_method.local(7) == 49
     assert obj.inited
     assert obj.entered
 
 
-inheritance_app = App()
+inheritance_stub = Stub()
 
 
 class BaseCls:
     @enter()
     def enter(self):
         self.x = 2
 
     @method()
     def run(self, y):
         return self.x * y
 
 
-@inheritance_app.cls()
+@inheritance_stub.cls()
 class DerivedCls(BaseCls):
     pass
 
 
 def test_derived_cls(client, servicer):
-    with inheritance_app.run(client=client):
+    with inheritance_stub.run(client=client):
         # default servicer fn just squares the number
         assert DerivedCls().run.remote(3) == 9
 
 
-inheritance_app_2 = App()
+inheritance_stub_2 = Stub()
 
 
-@inheritance_app_2.cls()
+@inheritance_stub_2.cls()
 class DerivedCls2(BaseCls2):
     pass
 
 
 def test_derived_cls_external_file(client, servicer):
-    with inheritance_app_2.run(client=client):
+    with inheritance_stub_2.run(client=client):
         # default servicer fn just squares the number
         assert DerivedCls2().run.remote(3) == 9
 
 
-def test_rehydrate(client, servicer, reset_container_app):
+def test_rehydrate(client, servicer):
     # Issue introduced in #922 - brief description in #931
 
     # Sanity check that local calls work
     obj = Foo()
     assert obj.bar.local(7) == 343
 
-    # Deploy app to get an app id
-    app_id = deploy_app(app, "my-cls-app", client=client).app_id
+    # Deploy stub to get an app id
+    app_id = deploy_stub(stub, "my-cls-app", client=client).app_id
 
     # Initialize a container
-    container_app = RunningApp(app_id=app_id)
+    app = ContainerApp()
+    app.init(client, app_id, "stub")
 
-    # Associate app with app
-    app._init_container(client, container_app)
+    # Associate app with stub
+    app.associate_stub_container(stub)
 
     # Hydration shouldn't overwrite local function definition
     obj = Foo()
     assert obj.bar.local(7) == 343
 
 
-app_unhydrated = App()
+stub_unhydrated = Stub()
 
 
-@app_unhydrated.cls()
+@stub_unhydrated.cls()
 class FooUnhydrated:
     @method()
     def bar(self):
         ...
 
 
 def test_unhydrated():
     foo = FooUnhydrated()
     with pytest.raises(ExecutionError, match="hydrated"):
         foo.bar.remote(42)
 
 
-app_method_args = App()
+stub_method_args = Stub()
 
 
-@app_method_args.cls()
+@stub_method_args.cls()
 class XYZ:
     @method(keep_warm=3)
     def foo(self):
         ...
 
     @method(keep_warm=7)
     def bar(self):
         ...
 
 
 def test_method_args(servicer, client):
-    with app_method_args.run(client=client):
+    with stub_method_args.run(client=client):
         funcs = servicer.app_functions.values()
         assert [f.function_name for f in funcs] == ["XYZ.foo", "XYZ.bar"]
         assert [f.warm_pool_size for f in funcs] == [3, 7]
 
 
 class ClsWith1Method:
     @method()
@@ -465,21 +459,21 @@
 
     @method()
     def bar(self):
         ...
 
 
 def test_keep_warm_depr():
-    app = App()
+    stub = Stub()
 
     # This should be fine
-    app.cls(keep_warm=2)(ClsWith1Method)
+    stub.cls(keep_warm=2)(ClsWith1Method)
 
     with pytest.warns(DeprecationError, match="@method"):
-        app.cls(keep_warm=2)(ClsWith2Methods)
+        stub.cls(keep_warm=2)(ClsWith2Methods)
 
 
 class ClsWithHandlers:
     @build()
     def my_build(self):
         pass
 
@@ -513,33 +507,33 @@
     pfs = _find_partial_methods_for_cls(ClsWithHandlers, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
     assert list(pfs.keys()) == ["my_enter", "my_build_and_enter"]
 
     pfs = _find_partial_methods_for_cls(ClsWithHandlers, _PartialFunctionFlags.EXIT)
     assert list(pfs.keys()) == ["my_exit"]
 
 
-handler_app = App("handler-app")
+handler_stub = Stub("handler-stub")
 
 
 image = Image.debian_slim().pip_install("xyz")
 
 
-@handler_app.cls(image=image)
+@handler_stub.cls(image=image)
 class ClsWithBuild:
     @build()
     def build(self):
         pass
 
     @method()
     def method(self):
         pass
 
 
 def test_build_image(client, servicer):
-    with handler_app.run(client=client):
+    with handler_stub.run(client=client):
         f_def = servicer.app_functions[ClsWithBuild.method.object_id]
         # The function image should have added a new layer with original image as the parent
         f_image = servicer.images[f_def.image_id]
         assert f_image.base_images[0].image_id == image.object_id
 
 
 @pytest.mark.parametrize("decorator", [build, enter, exit])
@@ -578,17 +572,17 @@
         enter_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
     assert [meth() for meth in enter_methods.values()] == [42, 43]
 
     with pytest.warns(DeprecationError, match="Using `__exit__`.+`modal.exit` decorator"):
         exit_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.EXIT)
     assert [meth(None, None, None) for meth in exit_methods.values()] == [44, 45]
 
-    app = App("deprecated-sync-cls")
+    stub = Stub("deprecated-sync-cls")
     with pytest.warns(DeprecationError):
-        app.cls()(ClsWithDeprecatedSyncMethods)()
+        stub.cls()(ClsWithDeprecatedSyncMethods)()
 
 
 @pytest.mark.asyncio
 async def test_deprecated_async_methods():
     with pytest.warns(DeprecationError, match="Support for decorating parameterized methods with `@exit`"):
 
         class ClsWithDeprecatedAsyncMethods:
@@ -612,25 +606,25 @@
         enter_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.ENTER_POST_CHECKPOINT)
     assert [await meth() for meth in enter_methods.values()] == [42, 43]
 
     with pytest.warns(DeprecationError, match=r"Using `__aexit__`.+`modal.exit` decorator \(on an async method\)"):
         exit_methods: Dict[str, Callable] = _find_callables_for_obj(obj, _PartialFunctionFlags.EXIT)
     assert [await meth(None, None, None) for meth in exit_methods.values()] == [44, 45]
 
-    app = App("deprecated-async-cls")
+    stub = Stub("deprecated-async-cls")
     with pytest.warns(DeprecationError):
-        app.cls()(ClsWithDeprecatedAsyncMethods)()
+        stub.cls()(ClsWithDeprecatedAsyncMethods)()
 
 
 class HasSnapMethod:
     @enter(snap=True)
     def enter(self):
         pass
 
     @method()
     def f(self):
         pass
 
 
 def test_snap_method_without_snapshot_enabled():
     with pytest.raises(InvalidError, match="A class must have `enable_memory_snapshot=True`"):
-        app.cls(enable_memory_snapshot=False)(HasSnapMethod)
+        stub.cls(enable_memory_snapshot=False)(HasSnapMethod)
```

## test/config_test.py

```diff
@@ -4,15 +4,15 @@
 import pytest
 import subprocess
 import sys
 
 import toml
 
 import modal
-from modal.config import Config, _lookup_workspace, config
+from modal.config import _lookup_workspace, config
 
 
 def _cli(args, env={}):
     lib_dir = pathlib.Path(modal.__file__).parent.parent
     args = [sys.executable, "-m", "modal.cli.entry_point"] + args
     env = {
         **os.environ,
@@ -130,20 +130,8 @@
     config.override_locally(key, value)
     assert os.getenv(key) == value
 
 
 @pytest.mark.asyncio
 async def test_workspace_lookup(servicer, server_url_env):
     resp = await _lookup_workspace(servicer.remote_addr, "ak-abc", "as-xyz")
-    assert resp.username == "test-username"
-
-
-@pytest.mark.parametrize("automount", ["false", "'false'", "'False'", "'0'", 0, "''"])
-def test_config_boolean(modal_config, automount):
-    modal_toml = f"""
-    [prof-1]
-    token_id = 'ak-abc'
-    token_secret = 'as_xyz'
-    automount = {automount}
-    """
-    with modal_config(modal_toml):
-        assert not Config().get("automount", "prof-1")
+    assert resp.workspace_name == "test-workspace"
```

## test/conftest.py

```diff
@@ -12,35 +12,34 @@
 import sys
 import tempfile
 import textwrap
 import threading
 import traceback
 from collections import defaultdict
 from pathlib import Path
-from typing import Dict, Iterator, Optional, get_args
+from typing import Dict, Iterator, Optional
 
 import aiohttp.web
 import aiohttp.web_runner
 import grpclib.server
 import pkg_resources
 import pytest_asyncio
 from google.protobuf.empty_pb2 import Empty
 from grpclib import GRPCError, Status
 
 import modal._serialization
 from modal import __version__, config
-from modal._container_io_manager import _ContainerIOManager
 from modal._serialization import serialize_data_format
 from modal._utils.async_utils import asyncify, synchronize_api
 from modal._utils.grpc_testing import patch_mock_servicer
 from modal._utils.grpc_utils import find_free_port
 from modal._utils.http_utils import run_temporary_http_server
 from modal._vendor import cloudpickle
+from modal.app import _ContainerApp
 from modal.client import Client
-from modal.image import ImageBuilderVersion
 from modal.mount import client_mount_name
 from modal_proto import api_grpc, api_pb2
 
 
 @dataclasses.dataclass
 class VolumeFile:
     data: bytes
@@ -59,15 +58,14 @@
     # TODO(erikbern): add more annotations
     container_inputs: list[api_pb2.FunctionGetInputsResponse]
     container_outputs: list[api_pb2.FunctionPutOutputsRequest]
     fc_data_in: defaultdict[str, asyncio.Queue[api_pb2.DataChunk]]
     fc_data_out: defaultdict[str, asyncio.Queue[api_pb2.DataChunk]]
 
     def __init__(self, blob_host, blobs):
-        self.use_blob_outputs = False
         self.put_outputs_barrier = threading.Barrier(
             1, timeout=10
         )  # set to non-1 to get lock-step of output pushing within a test
         self.get_inputs_barrier = threading.Barrier(
             1, timeout=10
         )  # set to non-1 to get lock-step of input releases within a test
 
@@ -119,15 +117,14 @@
 
         self.task_result = None
 
         self.nfs_files: Dict[str, Dict[str, api_pb2.SharedVolumePutFileRequest]] = defaultdict(dict)
         self.volume_files: Dict[str, Dict[str, VolumeFile]] = defaultdict(dict)
         self.images = {}
         self.image_build_function_ids = {}
-        self.image_builder_versions = {}
         self.force_built_images = []
         self.fail_blob_create = []
         self.blob_create_metadata = None
         self.blob_multipart_threshold = 10_000_000
 
         self.precreated_functions = set()
         self.app_functions = {}
@@ -302,21 +299,14 @@
 
     async def AppHeartbeat(self, stream):
         request: api_pb2.AppHeartbeatRequest = await stream.recv_message()
         self.requests.append(request)
         self.app_heartbeats[request.app_id] += 1
         await stream.send_message(Empty())
 
-    async def AppList(self, stream):
-        await stream.recv_message()
-        apps = []
-        for app_name, app_id in self.deployed_apps.items():
-            apps.append(api_pb2.AppStats(name=app_name, description=app_name, app_id=app_id))
-        await stream.send_message(api_pb2.AppListResponse(apps=apps))
-
     ### Checkpoint
 
     async def ContainerCheckpoint(self, stream):
         request: api_pb2.ContainerCheckpointRequest = await stream.recv_message()
         self.requests.append(request)
         self.container_checkpoint_requests += 1
         await stream.send_message(Empty())
@@ -327,15 +317,16 @@
         req = await stream.recv_message()
         # This is used to test retry_transient_errors, see grpc_utils_test.py
         self.blob_create_metadata = stream.metadata
         if len(self.fail_blob_create) > 0:
             status_code = self.fail_blob_create.pop()
             raise GRPCError(status_code, "foobar")
         elif req.content_length > self.blob_multipart_threshold:
-            blob_id = await self.next_blob_id()
+            self.n_blobs += 1
+            blob_id = f"bl-{self.n_blobs}"
             num_parts = (req.content_length + self.blob_multipart_threshold - 1) // self.blob_multipart_threshold
             upload_urls = []
             for part_number in range(num_parts):
                 upload_url = f"{self.blob_host}/upload?blob_id={blob_id}&part_number={part_number}"
                 upload_urls.append(upload_url)
 
             await stream.send_message(
@@ -345,23 +336,19 @@
                         part_length=self.blob_multipart_threshold,
                         upload_urls=upload_urls,
                         completion_url=f"{self.blob_host}/complete_multipart?blob_id={blob_id}",
                     ),
                 )
             )
         else:
-            blob_id = await self.next_blob_id()
+            self.n_blobs += 1
+            blob_id = f"bl-{self.n_blobs}"
             upload_url = f"{self.blob_host}/upload?blob_id={blob_id}"
             await stream.send_message(api_pb2.BlobCreateResponse(blob_id=blob_id, upload_url=upload_url))
 
-    async def next_blob_id(self):
-        self.n_blobs += 1
-        blob_id = f"bl-{self.n_blobs}"
-        return blob_id
-
     async def BlobGet(self, stream):
         request: api_pb2.BlobGetRequest = await stream.recv_message()
         download_url = f"{self.blob_host}/download?blob_id={request.blob_id}"
         await stream.send_message(api_pb2.BlobGetResponse(download_url=download_url))
 
     ### Class
 
@@ -389,29 +376,28 @@
     ### Client
 
     async def ClientHello(self, stream):
         request: Empty = await stream.recv_message()
         self.requests.append(request)
         self.client_create_metadata = stream.metadata
         client_version = stream.metadata["x-modal-client-version"]
-        image_builder_version = max(get_args(ImageBuilderVersion))
-        warning = ""
         assert stream.user_agent.startswith(f"modal-client/{__version__} ")
         if stream.metadata.get("x-modal-token-id") == "bad":
             raise GRPCError(Status.UNAUTHENTICATED, "bad bad bad")
+        elif client_version == "timeout":
+            await asyncio.sleep(60)
+            await stream.send_message(api_pb2.ClientHelloResponse())
         elif client_version == "unauthenticated":
             raise GRPCError(Status.UNAUTHENTICATED, "failed authentication")
         elif client_version == "deprecated":
-            warning = "SUPER OLD"
-        elif client_version == "timeout":
-            await asyncio.sleep(60)
+            await stream.send_message(api_pb2.ClientHelloResponse(warning="SUPER OLD"))
         elif pkg_resources.parse_version(client_version) < pkg_resources.parse_version(__version__):
             raise GRPCError(Status.FAILED_PRECONDITION, "Old client")
-        resp = api_pb2.ClientHelloResponse(warning=warning, image_builder_version=image_builder_version)
-        await stream.send_message(resp)
+        else:
+            await stream.send_message(api_pb2.ClientHelloResponse())
 
     # Container
 
     async def ContainerHeartbeat(self, stream):
         request: api_pb2.ContainerHeartbeatRequest = await stream.recv_message()
         self.requests.append(request)
         # Return earlier than the usual 15-second heartbeat to avoid suspending tests.
@@ -606,19 +592,15 @@
         await stream.send_message(api_pb2.FunctionMapResponse(function_call_id=function_call_id))
 
     async def FunctionPutInputs(self, stream):
         request: api_pb2.FunctionPutInputsRequest = await stream.recv_message()
         response_items = []
         function_call_inputs = self.client_calls.setdefault(request.function_call_id, [])
         for item in request.inputs:
-            if item.input.WhichOneof("args_oneof") == "args":
-                args, kwargs = modal._serialization.deserialize(item.input.args, None)
-            else:
-                args, kwargs = modal._serialization.deserialize(self.blobs[item.input.args_blob_id], None)
-
+            args, kwargs = modal._serialization.deserialize(item.input.args, None) if item.input.args else ((), {})
             input_id = f"in-{self.n_inputs}"
             self.n_inputs += 1
             response_items.append(api_pb2.FunctionPutInputsResponseItem(input_id=input_id, idx=item.idx))
             function_call_inputs.append(((item.idx, input_id), (args, kwargs)))
         if self.slow_put_inputs:
             await asyncio.sleep(0.001)
         await stream.send_message(api_pb2.FunctionPutInputsResponse(inputs=response_items))
@@ -666,27 +648,21 @@
                 output_exc = api_pb2.FunctionGetOutputsItem(
                     input_id=input_id, idx=idx, result=result, gen_index=0, data_format=api_pb2.DATA_FORMAT_PICKLE
                 )
 
             if output_exc:
                 output = output_exc
             else:
-                serialized_data = serialize_data_format(result, result_data_format)
-                if self.use_blob_outputs:
-                    blob_id = await self.next_blob_id()
-                    self.blobs[blob_id] = serialized_data
-                    data_kwargs = {
-                        "data_blob_id": blob_id,
-                    }
-                else:
-                    data_kwargs = {"data": serialized_data}
                 output = api_pb2.FunctionGetOutputsItem(
                     input_id=input_id,
                     idx=idx,
-                    result=api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS, **data_kwargs),
+                    result=api_pb2.GenericResult(
+                        status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
+                        data=serialize_data_format(result, result_data_format),
+                    ),
                     data_format=result_data_format,
                 )
 
             await stream.send_message(api_pb2.FunctionGetOutputsResponse(outputs=[output]))
         else:
             await stream.send_message(api_pb2.FunctionGetOutputsResponse(outputs=[]))
 
@@ -726,15 +702,14 @@
     async def ImageGetOrCreate(self, stream):
         request: api_pb2.ImageGetOrCreateRequest = await stream.recv_message()
         idx = len(self.images) + 1
         image_id = f"im-{idx}"
 
         self.images[image_id] = request.image
         self.image_build_function_ids[image_id] = request.build_function_id
-        self.image_builder_versions[image_id] = request.builder_version
         if request.force_build:
             self.force_built_images.append(image_id)
         await stream.send_message(api_pb2.ImageGetOrCreateResponse(image_id=image_id))
 
     async def ImageJoinStreaming(self, stream):
         await stream.recv_message()
         task_log_1 = api_pb2.TaskLogs(data="hello, world\n", file_descriptor=api_pb2.FILE_DESCRIPTOR_INFO)
@@ -842,25 +817,14 @@
             values = []
         await stream.send_message(api_pb2.QueueGetResponse(values=values))
 
     async def QueueLen(self, stream):
         await stream.recv_message()
         await stream.send_message(api_pb2.QueueLenResponse(len=len(self.queue)))
 
-    async def QueueNextItems(self, stream):
-        request: api_pb2.QueueNextItemsRequest = await stream.recv_message()
-        next_item_idx = int(request.last_entry_id) + 1 if request.last_entry_id else 0
-        if next_item_idx < len(self.queue):
-            item = api_pb2.QueueItem(value=self.queue[next_item_idx], entry_id=f"{next_item_idx}")
-            await stream.send_message(api_pb2.QueueNextItemsResponse(items=[item]))
-        else:
-            if request.item_poll_timeout > 0:
-                await asyncio.sleep(0.1)
-            await stream.send_message(api_pb2.QueueNextItemsResponse(items=[]))
-
     ### Sandbox
 
     async def SandboxCreate(self, stream):
         request: api_pb2.SandboxCreateRequest = await stream.recv_message()
         if request.definition.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL:
             self.sandbox_is_interactive = True
 
@@ -1026,15 +990,15 @@
             await stream.send_message(api_pb2.SharedVolumeGetFileResponse(data_blob_id=put_req.data_blob_id))
         else:
             await stream.send_message(api_pb2.SharedVolumeGetFileResponse(data=put_req.data))
 
     async def SharedVolumeListFilesStream(self, stream):
         req: api_pb2.SharedVolumeListFilesRequest = await stream.recv_message()
         for path in self.nfs_files[req.shared_volume_id].keys():
-            entry = api_pb2.FileEntry(path=path)
+            entry = api_pb2.SharedVolumeListFilesEntry(path=path)
             response = api_pb2.SharedVolumeListFilesResponse(entries=[entry])
             await stream.send_message(response)
 
     ### Task
 
     async def TaskCurrentInputs(
         self, stream: "grpclib.server.Stream[Empty, api_pb2.TaskCurrentInputsResponse]"
@@ -1060,15 +1024,17 @@
             api_pb2.TokenFlowWaitResponse(
                 token_id="abc",
                 token_secret="xyz",
             )
         )
 
     async def WorkspaceNameLookup(self, stream):
-        await stream.send_message(api_pb2.WorkspaceNameLookupResponse(username="test-username"))
+        await stream.send_message(
+            api_pb2.WorkspaceNameLookupResponse(workspace_name="test-workspace", username="test-username")
+        )
 
     ### Tunnel
 
     async def TunnelStart(self, stream):
         request: api_pb2.TunnelStartRequest = await stream.recv_message()
         port = request.port
         await stream.send_message(api_pb2.TunnelStartResponse(host=f"{port}.modal.test", port=443))
@@ -1165,15 +1131,19 @@
         await stream.send_message(Empty())
 
     async def VolumeListFiles(self, stream):
         req = await stream.recv_message()
         if req.path != "**":
             raise NotImplementedError("Only '**' listing is supported.")
         for k, vol_file in self.volume_files[req.volume_id].items():
-            entries = [api_pb2.FileEntry(path=k, type=api_pb2.FileEntry.FileType.FILE, size=len(vol_file.data))]
+            entries = [
+                api_pb2.VolumeListFilesEntry(
+                    path=k, type=api_pb2.VolumeListFilesEntry.FileType.FILE, size=len(vol_file.data)
+                )
+            ]
             await stream.send_message(api_pb2.VolumeListFilesResponse(entries=entries))
 
     async def VolumePutFiles(self, stream):
         req = await stream.recv_message()
         for file in req.files:
             blob_data = self.files_sha2data[file.sha256_hex]
 
@@ -1388,15 +1358,15 @@
 
 
 @pytest.fixture(autouse=True)
 def reset_container_app():
     try:
         yield
     finally:
-        _ContainerIOManager._reset_singleton()
+        _ContainerApp._reset_container()
 
 
 @pytest.fixture
 def repo_root(request):
     return Path(request.config.rootdir)
 
 
@@ -1444,16 +1414,7 @@
 
     return mock_modal_toml
 
 
 @pytest.fixture
 def supports_dir(test_dir):
     return test_dir / Path("supports")
-
-
-@pytest_asyncio.fixture
-async def set_env_client(client):
-    try:
-        Client.set_env_client(client)
-        yield
-    finally:
-        Client.set_env_client(None)
```

## test/container_app_test.py

```diff
@@ -1,50 +1,32 @@
 # Copyright Modal Labs 2022
 import pytest
-from typing import Dict
 
-from google.protobuf.empty_pb2 import Empty
-from google.protobuf.message import Message
-
-from modal import App, interact
-from modal._container_io_manager import ContainerIOManager
-from modal.running_app import RunningApp
+from modal import Stub
+from modal.app import container_app
 from modal_proto import api_pb2
 
 from .supports.skip import skip_windows_unix_socket
 
 
 def my_f_1(x):
     pass
 
 
 @skip_windows_unix_socket
 @pytest.mark.asyncio
-async def test_container_function_lazily_imported(container_client):
-    tag_to_object_id: Dict[str, str] = {
+async def test_container_function_lazily_imported(unix_servicer, container_client):
+    unix_servicer.app_objects["ap-123"] = {
         "my_f_1": "fu-123",
         "my_d": "di-123",
     }
-    object_handle_metadata: Dict[str, Message] = {
-        "fu-123": api_pb2.FunctionHandleMetadata(),
-    }
-    container_app = RunningApp(
-        app_id="ap-123", tag_to_object_id=tag_to_object_id, object_handle_metadata=object_handle_metadata
-    )
-    app = App()
+    unix_servicer.app_functions["fu-123"] = api_pb2.Function()
+
+    await container_app.init.aio(container_client, "ap-123")
+    stub = Stub()
 
     # This is normally done in _container_entrypoint
-    app._init_container(container_client, container_app)
+    container_app.associate_stub_container(stub)
 
     # Now, let's create my_f after the app started running and make sure it works
-    my_f_container = app.function()(my_f_1)
+    my_f_container = stub.function()(my_f_1)
     assert await my_f_container.remote.aio(42) == 1764  # type: ignore
-
-
-@skip_windows_unix_socket
-def test_interact(container_client, unix_servicer):
-    # Initialize container singleton
-    ContainerIOManager(api_pb2.ContainerArguments(), container_client)
-
-    with unix_servicer.intercept() as ctx:
-        ctx.add_response("FunctionStartPtyShell", Empty())
-        interact()
```

## test/container_test.py

```diff
@@ -17,29 +17,29 @@
 from typing import Any, Dict, List, Optional, Tuple
 from unittest import mock
 from unittest.mock import MagicMock
 
 from grpclib import Status
 from grpclib.exceptions import GRPCError
 
-from modal import Client, is_local
+from modal import Client
 from modal._container_entrypoint import UserException, main
 from modal._serialization import (
     deserialize,
     deserialize_data_format,
     serialize,
     serialize_data_format,
 )
 from modal._utils import async_utils
-from modal.app import _App
 from modal.exception import InvalidError
 from modal.partial_function import enter
+from modal.stub import _Stub
 from modal_proto import api_pb2
 
-from .helpers import deploy_app_externally
+from .helpers import deploy_stub_externally
 from .supports.skip import skip_windows_signals, skip_windows_unix_socket
 
 EXTRA_TOLERANCE_DELAY = 2.0 if sys.platform == "linux" else 5.0
 FUNCTION_CALL_ID = "fc-123"
 SLEEP_DELAY = 0.1
 
 
@@ -195,16 +195,16 @@
             with pathlib.Path(tmp_file_name).open("w") as target:
                 json.dump({}, target)
             env["MODAL_RESTORE_STATE_PATH"] = tmp_file_name
 
             # Override server URL to reproduce restore behavior.
             env["MODAL_SERVER_URL"] = servicer.remote_addr
 
-        # reset _App tracking state between runs
-        _App._all_apps.clear()
+        # reset _Stub tracking state between runs
+        _Stub._all_stubs = {}
 
         try:
             with mock.patch.dict(os.environ, env):
                 main(container_args, client)
         except UserException:
             # Handle it gracefully
             pass
@@ -722,22 +722,22 @@
 
     assert stdout == ""
     assert stderr == ""
 
 
 @skip_windows_unix_socket
 def test_function_sibling_hydration(unix_servicer):
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(unix_servicer, "test.supports.functions", "check_sibling_hydration")
     assert _unwrap_scalar(ret) is None
 
 
 @skip_windows_unix_socket
 def test_multistub(unix_servicer, caplog):
-    deploy_app_externally(unix_servicer, "test.supports.multistub", "a")
+    deploy_stub_externally(unix_servicer, "test.supports.multistub", "a")
     ret = _run_container(unix_servicer, "test.supports.multistub", "a_func")
     assert _unwrap_scalar(ret) is None
     assert len(caplog.messages) == 0
     # Note that the stub can be inferred from the function, even though there are multiple
     # stubs present in the file
 
 
@@ -905,28 +905,27 @@
         inputs=_get_inputs(((3,), {})),
     )
     assert _unwrap_scalar(ret) == 6
 
 
 @skip_windows_unix_socket
 def test_call_function_that_calls_function(unix_servicer):
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(
         unix_servicer,
         "test.supports.functions",
         "cube",
         inputs=_get_inputs(((42,), {})),
     )
     assert _unwrap_scalar(ret) == 42**3
 
 
 @skip_windows_unix_socket
-def test_call_function_that_calls_method(unix_servicer, set_env_client):
-    # TODO (elias): Remove set_env_client fixture dependency - shouldn't need an env client here?
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+def test_call_function_that_calls_method(unix_servicer):
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(
         unix_servicer,
         "test.supports.functions",
         "function_calling_method",
         inputs=_get_inputs(((42, "abc", 123), {})),
     )
     assert _unwrap_scalar(ret) == 123**2  # servicer's implementation of function calling
@@ -1019,15 +1018,15 @@
     volume_commit_rpcs = [r for r in unix_servicer.requests if isinstance(r, api_pb2.VolumeCommitRequest)]
     assert len(volume_commit_rpcs) == 3
     assert _unwrap_scalar(ret) == 42**2
 
 
 @skip_windows_unix_socket
 def test_function_dep_hydration(unix_servicer):
-    deploy_app_externally(unix_servicer, "test.supports.functions", "app")
+    deploy_stub_externally(unix_servicer, "test.supports.functions", "stub")
     ret = _run_container(
         unix_servicer,
         "test.supports.functions",
         "check_dep_hydration",
         deps=["im-1", "vo-0", "im-1", "im-2", "vo-0", "vo-1"],
     )
     assert _unwrap_scalar(ret) is None
@@ -1072,15 +1071,15 @@
     # set up spys to track synchronicity calls to _translate_scalar_in/out
     translate_in_spy = MagicMock(wraps=synchronizer._translate_scalar_in)
     monkeypatch.setattr(synchronizer, "_translate_scalar_in", translate_in_spy)
     translate_out_spy = MagicMock(wraps=synchronizer._translate_scalar_out)
     monkeypatch.setattr(synchronizer, "_translate_scalar_out", translate_out_spy)
 
     # don't do blobbing for this test
-    monkeypatch.setattr("modal._container_io_manager.MAX_OBJECT_SIZE_BYTES", 1e100)
+    monkeypatch.setattr("modal._container_entrypoint.MAX_OBJECT_SIZE_BYTES", 1e100)
 
     large_data_list = list(range(int(1e6)))  # large data set
 
     t0 = time.perf_counter()
     # pr = cProfile.Profile()
     # pr.enable()
     _run_container(
@@ -1245,15 +1244,15 @@
 
     assert len(ret.items) == 2
     assert ret.items[0].result.status == api_pb2.GenericResult.GENERIC_STATUS_SUCCESS
 
 
 @skip_windows_unix_socket
 def test_container_heartbeat_survives_grpc_deadlines(servicer, caplog, monkeypatch):
-    monkeypatch.setattr("modal._container_io_manager.HEARTBEAT_INTERVAL", 0.01)
+    monkeypatch.setattr("modal._container_entrypoint.HEARTBEAT_INTERVAL", 0.01)
     num_heartbeats = 0
 
     async def heartbeat_responder(servicer, stream):
         nonlocal num_heartbeats
         num_heartbeats += 1
         await stream.recv_message()
         raise GRPCError(Status.DEADLINE_EXCEEDED)
@@ -1281,17 +1280,17 @@
     numcalls = 0
 
     async def custom_heartbeater(self):
         nonlocal numcalls
         numcalls += 1
         raise Exception("oops")
 
-    monkeypatch.setattr("modal._container_io_manager.HEARTBEAT_INTERVAL", 0.01)
+    monkeypatch.setattr("modal._container_entrypoint.HEARTBEAT_INTERVAL", 0.01)
     monkeypatch.setattr(
-        "modal._container_io_manager._ContainerIOManager._heartbeat_handle_cancellations", custom_heartbeater
+        "modal._container_entrypoint._FunctionIOManager._heartbeat_handle_cancellations", custom_heartbeater
     )
 
     ret = _run_container(
         servicer,
         "test.supports.functions",
         "delay",
         inputs=_get_inputs(((0.5,), {})),
@@ -1384,21 +1383,7 @@
     stdout, stderr = container_process.communicate(timeout=5)
 
     assert len(servicer.container_outputs) == 1
     assert container_process.returncode == 0
     assert "[events:enter_sync,enter_async,delay,exit_sync,exit_async]" in stdout.decode()
     assert "Traceback" not in stderr.decode()
     assert servicer.task_result is None
-
-
-@skip_windows_unix_socket
-def test_sandbox(unix_servicer, event_loop):
-    ret = _run_container(unix_servicer, "test.supports.functions", "sandbox_f")
-    assert _unwrap_scalar(ret) == "sb-123"
-
-
-@skip_windows_unix_socket
-def test_is_local(unix_servicer, event_loop):
-    assert is_local() == True
-
-    ret = _run_container(unix_servicer, "test.supports.functions", "is_local_f")
-    assert _unwrap_scalar(ret) == False
```

## test/cpu_test.py

```diff
@@ -1,23 +1,23 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App
+from modal import Stub
 from modal.exception import InvalidError
 
 
 def dummy():
     pass
 
 
 def test_cpu_lower_bound(client, servicer):
-    app = App()
+    stub = Stub()
 
-    app.function(cpu=0.0)(dummy)
+    stub.function(cpu=0.0)(dummy)
 
     with pytest.raises(InvalidError):
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
-    app.function(cpu=42)(dummy)
-    with app.run(client=client):
+    stub.function(cpu=42)(dummy)
+    with stub.run(client=client):
         pass
```

## test/decorator_test.py

```diff
@@ -1,85 +1,85 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App, asgi_app, method, web_endpoint, wsgi_app
+from modal import Stub, asgi_app, method, web_endpoint, wsgi_app
 from modal.exception import InvalidError
 
 
 def test_local_entrypoint_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="local_entrypoint()"):
 
-        @app.local_entrypoint  # type: ignore
+        @stub.local_entrypoint  # type: ignore
         def f():
             pass
 
 
 def test_function_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="function()"):
 
-        @app.function  # type: ignore
+        @stub.function  # type: ignore
         def f():
             pass
 
 
 def test_cls_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="cls()"):
 
-        @app.cls  # type: ignore
+        @stub.cls  # type: ignore
         class XYZ:
             pass
 
 
 def test_method_forgot_parentheses():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="method()"):
 
-        @app.cls()
+        @stub.cls()
         class XYZ:
             @method  # type: ignore
             def f(self):
                 pass
 
 
 def test_invalid_web_decorator_usage():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="web_endpoint()"):
 
-        @app.function()  # type: ignore
+        @stub.function()  # type: ignore
         @web_endpoint  # type: ignore
         def my_handle():
             pass
 
     with pytest.raises(InvalidError, match="asgi_app()"):
 
-        @app.function()  # type: ignore
+        @stub.function()  # type: ignore
         @asgi_app  # type: ignore
         def my_handle_asgi():
             pass
 
     with pytest.raises(InvalidError, match="wsgi_app()"):
 
-        @app.function()  # type: ignore
+        @stub.function()  # type: ignore
         @wsgi_app  # type: ignore
         def my_handle_wsgi():
             pass
 
 
 def test_web_endpoint_method():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError, match="remove the `@method`"):
 
-        @app.cls()
+        @stub.cls()
         class Container:
             @method()  # type: ignore
             @web_endpoint()
             def generate(self):
                 pass
```

## test/dict_test.py

```diff
@@ -27,16 +27,7 @@
     with Dict.ephemeral({"bar": 123}, client=client, _heartbeat_sleep=1) as d:
         d["foo"] = 42
         assert d.len() == 2
         assert d["foo"] == 42
         assert d["bar"] == 123
         time.sleep(1.5)  # Make time for 2 heartbeats
     assert servicer.n_dict_heartbeats == 2
-
-
-def test_dict_lazy_hydrate_named(set_env_client, servicer):
-    with servicer.intercept() as ctx:
-        d = Dict.from_name("foo", create_if_missing=True)
-        assert len(ctx.get_requests("DictGetOrCreate")) == 0  # sanity check that the get request is lazy
-        d["foo"] = 42
-        assert d["foo"] == 42
-        assert len(ctx.get_requests("DictGetOrCreate")) == 1  # just sanity check that object is only hydrated once...
```

## test/function_serialization_test.py

```diff
@@ -1,25 +1,25 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App
+from modal import Stub
 from modal._serialization import deserialize
 
 
 @pytest.mark.asyncio
 async def test_serialize_deserialize_function(servicer, client):
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True, name="foo")
+    @stub.function(serialized=True, name="foo")
     def foo():
         2 * foo.remote()
 
     assert foo.object_id is None
 
-    with app.run(client=client):
+    with stub.run(client=client):
         object_id = foo.object_id
 
     assert object_id is not None
     assert {object_id} == servicer.precreated_functions
 
     foo_def = servicer.app_functions[object_id]
```

## test/function_test.py

```diff
@@ -1,234 +1,134 @@
 # Copyright Modal Labs 2022
 import asyncio
 import inspect
-import os
 import pytest
 import time
 import typing
-from contextlib import contextmanager
 
 from synchronicity.exceptions import UserCodeException
 
 import modal
-from modal import App, Image, Mount, NetworkFileSystem, Proxy, web_endpoint
-from modal._utils.async_utils import synchronize_api
+from modal import Image, Mount, NetworkFileSystem, Proxy, Stub, web_endpoint
 from modal._vendor import cloudpickle
 from modal.exception import ExecutionError, InvalidError
 from modal.functions import Function, FunctionCall, gather
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-if os.environ.get("GITHUB_ACTIONS") == "true":
-    TIME_TOLERANCE = 0.25
-else:
-    TIME_TOLERANCE = 0.05
-
-
-@app.function()
+@stub.function()
 def foo(p, q):
     return p + q + 11  # not actually used in test (servicer returns sum of square of all args)
 
 
-@app.function()
+@stub.function()
 async def async_foo(p, q):
     return p + q + 12
 
 
 def dummy():
     pass  # not actually used in test (servicer returns sum of square of all args)
 
 
 def test_run_function(client, servicer):
     assert len(servicer.cleared_function_calls) == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         assert foo.remote(2, 4) == 20
         assert len(servicer.cleared_function_calls) == 1
 
 
 @pytest.mark.asyncio
 async def test_call_function_locally(client, servicer):
     assert foo.local(22, 44) == 77  # call it locally
     assert await async_foo.local(22, 44) == 78
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert foo.remote(2, 4) == 20
         assert async_foo.remote(2, 4) == 20
         assert await async_foo.remote.aio(2, 4) == 20
 
 
 @pytest.mark.parametrize("slow_put_inputs", [False, True])
 @pytest.mark.timeout(120)
 def test_map(client, servicer, slow_put_inputs):
     servicer.slow_put_inputs = slow_put_inputs
 
-    app = App()
-    dummy_modal = app.function()(dummy)
+    stub = Stub()
+    dummy_modal = stub.function()(dummy)
 
     assert len(servicer.cleared_function_calls) == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         assert list(dummy_modal.map([5, 2], [4, 3])) == [41, 13]
         assert len(servicer.cleared_function_calls) == 1
         assert set(dummy_modal.map([5, 2], [4, 3], order_outputs=False)) == {13, 41}
         assert len(servicer.cleared_function_calls) == 2
 
 
-@pytest.mark.asyncio
-async def test_map_async_generator(client):
-    app = App()
-    dummy_modal = app.function()(dummy)
-
-    async def gen_num():
-        yield 2
-        yield 3
-
-    async with app.run(client=client):
-        res = [num async for num in dummy_modal.map.aio(gen_num())]
-        assert res == [4, 9]
-
-
-def _pow2(x: int):
-    return x**2
-
-
-@contextmanager
-def synchronicity_loop_delay_tracker():
-    done = False
-
-    async def _track_eventloop_blocking():
-        max_dur = 0.0
-        BLOCK_TIME = 0.01
-        while not done:
-            t0 = time.perf_counter()
-            await asyncio.sleep(BLOCK_TIME)
-            max_dur = max(max_dur, time.perf_counter() - t0)
-        return max_dur - BLOCK_TIME  # if it takes exactly BLOCK_TIME we would have zero delay
-
-    track_eventloop_blocking = synchronize_api(_track_eventloop_blocking)
-    yield track_eventloop_blocking(_future=True)
-    done = True
-
-
-def test_map_blocking_iterator_blocking_synchronicity_loop(client):
-    app = App()
-    SLEEP_DUR = 0.5
-
-    def blocking_iter():
-        yield 1
-        time.sleep(SLEEP_DUR)
-        yield 2
-
-    pow2 = app.function()(_pow2)
-
-    with app.run(client=client):
-        t0 = time.monotonic()
-        with synchronicity_loop_delay_tracker() as max_delay:
-            for _ in pow2.map(blocking_iter()):
-                pass
-        dur = time.monotonic() - t0
-    assert dur >= SLEEP_DUR
-    assert max_delay.result() < TIME_TOLERANCE  # should typically be much smaller than this
-
-
-@pytest.mark.asyncio
-async def test_map_blocking_iterator_blocking_synchronicity_loop_async(client):
-    app = App()
-    SLEEP_DUR = 0.5
-
-    def blocking_iter():
-        yield 1
-        time.sleep(SLEEP_DUR)
-        yield 2
-
-    pow2 = app.function()(_pow2)
-
-    async with app.run(client=client):
-        t0 = time.monotonic()
-        with synchronicity_loop_delay_tracker() as max_delay:
-            async for _ in pow2.map.aio(blocking_iter()):
-                pass
-        dur = time.monotonic() - t0
-    assert dur >= SLEEP_DUR
-    assert max_delay.result() < TIME_TOLERANCE  # should typically be much smaller than this
-
-
 _side_effect_count = 0
 
 
 def side_effect(_):
     global _side_effect_count
     _side_effect_count += 1
 
 
 def test_for_each(client, servicer):
-    app = App()
-    side_effect_modal = app.function()(servicer.function_body(side_effect))
+    stub = Stub()
+    side_effect_modal = stub.function()(servicer.function_body(side_effect))
     assert _side_effect_count == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         side_effect_modal.for_each(range(10))
 
     assert _side_effect_count == 10
 
 
 def custom_function(x):
     if x % 2 == 0:
         return x
 
 
 def test_map_none_values(client, servicer):
-    app = App()
+    stub = Stub()
 
-    custom_function_modal = app.function()(servicer.function_body(custom_function))
+    custom_function_modal = stub.function()(servicer.function_body(custom_function))
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert list(custom_function_modal.map(range(4))) == [0, None, 2, None]
 
 
 def test_starmap(client):
-    app = App()
+    stub = Stub()
 
-    dummy_modal = app.function()(dummy)
-    with app.run(client=client):
+    dummy_modal = stub.function()(dummy)
+    with stub.run(client=client):
         assert list(dummy_modal.starmap([[5, 2], [4, 3]])) == [29, 25]
 
 
 def test_function_memory_request(client):
-    app = App()
-    app.function(memory=2048)(dummy)
-
-
-def test_function_memory_limit(client):
-    app = App()
-    f = app.function(memory=(2048, 4096))(dummy)
-
-    with app.run(client=client):
-        f.remote()
-
-    g = app.function(memory=(2048, 2048 - 1))(custom_function)
-    with pytest.raises(InvalidError), app.run(client=client):
-        g.remote()
+    stub = Stub()
+    stub.function(memory=2048)(dummy)
 
 
 def test_function_cpu_request(client):
-    app = App()
-    app.function(cpu=2.0)(dummy)
+    stub = Stub()
+    stub.function(cpu=2.0)(dummy)
 
 
 def later():
     return "hello"
 
 
 def test_function_future(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_modal = app.function()(servicer.function_body(later))
-    with app.run(client=client):
+    later_modal = stub.function()(servicer.function_body(later))
+    with stub.run(client=client):
         future = later_modal.spawn()
         assert isinstance(future, FunctionCall)
 
         servicer.function_is_running = True
         assert future.object_id == "fc-1"
 
         with pytest.raises(TimeoutError):
@@ -247,19 +147,19 @@
         assert "fc-2" in servicer.cancelled_calls
 
         assert future.object_id not in servicer.cleared_function_calls
 
 
 @pytest.mark.asyncio
 async def test_function_future_async(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_modal = app.function()(servicer.function_body(later))
+    later_modal = stub.function()(servicer.function_body(later))
 
-    async with app.run(client=client):
+    async with stub.run(client=client):
         future = await later_modal.spawn.aio()
         servicer.function_is_running = True
 
         with pytest.raises(TimeoutError):
             await future.get.aio(0.01)
 
         servicer.function_is_running = False
@@ -273,90 +173,90 @@
 
 async def async_later_gen():
     yield "foo"
 
 
 @pytest.mark.asyncio
 async def test_generator(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_gen_modal = app.function()(later_gen)
+    later_gen_modal = stub.function()(later_gen)
 
     def dummy():
         yield "bar"
         yield "baz"
         yield "boo"
 
     servicer.function_body(dummy)
 
     assert len(servicer.cleared_function_calls) == 0
-    with app.run(client=client):
+    with stub.run(client=client):
         assert later_gen_modal.is_generator
         res: typing.Generator = later_gen_modal.remote_gen()  # type: ignore
         # Generators fulfil the *iterator protocol*, which requires both these methods.
         # https://docs.python.org/3/library/stdtypes.html#typeiter
         assert hasattr(res, "__iter__")  # strangely inspect.isgenerator returns false
         assert hasattr(res, "__next__")
         assert next(res) == "bar"
         assert list(res) == ["baz", "boo"]
         assert len(servicer.cleared_function_calls) == 1
 
 
-def test_generator_map_invalid(client, servicer):
-    app = App()
+@pytest.mark.asyncio
+async def test_generator_map_invalid(client, servicer):
+    stub = Stub()
 
-    later_gen_modal = app.function()(later_gen)
+    later_gen_modal = stub.function()(later_gen)
 
     def dummy(x):
         yield x
 
     servicer.function_body(dummy)
 
-    with app.run(client=client):
-        with pytest.raises(InvalidError, match="A generator function cannot be called with"):
+    with stub.run(client=client):
+        with pytest.raises(InvalidError):
             # Support for .map() on generators was removed in version 0.57
             for _ in later_gen_modal.map([1, 2, 3]):
                 pass
-
-        with pytest.raises(InvalidError, match="A generator function cannot be called with"):
+        with pytest.raises(InvalidError):
             later_gen_modal.for_each([1, 2, 3])
 
 
 @pytest.mark.asyncio
 async def test_generator_async(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_gen_modal = app.function()(async_later_gen)
+    later_gen_modal = stub.function()(async_later_gen)
 
     async def async_dummy():
         yield "bar"
         yield "baz"
 
     servicer.function_body(async_dummy)
 
     assert len(servicer.cleared_function_calls) == 0
-    async with app.run(client=client):
+    async with stub.run(client=client):
         assert later_gen_modal.is_generator
         res = later_gen_modal.remote_gen.aio()
         # Async generators fulfil the *asynchronous iterator protocol*, which requires both these methods.
         # https://peps.python.org/pep-0525/#support-for-asynchronous-iteration-protocol
         assert hasattr(res, "__aiter__")
         assert hasattr(res, "__anext__")
         # TODO(Jonathon): This works outside of testing, but here gives:
         # `TypeError: cannot pickle 'async_generator' object`
         # await res.__anext__() == "bar"
         # assert len(servicer.cleared_function_calls) == 1
 
 
 @pytest.mark.asyncio
 async def test_generator_future(client, servicer):
-    app = App()
+    stub = Stub()
 
-    later_gen_modal = app.function()(later_gen)
-    with app.run(client=client):
+    later_gen_modal = stub.function()(later_gen)
+    with stub.run(client=client):
         assert later_gen_modal.spawn() is None  # until we have a nice interface for polling generator futures
 
 
 def gen_with_arg(i):
     yield "foo"
 
 
@@ -364,58 +264,58 @@
     # need to use async function body in client test to run stuff in parallel
     # but calling interface is still non-asyncio
     await asyncio.sleep(sleep_seconds)
     return sleep_seconds
 
 
 def test_sync_parallelism(client, servicer):
-    app = App()
+    stub = Stub()
 
-    slo1_modal = app.function()(servicer.function_body(slo1))
-    with app.run(client=client):
+    slo1_modal = stub.function()(servicer.function_body(slo1))
+    with stub.run(client=client):
         t0 = time.time()
         # NOTE tests breaks in macOS CI if the smaller time is smaller than ~300ms
         res = gather(slo1_modal.spawn(0.31), slo1_modal.spawn(0.3))
         t1 = time.time()
         assert res == [0.31, 0.3]  # results should be ordered as inputs, not by completion time
         assert t1 - t0 < 0.6  # less than the combined runtime, make sure they run in parallel
 
 
 def test_proxy(client, servicer):
-    app = App()
+    stub = Stub()
 
-    app.function(proxy=Proxy.from_name("my-proxy"))(dummy)
-    with app.run(client=client):
+    stub.function(proxy=Proxy.from_name("my-proxy"))(dummy)
+    with stub.run(client=client):
         pass
 
 
 class CustomException(Exception):
     pass
 
 
 def failure():
     raise CustomException("foo!")
 
 
 def test_function_exception(client, servicer):
-    app = App()
+    stub = Stub()
 
-    failure_modal = app.function()(servicer.function_body(failure))
-    with app.run(client=client):
+    failure_modal = stub.function()(servicer.function_body(failure))
+    with stub.run(client=client):
         with pytest.raises(CustomException) as excinfo:
             failure_modal.remote()
         assert "foo!" in str(excinfo.value)
 
 
 @pytest.mark.asyncio
 async def test_function_exception_async(client, servicer):
-    app = App()
+    stub = Stub()
 
-    failure_modal = app.function()(servicer.function_body(failure))
-    async with app.run(client=client):
+    failure_modal = stub.function()(servicer.function_body(failure))
+    async with stub.run(client=client):
         with pytest.raises(CustomException) as excinfo:
             coro = failure_modal.remote.aio()
             assert inspect.isawaitable(
                 coro
             )  # mostly for mypy, since output could technically be an async generator which isn't awaitable in the same sense
             await coro
         assert "foo!" in str(excinfo.value)
@@ -424,19 +324,19 @@
 def custom_exception_function(x):
     if x == 4:
         raise CustomException("bad")
     return x * x
 
 
 def test_map_exceptions(client, servicer):
-    app = App()
+    stub = Stub()
 
-    custom_function_modal = app.function()(servicer.function_body(custom_exception_function))
+    custom_function_modal = stub.function()(servicer.function_body(custom_exception_function))
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert list(custom_function_modal.map(range(4))) == [0, 1, 4, 9]
 
         with pytest.raises(CustomException) as excinfo:
             list(custom_function_modal.map(range(6)))
         assert "bad" in str(excinfo.value)
 
         res = list(custom_function_modal.map(range(6), return_exceptions=True))
@@ -445,56 +345,56 @@
 
 
 def import_failure():
     raise ImportError("attempted relative import with no known parent package")
 
 
 def test_function_relative_import_hint(client, servicer):
-    app = App()
+    stub = Stub()
 
-    import_failure_modal = app.function()(servicer.function_body(import_failure))
+    import_failure_modal = stub.function()(servicer.function_body(import_failure))
 
-    with app.run(client=client):
+    with stub.run(client=client):
         with pytest.raises(ImportError) as excinfo:
             import_failure_modal.remote()
         assert "HINT" in str(excinfo.value)
 
 
 def test_nonglobal_function():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError) as excinfo:
 
-        @app.function()
+        @stub.function()
         def f():
             pass
 
     assert "global scope" in str(excinfo.value)
 
 
 def test_non_global_serialized_function():
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     def f():
         pass
 
 
 def test_closure_valued_serialized_function(client, servicer):
-    app = App()
+    stub = Stub()
 
     def make_function(s):
-        @app.function(name=f"ret_{s}", serialized=True)
+        @stub.function(name=f"ret_{s}", serialized=True)
         def returner():
             return s
 
     for s in ["foo", "bar"]:
         make_function(s)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
     functions = {}
     for func in servicer.app_functions.values():
         functions[func.function_name] = cloudpickle.loads(func.function_serialized)
 
     assert len(functions) == 2
@@ -504,104 +404,104 @@
 
 def test_new_hydrated_internal(client, servicer):
     obj = FunctionCall._new_hydrated("fc-123", client, None)
     assert obj.object_id == "fc-123"
 
 
 def test_from_id(client, servicer):
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @web_endpoint()
     def foo():
         pass
 
-    deploy_app(app, "dummy", client=client)
+    deploy_stub(stub, "dummy", client=client)
 
     function_id = foo.object_id
     assert function_id
     assert foo.web_url
 
     function_call = foo.spawn()
     assert function_call.object_id
     # Used in a few examples to construct FunctionCall objects
     rehydrated_function_call = FunctionCall.from_id(function_call.object_id, client)
     assert rehydrated_function_call.object_id == function_call.object_id
 
 
-lc_app = App()
+lc_stub = Stub()
 
 
-@lc_app.function()
+@lc_stub.function()
 def f(x):
     return x**2
 
 
 def test_allow_cross_region_volumes(client, servicer):
-    app = App()
+    stub = Stub()
     vol1 = NetworkFileSystem.from_name("xyz-1", create_if_missing=True)
     vol2 = NetworkFileSystem.from_name("xyz-2", create_if_missing=True)
     # Should pass flag for all the function's NetworkFileSystemMounts
-    app.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(dummy)
+    stub.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert len(servicer.app_functions) == 1
         for func in servicer.app_functions.values():
             assert len(func.shared_volume_mounts) == 2
             for svm in func.shared_volume_mounts:
                 assert svm.allow_cross_region
 
 
 def test_allow_cross_region_volumes_webhook(client, servicer):
     # TODO(erikbern): this test seems a bit redundant
-    app = App()
+    stub = Stub()
     vol1 = NetworkFileSystem.from_name("xyz-1", create_if_missing=True)
     vol2 = NetworkFileSystem.from_name("xyz-2", create_if_missing=True)
     # Should pass flag for all the function's NetworkFileSystemMounts
-    app.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(
+    stub.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(
         web_endpoint()(dummy)
     )
 
-    with app.run(client=client):
+    with stub.run(client=client):
         assert len(servicer.app_functions) == 1
         for func in servicer.app_functions.values():
             assert len(func.shared_volume_mounts) == 2
             for svm in func.shared_volume_mounts:
                 assert svm.allow_cross_region
 
 
 def test_serialize_deserialize_function_handle(servicer, client):
     from modal._serialization import deserialize, serialize
 
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @web_endpoint()
     def my_handle():
         pass
 
     with pytest.raises(InvalidError, match="hasn't been created"):
         serialize(my_handle)  # handle is not "live" yet! should not be serializable yet
 
-    with app.run(client=client):
+    with stub.run(client=client):
         blob = serialize(my_handle)
 
         rehydrated_function_handle = deserialize(blob, client)
         assert rehydrated_function_handle.object_id == my_handle.object_id
         assert isinstance(rehydrated_function_handle, Function)
         assert rehydrated_function_handle.web_url == "http://xyz.internal"
 
 
 def test_default_cloud_provider(client, servicer, monkeypatch):
-    app = App()
+    stub = Stub()
 
     monkeypatch.setenv("MODAL_DEFAULT_CLOUD", "oci")
-    app.function()(dummy)
-    with app.run(client=client):
-        object_id: str = app.indexed_objects["dummy"].object_id
+    stub.function()(dummy)
+    with stub.run(client=client):
+        object_id: str = stub.indexed_objects["dummy"].object_id
         f = servicer.app_functions[object_id]
 
     assert f.cloud_provider == api_pb2.CLOUD_PROVIDER_OCI
 
 
 def test_not_hydrated():
     with pytest.raises(ExecutionError):
@@ -611,86 +511,86 @@
 def test_invalid_large_serialization(client):
     big_data = b"1" * 500000
 
     def f():
         return big_data
 
     with pytest.warns(UserWarning, match="larger than the recommended limit"):
-        app = App()
-        app.function(serialized=True)(f)
-        with app.run(client=client):
+        stub = Stub()
+        stub.function(serialized=True)(f)
+        with stub.run(client=client):
             pass
 
     bigger_data = b"1" * 50000000
 
     def g():
         return bigger_data
 
     with pytest.raises(InvalidError):
-        app = App()
-        app.function(serialized=True)(g)
-        with app.run(client=client):
+        stub = Stub()
+        stub.function(serialized=True)(g)
+        with stub.run(client=client):
             pass
 
 
 def test_call_unhydrated_function():
     with pytest.raises(ExecutionError, match="hydrated"):
         foo.remote(123)
 
 
 def test_deps_explicit(client, servicer):
-    app = App()
+    stub = Stub()
 
     image = Image.debian_slim()
     nfs_1 = NetworkFileSystem.from_name("nfs-1", create_if_missing=True)
     nfs_2 = NetworkFileSystem.from_name("nfs-2", create_if_missing=True)
 
-    app.function(image=image, network_file_systems={"/nfs_1": nfs_1, "/nfs_2": nfs_2})(dummy)
+    stub.function(image=image, network_file_systems={"/nfs_1": nfs_1, "/nfs_2": nfs_2})(dummy)
 
-    with app.run(client=client):
-        object_id: str = app.indexed_objects["dummy"].object_id
+    with stub.run(client=client):
+        object_id: str = stub.indexed_objects["dummy"].object_id
         f = servicer.app_functions[object_id]
 
     dep_object_ids = set(d.object_id for d in f.object_dependencies)
     assert dep_object_ids == set([image.object_id, nfs_1.object_id, nfs_2.object_id])
 
 
 nfs = NetworkFileSystem.from_name("my-persisted-nfs", create_if_missing=True)
 
 
 def dummy_closurevars():
     nfs.listdir("/")
 
 
 def test_deps_closurevars(client, servicer):
-    app = App()
+    stub = Stub()
 
     image = Image.debian_slim()
-    modal_f = app.function(image=image)(dummy_closurevars)
+    modal_f = stub.function(image=image)(dummy_closurevars)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         f = servicer.app_functions[modal_f.object_id]
 
     assert set(d.object_id for d in f.object_dependencies) == set([nfs.object_id, image.object_id])
 
 
 def assert_is_wrapped_dict(some_arg):
     assert type(some_arg) == modal.Dict  # this should not be a modal._Dict unwrapped instance!
     return some_arg
 
 
 def test_calls_should_not_unwrap_modal_objects(servicer, client):
     some_modal_object = modal.Dict.lookup("blah", create_if_missing=True, client=client)
 
-    app = App()
-    foo = app.function()(assert_is_wrapped_dict)
+    stub = Stub()
+    foo = stub.function()(assert_is_wrapped_dict)
     servicer.function_body(assert_is_wrapped_dict)
 
     # make sure the serialized object is an actual Dict and not a _Dict in all user code contexts
-    with app.run(client=client):
+    with stub.run(client=client):
         assert type(foo.remote(some_modal_object)) == modal.Dict
         fc = foo.spawn(some_modal_object)
         assert type(fc.get()) == modal.Dict
         for ret in foo.map([some_modal_object]):
             assert type(ret) == modal.Dict
         for ret in foo.starmap([[some_modal_object]]):
             assert type(ret) == modal.Dict
@@ -703,89 +603,51 @@
     assert type(some_arg) == modal.Dict  # this should not be a modal._Dict unwrapped instance!
     yield some_arg
 
 
 def test_calls_should_not_unwrap_modal_objects_gen(servicer, client):
     some_modal_object = modal.Dict.lookup("blah", create_if_missing=True, client=client)
 
-    app = App()
-    foo = app.function()(assert_is_wrapped_dict_gen)
+    stub = Stub()
+    foo = stub.function()(assert_is_wrapped_dict_gen)
     servicer.function_body(assert_is_wrapped_dict_gen)
 
     # make sure the serialized object is an actual Dict and not a _Dict in all user code contexts
-    with app.run(client=client):
+    with stub.run(client=client):
         assert type(next(foo.remote_gen(some_modal_object))) == modal.Dict
         foo.spawn(some_modal_object)  # spawn on generator returns None, but starts the generator
 
     assert len(servicer.client_calls) == 2
 
 
 def test_mount_deps_have_ids(client, servicer, monkeypatch, test_dir):
     # This test can possibly break if a function's deps diverge between
     # local and remote environments
     monkeypatch.syspath_prepend(test_dir / "supports")
-    app = App()
-    app.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
+    stub = Stub()
+    stub.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
 
     with servicer.intercept() as ctx:
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
     function_create = ctx.pop_request("FunctionCreate")
     for dep in function_create.function.object_dependencies:
         assert dep.object_id
 
 
 def test_no_state_reuse(client, servicer, supports_dir):
     # two separate instances of the same mount content - triggers deduplication logic
     mount_instance_1 = Mount.from_local_file(supports_dir / "pyproject.toml")
     mount_instance_2 = Mount.from_local_file(supports_dir / "pyproject.toml")
 
-    app = App("reuse-mount-app")
-    app.function(mounts=[mount_instance_1, mount_instance_2])(dummy)
+    stub = Stub("reuse-mount-stub")
+    stub.function(mounts=[mount_instance_1, mount_instance_2])(dummy)
 
-    deploy_app(app, client=client, show_progress=False)
+    deploy_stub(stub, client=client, show_progress=False)
     first_deploy = {mount_instance_1.object_id, mount_instance_2.object_id}
 
-    deploy_app(app, client=client, show_progress=False)
+    deploy_stub(stub, client=client, show_progress=False)
     second_deploy = {mount_instance_1.object_id, mount_instance_2.object_id}
 
     # mount ids should not overlap between first and second deploy
     assert not (first_deploy & second_deploy)
-
-
-@pytest.mark.asyncio
-async def test_map_large_inputs(client, servicer, monkeypatch, blob_server):
-    # TODO: tests making use of mock blob server currently have to be async, since the
-    #  blob server runs as an async pytest fixture which will have its event loop blocked
-    #  by the test itself otherwise... Should move to its own thread.
-    monkeypatch.setattr("modal.functions.MAX_OBJECT_SIZE_BYTES", 1)
-    servicer.use_blob_outputs = True
-    app = App()
-    dummy_modal = app.function()(dummy)
-
-    _, blobs = blob_server
-    async with app.run.aio(client=client):
-        assert len(blobs) == 0
-        assert [a async for a in dummy_modal.map.aio(range(100))] == [i**2 for i in range(100)]
-        assert len(servicer.cleared_function_calls) == 1
-
-    assert len(blobs) == 200  # inputs + outputs
-
-
-@pytest.mark.asyncio
-async def test_non_aio_map_in_async_caller_error(client):
-    dummy_function = app.function()(dummy)
-
-    with app.run(client=client):
-        with pytest.raises(InvalidError, match=".map.aio"):
-            for _ in dummy_function.map([1, 2, 3]):
-                pass
-
-        # using .aio should be ok:
-        res = [r async for r in dummy_function.map.aio([1, 2, 3])]
-        assert res == [1, 4, 9]
-
-        # we might want to deprecate this syntax (async for ... in map without .aio),
-        # but we support it for backwards compatibility for now:
-        res = [r async for r in dummy_function.map([1, 2, 4])]
-        assert res == [1, 4, 16]
```

## test/gpu_test.py

```diff
@@ -1,109 +1,109 @@
 # Copyright Modal Labs 2022
 import pytest
 
-from modal import App
+from modal import Stub
 from modal.exception import DeprecationError, InvalidError
 from modal_proto import api_pb2
 
 
 def dummy():
     pass  # not actually used in test (servicer returns sum of square of all args)
 
 
 def test_gpu_true_function(client, servicer):
-    app = App()
+    stub = Stub()
 
     with pytest.raises(DeprecationError):
-        app.function(gpu=True)(dummy)
+        stub.function(gpu=True)(dummy)
 
 
 def test_gpu_any_function(client, servicer):
-    app = App()
+    stub = Stub()
 
-    app.function(gpu="any")(dummy)
-    with app.run(client=client):
+    stub.function(gpu="any")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_ANY
 
 
 def test_gpu_string_config(client, servicer):
-    app = App()
+    stub = Stub()
 
     # Invalid enum value.
     with pytest.raises(InvalidError):
-        app.function(gpu="foo")(dummy)
+        stub.function(gpu="foo")(dummy)
 
-    app.function(gpu="A100")(dummy)
-    with app.run(client=client):
+    stub.function(gpu="A100")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
 
 
 def test_gpu_string_count_config(client, servicer):
-    app = App()
+    stub = Stub()
 
     # Invalid count values.
     with pytest.raises(InvalidError):
-        app.function(gpu="A10G:hello")(dummy)
+        stub.function(gpu="A10G:hello")(dummy)
     with pytest.raises(InvalidError):
-        app.function(gpu="Nonexistent:2")(dummy)
+        stub.function(gpu="Nonexistent:2")(dummy)
 
-    app.function(gpu="A10G:4")(dummy)
-    with app.run(client=client):
+    stub.function(gpu="A10G:4")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 4
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A10G
 
 
 def test_gpu_config_function(client, servicer):
     import modal
 
-    app = App()
+    stub = Stub()
 
-    app.function(gpu=modal.gpu.A100())(dummy)
-    with app.run(client=client):
+    stub.function(gpu=modal.gpu.A100())(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
 
 
 def test_cloud_provider_selection(client, servicer):
     import modal
 
-    app = App()
+    stub = Stub()
 
-    app.function(gpu=modal.gpu.A100(), cloud="gcp")(dummy)
-    with app.run(client=client):
+    stub.function(gpu=modal.gpu.A100(), cloud="gcp")(dummy)
+    with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 1
     func_def = next(iter(servicer.app_functions.values()))
     assert func_def.cloud_provider == api_pb2.CLOUD_PROVIDER_GCP
 
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
 
     # Invalid enum value.
     with pytest.raises(InvalidError):
-        app.function(cloud="foo")(dummy)
+        stub.function(cloud="foo")(dummy)
 
 
 @pytest.mark.parametrize(
     "memory_arg,gpu_type,memory_gb",
     [
         (0, api_pb2.GPU_TYPE_A100, 40),
         (40, api_pb2.GPU_TYPE_A100, 40),
@@ -111,49 +111,49 @@
         ("40GB", api_pb2.GPU_TYPE_A100, 40),
         ("80GB", api_pb2.GPU_TYPE_A100_80GB, 80),
     ],
 )
 def test_memory_selection_gpu_variant(client, servicer, memory_arg, gpu_type, memory_gb):
     import modal
 
-    app = App()
+    stub = Stub()
     if isinstance(memory_arg, int):
-        app.function(gpu=modal.gpu.A100(memory=memory_arg))(dummy)
+        stub.function(gpu=modal.gpu.A100(memory=memory_arg))(dummy)
     elif isinstance(memory_arg, str):
-        app.function(gpu=modal.gpu.A100(size=memory_arg))(dummy)
+        stub.function(gpu=modal.gpu.A100(size=memory_arg))(dummy)
     else:
         raise RuntimeError(f"Unexpected test parameterization arg type {type(memory_arg)}")
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
     func_def = next(iter(servicer.app_functions.values()))
 
     assert func_def.resources.gpu_config.count == 1
     assert func_def.resources.gpu_config.type == gpu_type
     assert func_def.resources.gpu_config.memory == memory_gb
 
 
 def test_a100_20gb_gpu_unsupported():
     import modal
 
-    app = App()
+    stub = Stub()
 
     with pytest.raises(ValueError, match="A100 20GB is unsupported, consider"):
-        app.function(gpu=modal.gpu.A100(memory=20))(dummy)
+        stub.function(gpu=modal.gpu.A100(memory=20))(dummy)
 
 
 @pytest.mark.parametrize("count", [1, 2, 3, 4])
 def test_gpu_type_selection_from_count(client, servicer, count):
     import modal
 
-    app = App()
+    stub = Stub()
 
     # Task type does not change when user asks more than 1 GPU on an A100.
-    app.function(gpu=modal.gpu.A100(count=count))(dummy)
-    with app.run(client=client):
+    stub.function(gpu=modal.gpu.A100(count=count))(dummy)
+    with stub.run(client=client):
         pass
 
     func_def = next(iter(servicer.app_functions.values()))
 
     assert func_def.resources.gpu_config.count == count
     assert func_def.resources.gpu_config.type == api_pb2.GPU_TYPE_A100
```

## test/helpers.py

```diff
@@ -2,45 +2,41 @@
 import os
 import pathlib
 import subprocess
 import sys
 from typing import Optional
 
 
-def deploy_app_externally(
+def deploy_stub_externally(
     servicer,
     file_or_module: str,
-    app_variable: Optional[str] = None,
+    stub_variable: Optional[str] = None,
     deployment_name="Deployment",
     cwd=None,
     env={},
     capture_output=True,
 ) -> Optional[str]:
-    # deploys a app from another interpreter to prevent leaking state from client into a container process (apart from what goes through the servicer)
+    # deploys a stub from another interpreter to prevent leaking state from client into a container process (apart from what goes through the servicer)
     # also has the advantage that no modules imported by the test files themselves will be added to sys.modules and included in mounts etc.
     windows_support: dict[str, str] = {}
 
     if sys.platform == "win32":
         windows_support = {
             **os.environ.copy(),
             **{"PYTHONUTF8": "1"},
         }  # windows apparently needs a bunch of env vars to start python...
 
     env = {**windows_support, "MODAL_SERVER_URL": servicer.remote_addr, **env}
     if cwd is None:
         cwd = pathlib.Path(__file__).parent.parent
 
-    app_ref = file_or_module if app_variable is None else f"{file_or_module}::{app_variable}"
+    stub_ref = file_or_module if stub_variable is None else f"{file_or_module}::{stub_variable}"
 
     p = subprocess.Popen(
-        [sys.executable, "-m", "modal.cli.entry_point", "deploy", app_ref, "--name", deployment_name],
+        [sys.executable, "-m", "modal.cli.entry_point", "deploy", stub_ref, "--name", deployment_name],
         cwd=cwd,
         env=env,
         stderr=subprocess.STDOUT,
         stdout=subprocess.PIPE if capture_output else None,
     )
-    stdout_b, stderr_b = p.communicate()
-    stdout_s, stderr_s = (b.decode() if b is not None else None for b in (stdout_b, stderr_b))
-    if p.returncode != 0:
-        print(f"Deploying app failed!\n### stdout ###\n{stdout_s}\n### stderr ###\n{stderr_s}")
-        raise Exception("Test helper failed to deploy app")
-    return stdout_s
+    stdout, _ = p.communicate()
+    return stdout.decode("utf8")
```

## test/image_test.py

```diff
@@ -1,38 +1,32 @@
 # Copyright Modal Labs 2022
 import os
 import pytest
-import re
 import sys
 import threading
 from hashlib import sha256
 from tempfile import NamedTemporaryFile
-from typing import List, Literal, get_args
+from typing import List
 from unittest import mock
 
-from modal import App, Image, Mount, Secret, build, gpu, method
+from modal import Image, Mount, Secret, Stub, build, gpu, method
 from modal._serialization import serialize
-from modal.client import Client
-from modal.exception import DeprecationError, InvalidError, VersionError
-from modal.image import (
-    SUPPORTED_PYTHON_SERIES,
-    ImageBuilderVersion,
-    _dockerhub_debian_codename,
-    _dockerhub_python_version,
-    _get_modal_requirements_path,
-    _validate_python_version,
-)
-from modal.mount import PYTHON_STANDALONE_VERSIONS
+from modal.exception import DeprecationError, InvalidError
+from modal.image import _dockerhub_python_version, _get_client_requirements_path
 from modal_proto import api_pb2
 
 from .supports.skip import skip_windows
 
 
-def test_supported_python_series():
-    assert SUPPORTED_PYTHON_SERIES == PYTHON_STANDALONE_VERSIONS.keys()
+def test_python_version():
+    assert _dockerhub_python_version("3.9.1") == "3.9.1"
+    assert _dockerhub_python_version("3.9") == "3.9.15"
+    v = _dockerhub_python_version().split(".")
+    assert len(v) == 3
+    assert (int(v[0]), int(v[1])) == sys.version_info[:2]
 
 
 def get_image_layers(image_id: str, servicer) -> List[api_pb2.Image]:
     """Follow pointers to the previous image recursively in the servicer's list of images,
     and return a list of image layers from top to bottom."""
 
     result = []
@@ -48,380 +42,273 @@
             break
 
         image_id = image.base_images[0].image_id
 
     return result
 
 
-def get_all_dockerfile_commands(image_id: str, servicer) -> str:
-    layers = get_image_layers(image_id, servicer)
-    return "\n".join([cmd for layer in layers for cmd in layer.dockerfile_commands])
-
-
-@pytest.fixture(params=get_args(ImageBuilderVersion))
-def builder_version(request, server_url_env, modal_config):
-    version = request.param
-    with modal_config():
-        with mock.patch("test.conftest.ImageBuilderVersion", Literal[version]):  # type: ignore
-            yield version
-
-
-def test_python_version_validation():
-    assert _validate_python_version(None) == "{0}.{1}".format(*sys.version_info)
-    assert _validate_python_version("3.12") == "3.12"
-    assert _validate_python_version("3.12.0") == "3.12.0"
-
-    with pytest.raises(InvalidError, match="Unsupported Python version"):
-        _validate_python_version("3.7")
-
-    with pytest.raises(InvalidError, match="Python version must be specified as a string"):
-        _validate_python_version(3.10)  # type: ignore
-
-    with pytest.raises(InvalidError, match="Invalid Python version"):
-        _validate_python_version("3.10.2.9")
-
-    with pytest.raises(InvalidError, match="Invalid Python version"):
-        _validate_python_version("3.10.x")
-
-    with pytest.raises(InvalidError, match="Python version must be specified as 'major.minor'"):
-        _validate_python_version("3.10.5", allow_micro_granularity=False)
-
-
-def test_dockerhub_python_version(builder_version):
-    assert _dockerhub_python_version(builder_version, "3.9.1") == "3.9.1"
-
-    expected_39_full = {"2023.12": "3.9.15", "2024.04": "3.9.19"}[builder_version]
-    assert _dockerhub_python_version(builder_version, "3.9") == expected_39_full
-
-    v = _dockerhub_python_version(builder_version, None).split(".")
-    assert len(v) == 3
-    assert (int(v[0]), int(v[1])) == sys.version_info[:2]
-
-
-def test_image_base(builder_version, servicer, client, test_dir):
-    app = App()
-    constructors = [
-        (Image.debian_slim, ()),
-        (Image.from_registry, ("ubuntu",)),
-        (Image.from_dockerfile, (test_dir / "supports" / "test-dockerfile",)),
-        (Image.conda, ()),
-        (Image.micromamba, ()),
-    ]
-    for meth, args in constructors:
-        app.image = meth(*args)  # type: ignore
-        with app.run(client=client):
-            commands = get_all_dockerfile_commands(app.image.object_id, servicer)
-            assert "COPY /modal_requirements.txt /modal_requirements.txt" in commands
-            if builder_version == "2023.12":
-                assert "pip install -r /modal_requirements.txt" in commands
-            else:
-                assert "pip install --no-cache --no-deps -r /modal_requirements.txt" in commands
-                assert "rm /modal_requirements.txt" in commands
-
-
-@pytest.mark.parametrize("python_version", [None, "3.10", "3.11.4"])
-def test_python_version(builder_version, servicer, client, python_version):
-    local_python = "{0}.{1}".format(*sys.version_info)
-    expected_python = local_python if python_version is None else python_version
-
-    app = App()
-    app.image = Image.debian_slim() if python_version is None else Image.debian_slim(python_version)
-    expected_dockerhub_python = _dockerhub_python_version(builder_version, expected_python)
-    expected_dockerhub_debian = _dockerhub_debian_codename(builder_version)
-    assert expected_dockerhub_python.startswith(expected_python)
-    with app.run(client):
-        commands = get_all_dockerfile_commands(app.image.object_id, servicer)
-        assert re.match(rf"FROM python:{expected_dockerhub_python}-slim-{expected_dockerhub_debian}", commands)
-
-    for constructor in [Image.conda, Image.micromamba]:
-        app.image = constructor() if python_version is None else constructor(python_version)
-        if python_version is None and builder_version == "2023.12":
-            expected_python = "3.9"
-        with app.run(client):
-            commands = get_all_dockerfile_commands(app.image.object_id, servicer)
-            assert re.search(rf"install.* python={expected_python}", commands)
-
-
-def test_image_python_packages(builder_version, servicer, client):
-    app = App()
-    app.image = (
+def test_image_python_packages(client, servicer):
+    stub = Stub()
+    stub.image = (
         Image.debian_slim()
         .pip_install("sklearn[xyz]")
         .pip_install("numpy", "scipy", extra_index_url="https://xyz", find_links="https://abc?q=123", pre=True)
     )
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert any("pip install 'sklearn[xyz]'" in cmd for cmd in layers[1].dockerfile_commands)
         assert any(
             "pip install numpy scipy --find-links 'https://abc?q=123' --extra-index-url https://xyz --pre" in cmd
             for cmd in layers[0].dockerfile_commands
         )
 
 
-def test_image_kwargs_validation(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_commands(
+def test_image_kwargs_validation(servicer, client):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_commands(
         "echo hi", secrets=[Secret.from_dict({"xyz": "123"}), Secret.from_name("foo")]
     )
     with pytest.raises(InvalidError):
-        app.image = Image.debian_slim().run_commands(
+        stub.image = Image.debian_slim().run_commands(
             "echo hi",
             secrets=[
                 Secret.from_dict({"xyz": "123"}),
                 Secret.from_name("foo"),
                 Mount.from_local_dir("/", remote_path="/"),  # type: ignore
             ],  # Mount is not a valid Secret
         )
 
-    app = App()
-    app.image = Image.debian_slim().copy_local_dir("/", remote_path="/dummy")
-    app.image = Image.debian_slim().copy_mount(Mount.from_name("foo"), remote_path="/dummy")
+    stub = Stub()
+    stub.image = Image.debian_slim().copy_local_dir("/", remote_path="/dummy")
+    stub.image = Image.debian_slim().copy_mount(Mount.from_name("foo"), remote_path="/dummy")
     with pytest.raises(InvalidError):
         # Secret is not a valid Mount
-        app.image = Image.debian_slim().copy_mount(Secret.from_dict({"xyz": "123"}), remote_path="/dummy")  # type: ignore
+        stub.image = Image.debian_slim().copy_mount(Secret.from_dict({"xyz": "123"}), remote_path="/dummy")  # type: ignore
 
 
-def test_wrong_type(builder_version, servicer, client):
+def test_wrong_type(servicer, client):
     image = Image.debian_slim()
     for m in [image.pip_install, image.apt_install, image.run_commands]:
         m(["xyz"])  # type: ignore
         m("xyz")  # type: ignore
         m("xyz", ["def", "foo"], "ghi")  # type: ignore
         with pytest.raises(InvalidError):
             m(3)  # type: ignore
         with pytest.raises(InvalidError):
             m([3])  # type: ignore
         with pytest.raises(InvalidError):
             m([["double-nested-package"]])  # type: ignore
 
 
-def test_image_requirements_txt(builder_version, servicer, client):
+def test_image_requirements_txt(servicer, client):
     requirements_txt = os.path.join(os.path.dirname(__file__), "supports/test-requirements.txt")
 
-    app = App()
-    app.image = Image.debian_slim().pip_install_from_requirements(requirements_txt)
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install_from_requirements(requirements_txt)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("COPY /.requirements.txt /.requirements.txt" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("pip install -r /.requirements.txt" in cmd for cmd in layers[0].dockerfile_commands)
         assert any(b"banana" in f.data for f in layers[0].context_files)
 
 
-def test_empty_install(builder_version, servicer, client):
+def test_empty_install(servicer, client):
     # Install functions with no packages should be ignored.
-    app = App(
+    stub = Stub(
         image=Image.debian_slim()
         .pip_install()
         .pip_install([], [], [], [])
         .apt_install([])
         .run_commands()
         .conda_install()
     )
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert len(layers) == 1
 
 
-def test_debian_slim_apt_install(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().pip_install("numpy").apt_install("git", "ssh").pip_install("scikit-learn"))
+def test_debian_slim_apt_install(servicer, client):
+    stub = Stub(image=Image.debian_slim().pip_install("numpy").apt_install("git", "ssh").pip_install("scikit-learn"))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("pip install scikit-learn" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("apt-get install -y git ssh" in cmd for cmd in layers[1].dockerfile_commands)
         assert any("pip install numpy" in cmd for cmd in layers[2].dockerfile_commands)
 
 
-def test_image_pip_install_pyproject(builder_version, servicer, client):
+def test_image_pip_install_pyproject(servicer, client):
     pyproject_toml = os.path.join(os.path.dirname(__file__), "supports/test-pyproject.toml")
 
-    app = App()
-    app.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml)
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         print(layers[0].dockerfile_commands)
         assert any("pip install 'banana >=1.2.0' 'potato >=0.1.0'" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-def test_image_pip_install_pyproject_with_optionals(builder_version, servicer, client):
+def test_image_pip_install_pyproject_with_optionals(servicer, client):
     pyproject_toml = os.path.join(os.path.dirname(__file__), "supports/test-pyproject.toml")
 
-    app = App()
-    app.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml, optional_dependencies=["dev", "test"])
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install_from_pyproject(pyproject_toml, optional_dependencies=["dev", "test"])
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         print(layers[0].dockerfile_commands)
         assert any(
             "pip install 'banana >=1.2.0' 'linting-tool >=0.0.0' 'potato >=0.1.0' 'pytest >=1.2.0'" in cmd
             for cmd in layers[0].dockerfile_commands
         )
         assert not (any("'mkdocs >=1.4.2'" in cmd for cmd in layers[0].dockerfile_commands))
 
 
-def test_image_pip_install_private_repos(builder_version, servicer, client):
-    app = App()
+def test_image_pip_install_private_repos(servicer, client):
+    stub = Stub()
     with pytest.raises(InvalidError):
-        app.image = Image.debian_slim().pip_install_private_repos(
+        stub.image = Image.debian_slim().pip_install_private_repos(
             "github.com/ecorp/private-one@1.0.0",
             git_user="erikbern",
             secrets=[],  # Invalid: missing secret
         )
 
     bad_repo_refs = [
         "ecorp/private-one@1.0.0",
         "gitspace.com/corp/private-one@1.0.0",
     ]
     for invalid_ref in bad_repo_refs:
         with pytest.raises(InvalidError):
-            app.image = Image.debian_slim().pip_install_private_repos(
+            stub.image = Image.debian_slim().pip_install_private_repos(
                 invalid_ref,
                 git_user="erikbern",
                 secrets=[Secret.from_name("test-gh-read")],
             )
 
-    app.image = Image.debian_slim().pip_install_private_repos(
+    stub.image = Image.debian_slim().pip_install_private_repos(
         "github.com/corp/private-one@1.0.0",
         "gitlab.com/corp2/private-two@0.0.2",
         git_user="erikbern",
         secrets=[
             Secret.from_dict({"GITHUB_TOKEN": "not-a-secret"}),
             Secret.from_dict({"GITLAB_TOKEN": "not-a-secret"}),
         ],
     )
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert len(layers[0].secret_ids) == 2
         assert any(
             'pip install "git+https://erikbern:$GITHUB_TOKEN@github.com/corp/private-one@1.0.0"' in cmd
             for cmd in layers[0].dockerfile_commands
         )
         assert any(
             'pip install "git+https://erikbern:$GITLAB_TOKEN@gitlab.com/corp2/private-two@0.0.2"' in cmd
             for cmd in layers[0].dockerfile_commands
         )
 
 
-def test_conda_install(builder_version, servicer, client):
-    app = App(image=Image.conda().pip_install("numpy").conda_install("pymc3", "theano").pip_install("scikit-learn"))
+def test_conda_install(servicer, client):
+    stub = Stub(image=Image.conda().pip_install("numpy").conda_install("pymc3", "theano").pip_install("scikit-learn"))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("pip install scikit-learn" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("conda install pymc3 theano --yes" in cmd for cmd in layers[1].dockerfile_commands)
         assert any("pip install numpy" in cmd for cmd in layers[2].dockerfile_commands)
 
 
-def test_dockerfile_image(builder_version, servicer, client):
+def test_dockerfile_image(servicer, client):
     path = os.path.join(os.path.dirname(__file__), "supports/test-dockerfile")
 
-    app = App(image=Image.from_dockerfile(path))
+    stub = Stub(image=Image.from_dockerfile(path))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("RUN pip install numpy" in cmd for cmd in layers[1].dockerfile_commands)
 
 
-def test_conda_update_from_environment(builder_version, servicer, client):
+def test_conda_update_from_environment(servicer, client):
     path = os.path.join(os.path.dirname(__file__), "supports/test-conda-environment.yml")
 
-    app = App(image=Image.conda().conda_update_from_environment(path))
+    stub = Stub(image=Image.conda().conda_update_from_environment(path))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("RUN conda env update" in cmd for cmd in layers[0].dockerfile_commands)
         assert any(b"foo=1.0" in f.data for f in layers[0].context_files)
         assert any(b"bar=2.1" in f.data for f in layers[0].context_files)
 
 
-def test_run_commands(builder_version, servicer, client):
-    base = Image.debian_slim()
+def test_dockerhub_install(servicer, client):
+    stub = Stub(image=Image.from_registry("gisops/valhalla:latest", setup_dockerfile_commands=["RUN apt-get update"]))
 
-    command = "echo 'Hello Modal'"
-    app = App(image=base.run_commands(command))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
-        assert layers[0].dockerfile_commands[1] == f"RUN {command}"
-
-    commands = ["echo 'Hello world'", "touch agi.yaml"]
-    for image in [base.run_commands(commands), base.run_commands(*commands)]:
-        app = App(image=image)
-        with app.run(client=client):
-            layers = get_image_layers(app.image.object_id, servicer)
-            for i, cmd in enumerate(commands, 1):
-                assert layers[0].dockerfile_commands[i] == f"RUN {cmd}"
-
-
-def test_dockerhub_install(builder_version, servicer, client):
-    app = App(image=Image.from_registry("gisops/valhalla:latest", setup_dockerfile_commands=["RUN apt-get update"]))
-
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("FROM gisops/valhalla:latest" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("RUN apt-get update" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-def test_ecr_install(builder_version, servicer, client):
+def test_ecr_install(servicer, client):
     image_tag = "000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:latest"
-    app = App(
+    stub = Stub(
         image=Image.from_aws_ecr(
             image_tag,
             setup_dockerfile_commands=["RUN apt-get update"],
             secret=Secret.from_dict({"AWS_ACCESS_KEY_ID": "", "AWS_SECRET_ACCESS_KEY": ""}),
         )
     )
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any(f"FROM {image_tag}" in cmd for cmd in layers[0].dockerfile_commands)
         assert any("RUN apt-get update" in cmd for cmd in layers[0].dockerfile_commands)
 
 
 def run_f():
     print("foo!")
 
 
-def test_image_run_function(builder_version, servicer, client):
-    app = App()
-    app.image = (
+def test_image_run_function(client, servicer):
+    stub = Stub()
+    stub.image = (
         Image.debian_slim().pip_install("pandas").run_function(run_f, secrets=[Secret.from_dict({"xyz": "123"})])
     )
 
-    with app.run(client=client):
-        image_id = app.image.object_id
+    with stub.run(client=client):
+        image_id = stub.image.object_id
         layers = get_image_layers(image_id, servicer)
         assert "foo!" in layers[0].build_function.definition
         assert "Secret.from_dict([xyz])" in layers[0].build_function.definition
         # globals is none when no globals are referenced
         assert layers[0].build_function.globals == b""
 
     function_id = servicer.image_build_function_ids[image_id]
     assert function_id
     assert servicer.app_functions[function_id].function_name == "run_f"
     assert len(servicer.app_functions[function_id].secret_ids) == 1
 
 
-def test_image_run_function_interactivity(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().pip_install("pandas").run_function(run_f)
+def test_image_run_function_interactivity(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().pip_install("pandas").run_function(run_f)
 
-    from modal.runner import run_app
+    from modal.runner import run_stub
 
-    with run_app(app, client=client, shell=True):
-        image_id = app.image.object_id
+    with run_stub(stub, client=client, shell=True):
+        image_id = stub.image.object_id
         layers = get_image_layers(image_id, servicer)
         assert "foo!" in layers[0].build_function.definition
 
     function_id = servicer.image_build_function_ids[image_id]
     assert function_id
     assert servicer.app_functions[function_id].function_name == "run_f"
     assert not servicer.app_functions[function_id].pty_info.enabled
@@ -431,70 +318,70 @@
 VARIABLE_2 = 3
 
 
 def run_f_globals():
     print("foo!", VARIABLE_1)
 
 
-def test_image_run_function_globals(builder_version, servicer, client):
+def test_image_run_function_globals(client, servicer):
     global VARIABLE_1, VARIABLE_2
 
-    app = App()
-    app.image = Image.debian_slim().run_function(run_f_globals)
+    stub = Stub()
+    stub.image = Image.debian_slim().run_function(run_f_globals)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         old_globals = layers[0].build_function.globals
         assert b"VARIABLE_1" in old_globals
         assert b"VARIABLE_2" not in old_globals
 
     VARIABLE_1 = 3
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].build_function.globals != old_globals
 
     VARIABLE_1 = 1
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].build_function.globals == old_globals
 
 
 VARIABLE_3 = threading.Lock()
 VARIABLE_4 = "bar"
 
 
 def run_f_unserializable_globals():
     print("foo!", VARIABLE_3, VARIABLE_4)
 
 
-def test_image_run_unserializable_function(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_function(run_f_unserializable_globals)
+def test_image_run_unserializable_function(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_function(run_f_unserializable_globals)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         old_globals = layers[0].build_function.globals
         assert b"VARIABLE_4" in old_globals
 
 
 def run_f_with_args(arg, *, kwarg):
     print("building!", arg, kwarg)
 
 
-def test_image_run_function_with_args(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_function(run_f_with_args, args=("foo",), kwargs={"kwarg": "bar"})
+def test_image_run_function_with_args(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_function(run_f_with_args, args=("foo",), kwargs={"kwarg": "bar"})
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         input = layers[0].build_function.input
         assert input.args == serialize((("foo",), {"kwarg": "bar"}))
 
 
-def test_poetry(builder_version, servicer, client):
+def test_poetry(client, servicer):
     path = os.path.join(os.path.dirname(__file__), "supports/pyproject.toml")
 
     # No lockfile provided and there's no lockfile found
     # TODO we deferred the exception until _load runs, not sure how to test that here
     # with pytest.raises(NotFoundError):
     #     Image.debian_slim().poetry_install_from_file(path)
 
@@ -502,126 +389,126 @@
     Image.debian_slim().poetry_install_from_file(path, ignore_lockfile=True)
 
     # Provide lockfile explicitly - this should also work
     lockfile_path = os.path.join(os.path.dirname(__file__), "supports/special_poetry.lock")
     image = Image.debian_slim().poetry_install_from_file(path, lockfile_path)
 
     # Build iamge
-    app = App()
-    app.image = image
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub()
+    stub.image = image
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         context_files = {f.filename for layer in layers for f in layer.context_files}
         assert context_files == {"/.poetry.lock", "/.pyproject.toml", "/modal_requirements.txt"}
 
 
 @pytest.fixture
 def tmp_path_with_content(tmp_path):
     (tmp_path / "data.txt").write_text("hello")
     (tmp_path / "data").mkdir()
     (tmp_path / "data" / "sub").write_text("world")
     return tmp_path
 
 
-def test_image_copy_local_dir(builder_version, servicer, client, tmp_path_with_content):
-    app = App()
-    app.image = Image.debian_slim().copy_local_dir(tmp_path_with_content, remote_path="/dummy")
+def test_image_copy_local_dir(client, servicer, tmp_path_with_content):
+    stub = Stub()
+    stub.image = Image.debian_slim().copy_local_dir(tmp_path_with_content, remote_path="/dummy")
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert "COPY . /dummy" in layers[0].dockerfile_commands
         assert set(servicer.mount_contents["mo-1"].keys()) == {"/data.txt", "/data/sub"}
 
 
-def test_image_docker_command_copy(builder_version, servicer, client, tmp_path_with_content):
-    app = App()
+def test_image_docker_command_copy(client, servicer, tmp_path_with_content):
+    stub = Stub()
     data_mount = Mount.from_local_dir(tmp_path_with_content, remote_path="/")
-    app.image = Image.debian_slim().dockerfile_commands(["COPY . /dummy"], context_mount=data_mount)
+    stub.image = Image.debian_slim().dockerfile_commands(["COPY . /dummy"], context_mount=data_mount)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert "COPY . /dummy" in layers[0].dockerfile_commands
         files = {f.mount_filename: f.content for f in Mount._get_files(data_mount.entries)}
         assert files == {"/data.txt": b"hello", "/data/sub": b"world"}
 
 
-def test_image_dockerfile_copy(builder_version, servicer, client, tmp_path_with_content):
+def test_image_dockerfile_copy(client, servicer, tmp_path_with_content):
     dockerfile = NamedTemporaryFile("w", delete=False)
     dockerfile.write("COPY . /dummy\n")
     dockerfile.close()
 
-    app = App()
+    stub = Stub()
     data_mount = Mount.from_local_dir(tmp_path_with_content, remote_path="/")
-    app.image = Image.debian_slim().from_dockerfile(dockerfile.name, context_mount=data_mount)
+    stub.image = Image.debian_slim().from_dockerfile(dockerfile.name, context_mount=data_mount)
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert "COPY . /dummy" in layers[1].dockerfile_commands
         files = {f.mount_filename: f.content for f in Mount._get_files(data_mount.entries)}
         assert files == {"/data.txt": b"hello", "/data/sub": b"world"}
 
 
-def test_image_env(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().env({"HELLO": "world!"}))
+def test_image_env(client, servicer):
+    stub = Stub(image=Image.debian_slim().env({"HELLO": "world!"}))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert any("ENV HELLO=" in cmd and "world!" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-def test_image_gpu(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().run_commands("echo 0"))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+def test_image_gpu(client, servicer):
+    stub = Stub(image=Image.debian_slim().run_commands("echo 0"))
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].gpu_config.type == api_pb2.GPU_TYPE_UNSPECIFIED
 
     with pytest.warns(DeprecationError):
-        app = App(image=Image.debian_slim().run_commands("echo 1", gpu=True))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+        stub = Stub(image=Image.debian_slim().run_commands("echo 1", gpu=True))
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].gpu_config.type == api_pb2.GPU_TYPE_ANY
 
-    app = App(image=Image.debian_slim().run_commands("echo 2", gpu=gpu.A10G()))
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    stub = Stub(image=Image.debian_slim().run_commands("echo 2", gpu=gpu.A10G()))
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
         assert layers[0].gpu_config.type == api_pb2.GPU_TYPE_A10G
 
 
-def test_image_force_build(builder_version, servicer, client):
-    app = App()
-    app.image = Image.debian_slim().run_commands("echo 1").pip_install("foo", force_build=True).run_commands("echo 2")
-    with app.run(client=client):
+def test_image_force_build(client, servicer):
+    stub = Stub()
+    stub.image = Image.debian_slim().run_commands("echo 1").pip_install("foo", force_build=True).run_commands("echo 2")
+    with stub.run(client=client):
         assert servicer.force_built_images == ["im-3", "im-4"]
 
-    app.image = (
+    stub.image = (
         Image.from_gcp_artifact_registry("foo", force_build=True)
         .run_commands("python_packagesecho 1")
         .pip_install("foo", force_build=True)
         .run_commands("echo 2")
     )
-    with app.run(client=client):
+    with stub.run(client=client):
         assert servicer.force_built_images == ["im-3", "im-4", "im-5", "im-6", "im-7", "im-8"]
 
 
-def test_workdir(builder_version, servicer, client):
-    app = App(image=Image.debian_slim().workdir("/foo/bar"))
+def test_workdir(servicer, client):
+    stub = Stub(image=Image.debian_slim().workdir("/foo/bar"))
 
-    with app.run(client=client):
-        layers = get_image_layers(app.image.object_id, servicer)
+    with stub.run(client=client):
+        layers = get_image_layers(stub.image.object_id, servicer)
 
         assert any("WORKDIR /foo/bar" in cmd for cmd in layers[0].dockerfile_commands)
 
 
-cls_app = App()
+cls_stub = Stub()
 
 VARIABLE_5 = 1
 VARIABLE_6 = 1
 
 
-@cls_app.cls(
+@cls_stub.cls(
     image=Image.debian_slim().pip_install("pandas"),
     secrets=[Secret.from_dict({"xyz": "123"})],
 )
 class Foo:
     @build()
     def build_func(self):
         global VARIABLE_5
@@ -632,15 +519,15 @@
     def f(self):
         global VARIABLE_6
 
         print("bar!", VARIABLE_6)
 
 
 def test_image_build_snapshot(client, servicer):
-    with cls_app.run(client=client):
+    with cls_stub.run(client=client):
         image_id = list(servicer.images.keys())[-1]
         layers = get_image_layers(image_id, servicer)
 
         assert "foo!" in layers[0].build_function.definition
         assert "Secret.from_dict([xyz])" in layers[0].build_function.definition
         assert any("pip install pandas" in cmd for cmd in layers[1].dockerfile_commands)
 
@@ -671,17 +558,18 @@
             raise ImportError("bar")
 
         # non-ImportErrors should trigger a warning
         with pytest.warns(match="ImportError"):
             with image_2.imports():
                 raise Exception("foo")
 
-        # Old one raises
-        with pytest.raises(DeprecationError, match="imports()"):
-            image_1.run_inside()
+        # Make sure run_inside works but is depreated
+        with pytest.warns(DeprecationError, match="imports()"):
+            with image_1.run_inside():
+                pass
 
         # Hydration of the image should raise the exception
         with pytest.raises(ImportError, match="foo"):
             image_1._hydrate("im-123", client, None)
 
         # Should not raise since it's a different image
         image_2._hydrate("im-456", client, None)
@@ -702,66 +590,33 @@
                 raise ImportError("baz")
 
         # We're not inside this image so this should be swallowed
         with image_2.imports():
             raise ImportError("bar")
 
 
-@pytest.mark.parametrize("python_version", ["3.11", "3.12", "3.12.1", "3.12.1-gnu"])
-def test_get_modal_requirements_path(builder_version, python_version):
-    path = _get_modal_requirements_path(builder_version, python_version)
-    if builder_version == "2023.12" and python_version.startswith("3.12"):
-        assert path.endswith("2023.12.312.txt")
-    else:
-        assert path.endswith(f"{builder_version}.txt")
-
-
-def test_image_builder_version(servicer, test_dir, modal_config):
-    app = App(image=Image.debian_slim())
-    # TODO use a single with statement and tuple of managers when we drop Py3.8
-    test_requirements = str(test_dir / "supports" / "test-requirements.txt")
-    with mock.patch("modal.image._get_modal_requirements_path", lambda *_, **__: test_requirements):
-        with mock.patch("modal.image._dockerhub_python_version", lambda *_, **__: "3.11.0"):
-            with mock.patch("modal.image._dockerhub_debian_codename", lambda *_, **__: "bullseye"):
-                with mock.patch("test.conftest.ImageBuilderVersion", Literal["2000.01"]):
-                    with mock.patch("modal.image.ImageBuilderVersion", Literal["2000.01"]):
-                        with Client(
-                            servicer.remote_addr, api_pb2.CLIENT_TYPE_CONTAINER, ("ak-123", "as-xyz")
-                        ) as client:
-                            with modal_config():
-                                with app.run(client=client):
-                                    assert servicer.image_builder_versions
-                                    for version in servicer.image_builder_versions.values():
-                                        assert version == "2000.01"
-
-
-def test_image_builder_supported_versions(servicer):
-    app = App(image=Image.debian_slim())
-    # TODO use a single with statement and tuple of managers when we drop Py3.8
-    with pytest.raises(VersionError, match=r"This version of the modal client supports.+{'2000.01'}"):
-        with mock.patch("modal.image.ImageBuilderVersion", Literal["2000.01"]):
-            with mock.patch("test.conftest.ImageBuilderVersion", Literal["2023.11"]):
-                with Client(servicer.remote_addr, api_pb2.CLIENT_TYPE_CONTAINER, ("ak-123", "as-xyz")) as client:
-                    with app.run(client=client):
-                        pass
-
-
-@pytest.fixture
-def force_2023_12(modal_config):
-    with mock.patch("test.conftest.ImageBuilderVersion", Literal["2023.12"]):
-        with modal_config():
-            yield
+@pytest.mark.parametrize(
+    "version,expected",
+    [
+        ("3.12", "requirements.312.txt"),
+        ("3.12.1", "requirements.312.txt"),
+        ("3.12.1-gnu", "requirements.312.txt"),
+    ],
+)
+def test_get_client_requirements_path(version, expected):
+    path = _get_client_requirements_path(version)
+    assert os.path.basename(path) == expected
 
 
 @skip_windows("Different hash values for context file paths")
-def test_image_stability_on_2023_12(force_2023_12, servicer, client, test_dir):
+def test_image_stability_on_2023_12(servicer, client, test_dir):
     def get_hash(img: Image) -> str:
-        app = App(image=img)
-        with app.run(client=client):
-            layers = get_image_layers(app.image.object_id, servicer)
+        stub = Stub(image=img)
+        with stub.run(client=client):
+            layers = get_image_layers(stub.image.object_id, servicer)
             commands = [layer.dockerfile_commands for layer in layers]
             context_files = [[(f.filename, f.data) for f in layer.context_files] for layer in layers]
         return sha256(repr(list(zip(commands, context_files))).encode()).hexdigest()
 
     if sys.version_info[:2] == (3, 11):
         # Matches my development environment default is to match Python version from local system
         img = Image.debian_slim()
```

## test/live_reload_test.py

```diff
@@ -2,78 +2,78 @@
 import asyncio
 import pytest
 import threading
 import time
 from unittest import mock
 
 from modal import Function
-from modal.serving import serve_app
+from modal.serving import serve_stub
 
-from .supports.app_run_tests.webhook import app
+from .supports.app_run_tests.webhook import stub
 from .supports.skip import skip_windows
 
 
 @pytest.fixture
-def app_ref(test_dir):
+def stub_ref(test_dir):
     return str(test_dir / "supports" / "app_run_tests" / "webhook.py")
 
 
 @pytest.mark.asyncio
-async def test_live_reload(app_ref, server_url_env, servicer):
-    async with serve_app.aio(app, app_ref):
+async def test_live_reload(stub_ref, server_url_env, servicer):
+    async with serve_stub.aio(stub, stub_ref):
         await asyncio.sleep(3.0)
     assert servicer.app_set_objects_count == 1
     assert servicer.app_client_disconnect_count == 1
     assert servicer.app_get_logs_initial_count == 1
 
 
 @skip_windows("live-reload not supported on windows")
-def test_file_changes_trigger_reloads(app_ref, server_url_env, servicer):
+def test_file_changes_trigger_reloads(stub_ref, server_url_env, servicer):
     watcher_done = threading.Event()
 
     async def fake_watch():
         for i in range(3):
             yield {"/some/file"}
         watcher_done.set()
 
-    with serve_app(app, app_ref, _watcher=fake_watch()):
+    with serve_stub(stub, stub_ref, _watcher=fake_watch()):
         watcher_done.wait()  # wait until watcher loop is done
 
     # TODO ideally we would assert the specific expected number here, but this test
     # is consistently flaking in CI and I cannot reproduce locally to debug.
     # I'm relaxing the assertion for now to stop the test from blocking deployments.
     # assert servicer.app_set_objects_count == 4  # 1 + number of file changes
     assert servicer.app_set_objects_count > 1
     assert servicer.app_client_disconnect_count == 1
     assert servicer.app_get_logs_initial_count == 1
-    foo = app.indexed_objects["foo"]
+    foo = stub.indexed_objects["foo"]
     assert isinstance(foo, Function)
     assert foo.web_url.startswith("http://")
 
 
 @pytest.mark.asyncio
-async def test_no_change(app_ref, server_url_env, servicer):
+async def test_no_change(stub_ref, server_url_env, servicer):
     async def fake_watch():
         # Iterator that returns immediately, yielding nothing
         if False:
             yield
 
-    async with serve_app.aio(app, app_ref, _watcher=fake_watch()):
+    async with serve_stub.aio(stub, stub_ref, _watcher=fake_watch()):
         pass
 
     assert servicer.app_set_objects_count == 1  # Should create the initial app once
     assert servicer.app_client_disconnect_count == 1
     assert servicer.app_get_logs_initial_count == 1
 
 
 @pytest.mark.asyncio
-async def test_heartbeats(app_ref, server_url_env, servicer):
+async def test_heartbeats(stub_ref, server_url_env, servicer):
     with mock.patch("modal.runner.HEARTBEAT_INTERVAL", 1):
         t0 = time.time()
-        async with serve_app.aio(app, app_ref):
+        async with serve_stub.aio(stub, stub_ref):
             await asyncio.sleep(3.1)
         total_secs = int(time.time() - t0)
 
     apps = list(servicer.app_heartbeats.keys())
     assert len(apps) == 1
     # Typically [0s, 1s, 2s, 3s], but asyncio.sleep may lag.
     actual_heartbeats = servicer.app_heartbeats[apps[0]]
```

## test/lookup_test.py

```diff
@@ -1,13 +1,13 @@
 # Copyright Modal Labs 2023
 import pytest
 
-from modal import App, Function, Volume, web_endpoint
+from modal import Function, Stub, Volume, web_endpoint
 from modal.exception import ExecutionError, NotFoundError
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 
 
 def test_persistent_object(servicer, client):
     volume_id = Volume.create_deployed("my-volume", client=client)
 
     v: Volume = Volume.lookup("my-volume", client=client)
     assert isinstance(v, Volume)
@@ -19,18 +19,18 @@
 
 def square(x):
     # This function isn't deployed anyway
     pass
 
 
 def test_lookup_function(servicer, client):
-    app = App()
+    stub = Stub()
 
-    app.function()(square)
-    deploy_app(app, "my-function", client=client)
+    stub.function()(square)
+    deploy_stub(stub, "my-function", client=client)
 
     f = Function.lookup("my-function", "square", client=client)
     assert f.object_id == "fu-1"
 
     # Call it using two arguments
     f = Function.lookup("my-function", "square", client=client)
     assert f.object_id == "fu-1"
@@ -43,17 +43,17 @@
 
     # Make sure the new-style local calls raise an error
     with pytest.raises(ExecutionError):
         assert f.local(2, 4) == 20
 
 
 def test_webhook_lookup(servicer, client):
-    app = App()
-    app.function()(web_endpoint(method="POST")(square))
-    deploy_app(app, "my-webhook", client=client)
+    stub = Stub()
+    stub.function()(web_endpoint(method="POST")(square))
+    deploy_stub(stub, "my-webhook", client=client)
 
     f = Function.lookup("my-webhook", "square", client=client)
     assert f.web_url
 
 
 def test_deploy_exists(servicer, client):
     with pytest.raises(NotFoundError):
```

## test/mount_test.py

```diff
@@ -2,15 +2,15 @@
 import hashlib
 import os
 import platform
 import pytest
 import sys
 from pathlib import Path
 
-from modal import App
+from modal import Stub
 from modal._utils.blob_utils import LARGE_FILE_LIMIT
 from modal.exception import ModuleNotMountable
 from modal.mount import Mount
 
 
 @pytest.mark.asyncio
 async def test_get_files(servicer, client, tmpdir):
@@ -85,62 +85,62 @@
 
 
 def dummy():
     pass
 
 
 def test_from_local_python_packages(servicer, client, test_dir):
-    app = App()
+    stub = Stub()
 
     sys.path.append((test_dir / "supports").as_posix())
 
-    app.function(mounts=[Mount.from_local_python_packages("pkg_a", "pkg_b", "standalone_file")])(dummy)
+    stub.function(mounts=[Mount.from_local_python_packages("pkg_a", "pkg_b", "standalone_file")])(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         files = set(servicer.files_name2sha.keys())
         expected_files = {
             "/root/pkg_a/a.py",
             "/root/pkg_a/b/c.py",
             "/root/pkg_b/f.py",
             "/root/pkg_b/g/h.py",
             "/root/standalone_file.py",
         }
         assert expected_files.issubset(files)
 
         assert "/root/pkg_c/i.py" not in files
         assert "/root/pkg_c/j/k.py" not in files
 
 
-def test_app_mounts(servicer, client, test_dir):
+def test_stub_mounts(servicer, client, test_dir):
     sys.path.append((test_dir / "supports").as_posix())
 
-    app = App(mounts=[Mount.from_local_python_packages("pkg_b")])
+    stub = Stub(mounts=[Mount.from_local_python_packages("pkg_b")])
 
-    app.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
+    stub.function(mounts=[Mount.from_local_python_packages("pkg_a")])(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         files = set(servicer.files_name2sha.keys())
         expected_files = {
             "/root/pkg_a/a.py",
             "/root/pkg_a/b/c.py",
             "/root/pkg_b/f.py",
             "/root/pkg_b/g/h.py",
         }
         assert expected_files.issubset(files)
 
         assert "/root/pkg_c/i.py" not in files
         assert "/root/pkg_c/j/k.py" not in files
 
 
 def test_from_local_python_packages_missing_module(servicer, client, test_dir, server_url_env):
-    app = App()
-    app.function(mounts=[Mount.from_local_python_packages("nonexistent_package")])(dummy)
+    stub = Stub()
+    stub.function(mounts=[Mount.from_local_python_packages("nonexistent_package")])(dummy)
 
     with pytest.raises(ModuleNotMountable):
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
 
 def test_chained_entries(test_dir):
     a_txt = str(test_dir / "a.txt")
     b_txt = str(test_dir / "b.txt")
     with open(a_txt, "w") as f:
```

## test/mounted_files_test.py

```diff
@@ -52,15 +52,15 @@
         async for file_info in mount._get_files(mount.entries):
             filenames.append(file_info.mount_filename)
 
     return filenames
 
 
 def test_mounted_files_script(servicer, supports_dir, env_mount_files, server_url_env):
-    helpers.deploy_app_externally(servicer, script_path, cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, script_path, cwd=supports_dir)
     files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
 
     # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
     assert files == {
         "/root/a.py",
         "/root/b/c.py",
         "/root/b/e.py",
@@ -71,15 +71,15 @@
     }
 
 
 serialized_fn_path = "pkg_a/serialized_fn.py"
 
 
 def test_mounted_files_serialized(servicer, supports_dir, env_mount_files, server_url_env):
-    helpers.deploy_app_externally(servicer, serialized_fn_path, cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, serialized_fn_path, cwd=supports_dir)
     files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
 
     # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
     assert (
         files
         == {
             "/root/serialized_fn.py",  # should serialized_fn be included? It's not needed to run the function, but it's loaded into sys.modules at definition time...
@@ -200,24 +200,24 @@
     files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
     assert files == {
         "/root/script.py",
     }
 
 
 def test_e2e_modal_run_py_file_mounts(servicer, supports_dir):
-    helpers.deploy_app_externally(servicer, "hello.py", cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, "hello.py", cwd=supports_dir)
     # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
     # assert len(servicer.files_name2sha) == 1
     # assert servicer.n_mounts == 1  # there should be a single mount
     # assert servicer.n_mount_files == 1
     assert "/root/hello.py" in servicer.files_name2sha
 
 
 def test_e2e_modal_run_py_module_mounts(servicer, supports_dir):
-    helpers.deploy_app_externally(servicer, "hello", cwd=supports_dir)
+    helpers.deploy_stub_externally(servicer, "hello", cwd=supports_dir)
     # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
     # assert len(servicer.files_name2sha) == 1
     # assert servicer.n_mounts == 1  # there should be a single mount
     # assert servicer.n_mount_files == 1
     assert "/root/hello.py" in servicer.files_name2sha
 
 
@@ -231,20 +231,20 @@
 
     def mock_get_files_to_upload(self):
         r = list(original(self))
         return_values.append(r)
         return r
 
     monkeypatch.setattr("modal.mount._MountDir.get_files_to_upload", mock_get_files_to_upload)
-    app = modal.App()
+    stub = modal.Stub()
     mount_with_many_files = Mount.from_local_dir(test_dir, remote_path="/test")
-    app.function(mounts=[mount_with_many_files])(foo)
+    stub.function(mounts=[mount_with_many_files])(foo)
     assert len(return_values) == 0  # ensure we don't look at the files yet
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
     assert return_values  # at this point we should have gotten all the mount files
     # flatten inspected files
     files = set()
     for r in return_values:
         for fn, _ in r:
@@ -254,15 +254,15 @@
 
 
 def test_mount_dedupe(servicer, test_dir, server_url_env):
     supports_dir = test_dir / "supports"
     normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
     normally_not_included_file.touch(exist_ok=True)
     print(
-        helpers.deploy_app_externally(
+        helpers.deploy_stub_externally(
             # no explicit mounts, rely on auto-mounting
             servicer,
             "mount_dedupe.py",
             cwd=test_dir / "supports",
             env={"USE_EXPLICIT": "0"},
         )
     )
@@ -275,15 +275,15 @@
 
 
 def test_mount_dedupe_explicit(servicer, test_dir, server_url_env):
     supports_dir = test_dir / "supports"
     normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
     normally_not_included_file.touch(exist_ok=True)
     print(
-        helpers.deploy_app_externally(
+        helpers.deploy_stub_externally(
             # two explicit mounts of the same package
             servicer,
             "mount_dedupe.py",
             cwd=supports_dir,
             env={"USE_EXPLICIT": "1"},
         )
     )
```

## test/network_file_system_test.py

```diff
@@ -1,47 +1,46 @@
 # Copyright Modal Labs 2022
 import pytest
 import time
-from io import BytesIO
 from unittest import mock
 
 import modal
 from modal.exception import DeprecationError, InvalidError, NotFoundError
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 
 
 def dummy():
     pass
 
 
 def test_network_file_system_files(client, test_dir, servicer):
-    app = modal.App()
+    stub = modal.Stub()
     nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)
 
-    dummy_modal = app.function(network_file_systems={"/root/foo": nfs})(dummy)
+    dummy_modal = stub.function(network_file_systems={"/root/foo": nfs})(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         dummy_modal.remote()
 
 
 def test_network_file_system_bad_paths():
-    app = modal.App()
+    stub = modal.Stub()
     nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)
 
     def _f():
         pass
 
     with pytest.raises(InvalidError):
-        app.function(network_file_systems={"/root/../../foo": nfs})(dummy)
+        stub.function(network_file_systems={"/root/../../foo": nfs})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(network_file_systems={"/": nfs})(dummy)
+        stub.function(network_file_systems={"/": nfs})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(network_file_systems={"/tmp/": nfs})(dummy)
+        stub.function(network_file_systems={"/tmp/": nfs})(dummy)
 
 
 def test_network_file_system_handle_single_file(client, tmp_path, servicer):
     local_file_path = tmp_path / "some_file"
     local_file_path.write_text("hello world")
 
     with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
@@ -94,46 +93,46 @@
         assert servicer.nfs_files[object_id]["/bigfile"].data_blob_id == "bl-1"
 
         _, blobs = blob_server
         assert blobs["bl-1"] == b"hello world, this is a lot of text"
 
 
 def test_old_syntax(client, servicer):
-    app = modal.App()
+    stub = modal.Stub()
     with pytest.raises(DeprecationError):
-        app.vol1 = modal.SharedVolume()  # type: ignore  # This is just a post-deprecation husk
+        stub.vol1 = modal.SharedVolume()  # type: ignore  # This is just a post-deprecation husk
     with pytest.raises(DeprecationError):
-        app.vol2 = modal.SharedVolume.new()
+        stub.vol2 = modal.SharedVolume.new()
 
 
 def test_redeploy(servicer, client):
-    app = modal.App()
+    stub = modal.Stub()
     with pytest.warns(DeprecationError):
         n1 = modal.NetworkFileSystem.new()
         n2 = modal.NetworkFileSystem.new()
         n3 = modal.NetworkFileSystem.new()
-        app.n1, app.n2, app.n3 = n1, n2, n3
+        stub.n1, stub.n2, stub.n3 = n1, n2, n3
 
     # Deploy app once
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app1_ids = [n1.object_id, n2.object_id, n3.object_id]
 
     # Deploy app again
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app2_ids = [n1.object_id, n2.object_id, n3.object_id]
 
     # Make sure ids are stable
     assert app1_ids == app2_ids
 
     # Make sure ids are unique
     assert len(set(app1_ids)) == 3
     assert len(set(app2_ids)) == 3
 
     # Deploy to a different app
-    deploy_app(app, "my-other-app", client=client)
+    deploy_stub(stub, "my-other-app", client=client)
     app3_ids = [n1.object_id, n2.object_id, n3.object_id]
 
     # Should be unique and different
     assert len(set(app3_ids)) == 3
     assert set(app1_ids) & set(app3_ids) == set()
 
 
@@ -176,13 +175,7 @@
         assert nfs.listdir("/") == []
         nfs.write_file("xyz.txt", open(local_file_path, "rb"))
         (entry,) = nfs.listdir("/")
         assert entry.path == "xyz.txt"
 
         time.sleep(1.5)  # Make time for 2 heartbeats
     assert servicer.n_nfs_heartbeats == 2
-
-
-def test_nfs_lazy_hydration_from_name(set_env_client):
-    nfs = modal.NetworkFileSystem.from_name("nfs", create_if_missing=True)
-    bio = BytesIO(b"content")
-    nfs.write_file("blah", bio)
```

## test/object_test.py

```diff
@@ -1,22 +1,22 @@
 # Copyright Modal Labs 2022
 import pytest
 
-from modal import App, Queue, Secret
+from modal import Queue, Secret, Stub
 from modal.exception import DeprecationError, InvalidError
 
 
 @pytest.mark.asyncio
 async def test_async_factory(client):
-    app = App()
+    stub = Stub()
     with pytest.warns(DeprecationError):
-        app.my_factory = Queue.new()
-        async with app.run(client=client):
-            assert isinstance(app.my_factory, Queue)
-            assert app.my_factory.object_id == "qu-1"
+        stub.my_factory = Queue.new()
+        async with stub.run(client=client):
+            assert isinstance(stub.my_factory, Queue)
+            assert stub.my_factory.object_id == "qu-1"
 
 
 def test_new_hydrated(client):
     from modal.dict import _Dict
     from modal.object import _Object
     from modal.queue import _Queue
```

## test/queue_test.py

```diff
@@ -15,21 +15,14 @@
     q.put(42)
     assert q.len() == 1
     assert q.get() == 42
     with pytest.raises(queue.Empty):
         q.get(timeout=0)
     assert q.len() == 0
 
-    # test iter
-    q.put_many([1, 2, 3])
-    t0 = time.time()
-    assert [v for v in q.iterate(item_poll_timeout=1.0)] == [1, 2, 3]
-    assert 1.0 < time.time() - t0 < 2.0
-    assert [v for v in q.iterate(item_poll_timeout=0.0)] == [1, 2, 3]
-
 
 def test_queue_ephemeral(servicer, client):
     with Queue.ephemeral(client=client, _heartbeat_sleep=1) as q:
         q.put("hello")
         assert q.len() == 1
         assert q.get() == "hello"
         time.sleep(1.5)  # enough to trigger two heartbeats
@@ -98,13 +91,7 @@
     assert str(servicer.queue_max_len) in str(excinfo.value)
     assert i == servicer.queue_max_len
 
 
 def test_queue_deploy(servicer, client):
     d = Queue.lookup("xyz", create_if_missing=True, client=client)
     d.put(123)
-
-
-def test_queue_lazy_hydrate_from_name(set_env_client):
-    q = Queue.from_name("foo", create_if_missing=True)
-    q.put(123)
-    assert q.get() == 123
```

## test/resolver_test.py

```diff
@@ -5,15 +5,14 @@
 from typing import Optional
 
 from modal._output import OutputManager
 from modal._resolver import Resolver
 from modal.object import _Object
 
 
-@pytest.mark.flaky(max_runs=2)
 @pytest.mark.asyncio
 async def test_multi_resolve_sequential_loads_once():
     output_manager = OutputManager(None, show_progress=False)
     resolver = Resolver(None, output_mgr=output_manager, environment_name="", app_id=None)
 
     load_count = 0
```

## test/retries_test.py

```diff
@@ -26,42 +26,42 @@
 
 
 def zero_retries():
     pass
 
 
 def test_retries(client):
-    app = modal.App()
+    stub = modal.Stub()
 
-    default_retries_from_int_modal = app.function(retries=5)(default_retries_from_int)
-    fixed_delay_retries_modal = app.function(retries=modal.Retries(max_retries=5, backoff_coefficient=1.0))(
+    default_retries_from_int_modal = stub.function(retries=5)(default_retries_from_int)
+    fixed_delay_retries_modal = stub.function(retries=modal.Retries(max_retries=5, backoff_coefficient=1.0))(
         fixed_delay_retries
     )
 
-    exponential_backoff_modal = app.function(
+    exponential_backoff_modal = stub.function(
         retries=modal.Retries(max_retries=2, initial_delay=2.0, backoff_coefficient=2.0)
     )(exponential_backoff)
 
-    exponential_with_max_delay_modal = app.function(
+    exponential_with_max_delay_modal = stub.function(
         retries=modal.Retries(max_retries=2, backoff_coefficient=2.0, max_delay=30.0)
     )(exponential_with_max_delay)
 
-    zero_retries_modal = app.function(retries=0)(zero_retries)
+    zero_retries_modal = stub.function(retries=0)(zero_retries)
 
     with pytest.raises(TypeError):
         # Reject no-args constructions, which is unreadable and harder to support long-term
-        app.function(retries=modal.Retries())(dummy)  # type: ignore
+        stub.function(retries=modal.Retries())(dummy)  # type: ignore
 
     # Reject weird inputs:
     # Don't need server to detect and reject nonsensical input. Can do client-side.
     with pytest.raises(InvalidError):
-        app.function(retries=modal.Retries(max_retries=-2))(dummy)
+        stub.function(retries=modal.Retries(max_retries=-2))(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(retries=modal.Retries(max_retries=2, backoff_coefficient=0.0))(dummy)
+        stub.function(retries=modal.Retries(max_retries=2, backoff_coefficient=0.0))(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         default_retries_from_int_modal.remote()
         fixed_delay_retries_modal.remote()
         exponential_backoff_modal.remote()
         exponential_with_max_delay_modal.remote()
         zero_retries_modal.remote()
```

## test/runner_test.py

```diff
@@ -1,79 +1,71 @@
 # Copyright Modal Labs 2023
 import pytest
 import typing
 
 import modal
-from modal.client import Client
-from modal.exception import ExecutionError
-from modal.runner import run_app
+from modal.runner import run_stub
 from modal_proto import api_pb2
 
 T = typing.TypeVar("T")
 
 
-def test_run_app(servicer, client):
-    dummy_app = modal.App()
+def test_run_stub(servicer, client):
+    dummy_stub = modal.Stub()
     with servicer.intercept() as ctx:
-        with run_app(dummy_app, client=client):
+        with run_stub(dummy_stub, client=client):
             pass
 
     ctx.pop_request("AppCreate")
     ctx.pop_request("AppSetObjects")
     ctx.pop_request("AppClientDisconnect")
 
 
-def test_run_app_unauthenticated(servicer):
-    dummy_app = modal.App()
-    with Client.anonymous(servicer.remote_addr) as client:
-        with pytest.raises(ExecutionError, match=".+unauthenticated client"):
-            with run_app(dummy_app, client=client):
-                pass
-
-
 def dummy():
     ...
 
 
-def test_run_app_profile_env_with_refs(servicer, client, monkeypatch):
+def test_run_stub_profile_env_with_refs(servicer, client, monkeypatch):
     monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
     with servicer.intercept() as ctx:
-        dummy_app = modal.App()
+        dummy_stub = modal.Stub()
         ref = modal.Secret.from_name("some_secret")
-        dummy_app.function(secrets=[ref])(dummy)
+        dummy_stub.function(secrets=[ref])(dummy)
 
     assert ctx.calls == []  # all calls should be deferred
 
     with servicer.intercept() as ctx:
         ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
-        with run_app(dummy_app, client=client):
+        with run_stub(dummy_stub, client=client):
             pass
 
     with pytest.raises(Exception):
         ctx.pop_request("SecretCreate")  # should not create a new secret...
 
     app_create = ctx.pop_request("AppCreate")
     assert app_create.environment_name == "profile_env"
 
     secret_get_or_create = ctx.pop_request("SecretGetOrCreate")
     assert secret_get_or_create.environment_name == "profile_env"
 
 
-def test_run_app_custom_env_with_refs(servicer, client, monkeypatch):
+def test_run_stub_custom_env_with_refs(servicer, client, monkeypatch):
     monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
-    dummy_app = modal.App()
+    dummy_stub = modal.Stub()
     own_env_secret = modal.Secret.from_name("own_env_secret")
-    other_env_secret = modal.Secret.from_name("other_env_secret", environment_name="third")  # explicit lookup
+    other_env_secret = modal.Secret.from_name(
+        "other_env_secret", environment_name="third"
+    )  # explicit lookup
 
-    dummy_app.function(secrets=[own_env_secret, other_env_secret])(dummy)
+    dummy_stub.function(secrets=[own_env_secret, other_env_secret])(dummy)
 
     with servicer.intercept() as ctx:
         ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
         ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-456"))
-        with run_app(dummy_app, client=client, environment_name="custom"):
+        with run_stub(dummy_stub, client=client, environment_name="custom"):
             pass
 
     with pytest.raises(Exception):
         ctx.pop_request("SecretCreate")
 
     app_create = ctx.pop_request("AppCreate")
     assert app_create.environment_name == "custom"
```

## test/sandbox_test.py

```diff
@@ -2,27 +2,27 @@
 
 import hashlib
 import platform
 import pytest
 import time
 from pathlib import Path
 
-from modal import App, Image, Mount, NetworkFileSystem, Sandbox, Secret
+from modal import Image, Mount, NetworkFileSystem, Sandbox, Secret, Stub
 from modal.exception import InvalidError
 
-app = App()
+stub = Stub()
 
 
 skip_non_linux = pytest.mark.skipif(platform.system() != "Linux", reason="sandbox mock uses subprocess")
 
 
 @skip_non_linux
 def test_spawn_sandbox(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo bye >&2 && sleep 1 && echo hi && exit 42", timeout=600)
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo bye >&2 && sleep 1 && echo hi && exit 42", timeout=600)
 
         assert sb.poll() is None
 
         t0 = time.time()
         sb.wait()
         # Test that we actually waited for the sandbox to finish.
         assert time.time() - t0 > 0.3
@@ -37,86 +37,86 @@
         assert sb.poll() == 42
 
 
 @skip_non_linux
 def test_sandbox_mount(client, servicer, tmpdir):
     tmpdir.join("a.py").write(b"foo")
 
-    with app.run(client=client):
-        sb = app.spawn_sandbox(
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox(
             "echo",
             "hi",
             mounts=[Mount.from_local_dir(Path(tmpdir), remote_path="/m")],
         )
         sb.wait()
 
     sha = hashlib.sha256(b"foo").hexdigest()
     assert servicer.files_sha2data[sha]["data"] == b"foo"
 
 
 @skip_non_linux
 def test_sandbox_image(client, servicer, tmpdir):
     tmpdir.join("a.py").write(b"foo")
 
-    with app.run(client=client):
-        sb = app.spawn_sandbox("echo", "hi", image=Image.debian_slim().pip_install("foo", "bar", "potato"))
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("echo", "hi", image=Image.debian_slim().pip_install("foo", "bar", "potato"))
         sb.wait()
 
     idx = max(servicer.images.keys())
     last_image = servicer.images[idx]
 
     assert all(c in last_image.dockerfile_commands[-1] for c in ["foo", "bar", "potato"])
 
 
 @skip_non_linux
 def test_sandbox_secret(client, servicer, tmpdir):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("echo", "$FOO", secrets=[Secret.from_dict({"FOO": "BAR"})])
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("echo", "$FOO", secrets=[Secret.from_dict({"FOO": "BAR"})])
         sb.wait()
 
     assert len(servicer.sandbox_defs[0].secret_ids) == 1
 
 
 @skip_non_linux
 def test_sandbox_nfs(client, servicer, tmpdir):
-    with app.run(client=client):
+    with stub.run(client=client):
         with NetworkFileSystem.ephemeral(client=client) as nfs:
             with pytest.raises(InvalidError):
-                app.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/": nfs})
+                stub.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/": nfs})
 
-            app.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/cache": nfs})
+            stub.spawn_sandbox("echo", "foo > /cache/a.txt", network_file_systems={"/cache": nfs})
 
     assert len(servicer.sandbox_defs[0].nfs_mounts) == 1
 
 
 @skip_non_linux
 def test_sandbox_from_id(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo foo && exit 42", timeout=600)
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo foo && exit 42", timeout=600)
         sb.wait()
 
     sb2 = Sandbox.from_id(sb.object_id, client=client)
     assert sb2.stdout.read() == "foo\n"
     assert sb2.returncode == 42
 
 
 @skip_non_linux
 def test_sandbox_terminate(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "sleep 10000")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "sleep 10000")
         sb.terminate()
 
         assert sb.returncode != 0
 
 
 @skip_non_linux
 @pytest.mark.asyncio
 async def test_sandbox_stdin_async(client, servicer):
-    async with app.run.aio(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
+    async with stub.run.aio(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
 
         sb.stdin.write(b"foo\n")
         sb.stdin.write(b"bar\n")
 
         sb.stdin.write_eof()
 
         await sb.stdin.drain.aio()
@@ -125,16 +125,16 @@
 
         assert sb.stdout.read() == "foo\nbar\n"
         assert sb.returncode == 13
 
 
 @skip_non_linux
 def test_sandbox_stdin(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "while read line; do echo $line; done && exit 13")
 
         sb.stdin.write(b"foo\n")
         sb.stdin.write(b"bar\n")
 
         sb.stdin.write_eof()
 
         sb.stdin.drain()
@@ -143,34 +143,34 @@
 
         assert sb.stdout.read() == "foo\nbar\n"
         assert sb.returncode == 13
 
 
 @skip_non_linux
 def test_sandbox_stdin_invalid_write(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo foo")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo foo")
         with pytest.raises(TypeError):
             sb.stdin.write("foo\n")  # type: ignore
 
 
 @skip_non_linux
 def test_sandbox_stdin_write_after_eof(client, servicer):
-    with app.run(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo foo")
+    with stub.run(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo foo")
         sb.stdin.write_eof()
         with pytest.raises(EOFError):
             sb.stdin.write(b"foo")
 
 
 @skip_non_linux
 @pytest.mark.asyncio
 async def test_sandbox_async_for(client, servicer):
-    async with app.run.aio(client=client):
-        sb = app.spawn_sandbox("bash", "-c", "echo hello && echo world && echo bye >&2")
+    async with stub.run.aio(client=client):
+        sb = stub.spawn_sandbox("bash", "-c", "echo hello && echo world && echo bye >&2")
 
         out = ""
 
         async for message in sb.stdout:
             out += message
         assert out == "hello\nworld\n"
```

## test/schedule_test.py

```diff
@@ -1,15 +1,15 @@
 # Copyright Modal Labs 2022
-from modal import App, Period
+from modal import Period, Stub
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-@app.function(schedule=Period(seconds=5))
+@stub.function(schedule=Period(seconds=5))
 def f():
     pass
 
 
 def test_schedule(servicer, client):
-    with app.run(client=client):
+    with stub.run(client=client):
         assert servicer.function2schedule == {"fu-1": api_pb2.Schedule(period=api_pb2.Schedule.Period(seconds=5.0))}
```

## test/scheduler_placement_test.py

```diff
@@ -1,28 +1,28 @@
 # Copyright Modal Labs 2024
-from modal import App, SchedulerPlacement
+from modal import SchedulerPlacement, Stub
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-@app.function(
+@stub.function(
     _experimental_scheduler=True,
     _experimental_scheduler_placement=SchedulerPlacement(
         region="us-east-1",
         zone="us-east-1a",
         spot=False,
     ),
 )
 def f():
     pass
 
 
 def test_scheduler_placement(servicer, client):
-    with app.run(client=client):
+    with stub.run(client=client):
         assert len(servicer.app_functions) == 1
         fn = servicer.app_functions["fu-1"]
         assert fn._experimental_scheduler
         assert fn._experimental_scheduler_placement == api_pb2.SchedulerPlacement(
             _region="us-east-1",
             _zone="us-east-1a",
             _lifecycle="on-demand",
```

## test/secret_test.py

```diff
@@ -1,78 +1,78 @@
 # Copyright Modal Labs 2022
 import os
 import pytest
 import tempfile
 from unittest import mock
 
-from modal import App, Secret
+from modal import Secret, Stub
 from modal.exception import InvalidError
 
 from .supports.skip import skip_old_py
 
 
 def dummy():
     ...
 
 
 def test_secret_from_dict(servicer, client):
-    app = App()
+    stub = Stub()
     secret = Secret.from_dict({"FOO": "hello, world"})
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert secret.object_id == "st-0"
         assert servicer.secrets["st-0"] == {"FOO": "hello, world"}
 
 
 @skip_old_py("python-dotenv requires python3.8 or higher", (3, 8))
 def test_secret_from_dotenv(servicer, client):
     with tempfile.TemporaryDirectory() as tmpdirname:
         with open(os.path.join(tmpdirname, ".env"), "w") as f:
             f.write("# My settings\nUSER=user\nPASSWORD=abc123\n")
-        app = App()
+        stub = Stub()
         secret = Secret.from_dotenv(tmpdirname)
-        app.function(secrets=[secret])(dummy)
-        with app.run(client=client):
+        stub.function(secrets=[secret])(dummy)
+        with stub.run(client=client):
             assert secret.object_id == "st-0"
             assert servicer.secrets["st-0"] == {"USER": "user", "PASSWORD": "abc123"}
 
 @mock.patch.dict(os.environ, {"FOO": "easy", "BAR": "1234"})
 def test_secret_from_local_environ(servicer, client):
-    app = App()
+    stub = Stub()
     secret = Secret.from_local_environ(["FOO", "BAR"])
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert secret.object_id == "st-0"
         assert servicer.secrets["st-0"] == {"FOO": "easy", "BAR": "1234"}
 
     with pytest.raises(InvalidError, match="NOTFOUND"):
         Secret.from_local_environ(["FOO", "NOTFOUND"])
 
 
 
 def test_init_types():
     with pytest.raises(InvalidError):
         Secret.from_dict({"foo": 1.0})  # type: ignore
 
 
 def test_secret_from_dict_none(servicer, client):
-    app = App()
+    stub = Stub()
     secret = Secret.from_dict({"FOO": os.getenv("xyz"), "BAR": os.environ.get("abc"), "BAZ": "baz"})
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert servicer.secrets["st-0"] == {"BAZ": "baz"}
 
 
 def test_secret_from_name(servicer, client):
     # Deploy secret
     secret_id = Secret.create_deployed("my-secret", {"FOO": "123"}, client=client)
 
     # Look up secret
     secret = Secret.lookup("my-secret", client=client)
     assert secret.object_id == secret_id
 
     # Look up secret through app
-    app = App()
+    stub = Stub()
     secret = Secret.from_name("my-secret")
-    app.function(secrets=[secret])(dummy)
-    with app.run(client=client):
+    stub.function(secrets=[secret])(dummy)
+    with stub.run(client=client):
         assert secret.object_id == secret_id
```

## test/stub_composition_test.py

```diff
@@ -1,10 +1,10 @@
 # Copyright Modal Labs 2024
-from test.helpers import deploy_app_externally
+from test.helpers import deploy_stub_externally
 
 
-def test_app_composition_includes_all_functions(servicer, supports_dir, monkeypatch, client):
-    print(deploy_app_externally(servicer, "main.py", cwd=supports_dir / "multifile_project"))
+def test_stub_composition_includes_all_functions(servicer, supports_dir, monkeypatch, client):
+    print(deploy_stub_externally(servicer, "main.py", cwd=supports_dir / "multifile_project"))
     assert servicer.n_functions == 3
     assert {"/root/main.py", "/root/a.py", "/root/b.py", "/root/c.py"} == set(servicer.files_name2sha.keys())
     assert len(servicer.secrets) == 1  # secret from B should be included
     assert servicer.n_mounts == 4  # mounts should not be duplicated
```

## test/stub_test.py

```diff
@@ -2,176 +2,198 @@
 import asyncio
 import logging
 import pytest
 
 from google.protobuf.empty_pb2 import Empty
 from grpclib import GRPCError, Status
 
-from modal import App, Dict, Image, Mount, Queue, Secret, Volume, web_endpoint
-from modal.app import list_apps  # type: ignore
+import modal.app
+from modal import Dict, Image, Queue, Stub, web_endpoint
 from modal.config import config
 from modal.exception import DeprecationError, ExecutionError, InvalidError, NotFoundError
 from modal.partial_function import _parse_custom_domains
-from modal.runner import deploy_app
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
 from .supports import module_1, module_2
 
 
 @pytest.mark.asyncio
 async def test_kwargs(servicer, client):
-    with pytest.raises(DeprecationError):
-        App(
+    with pytest.warns(DeprecationError):
+        stub = Stub(
             d=Dict.new(),
             q=Queue.new(),
         )
+    async with stub.run(client=client):
+        with pytest.warns(DeprecationError):
+            await stub["d"].put.aio("foo", "bar")  # type: ignore
+            await stub["q"].put.aio("baz")  # type: ignore
+            assert await stub["d"].get.aio("foo") == "bar"  # type: ignore
+            assert await stub["q"].get.aio() == "baz"  # type: ignore
 
 
 @pytest.mark.asyncio
 async def test_attrs(servicer, client):
-    app = App()
+    stub = Stub()
     with pytest.warns(DeprecationError):
-        app.d = Dict.new()
-        app.q = Queue.new()
-    async with app.run(client=client):
+        stub.d = Dict.new()
+        stub.q = Queue.new()
+    async with stub.run(client=client):
         with pytest.warns(DeprecationError):
-            await app.d.put.aio("foo", "bar")  # type: ignore
-            await app.q.put.aio("baz")  # type: ignore
-            assert await app.d.get.aio("foo") == "bar"  # type: ignore
-            assert await app.q.get.aio() == "baz"  # type: ignore
+            await stub.d.put.aio("foo", "bar")  # type: ignore
+            await stub.q.put.aio("baz")  # type: ignore
+            assert await stub.d.get.aio("foo") == "bar"  # type: ignore
+            assert await stub.q.get.aio() == "baz"  # type: ignore
+
+
+@pytest.mark.asyncio
+async def test_stub_type_validation(servicer, client):
+    with pytest.raises(InvalidError):
+        with pytest.warns(DeprecationError):
+            stub = Stub(
+                foo=4242,  # type: ignore
+            )
+
+    stub = Stub()
+
+    with pytest.raises(InvalidError) as excinfo:
+        stub.bar = 4242  # type: ignore
+
+    assert "4242" in str(excinfo.value)
 
 
 def square(x):
     return x**2
 
 
 @pytest.mark.asyncio
 async def test_redeploy(servicer, client):
-    app = App(image=Image.debian_slim().pip_install("pandas"))
-    app.function()(square)
+    stub = Stub(image=Image.debian_slim().pip_install("pandas"))
+    stub.function()(square)
 
     # Deploy app
-    res = await deploy_app.aio(app, "my-app", client=client)
-    assert res.app_id == "ap-1"
+    app = await deploy_stub.aio(stub, "my-app", client=client)
+    assert app.app_id == "ap-1"
     assert servicer.app_objects["ap-1"]["square"] == "fu-1"
-    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
+    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
     # Redeploy, make sure all ids are the same
-    res = await deploy_app.aio(app, "my-app", client=client)
-    assert res.app_id == "ap-1"
+    app = await deploy_stub.aio(stub, "my-app", client=client)
+    assert app.app_id == "ap-1"
     assert servicer.app_objects["ap-1"]["square"] == "fu-1"
-    assert servicer.app_state_history[res.app_id] == [
+    assert servicer.app_state_history[app.app_id] == [
         api_pb2.APP_STATE_INITIALIZING,
         api_pb2.APP_STATE_DEPLOYED,
         api_pb2.APP_STATE_DEPLOYED,
     ]
 
     # Deploy to a different name, ids should change
-    res = await deploy_app.aio(app, "my-app-xyz", client=client)
-    assert res.app_id == "ap-2"
+    app = await deploy_stub.aio(stub, "my-app-xyz", client=client)
+    assert app.app_id == "ap-2"
     assert servicer.app_objects["ap-2"]["square"] == "fu-2"
-    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
+    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
 
 def dummy():
     pass
 
 
 # Should exit without waiting for the "logs_timeout" grace period.
 @pytest.mark.timeout(5)
 def test_create_object_exception(servicer, client):
     servicer.function_create_error = True
 
-    app = App()
-    app.function()(dummy)
+    stub = Stub()
+    stub.function()(dummy)
 
     with pytest.raises(GRPCError) as excinfo:
-        with app.run(client=client):
+        with stub.run(client=client):
             pass
 
     assert excinfo.value.status == Status.INTERNAL
 
 
 def test_deploy_falls_back_to_app_name(servicer, client):
-    named_app = App(name="foo_app")
-    deploy_app(named_app, client=client)
+    named_stub = Stub(name="foo_app")
+    deploy_stub(named_stub, client=client)
     assert "foo_app" in servicer.deployed_apps
 
 
 def test_deploy_uses_deployment_name_if_specified(servicer, client):
-    named_app = App(name="foo_app")
-    deploy_app(named_app, "bar_app", client=client)
+    named_stub = Stub(name="foo_app")
+    deploy_stub(named_stub, "bar_app", client=client)
     assert "bar_app" in servicer.deployed_apps
     assert "foo_app" not in servicer.deployed_apps
 
 
 def test_run_function_without_app_error():
-    app = App()
-    dummy_modal = app.function()(dummy)
+    stub = Stub()
+    dummy_modal = stub.function()(dummy)
 
     with pytest.raises(ExecutionError) as excinfo:
         dummy_modal.remote()
 
     assert "hydrated" in str(excinfo.value)
 
 
 def test_is_inside_basic():
-    app = App()
+    stub = Stub()
     with pytest.raises(DeprecationError, match="imports()"):
-        app.is_inside()
+        stub.is_inside()
 
 
 def test_missing_attr():
-    """Trying to call a non-existent function on the App should produce
+    """Trying to call a non-existent function on the Stub should produce
     an understandable error message."""
 
-    app = App()
+    stub = Stub()
     with pytest.raises(AttributeError):
-        app.fun()  # type: ignore
+        stub.fun()  # type: ignore
 
 
 def test_same_function_name(caplog):
-    app = App()
+    stub = Stub()
 
     # Add first function
     with caplog.at_level(logging.WARNING):
-        app.function()(module_1.square)
+        stub.function()(module_1.square)
     assert len(caplog.records) == 0
 
     # Add second function: check warning
     with caplog.at_level(logging.WARNING):
-        app.function()(module_2.square)
+        stub.function()(module_2.square)
     assert len(caplog.records) == 1
     assert "module_1" in caplog.text
     assert "module_2" in caplog.text
     assert "square" in caplog.text
 
 
 def test_run_state(client, servicer):
-    app = App()
-    with app.run(client=client):
-        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_EPHEMERAL]
+    stub = Stub()
+    with stub.run(client=client):
+        assert servicer.app_state_history[stub.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_EPHEMERAL]
 
 
 def test_deploy_state(client, servicer):
-    app = App()
-    res = deploy_app(app, "foobar", client=client)
-    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
+    stub = Stub()
+    app = deploy_stub(stub, "foobar", client=client)
+    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
 
 
 def test_detach_state(client, servicer):
-    app = App()
-    with app.run(client=client, detach=True):
-        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]
+    stub = Stub()
+    with stub.run(client=client, detach=True):
+        assert servicer.app_state_history[stub.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]
 
 
 @pytest.mark.asyncio
 async def test_grpc_protocol(client, servicer):
-    app = App()
-    async with app.run(client=client):
+    stub = Stub()
+    async with stub.run(client=client):
         await asyncio.sleep(0.01)  # wait for heartbeat
     assert len(servicer.requests) == 4
     assert isinstance(servicer.requests[0], Empty)  # ClientHello
     assert isinstance(servicer.requests[1], api_pb2.AppCreateRequest)
     assert isinstance(servicer.requests[2], api_pb2.AppHeartbeatRequest)
     assert isinstance(servicer.requests[3], api_pb2.AppClientDisconnectRequest)
 
@@ -181,169 +203,158 @@
 
 
 async def web2(x):
     return {"cube": x**3}
 
 
 def test_registered_web_endpoints(client, servicer):
-    app = App()
-    app.function()(square)
-    app.function()(web_endpoint()(web1))
-    app.function()(web_endpoint()(web2))
+    stub = Stub()
+    stub.function()(square)
+    stub.function()(web_endpoint()(web1))
+    stub.function()(web_endpoint()(web2))
 
-    assert app.registered_web_endpoints == ["web1", "web2"]
+    assert stub.registered_web_endpoints == ["web1", "web2"]
 
 
 def test_init_types():
     with pytest.raises(InvalidError):
         # singular secret to plural argument
-        App(secrets=Secret.from_dict())  # type: ignore
+        Stub(secrets=modal.Secret.from_dict())  # type: ignore
     with pytest.raises(InvalidError):
         # not a Secret Object
-        App(secrets=[{"foo": "bar"}])  # type: ignore
+        Stub(secrets=[{"foo": "bar"}])  # type: ignore
+    with pytest.raises(InvalidError):
+        # blueprint needs to use _Providers
+        with pytest.warns(DeprecationError):
+            Stub(some_arg=5)  # type: ignore
     with pytest.raises(InvalidError):
         # should be an Image
-        App(image=Secret.from_dict())  # type: ignore
+        Stub(image=modal.Secret.from_dict())  # type: ignore
 
-    App(
-        image=Image.debian_slim().pip_install("pandas"),
-        secrets=[Secret.from_dict()],
-        mounts=[Mount.from_local_file(__file__)],
-    )
+    with pytest.warns(DeprecationError):
+        Stub(
+            image=modal.Image.debian_slim().pip_install("pandas"),
+            secrets=[modal.Secret.from_dict()],
+            mounts=[modal.Mount.from_local_file(__file__)],
+            some_dict=modal.Dict.new(),
+            some_queue=modal.Queue.new(),
+        )
 
 
-def test_set_image_on_app_as_attribute():
+def test_set_image_on_stub_as_attribute():
     # TODO: do we want to deprecate this syntax? It's kind of random for image to
     #     have a reserved name in the blueprint, and being the only of the construction
     #     arguments that can be set on the instance after construction
-    custom_img = Image.debian_slim().apt_install("emacs")
-    app = App(image=custom_img)
-    assert app._get_default_image() == custom_img
+    custom_img = modal.Image.debian_slim().apt_install("emacs")
+    stub = Stub(image=custom_img)
+    assert stub._get_default_image() == custom_img
 
 
 def test_redeploy_delete_objects(servicer, client):
     # Deploy an app with objects d1 and d2
-    app = App()
-    app.function(name="d1")(dummy)
-    app.function(name="d2")(dummy)
-    res = deploy_app(app, "xyz", client=client)
+    stub = Stub()
+    stub.function(name="d1")(dummy)
+    stub.function(name="d2")(dummy)
+    app = deploy_stub(stub, "xyz", client=client)
 
     # Check objects
-    assert set(servicer.app_objects[res.app_id].keys()) == set(["d1", "d2"])
+    assert set(servicer.app_objects[app.app_id].keys()) == set(["d1", "d2"])
 
     # Deploy an app with objects d2 and d3
-    app = App()
-    app.function(name="d2")(dummy)
-    app.function(name="d3")(dummy)
-    res = deploy_app(app, "xyz", client=client)
+    stub = Stub()
+    stub.function(name="d2")(dummy)
+    stub.function(name="d3")(dummy)
+    app = deploy_stub(stub, "xyz", client=client)
 
     # Make sure d1 is deleted
-    assert set(servicer.app_objects[res.app_id].keys()) == set(["d2", "d3"])
+    assert set(servicer.app_objects[app.app_id].keys()) == set(["d2", "d3"])
 
 
 @pytest.mark.asyncio
 async def test_unhydrate(servicer, client):
-    app = App()
+    stub = Stub()
 
-    f = app.function()(dummy)
+    f = stub.function()(dummy)
 
     assert not f.is_hydrated
-    async with app.run(client=client):
+    async with stub.run(client=client):
         assert f.is_hydrated
 
     # After app finishes, it should unhydrate
     assert not f.is_hydrated
 
 
 def test_keyboard_interrupt(servicer, client):
-    app = App()
-    app.function()(square)
-    with app.run(client=client):
+    stub = Stub()
+    stub.function()(square)
+    with stub.run(client=client):
         # The exit handler should catch this interrupt and exit gracefully
         raise KeyboardInterrupt()
 
 
 def test_function_image_positional():
-    app = App()
+    stub = Stub()
     image = Image.debian_slim()
 
     with pytest.raises(InvalidError) as excinfo:
 
-        @app.function(image)  # type: ignore
+        @stub.function(image)  # type: ignore
         def f():
             pass
 
     assert "function(image=image)" in str(excinfo.value)
 
 
 @pytest.mark.asyncio
 async def test_deploy_disconnect(servicer, client):
-    app = App()
-    app.function(secrets=[Secret.from_name("nonexistent-secret")])(square)
+    stub = Stub()
+    stub.function(secrets=[modal.Secret.from_name("nonexistent-secret")])(square)
 
     with pytest.raises(NotFoundError):
-        await deploy_app.aio(app, "my-app", client=client)
+        await deploy_stub.aio(stub, "my-app", client=client)
 
     assert servicer.app_state_history["ap-1"] == [
         api_pb2.APP_STATE_INITIALIZING,
         api_pb2.APP_STATE_STOPPED,
     ]
 
 
 def test_redeploy_from_name_change(servicer, client):
     # Deploy queue
-    Queue.lookup("foo-queue", create_if_missing=True, client=client)
+    modal.Queue.lookup("foo-queue", create_if_missing=True, client=client)
 
-    # Use it from app
-    app = App()
+    # Use it from stub
+    stub = Stub()
     with pytest.warns(DeprecationError):
-        app.q = Queue.from_name("foo-queue")
-    deploy_app(app, "my-app", client=client)
+        stub.q = modal.Queue.from_name("foo-queue")
+    deploy_stub(stub, "my-app", client=client)
 
     # Change the object id of foo-queue
     k = ("foo-queue", api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, config.get("environment"))
     assert servicer.deployed_queues[k]
     servicer.deployed_queues[k] = "qu-baz123"
 
     # Redeploy app
     # This should not fail because the object_id changed - it's a different app
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
 
 
 def test_parse_custom_domains():
     assert len(_parse_custom_domains(None)) == 0
     assert len(_parse_custom_domains(["foo.com", "bar.com"])) == 2
     with pytest.raises(AssertionError):
         assert _parse_custom_domains("foo.com")
 
 
 def test_hydrated_other_app_object_gets_referenced(servicer, client):
-    app = App("my-app")
+    stub = Stub("my-stub")
     with servicer.intercept() as ctx:
-        with Volume.ephemeral(client=client) as vol:
-            app.function(volumes={"/vol": vol})(dummy)  # implicitly load vol
-            deploy_app(app, client=client)
+        with modal.Volume.ephemeral(client=client) as vol:
+            stub.function(volumes={"/vol": vol})(dummy)  # implicitly load vol
+            deploy_stub(stub, client=client)
             app_set_objects_req = ctx.pop_request("AppSetObjects")
             assert vol.object_id in app_set_objects_req.unindexed_object_ids
 
 
 def test_hasattr():
-    app = App()
-    assert not hasattr(app, "xyz")
-
-
-def test_app(client):
-    app = App()
-    square_modal = app.function()(square)
-
-    with app.run(client=client):
-        square_modal.remote(42)
-
-
-def test_list_apps(client):
-    apps_0 = [app.name for app in list_apps(client=client)]
-    app = App()
-    deploy_app(app, "foobar", client=client)
-    apps_1 = [app.name for app in list_apps(client=client)]
-
-    assert len(apps_1) == len(apps_0) + 1
-    assert set(apps_1) - set(apps_0) == set(["foobar"])
+    stub = Stub()
+    assert not hasattr(stub, "xyz")
```

## test/volume_test.py

```diff
@@ -1,60 +1,54 @@
 # Copyright Modal Labs 2023
-import asyncio
 import io
-import os
-import platform
 import pytest
-import re
-import sys
 import time
 from pathlib import Path
 from unittest import mock
 
 import modal
 from modal.exception import DeprecationError, InvalidError, NotFoundError, VolumeUploadTimeoutError
-from modal.runner import deploy_app
-from modal.volume import _open_files_error_annotation
+from modal.runner import deploy_stub
 from modal_proto import api_pb2
 
 
 def dummy():
     pass
 
 
 def test_volume_mount(client, servicer):
-    app = modal.App()
+    stub = modal.Stub()
     vol = modal.Volume.from_name("xyz", create_if_missing=True)
 
-    _ = app.function(volumes={"/root/foo": vol})(dummy)
+    _ = stub.function(volumes={"/root/foo": vol})(dummy)
 
-    with app.run(client=client):
+    with stub.run(client=client):
         pass
 
 
 def test_volume_bad_paths():
-    app = modal.App()
+    stub = modal.Stub()
     vol = modal.Volume.from_name("xyz")
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/root/../../foo": vol})(dummy)
+        stub.function(volumes={"/root/../../foo": vol})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/": vol})(dummy)
+        stub.function(volumes={"/": vol})(dummy)
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/tmp/": vol})(dummy)
+        stub.function(volumes={"/tmp/": vol})(dummy)
 
 
 def test_volume_duplicate_mount():
-    app = modal.App()
+    stub = modal.Stub()
     vol = modal.Volume.from_name("xyz")
 
     with pytest.raises(InvalidError):
-        app.function(volumes={"/foo": vol, "/bar": vol})(dummy)
+        stub.function(volumes={"/foo": vol, "/bar": vol})(dummy)
 
 
 @pytest.mark.parametrize("skip_reload", [False, True])
 def test_volume_commit(client, servicer, skip_reload):
     with servicer.intercept() as ctx:
         ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))
         ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))
@@ -105,39 +99,39 @@
         # Note that in practice this will not work unless run in a task.
         vol.reload()
 
         assert servicer.volume_reloads[vol.object_id] == 1
 
 
 def test_redeploy(servicer, client):
-    app = modal.App()
+    stub = modal.Stub()
 
     with pytest.warns(DeprecationError):
         v1 = modal.Volume.new()
         v2 = modal.Volume.new()
         v3 = modal.Volume.new()
-        app.v1, app.v2, app.v3 = v1, v2, v3
+        stub.v1, stub.v2, stub.v3 = v1, v2, v3
 
     # Deploy app once
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app1_ids = [v1.object_id, v2.object_id, v3.object_id]
 
     # Deploy app again
-    deploy_app(app, "my-app", client=client)
+    deploy_stub(stub, "my-app", client=client)
     app2_ids = [v1.object_id, v2.object_id, v3.object_id]
 
     # Make sure ids are stable
     assert app1_ids == app2_ids
 
     # Make sure ids are unique
     assert len(set(app1_ids)) == 3
     assert len(set(app2_ids)) == 3
 
     # Deploy to a different app
-    deploy_app(app, "my-other-app", client=client)
+    deploy_stub(stub, "my-other-app", client=client)
     app3_ids = [v1.object_id, v2.object_id, v3.object_id]
 
     # Should be unique and different
     assert len(set(app3_ids)) == 3
     assert set(app1_ids) & set(app3_ids) == set()
 
 
@@ -341,52 +335,7 @@
 def test_ephemeral(servicer, client):
     assert servicer.n_vol_heartbeats == 0
     with modal.Volume.ephemeral(client=client, _heartbeat_sleep=1) as vol:
         assert vol.listdir("**") == []
         # TODO(erikbern): perform some operations
         time.sleep(1.5)  # Make time for 2 heartbeats
     assert servicer.n_vol_heartbeats == 2
-
-
-def test_lazy_hydration_from_named(set_env_client):
-    vol = modal.Volume.from_name("my-vol", create_if_missing=True)
-    assert vol.listdir("**") == []
-
-
-@pytest.mark.skipif(platform.system() != "Linux", reason="needs /proc")
-@pytest.mark.asyncio
-async def test_open_files_error_annotation(tmp_path):
-    assert _open_files_error_annotation(tmp_path) is None
-
-    # Current process keeps file open
-    with (tmp_path / "foo.txt").open("w") as _f:
-        assert _open_files_error_annotation(tmp_path) == "path foo.txt is open"
-
-    # cwd of current process is inside volume
-    cwd = os.getcwd()
-    os.chdir(tmp_path)
-    assert _open_files_error_annotation(tmp_path) == "cwd is inside volume"
-    os.chdir(cwd)
-
-    # Subprocess keeps open file
-    open_path = tmp_path / "bar.txt"
-    open_path.write_text("")
-    proc = await asyncio.create_subprocess_exec("tail", "-f", open_path.as_posix())
-    await asyncio.sleep(0.01)  # Give process some time to start
-    assert _open_files_error_annotation(tmp_path) == f"path bar.txt is open from 'tail -f {open_path.as_posix()}'"
-    proc.kill()
-    await proc.wait()
-    assert _open_files_error_annotation(tmp_path) is None
-
-    # Subprocess cwd inside volume
-    proc = await asyncio.create_subprocess_exec(
-        sys.executable, "-c", f"import time; import os; os.chdir('{tmp_path}'); time.sleep(60)"
-    )
-    # Wait for process to chdir
-    for _ in range(100):
-        if os.readlink(f"/proc/{proc.pid}/cwd") == tmp_path.as_posix():
-            break
-        await asyncio.sleep(0.05)
-    assert re.match(f"^cwd of '{sys.executable} -c .*' is inside volume$", _open_files_error_annotation(tmp_path))
-    proc.kill()
-    await proc.wait()
-    assert _open_files_error_annotation(tmp_path) is None
```

## test/webhook_test.py

```diff
@@ -2,46 +2,47 @@
 import pathlib
 import pytest
 import subprocess
 import sys
 
 from fastapi.testclient import TestClient
 
-from modal import App, asgi_app, web_endpoint, wsgi_app
+from modal import Stub, asgi_app, web_endpoint, wsgi_app
 from modal._asgi import webhook_asgi_app
+from modal.app import ContainerApp
 from modal.exception import InvalidError
 from modal.functions import Function
-from modal.running_app import RunningApp
 from modal_proto import api_pb2
 
-app = App()
+stub = Stub()
 
 
-@app.function(cpu=42)
+@stub.function(cpu=42)
 @web_endpoint(method="PATCH")
 async def f(x):
     return {"square": x**2}
 
 
 @pytest.mark.asyncio
-async def test_webhook(servicer, client, reset_container_app):
-    async with app.run(client=client):
+async def test_webhook(servicer, client):
+    async with stub.run(client=client):
         assert f.web_url
 
         assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION
         assert servicer.app_functions["fu-1"].webhook_config.method == "PATCH"
 
         # Make sure we can call the webhooks
         # TODO: reinstate `.remote` check when direct webhook fn invocation is fixed.
         # assert await f.remote(10)
         assert await f.local(100) == {"square": 10000}
 
         # Make sure the container gets the app id as well
-        container_app = RunningApp(app_id=app.app_id)
-        app._init_container(client, container_app)
+        container_app = ContainerApp()
+        await ContainerApp.init.aio(client, stub.app_id)
+        container_app.associate_stub_container(stub)
         assert isinstance(f, Function)
         assert f.web_url
 
 
 def test_webhook_cors():
     def handler():
         return {"message": "Hello, World!"}
@@ -74,19 +75,19 @@
     app = webhook_asgi_app(handler, method="GET")
     client = TestClient(app)
     assert client.get("/docs").status_code == 404
     assert client.get("/redoc").status_code == 404
 
 
 def test_webhook_generator():
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError) as excinfo:
 
-        @app.function(serialized=True)
+        @stub.function(serialized=True)
         @web_endpoint()
         def web_gen():
             yield None
 
     assert "streaming" in str(excinfo.value).lower()
 
 
@@ -98,41 +99,41 @@
     stderr = ret.stderr.decode()
     assert "absent_minded_function" in stderr
     assert "@stub.function" in stderr
 
 
 @pytest.mark.asyncio
 async def test_webhook_decorator_in_wrong_order(servicer, client):
-    app = App()
+    stub = Stub()
 
     with pytest.raises(InvalidError) as excinfo:
 
         @web_endpoint()  # type: ignore
-        @app.function(serialized=True)
+        @stub.function(serialized=True)
         async def g(x):
             pass
 
     assert "wrong order" in str(excinfo.value).lower()
 
 
 @pytest.mark.asyncio
 async def test_asgi_wsgi(servicer, client):
-    app = App()
+    stub = Stub()
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @asgi_app()
     async def my_asgi(x):
         pass
 
-    @app.function(serialized=True)
+    @stub.function(serialized=True)
     @wsgi_app()
     async def my_wsgi(x):
         pass
 
-    async with app.run(client=client):
+    async with stub.run(client=client):
         pass
 
     assert len(servicer.app_functions) == 2
     assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP
     assert servicer.app_functions["fu-2"].webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP
```

## Comparing `modal/_container_io_manager.py` & `modal/stub.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,666 +1,783 @@
-# Copyright Modal Labs 2024
-import asyncio
-import json
-import math
+# Copyright Modal Labs 2022
+import inspect
 import os
-import signal
-import time
-import traceback
-from pathlib import Path
-from typing import Any, AsyncGenerator, AsyncIterator, Callable, ClassVar, List, Optional, Set, Tuple
-
-from google.protobuf.empty_pb2 import Empty
-from google.protobuf.message import Message
-from grpclib import Status
+import typing
+from pathlib import PurePosixPath
+from typing import Any, AsyncGenerator, Callable, ClassVar, Dict, List, Optional, Sequence, Tuple, Union
+
 from synchronicity.async_wrap import asynccontextmanager
 
 from modal_proto import api_pb2
 
-from ._serialization import deserialize, deserialize_data_format, serialize, serialize_data_format
-from ._traceback import extract_traceback
-from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
-from ._utils.blob_utils import MAX_OBJECT_SIZE_BYTES, blob_download, blob_upload
-from ._utils.function_utils import _stream_function_call_data
-from ._utils.grpc_utils import get_proto_oneof, retry_transient_errors
-from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
-from .config import config, logger
-from .exception import InputCancellation, InvalidError
-from .running_app import RunningApp
+from ._ipython import is_notebook
+from ._output import OutputManager
+from ._resolver import Resolver
+from ._utils.async_utils import synchronize_api
+from ._utils.function_utils import FunctionInfo
+from ._utils.mount_utils import validate_volumes
+from .app import _ContainerApp, _LocalApp
+from .client import _Client
+from .cls import _Cls
+from .config import logger
+from .exception import InvalidError, deprecation_error, deprecation_warning
+from .functions import _Function
+from .gpu import GPU_T
+from .image import _Image
+from .mount import _Mount
+from .network_file_system import _NetworkFileSystem
+from .object import _Object
+from .partial_function import PartialFunction, _find_callables_for_cls, _PartialFunction, _PartialFunctionFlags
+from .proxy import _Proxy
+from .retries import Retries
+from .runner import _run_stub
+from .sandbox import _Sandbox
+from .schedule import Schedule
+from .scheduler_placement import SchedulerPlacement
+from .secret import _Secret
+from .volume import _Volume
+
+_default_image: _Image = _Image.debian_slim()
+
+
+class _LocalEntrypoint:
+    _info: FunctionInfo
+    _stub: "_Stub"
+
+    def __init__(self, info, stub):
+        self._info = info  # type: ignore
+        self._stub = stub
+
+    def __call__(self, *args, **kwargs):
+        return self._info.raw_f(*args, **kwargs)
+
+    @property
+    def info(self) -> FunctionInfo:
+        return self._info
+
+    @property
+    def stub(self) -> "_Stub":
+        return self._stub
+
+
+LocalEntrypoint = synchronize_api(_LocalEntrypoint)
+
+
+def check_sequence(items: typing.Sequence[typing.Any], item_type: typing.Type[typing.Any], error_msg: str):
+    if not isinstance(items, (list, tuple)):
+        raise InvalidError(error_msg)
+    if not all(isinstance(v, item_type) for v in items):
+        raise InvalidError(error_msg)
+
+
+CLS_T = typing.TypeVar("CLS_T", bound=typing.Type)
+
+
+class _Stub:
+    """A `Stub` is a description of how to create a Modal application.
+
+    The stub object principally describes Modal objects (`Function`, `Image`,
+    `Secret`, etc.) associated with the application. It has three responsibilities:
+
+    * Syncing of identities across processes (your local Python interpreter and
+      every Modal worker active in your application).
+    * Making Objects stay alive and not be garbage collected for as long as the
+      app lives (see App lifetime below).
+    * Manage log collection for everything that happens inside your code.
+
+    **Registering functions with an app**
+
+    The most common way to explicitly register an Object with an app is through the
+    `@stub.function()` decorator. It both registers the annotated function itself and
+    other passed objects, like schedules and secrets, with the app:
+
+    ```python
+    import modal
+
+    stub = modal.Stub()
+
+    @stub.function(
+        secrets=[modal.Secret.from_name("some_secret")],
+        schedule=modal.Period(days=1),
+    )
+    def foo():
+        pass
+    ```
 
-MAX_OUTPUT_BATCH_SIZE: int = 49
+    In this example, the secret and schedule are registered with the app.
+    """
 
-RTT_S: float = 0.5  # conservative estimate of RTT in seconds.
+    _name: Optional[str]
+    _description: Optional[str]
+    _indexed_objects: Dict[str, _Object]
+    _function_mounts: Dict[str, _Mount]
+    _mounts: Sequence[_Mount]
+    _secrets: Sequence[_Secret]
+    _volumes: Dict[Union[str, PurePosixPath], _Volume]
+    _web_endpoints: List[str]  # Used by the CLI
+    _local_entrypoints: Dict[str, _LocalEntrypoint]
+    _container_app: Optional[_ContainerApp]
+    _local_app: Optional[_LocalApp]
+    _all_stubs: ClassVar[Dict[Optional[str], List["_Stub"]]] = {}
 
+    def __init__(
+        self,
+        name: Optional[str] = None,
+        *,
+        image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)
+        mounts: Sequence[_Mount] = [],  # default mounts for all functions
+        secrets: Sequence[_Secret] = [],  # default secrets for all functions
+        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # default volumes for all functions
+        **indexed_objects: _Object,  # any Modal Object dependencies (Dict, Queue, etc.)
+    ) -> None:
+        """Construct a new app stub, optionally with default image, mounts, secrets
 
-class UserException(Exception):
-    """Used to shut down the task gracefully."""
+        Any "indexed_objects" objects are loaded as part of running or deploying the app,
+        and are accessible by name on the running container app, e.g.:
+        ```python
+        stub = modal.Stub(key_value_store=modal.Dict.new())
+
+        @stub.function()
+        def store_something(key: str, value: str):
+            stub.app.key_value_store.put(key, value)
+        ```
+        """
 
+        self._name = name
+        self._description = name
 
-class Sentinel:
-    """Used to get type-stubs to work with this object."""
+        check_sequence(mounts, _Mount, "mounts has to be a list or tuple of Mount objects")
+        check_sequence(secrets, _Secret, "secrets has to be a list or tuple of Secret objects")
+        validate_volumes(volumes)
+
+        if image is not None and not isinstance(image, _Image):
+            raise InvalidError("image has to be a modal Image or AioImage object")
+
+        if indexed_objects:
+            deprecation_warning(
+                (2023, 12, 13),
+                "Passing **kwargs to a stub is deprecated. In most cases, you can just define the objects in global scope.",
+            )
 
+        for k, v in indexed_objects.items():
+            self._validate_blueprint_value(k, v)
 
-class _ContainerIOManager:
-    """Synchronizes all RPC calls and network operations for a running container.
+        self._indexed_objects = indexed_objects
+        if image is not None:
+            self._indexed_objects["image"] = image  # backward compatibility since "image" used to be on the blueprint
+
+        self._mounts = mounts
+
+        self._secrets = secrets
+        self._volumes = volumes
+        self._local_entrypoints = {}
+        self._web_endpoints = []
+        self._local_app = None  # when this is the launcher process
+        self._container_app = None  # when this is inside a container
+
+        # Register this stub. This is used to look up the stub in the container, when we can't get it from the function
+        _Stub._all_stubs.setdefault(self._name, []).append(self)
+
+    @property
+    def name(self) -> Optional[str]:
+        """The user-provided name of the Stub."""
+        return self._name
+
+    @property
+    def is_interactive(self) -> bool:
+        """Whether the current app for the stub is running in interactive mode."""
+        # return self._name
+        if self._local_app:
+            return self._local_app.is_interactive
+        else:
+            return False
 
-    TODO: maybe we shouldn't synchronize the whole class.
-    Then we could potentially move a bunch of the global functions onto it.
-    """
+    @property
+    def app_id(self) -> Optional[str]:
+        """Return the app_id, if the stub is running."""
+        if self._container_app:
+            return self._container_app._app_id
+        elif self._local_app:
+            return self._local_app._app_id
+        else:
+            return None
 
-    cancelled_input_ids: Set[str]
-    task_id: str
-    function_id: str
-    app_id: str
-    function_def: api_pb2.Function
-    checkpoint_id: Optional[str]
-
-    calls_completed: int
-    total_user_time: float
-    current_input_id: Optional[str]
-    current_input_started_at: Optional[float]
-
-    _input_concurrency: Optional[int]
-    _semaphore: Optional[asyncio.Semaphore]
-    _environment_name: str
-    _waiting_for_checkpoint: bool
-    _heartbeat_loop: Optional[asyncio.Task]
-
-    _is_interactivity_enabled: bool
-    _fetching_inputs: bool
-
-    _client: _Client
-
-    _GENERATOR_STOP_SENTINEL: ClassVar[Sentinel] = Sentinel()
-    _singleton: ClassVar[Optional["_ContainerIOManager"]] = None
-
-    def _init(self, container_args: api_pb2.ContainerArguments, client: _Client):
-        self.cancelled_input_ids = set()
-        self.task_id = container_args.task_id
-        self.function_id = container_args.function_id
-        self.app_id = container_args.app_id
-        self.function_def = container_args.function_def
-        self.checkpoint_id = container_args.checkpoint_id or None
-
-        self.calls_completed = 0
-        self.total_user_time = 0.0
-        self.current_input_id = None
-        self.current_input_started_at = None
-
-        self._input_concurrency = None
-
-        self._semaphore = None
-        self._environment_name = container_args.environment_name
-        self._waiting_for_checkpoint = False
-        self._heartbeat_loop = None
-
-        self._is_interactivity_enabled = False
-        self._fetching_inputs = True
-
-        self._client = client
-        assert isinstance(self._client, _Client)
-
-    def __new__(cls, container_args: api_pb2.ContainerArguments, client: _Client) -> "_ContainerIOManager":
-        cls._singleton = super().__new__(cls)
-        cls._singleton._init(container_args, client)
-        return cls._singleton
-
-    @classmethod
-    def _reset_singleton(cls):
-        """Only used for tests."""
-        cls._singleton = None
-
-    async def _run_heartbeat_loop(self):
-        while 1:
-            t0 = time.monotonic()
-            try:
-                if await self._heartbeat_handle_cancellations():
-                    # got a cancellation event, fine to start another heartbeat immediately
-                    # since the cancellation queue should be empty on the worker server
-                    # however, we wait at least 1s to prevent short-circuiting the heartbeat loop
-                    # in case there is ever a bug. This means it will take at least 1s between
-                    # two subsequent cancellations on the same task at the moment
-                    await asyncio.sleep(1.0)
-                    continue
-            except Exception as exc:
-                # don't stop heartbeat loop if there are transient exceptions!
-                time_elapsed = time.monotonic() - t0
-                error = exc
-                logger.warning(f"Heartbeat attempt failed ({time_elapsed=}, {error=})")
-
-            heartbeat_duration = time.monotonic() - t0
-            time_until_next_hearbeat = max(0.0, HEARTBEAT_INTERVAL - heartbeat_duration)
-            await asyncio.sleep(time_until_next_hearbeat)
-
-    async def _heartbeat_handle_cancellations(self) -> bool:
-        # Return True if a cancellation event was received, in that case we shouldn't wait too long for another heartbeat
-
-        # Don't send heartbeats for tasks waiting to be checkpointed.
-        # Calling gRPC methods open new connections which block the
-        # checkpointing process.
-        if self._waiting_for_checkpoint:
-            return False
+    @property
+    def description(self) -> Optional[str]:
+        """The Stub's `name`, if available, or a fallback descriptive identifier."""
+        return self._description
+
+    def set_description(self, description: str):
+        self._description = description
+
+    def _validate_blueprint_value(self, key: str, value: Any):
+        if not isinstance(value, _Object):
+            raise InvalidError(f"Stub attribute {key} with value {value} is not a valid Modal object")
+
+    def _add_object(self, tag, obj):
+        if self._container_app:
+            # If this is inside a container, then objects can be defined after app initialization.
+            # So we may have to initialize objects once they get bound to the stub.
+            if self._container_app._has_object(tag):
+                self._container_app._hydrate_object(obj, tag)
+
+        self._indexed_objects[tag] = obj
+
+    def __getitem__(self, tag: str):
+        """Stub assignments of the form `stub.x` or `stub["x"]` are deprecated!
+
+        The only use cases for these assignments is in conjunction with `.new()`, which is now
+        in itself deprecated. If you are constructing objects with `.from_name(...)`, there is no
+        need to assign those objects to the stub. Example:
+
+        ```python
+        d = modal.Dict.from_name("my-dict", create_if_missing=True)
+
+        @stub.function()
+        def f(x, y):
+            d[x] = y  # Refer to d in global scope
+        ```
+        """
+        deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+        return self._indexed_objects[tag]
 
-        request = api_pb2.ContainerHeartbeatRequest(supports_graceful_input_cancellation=True)
-        if self.current_input_id is not None:
-            request.current_input_id = self.current_input_id
-        if self.current_input_started_at is not None:
-            request.current_input_started_at = self.current_input_started_at
-
-        # TODO(erikbern): capture exceptions?
-        response = await retry_transient_errors(
-            self._client.stub.ContainerHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT
-        )
+    def __setitem__(self, tag: str, obj: _Object):
+        deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+        self._validate_blueprint_value(tag, obj)
+        # Deprecated ?
+        self._add_object(tag, obj)
+
+    def __getattr__(self, tag: str) -> _Object:
+        # TODO(erikbern): remove this method later
+        assert isinstance(tag, str)
+        if tag.startswith("__"):
+            # Hacky way to avoid certain issues, e.g. pickle will try to look this up
+            raise AttributeError(f"Stub has no member {tag}")
+        if tag not in self._indexed_objects:
+            # Primarily to make hasattr work
+            raise AttributeError(f"Stub has no member {tag}")
+        obj: _Object = self._indexed_objects[tag]
+        deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+        return obj
+
+    def __setattr__(self, tag: str, obj: _Object):
+        # TODO(erikbern): remove this method later
+        # Note that only attributes defined in __annotations__ are set on the object itself,
+        # everything else is registered on the indexed_objects
+        if tag in self.__annotations__:
+            object.__setattr__(self, tag, obj)
+        elif tag == "image":
+            self._indexed_objects["image"] = obj
+        else:
+            self._validate_blueprint_value(tag, obj)
+            deprecation_warning((2024, 3, 25), _Stub.__getitem__.__doc__)
+            self._add_object(tag, obj)
+
+    @property
+    def image(self) -> _Image:
+        # Exists to get the type inference working for `stub.image`
+        # Will also keep this one after we remove [get/set][item/attr]
+        return self._indexed_objects["image"]
+
+    @image.setter
+    def image(self, value):
+        self._indexed_objects["image"] = value
+
+    def get_objects(self) -> List[Tuple[str, _Object]]:
+        """Used by the container app to initialize objects."""
+        return list(self._indexed_objects.items())
+
+    def _uncreate_all_objects(self):
+        # TODO(erikbern): this doesn't unhydrate objects that aren't tagged
+        for obj in self._indexed_objects.values():
+            obj._unhydrate()
+
+    def is_inside(self, image: Optional[_Image] = None):
+        """Deprecated: use `Image.imports()` instead! Usage:
+        ```
+        my_image = modal.Image.debian_slim().pip_install("torch")
+        with my_image.imports():
+            import torch
+        ```
+        """
+        deprecation_error((2023, 11, 8), _Stub.is_inside.__doc__)
 
-        if response.HasField("cancel_input_event"):
-            # Pause processing of the current input by signaling self a SIGUSR1.
-            input_ids_to_cancel = response.cancel_input_event.input_ids
-            if input_ids_to_cancel:
-                if self._input_concurrency > 1:
-                    logger.info(
-                        "Shutting down task to stop some subset of inputs (concurrent functions don't support fine-grained cancellation)"
-                    )
-                    # This is equivalent to a task cancellation or preemption from worker code,
-                    # except we do not send a SIGKILL to forcefully exit after 30 seconds.
-                    #
-                    # SIGINT always interrupts the main thread, but not any auxiliary threads. On a
-                    # sync function without concurrent inputs, this raises a KeyboardInterrupt. When
-                    # there are concurrent inputs, we cannot interrupt the thread pool, but the
-                    # interpreter stops waiting for daemon threads and exits. On async functions,
-                    # this signal lands outside the event loop, stopping `run_until_complete()`.
-                    os.kill(os.getpid(), signal.SIGINT)
-
-                elif self.current_input_id in input_ids_to_cancel:
-                    # This goes to a registered signal handler for sync Modal functions, or to the
-                    # `SignalHandlingEventLoop` for async functions.
-                    #
-                    # We only send this signal on functions that do not have concurrent inputs enabled.
-                    # This allows us to do fine-grained input cancellation. On sync functions, the
-                    # SIGUSR1 signal should interrupt the main thread where user code is running,
-                    # raising an InputCancellation() exception. On async functions, the signal should
-                    # reach a handler in SignalHandlingEventLoop, which cancels the task.
-                    os.kill(os.getpid(), signal.SIGUSR1)
-            return True
-        return False
+    @asynccontextmanager
+    async def _set_local_app(self, app: _LocalApp) -> AsyncGenerator[None, None]:
+        self._local_app = app
+        try:
+            yield
+        finally:
+            self._local_app = None
 
     @asynccontextmanager
-    async def heartbeats(self) -> AsyncGenerator[None, None]:
-        async with TaskContext() as tc:
-            self._heartbeat_loop = t = tc.create_task(self._run_heartbeat_loop())
-            t.set_name("heartbeat loop")
-            try:
-                yield
-            finally:
-                t.cancel()
-
-    def stop_heartbeat(self):
-        if self._heartbeat_loop:
-            self._heartbeat_loop.cancel()
-
-    async def get_app_objects(self) -> RunningApp:
-        req = api_pb2.AppGetObjectsRequest(app_id=self.app_id, include_unindexed=True)
-        resp = await retry_transient_errors(self._client.stub.AppGetObjects, req)
-        logger.debug(f"AppGetObjects received {len(resp.items)} objects for app {self.app_id}")
-
-        tag_to_object_id = {}
-        object_handle_metadata = {}
-        for item in resp.items:
-            handle_metadata: Optional[Message] = get_proto_oneof(item.object, "handle_metadata_oneof")
-            object_handle_metadata[item.object.object_id] = handle_metadata
-            if item.tag:
-                tag_to_object_id[item.tag] = item.object.object_id
-
-        return RunningApp(
-            self.app_id,
-            environment_name=self._environment_name,
-            tag_to_object_id=tag_to_object_id,
-            object_handle_metadata=object_handle_metadata,
-        )
+    async def run(
+        self,
+        client: Optional[_Client] = None,
+        stdout=None,
+        show_progress: bool = True,
+        detach: bool = False,
+        output_mgr: Optional[OutputManager] = None,
+    ) -> AsyncGenerator["_Stub", None]:
+        """Context manager that runs an app on Modal.
+
+        Use this as the main entry point for your Modal application. All calls
+        to Modal functions should be made within the scope of this context
+        manager, and they will correspond to the current app.
+
+        Note that this method used to return a separate "App" object. This is
+        no longer useful since you can use the stub itself for access to all
+        objects. For backwards compatibility reasons, it returns the same stub.
+        """
+        # TODO(erikbern): deprecate this one too?
+        async with _run_stub(self, client, stdout, show_progress, detach, output_mgr):
+            yield self
+
+    def _get_default_image(self):
+        if "image" in self._indexed_objects:
+            return self._indexed_objects["image"]
+        else:
+            return _default_image
 
-    async def get_serialized_function(self) -> Tuple[Optional[Any], Callable]:
-        # Fetch the serialized function definition
-        request = api_pb2.FunctionGetSerializedRequest(function_id=self.function_id)
-        response = await self._client.stub.FunctionGetSerialized(request)
-        fun = self.deserialize(response.function_serialized)
+    def _get_watch_mounts(self):
+        all_mounts = [
+            *self._mounts,
+        ]
+        for function in self.registered_functions.values():
+            all_mounts.extend(function._all_mounts)
+
+        return [m for m in all_mounts if m.is_local()]
+
+    def _add_function(self, function: _Function):
+        if function.tag in self._indexed_objects:
+            old_function = self._indexed_objects[function.tag]
+            if isinstance(old_function, _Function):
+                if not is_notebook():
+                    logger.warning(
+                        f"Warning: Tag '{function.tag}' collision!"
+                        f" Overriding existing function [{old_function._info.module_name}].{old_function._info.function_name}"
+                        f" with new function [{function._info.module_name}].{function._info.function_name}"
+                    )
+            else:
+                logger.warning(f"Warning: tag {function.tag} exists but is overridden by function")
 
-        if response.class_serialized:
-            cls = self.deserialize(response.class_serialized)
-        else:
-            cls = None
+        self._add_object(function.tag, function)
 
-        return cls, fun
+    @property
+    def registered_functions(self) -> Dict[str, _Function]:
+        """All modal.Function objects registered on the stub."""
+        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Function)}
 
-    def serialize(self, obj: Any) -> bytes:
-        return serialize(obj)
+    @property
+    def registered_classes(self) -> Dict[str, _Function]:
+        """All modal.Cls objects registered on the stub."""
+        return {tag: obj for tag, obj in self._indexed_objects.items() if isinstance(obj, _Cls)}
 
-    def deserialize(self, data: bytes) -> Any:
-        return deserialize(data, self._client)
+    @property
+    def registered_entrypoints(self) -> Dict[str, _LocalEntrypoint]:
+        """All local CLI entrypoints registered on the stub."""
+        return self._local_entrypoints
 
-    @synchronizer.no_io_translation
-    def serialize_data_format(self, obj: Any, data_format: int) -> bytes:
-        return serialize_data_format(obj, data_format)
+    @property
+    def indexed_objects(self) -> Dict[str, _Object]:
+        return self._indexed_objects
 
-    def deserialize_data_format(self, data: bytes, data_format: int) -> Any:
-        return deserialize_data_format(data, data_format, self._client)
+    @property
+    def registered_web_endpoints(self) -> List[str]:
+        """Names of web endpoint (ie. webhook) functions registered on the stub."""
+        return self._web_endpoints
 
-    async def get_data_in(self, function_call_id: str) -> AsyncIterator[Any]:
-        """Read from the `data_in` stream of a function call."""
-        async for data in _stream_function_call_data(self._client, function_call_id, "data_in"):
-            yield data
+    def local_entrypoint(
+        self, _warn_parentheses_missing=None, *, name: Optional[str] = None
+    ) -> Callable[[Callable[..., Any]], None]:
+        """Decorate a function to be used as a CLI entrypoint for a Modal App.
 
-    async def put_data_out(
-        self,
-        function_call_id: str,
-        start_index: int,
-        data_format: int,
-        messages_bytes: List[Any],
-    ) -> None:
-        """Put data onto the `data_out` stream of a function call.
+        These functions can be used to define code that runs locally to set up the app,
+        and act as an entrypoint to start Modal functions from. Note that regular
+        Modal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,
+        those functions are executed remotely directly.
 
-        This is used for generator outputs, which includes web endpoint responses. Note that this
-        was introduced as a performance optimization in client version 0.57, so older clients will
-        still use the previous Postgres-backed system based on `FunctionPutOutputs()`.
-        """
-        data_chunks: List[api_pb2.DataChunk] = []
-        for i, message_bytes in enumerate(messages_bytes):
-            chunk = api_pb2.DataChunk(data_format=data_format, index=start_index + i)  # type: ignore
-            if len(message_bytes) > MAX_OBJECT_SIZE_BYTES:
-                chunk.data_blob_id = await blob_upload(message_bytes, self._client.stub)
-            else:
-                chunk.data = message_bytes
-            data_chunks.append(chunk)
+        **Example**
 
-        req = api_pb2.FunctionCallPutDataRequest(function_call_id=function_call_id, data_chunks=data_chunks)
-        await retry_transient_errors(self._client.stub.FunctionCallPutDataOut, req)
+        ```python
+        @stub.local_entrypoint()
+        def main():
+            some_modal_function.remote()
+        ```
 
-    async def generator_output_task(self, function_call_id: str, data_format: int, message_rx: asyncio.Queue) -> None:
-        """Task that feeds generator outputs into a function call's `data_out` stream."""
-        index = 1
-        received_sentinel = False
-        while not received_sentinel:
-            message = await message_rx.get()
-            if message is self._GENERATOR_STOP_SENTINEL:
-                break
-            # ASGI 'http.response.start' and 'http.response.body' msgs are observed to be separated by 1ms.
-            # If we don't sleep here for 1ms we end up with an extra call to .put_data_out().
-            if index == 1:
-                await asyncio.sleep(0.001)
-            messages_bytes = [serialize_data_format(message, data_format)]
-            total_size = len(messages_bytes[0]) + 512
-            while total_size < 16 * 1024 * 1024:  # 16 MiB, maximum size in a single message
-                try:
-                    message = message_rx.get_nowait()
-                except asyncio.QueueEmpty:
-                    break
-                if message is self._GENERATOR_STOP_SENTINEL:
-                    received_sentinel = True
-                    break
-                else:
-                    messages_bytes.append(serialize_data_format(message, data_format))
-                    total_size += len(messages_bytes[-1]) + 512  # 512 bytes for estimated framing overhead
-            await self.put_data_out(function_call_id, index, data_format, messages_bytes)
-            index += len(messages_bytes)
-
-    async def _queue_create(self, size: int) -> asyncio.Queue:
-        """Create a queue, on the synchronicity event loop (needed on Python 3.8 and 3.9)."""
-        return asyncio.Queue(size)
-
-    async def _queue_put(self, queue: asyncio.Queue, value: Any) -> None:
-        """Put a value onto a queue, using the synchronicity event loop."""
-        await queue.put(value)
-
-    async def populate_input_blobs(self, item: api_pb2.FunctionInput):
-        args = await blob_download(item.args_blob_id, self._client.stub)
-
-        # Mutating
-        item.ClearField("args_blob_id")
-        item.args = args
-        return item
-
-    def get_average_call_time(self) -> float:
-        if self.calls_completed == 0:
-            return 0
-
-        return self.total_user_time / self.calls_completed
-
-    def get_max_inputs_to_fetch(self):
-        if self.calls_completed == 0:
-            return 1
-
-        return math.ceil(RTT_S / max(self.get_average_call_time(), 1e-6))
-
-    @synchronizer.no_io_translation
-    async def _generate_inputs(self) -> AsyncIterator[Tuple[str, str, api_pb2.FunctionInput]]:
-        request = api_pb2.FunctionGetInputsRequest(function_id=self.function_id)
-        eof_received = False
-        iteration = 0
-        while not eof_received and self._fetching_inputs:
-            request.average_call_time = self.get_average_call_time()
-            request.max_values = self.get_max_inputs_to_fetch()  # Deprecated; remove.
-            request.input_concurrency = self._input_concurrency
-
-            await self._semaphore.acquire()
-            yielded = False
-            try:
-                # If number of active inputs is at max queue size, this will block.
-                iteration += 1
-                response: api_pb2.FunctionGetInputsResponse = await retry_transient_errors(
-                    self._client.stub.FunctionGetInputs, request
-                )
+        You can call the function using `modal run` directly from the CLI:
 
-                if response.rate_limit_sleep_duration:
-                    logger.info(
-                        "Task exceeded rate limit, sleeping for %.2fs before trying again."
-                        % response.rate_limit_sleep_duration
-                    )
-                    await asyncio.sleep(response.rate_limit_sleep_duration)
-                elif response.inputs:
-                    # for input cancellations and concurrency logic we currently assume
-                    # that there is no input buffering in the container
-                    assert len(response.inputs) == 1
-
-                    for item in response.inputs:
-                        if item.kill_switch:
-                            logger.debug(f"Task {self.task_id} input kill signal input.")
-                            eof_received = True
-                            break
-                        if item.input_id in self.cancelled_input_ids:
-                            continue
-
-                        # If we got a pointer to a blob, download it from S3.
-                        if item.input.WhichOneof("args_oneof") == "args_blob_id":
-                            input_pb = await self.populate_input_blobs(item.input)
-                        else:
-                            input_pb = item.input
-
-                        # If yielded, allow semaphore to be released via complete_call
-                        yield (item.input_id, item.function_call_id, input_pb)
-                        yielded = True
-
-                        # We only support max_inputs = 1 at the moment
-                        if item.input.final_input or self.function_def.max_inputs == 1:
-                            eof_received = True
-                            break
-            finally:
-                if not yielded:
-                    self._semaphore.release()
-
-    @synchronizer.no_io_translation
-    async def run_inputs_outputs(self, input_concurrency: int = 1) -> AsyncIterator[Tuple[str, str, Any, Any]]:
-        # Ensure we do not fetch new inputs when container is too busy.
-        # Before trying to fetch an input, acquire the semaphore:
-        # - if no input is fetched, release the semaphore.
-        # - or, when the output for the fetched input is sent, release the semaphore.
-        self._input_concurrency = input_concurrency
-        self._semaphore = asyncio.Semaphore(input_concurrency)
+        ```shell
+        modal run stub_module.py
+        ```
 
-        try:
-            async for input_id, function_call_id, input_pb in self._generate_inputs():
-                args, kwargs = self.deserialize(input_pb.args) if input_pb.args else ((), {})
-                self.current_input_id, self.current_input_started_at = (input_id, time.time())
-                yield input_id, function_call_id, args, kwargs
-                self.current_input_id, self.current_input_started_at = (None, None)
-        finally:
-            # collect all active input slots, meaning all inputs have wrapped up.
-            for _ in range(input_concurrency):
-                await self._semaphore.acquire()
-
-    async def _push_output(self, input_id, started_at: float, data_format=api_pb2.DATA_FORMAT_UNSPECIFIED, **kwargs):
-        # upload data to S3 if too big.
-        if "data" in kwargs and kwargs["data"] and len(kwargs["data"]) > MAX_OBJECT_SIZE_BYTES:
-            data_blob_id = await blob_upload(kwargs["data"], self._client.stub)
-            # mutating kwargs.
-            del kwargs["data"]
-            kwargs["data_blob_id"] = data_blob_id
-
-        output = api_pb2.FunctionPutOutputsItem(
-            input_id=input_id,
-            input_started_at=started_at,
-            output_created_at=time.time(),
-            result=api_pb2.GenericResult(**kwargs),
-            data_format=data_format,
-        )
+        Note that an explicit [`stub.run()`](/docs/reference/modal.Stub#run) is not needed, as an
+        [app](/docs/guide/apps) is automatically created for you.
 
-        await retry_transient_errors(
-            self._client.stub.FunctionPutOutputs,
-            api_pb2.FunctionPutOutputsRequest(outputs=[output]),
-            additional_status_codes=[Status.RESOURCE_EXHAUSTED],
-            max_retries=None,  # Retry indefinitely, trying every 1s.
-        )
+        **Multiple Entrypoints**
 
-    def serialize_exception(self, exc: BaseException) -> Optional[bytes]:
-        try:
-            return self.serialize(exc)
-        except Exception as serialization_exc:
-            logger.info(f"Failed to serialize exception {exc}: {serialization_exc}")
-            # We can't always serialize exceptions.
-            return None
+        If you have multiple `local_entrypoint` functions, you can qualify the name of your stub and function:
 
-    def serialize_traceback(self, exc: BaseException) -> Tuple[Optional[bytes], Optional[bytes]]:
-        serialized_tb, tb_line_cache = None, None
+        ```shell
+        modal run stub_module.py::stub.some_other_function
+        ```
 
-        try:
-            tb_dict, line_cache = extract_traceback(exc, self.task_id)
-            serialized_tb = self.serialize(tb_dict)
-            tb_line_cache = self.serialize(line_cache)
-        except Exception:
-            logger.info("Failed to serialize exception traceback.")
+        **Parsing Arguments**
 
-        return serialized_tb, tb_line_cache
+        If your entrypoint function take arguments with primitive types, `modal run` automatically parses them as
+        CLI options. For example, the following function can be called with `modal run stub_module.py --foo 1 --bar "hello"`:
 
-    @asynccontextmanager
-    async def handle_user_exception(self) -> AsyncGenerator[None, None]:
-        """Sets the task as failed in a way where it's not retried.
+        ```python
+        @stub.local_entrypoint()
+        def main(foo: int, bar: str):
+            some_modal_function.call(foo, bar)
+        ```
+
+        Currently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported. Use `modal run stub_module.py --help` for more
+        information on usage.
 
-        Used for handling exceptions from container lifecycle methods at the moment, which should
-        trigger a task failure state.
         """
-        try:
-            yield
-        except KeyboardInterrupt:
-            # Send no task result in case we get sigint:ed by the runner
-            # The status of the input should have been handled externally already in that case
-            raise
-        except BaseException as exc:
-            # Since this is on a different thread, sys.exc_info() can't find the exception in the stack.
-            traceback.print_exception(type(exc), exc, exc.__traceback__)
-
-            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
-
-            result = api_pb2.GenericResult(
-                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
-                data=self.serialize_exception(exc),
-                exception=repr(exc),
-                traceback="".join(traceback.format_exception(type(exc), exc, exc.__traceback__)),
-                serialized_tb=serialized_tb,
-                tb_line_cache=tb_line_cache,
+        if _warn_parentheses_missing:
+            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.local_entrypoint()`.")
+        if name is not None and not isinstance(name, str):
+            raise InvalidError("Invalid value for `name`: Must be string.")
+
+        def wrapped(raw_f: Callable[..., Any]) -> None:
+            info = FunctionInfo(raw_f)
+            tag = name if name is not None else raw_f.__qualname__
+            if tag in self._local_entrypoints:
+                # TODO: get rid of this limitation.
+                raise InvalidError(f"Duplicate local entrypoint name: {tag}. Local entrypoint names must be unique.")
+            entrypoint = self._local_entrypoints[tag] = _LocalEntrypoint(info, self)
+            return entrypoint
+
+        return wrapped
+
+    def function(
+        self,
+        _warn_parentheses_missing=None,
+        *,
+        image: Optional[_Image] = None,  # The image to run as the container for the function
+        schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
+        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
+        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
+        serialized: bool = False,  # Whether to send the function over using cloudpickle.
+        mounts: Sequence[_Mount] = (),  # Modal Mounts added to the container
+        network_file_systems: Dict[
+            Union[str, PurePosixPath], _NetworkFileSystem
+        ] = {},  # Mountpoints for Modal NetworkFileSystems
+        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # Mountpoints for Modal Volumes
+        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
+        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
+        memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
+        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
+        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
+        concurrency_limit: Optional[
+            int
+        ] = None,  # An optional maximum number of concurrent containers running the function (use keep_warm for minimum).
+        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
+        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
+        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
+        keep_warm: Optional[
+            int
+        ] = None,  # An optional minimum number of containers to always keep warm (use concurrency_limit for maximum).
+        name: Optional[str] = None,  # Sets the Modal name of the function within the stub
+        is_generator: Optional[
+            bool
+        ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
+        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
+        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
+        checkpointing_enabled: Optional[bool] = None,  # Deprecated
+        block_network: bool = False,  # Whether to block network access
+        max_inputs: Optional[
+            int
+        ] = None,  # Maximum number of inputs a container should handle before shutting down. With `max_inputs = 1`, containers will be single-use.
+        # The next group of parameters are deprecated; do not use in any new code
+        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
+        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
+        # Parameters below here are experimental. Use with caution!
+        _allow_background_volume_commits: bool = False,  # Experimental flag
+        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
+        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
+        _experimental_scheduler_placement: Optional[
+            SchedulerPlacement
+        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
+    ) -> Callable[..., _Function]:
+        """Decorator to register a new Modal function with this stub."""
+        if isinstance(_warn_parentheses_missing, _Image):
+            # Handle edge case where maybe (?) some users passed image as a positional arg
+            raise InvalidError("`image` needs to be a keyword argument: `@stub.function(image=image)`.")
+        if _warn_parentheses_missing:
+            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.function()`.")
+
+        if interactive:
+            deprecation_error(
+                (2024, 2, 29), "interactive=True has been deprecated. Set MODAL_INTERACTIVE_FUNCTIONS=1 instead."
             )
 
-            req = api_pb2.TaskResultRequest(result=result)
-            await retry_transient_errors(self._client.stub.TaskResult, req)
+        if image is None:
+            image = self._get_default_image()
 
-            # Shut down the task gracefully
-            raise UserException()
+        secrets = [*self._secrets, *secrets]
 
-    @asynccontextmanager
-    async def handle_input_exception(self, input_id, started_at: float) -> AsyncGenerator[None, None]:
-        """Handle an exception while processing a function input."""
-        try:
-            yield
-        except KeyboardInterrupt:
-            raise
-        except (InputCancellation, asyncio.CancelledError):
-            # just skip creating any output for this input and keep going with the next instead
-            # it should have been marked as cancelled already in the backend at this point so it
-            # won't be retried
-            logger.warning(f"The current input ({input_id=}) was cancelled by a user request")
-            await self.complete_call(started_at)
-            return
-        except BaseException as exc:
-            # print exception so it's logged
-            traceback.print_exc()
-            serialized_tb, tb_line_cache = self.serialize_traceback(exc)
-
-            # Note: we're not serializing the traceback since it contains
-            # local references that means we can't unpickle it. We *are*
-            # serializing the exception, which may have some issues (there
-            # was an earlier note about it that it might not be possible
-            # to unpickle it in some cases). Let's watch out for issues.
-            await self._push_output(
-                input_id,
-                started_at=started_at,
-                data_format=api_pb2.DATA_FORMAT_PICKLE,
-                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
-                data=self.serialize_exception(exc),
-                exception=repr(exc),
-                traceback=traceback.format_exc(),
-                serialized_tb=serialized_tb,
-                tb_line_cache=tb_line_cache,
+        def wrapped(
+            f: Union[_PartialFunction, Callable[..., Any]],
+            _cls: Optional[type] = None,  # Used for methods only
+        ) -> _Function:
+            nonlocal keep_warm, is_generator
+
+            if isinstance(f, _PartialFunction):
+                f.wrapped = True
+                info = FunctionInfo(f.raw_f, serialized=serialized, name_override=name, cls=_cls)
+                raw_f = f.raw_f
+                webhook_config = f.webhook_config
+                is_generator = f.is_generator
+                keep_warm = f.keep_warm or keep_warm
+
+                if webhook_config:
+                    if interactive:
+                        raise InvalidError("interactive=True is not supported with web endpoint functions")
+                    self._web_endpoints.append(info.get_tag())
+            else:
+                info = FunctionInfo(f, serialized=serialized, name_override=name, cls=_cls)
+                webhook_config = None
+                raw_f = f
+
+            if not _cls and not info.is_serialized() and "." in info.function_name:  # This is a method
+                raise InvalidError(
+                    "`stub.function` on methods is not allowed. See https://modal.com/docs/guide/lifecycle-functions instead"
+                )
+
+            if is_generator is None:
+                is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)
+
+            function = _Function.from_args(
+                info,
+                stub=self,
+                image=image,
+                secret=secret,
+                secrets=secrets,
+                schedule=schedule,
+                is_generator=is_generator,
+                gpu=gpu,
+                mounts=[*self._mounts, *mounts],
+                network_file_systems=network_file_systems,
+                allow_cross_region_volumes=allow_cross_region_volumes,
+                volumes={**self._volumes, **volumes},
+                memory=memory,
+                proxy=proxy,
+                retries=retries,
+                concurrency_limit=concurrency_limit,
+                allow_concurrent_inputs=allow_concurrent_inputs,
+                container_idle_timeout=container_idle_timeout,
+                timeout=timeout,
+                cpu=cpu,
+                keep_warm=keep_warm,
+                cloud=cloud,
+                webhook_config=webhook_config,
+                enable_memory_snapshot=enable_memory_snapshot,
+                checkpointing_enabled=checkpointing_enabled,
+                allow_background_volume_commits=_allow_background_volume_commits,
+                block_network=block_network,
+                max_inputs=max_inputs,
+                _experimental_boost=_experimental_boost,
+                _experimental_scheduler=_experimental_scheduler,
+                _experimental_scheduler_placement=_experimental_scheduler_placement,
             )
-            await self.complete_call(started_at)
 
-    async def complete_call(self, started_at):
-        self.total_user_time += time.time() - started_at
-        self.calls_completed += 1
-        self._semaphore.release()
-
-    @synchronizer.no_io_translation
-    async def push_output(self, input_id, started_at: float, data: Any, data_format: int) -> None:
-        await self._push_output(
-            input_id,
-            started_at=started_at,
-            data_format=data_format,
-            status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
-            data=self.serialize_data_format(data, data_format),
-        )
-        await self.complete_call(started_at)
+            self._add_function(function)
+            return function
 
-    async def restore(self) -> None:
-        # Busy-wait for restore. `/__modal/restore-state.json` is created
-        # by the worker process with updates to the container config.
-        restored_path = Path(config.get("restore_state_path"))
-        start = time.perf_counter()
-        while not restored_path.exists():
-            logger.debug(f"Waiting for restore (elapsed={time.perf_counter() - start:.3f}s)")
-            await asyncio.sleep(0.01)
-            continue
-
-        logger.debug("Container: restored")
-
-        # Look for state file and create new client with updated credentials.
-        # State data is serialized with key-value pairs, example: {"task_id": "tk-000"}
-        with restored_path.open("r") as file:
-            restored_state = json.load(file)
-
-        # Local ContainerIOManager state.
-        for key in ["task_id", "function_id"]:
-            if value := restored_state.get(key):
-                logger.debug(f"Updating ContainerIOManager.{key} = {value}")
-                setattr(self, key, restored_state[key])
-
-        # Env vars and global state.
-        for key, value in restored_state.items():
-            # Empty string indicates that value does not need to be updated.
-            if value != "":
-                config.override_locally(key, value)
-
-        # Restore input to default state.
-        self.current_input_id = None
-        self.current_input_started_at = None
-
-        self._client = await _Client.from_env()
-        self._waiting_for_checkpoint = False
-
-    async def checkpoint(self) -> None:
-        """Message server indicating that function is ready to be checkpointed."""
-        if self.checkpoint_id:
-            logger.debug(f"Checkpoint ID: {self.checkpoint_id}")
+        return wrapped
 
-        await self._client.stub.ContainerCheckpoint(
-            api_pb2.ContainerCheckpointRequest(checkpoint_id=self.checkpoint_id)
+    def cls(
+        self,
+        _warn_parentheses_missing=None,
+        *,
+        image: Optional[_Image] = None,  # The image to run as the container for the function
+        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
+        gpu: GPU_T = None,  # GPU specification as string ("any", "T4", "A10G", ...) or object (`modal.GPU.A100()`, ...)
+        serialized: bool = False,  # Whether to send the function over using cloudpickle.
+        mounts: Sequence[_Mount] = (),
+        network_file_systems: Dict[
+            Union[str, PurePosixPath], _NetworkFileSystem
+        ] = {},  # Mountpoints for Modal NetworkFileSystems
+        volumes: Dict[Union[str, PurePosixPath], _Volume] = {},  # Mountpoints for Modal Volumes
+        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
+        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
+        memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
+        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
+        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
+        concurrency_limit: Optional[int] = None,  # Limit for max concurrent containers running the function.
+        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
+        container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.
+        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
+        keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm.
+        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
+        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
+        checkpointing_enabled: Optional[bool] = None,  # Deprecated
+        block_network: bool = False,  # Whether to block network access
+        _allow_background_volume_commits: bool = False,
+        max_inputs: Optional[
+            int
+        ] = None,  # Limits the number of inputs a container handles before shutting down. Use `max_inputs = 1` for single-use containers.
+        # The next group of parameters are deprecated; do not use in any new code
+        interactive: bool = False,  # Deprecated: use the `modal.interact()` hook instead
+        secret: Optional[_Secret] = None,  # Deprecated: use `secrets`
+        # Parameters below here are experimental. Use with caution!
+        _experimental_boost: bool = False,  # Experimental flag for lower latency function execution (alpha).
+        _experimental_scheduler: bool = False,  # Experimental flag for more fine-grained scheduling (alpha).
+        _experimental_scheduler_placement: Optional[
+            SchedulerPlacement
+        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
+    ) -> Callable[[CLS_T], _Cls]:
+        if _warn_parentheses_missing:
+            raise InvalidError("Did you forget parentheses? Suggestion: `@stub.cls()`.")
+
+        decorator: Callable[[PartialFunction, type], _Function] = self.function(
+            image=image,
+            secret=secret,
+            secrets=secrets,
+            gpu=gpu,
+            serialized=serialized,
+            mounts=mounts,
+            network_file_systems=network_file_systems,
+            allow_cross_region_volumes=allow_cross_region_volumes,
+            volumes=volumes,
+            cpu=cpu,
+            memory=memory,
+            proxy=proxy,
+            retries=retries,
+            concurrency_limit=concurrency_limit,
+            allow_concurrent_inputs=allow_concurrent_inputs,
+            container_idle_timeout=container_idle_timeout,
+            timeout=timeout,
+            interactive=interactive,
+            keep_warm=keep_warm,
+            cloud=cloud,
+            enable_memory_snapshot=enable_memory_snapshot,
+            checkpointing_enabled=checkpointing_enabled,
+            block_network=block_network,
+            _allow_background_volume_commits=_allow_background_volume_commits,
+            max_inputs=max_inputs,
+            _experimental_boost=_experimental_boost,
+            _experimental_scheduler=_experimental_scheduler,
+            _experimental_scheduler_placement=_experimental_scheduler_placement,
         )
 
-        self._waiting_for_checkpoint = True
-        await self._client._close()
+        def wrapper(user_cls: CLS_T) -> _Cls:
+            cls: _Cls = _Cls.from_local(user_cls, self, decorator)
 
-        logger.debug("Checkpointing request sent. Connection closed.")
-        await self.restore()
-
-    async def volume_commit(self, volume_ids: List[str]) -> None:
-        """
-        Perform volume commit for given `volume_ids`.
-        Only used on container exit to persist uncommitted changes on behalf of user.
-        """
-        if not volume_ids:
-            return
-        await asyncify(os.sync)()
-        results = await asyncio.gather(
-            *[
-                retry_transient_errors(
-                    self._client.stub.VolumeCommit,
-                    api_pb2.VolumeCommitRequest(volume_id=v_id),
-                    max_retries=9,
-                    base_delay=0.25,
-                    max_delay=256,
-                    delay_factor=2,
+            if (
+                _find_callables_for_cls(user_cls, _PartialFunctionFlags.ENTER_PRE_CHECKPOINT)
+                and not enable_memory_snapshot
+            ):
+                raise InvalidError("A class must have `enable_memory_snapshot=True` to use `snap=True` on its methods.")
+
+            if len(cls._functions) > 1 and keep_warm is not None:
+                deprecation_warning(
+                    (2023, 10, 20),
+                    "`@stub.cls(keep_warm=...)` is deprecated when there is more than 1 method."
+                    " Use `@method(keep_warm=...)` on each method instead!",
                 )
-                for v_id in volume_ids
-            ],
-            return_exceptions=True,
-        )
-        for volume_id, res in zip(volume_ids, results):
-            if isinstance(res, Exception):
-                logger.error(f"modal.Volume background commit failed for {volume_id}. Exception: {res}")
-            else:
-                logger.debug(f"modal.Volume background commit success for {volume_id}.")
 
-    async def interact(self):
-        if self._is_interactivity_enabled:
-            # Currently, interactivity is enabled forever
-            return
-        self._is_interactivity_enabled = True
-
-        if not self.function_def.pty_info:
-            raise InvalidError(
-                "Interactivity is not enabled in this function. Use MODAL_INTERACTIVE_FUNCTIONS=1 to enable interactivity."
-            )
+            tag: str = user_cls.__name__
+            self._add_object(tag, cls)
+            return cls
 
-        if self.function_def.concurrency_limit > 1:
-            print(
-                "Warning: Interactivity is not supported on functions with concurrency > 1. You may experience unexpected behavior."
-            )
+        return wrapper
 
-        # todo(nathan): add warning if concurrency limit > 1. but idk how to check this here
-        # todo(nathan): check if function interactivity is enabled
-        try:
-            await self._client.stub.FunctionStartPtyShell(Empty())
-        except Exception as e:
-            print("Error: Failed to start PTY shell.")
-            raise e
+    async def spawn_sandbox(
+        self,
+        *entrypoint_args: str,
+        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
+        mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.
+        secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
+        network_file_systems: Dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
+        timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
+        workdir: Optional[str] = None,  # Working directory of the sandbox.
+        gpu: GPU_T = None,
+        cloud: Optional[str] = None,
+        cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
+        memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
+        block_network: bool = False,  # Whether to block network access
+        volumes: Dict[Union[str, os.PathLike], _Volume] = {},  # Volumes to mount in the sandbox.
+        _allow_background_volume_commits: bool = False,
+        pty_info: Optional[api_pb2.PTYInfo] = None,
+    ) -> _Sandbox:
+        """Sandboxes are a way to run arbitrary commands in dynamically defined environments.
 
-    @classmethod
-    def stop_fetching_inputs(cls):
-        assert cls._singleton
-        cls._singleton._fetching_inputs = False
+        This function returns a [SandboxHandle](/docs/reference/modal.Sandbox#modalsandboxsandbox), which can be used to interact with the running sandbox.
 
+        Refer to the [docs](/docs/guide/sandbox) on how to spawn and use sandboxes.
+        """
+        from .sandbox import _Sandbox
+        from .stub import _default_image
 
-ContainerIOManager = synchronize_api(_ContainerIOManager)
+        if self._local_app:
+            app_id = self._local_app.app_id
+            environment_name = self._local_app._environment_name
+            client = self._local_app.client
+        elif self._container_app:
+            app_id = self._container_app.app_id
+            environment_name = self._container_app._environment_name
+            client = self._container_app.client
+        else:
+            raise InvalidError("`stub.spawn_sandbox` requires a running app.")
 
+        # TODO(erikbern): pulling a lot of app internals here, let's clean up shortly
+        resolver = Resolver(client, environment_name=environment_name, app_id=app_id)
+        obj = _Sandbox._new(
+            entrypoint_args,
+            image=image or _default_image,
+            mounts=mounts,
+            secrets=secrets,
+            timeout=timeout,
+            workdir=workdir,
+            gpu=gpu,
+            cloud=cloud,
+            cpu=cpu,
+            memory=memory,
+            network_file_systems=network_file_systems,
+            block_network=block_network,
+            volumes=volumes,
+            allow_background_volume_commits=_allow_background_volume_commits,
+            pty_info=pty_info,
+        )
+        await resolver.load(obj)
+        return obj
 
-def is_local() -> bool:
-    """Returns if we are currently on the machine launching/deploying a Modal app
+    def include(self, /, other_stub: "_Stub"):
+        """Include another stub's objects in this one.
 
-    Returns `True` when executed locally on the user's machine.
-    Returns `False` when executed from a Modal container in the cloud.
-    """
-    return not _ContainerIOManager._singleton
+        Useful splitting up Modal apps across different self-contained files
 
+        ```python
+        stub_a = modal.Stub("a")
+        @stub.function()
+        def foo():
+            ...
+
+        stub_b = modal.Stub("b")
+        @stub.function()
+        def bar():
+            ...
+
+        stub_a.include(stub_b)
+
+        @stub_a.local_entrypoint()
+        def main():
+            # use function declared on the included stub
+            bar.remote()
+        ```
+        """
+        for tag, object in other_stub._indexed_objects.items():
+            existing_object = self._indexed_objects.get(tag)
+            if existing_object and existing_object != object:
+                logger.warning(
+                    f"Named app object {tag} with existing value {existing_object} is being overwritten by a different object {object}"
+                )
 
-async def _interact() -> None:
-    container_io_manager = _ContainerIOManager._singleton
-    if not container_io_manager:
-        raise InvalidError("Interactivity only works inside a Modal container.")
-    else:
-        await container_io_manager.interact()
+            self._add_object(tag, object)
 
 
-interact = synchronize_api(_interact)
+Stub = synchronize_api(_Stub)
```

## Comparing `modal/_container_io_manager.pyi` & `modal/_container_entrypoint.pyi`

 * *Files 20% similar despite different names*

```diff
@@ -1,71 +1,50 @@
-import _asyncio
-import asyncio.locks
 import asyncio.queues
+import collections.abc
+import modal.app
 import modal.client
-import modal.running_app
+import modal.functions
+import modal.stub
 import modal_proto.api_pb2
-import synchronicity.combined_types
 import typing
 import typing_extensions
 
 class UserException(Exception):
     ...
 
-class Sentinel:
-    ...
+class UserCodeEventLoop:
+    def __enter__(self):
+        ...
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        ...
 
-class _ContainerIOManager:
-    cancelled_input_ids: typing.Set[str]
-    task_id: str
-    function_id: str
-    app_id: str
-    function_def: modal_proto.api_pb2.Function
-    checkpoint_id: typing.Union[str, None]
-    calls_completed: int
-    total_user_time: float
-    current_input_id: typing.Union[str, None]
-    current_input_started_at: typing.Union[float, None]
-    _input_concurrency: typing.Union[int, None]
-    _semaphore: typing.Union[asyncio.locks.Semaphore, None]
-    _environment_name: str
-    _waiting_for_checkpoint: bool
-    _heartbeat_loop: typing.Union[_asyncio.Task, None]
-    _is_interactivity_enabled: bool
-    _fetching_inputs: bool
-    _client: modal.client._Client
-    _GENERATOR_STOP_SENTINEL: typing.ClassVar[Sentinel]
-    _singleton: typing.ClassVar[typing.Union[_ContainerIOManager, None]]
-
-    def _init(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client._Client):
+    def run(self, coro):
         ...
 
-    @staticmethod
-    def __new__(cls, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client._Client) -> _ContainerIOManager:
+
+class _FunctionIOManager:
+    def __init__(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client._Client):
         ...
 
-    @classmethod
-    def _reset_singleton(cls):
+    async def initialize_app(self) -> modal.app._ContainerApp:
         ...
 
     async def _run_heartbeat_loop(self):
         ...
 
     async def _heartbeat_handle_cancellations(self) -> bool:
         ...
 
-    def heartbeats(self) -> typing.AsyncContextManager[None]:
+    def heartbeats(self):
         ...
 
     def stop_heartbeat(self):
         ...
 
-    async def get_app_objects(self) -> modal.running_app.RunningApp:
-        ...
-
     async def get_serialized_function(self) -> typing.Tuple[typing.Union[typing.Any, None], typing.Callable]:
         ...
 
     def serialize(self, obj: typing.Any) -> bytes:
         ...
 
     def deserialize(self, data: bytes) -> typing.Any:
@@ -112,18 +91,18 @@
 
     def serialize_exception(self, exc: BaseException) -> typing.Union[bytes, None]:
         ...
 
     def serialize_traceback(self, exc: BaseException) -> typing.Tuple[typing.Union[bytes, None], typing.Union[bytes, None]]:
         ...
 
-    def handle_user_exception(self) -> typing.AsyncContextManager[None]:
+    def handle_user_exception(self) -> typing.AsyncGenerator[None, None]:
         ...
 
-    def handle_input_exception(self, input_id, started_at: float) -> typing.AsyncContextManager[None]:
+    def handle_input_exception(self, input_id, started_at: float) -> typing.AsyncGenerator[None, None]:
         ...
 
     async def complete_call(self, started_at):
         ...
 
     async def push_output(self, input_id, started_at: float, data: typing.Any, data_format: int) -> None:
         ...
@@ -133,53 +112,27 @@
 
     async def checkpoint(self) -> None:
         ...
 
     async def volume_commit(self, volume_ids: typing.List[str]) -> None:
         ...
 
-    async def interact(self):
-        ...
 
-    @classmethod
-    def stop_fetching_inputs(cls):
+class FunctionIOManager:
+    def __init__(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client.Client):
         ...
 
+    class __initialize_app_spec(typing_extensions.Protocol):
+        def __call__(self) -> modal.app.ContainerApp:
+            ...
 
-class ContainerIOManager:
-    cancelled_input_ids: typing.Set[str]
-    task_id: str
-    function_id: str
-    app_id: str
-    function_def: modal_proto.api_pb2.Function
-    checkpoint_id: typing.Union[str, None]
-    calls_completed: int
-    total_user_time: float
-    current_input_id: typing.Union[str, None]
-    current_input_started_at: typing.Union[float, None]
-    _input_concurrency: typing.Union[int, None]
-    _semaphore: typing.Union[asyncio.locks.Semaphore, None]
-    _environment_name: str
-    _waiting_for_checkpoint: bool
-    _heartbeat_loop: typing.Union[_asyncio.Task, None]
-    _is_interactivity_enabled: bool
-    _fetching_inputs: bool
-    _client: modal.client.Client
-    _GENERATOR_STOP_SENTINEL: typing.ClassVar[Sentinel]
-    _singleton: typing.ClassVar[typing.Union[ContainerIOManager, None]]
-
-    def __init__(self, /, *args, **kwargs):
-        ...
-
-    def _init(self, container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client.Client):
-        ...
+        async def aio(self, *args, **kwargs) -> modal.app.ContainerApp:
+            ...
 
-    @classmethod
-    def _reset_singleton(cls):
-        ...
+    initialize_app: __initialize_app_spec
 
     class ___run_heartbeat_loop_spec(typing_extensions.Protocol):
         def __call__(self):
             ...
 
         async def aio(self, *args, **kwargs):
             ...
@@ -191,35 +144,20 @@
             ...
 
         async def aio(self, *args, **kwargs) -> bool:
             ...
 
     _heartbeat_handle_cancellations: ___heartbeat_handle_cancellations_spec
 
-    class __heartbeats_spec(typing_extensions.Protocol):
-        def __call__(self) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
-            ...
-
-        def aio(self) -> typing.AsyncContextManager[None]:
-            ...
-
-    heartbeats: __heartbeats_spec
+    def heartbeats(self):
+        ...
 
     def stop_heartbeat(self):
         ...
 
-    class __get_app_objects_spec(typing_extensions.Protocol):
-        def __call__(self) -> modal.running_app.RunningApp:
-            ...
-
-        async def aio(self, *args, **kwargs) -> modal.running_app.RunningApp:
-            ...
-
-    get_app_objects: __get_app_objects_spec
-
     class __get_serialized_function_spec(typing_extensions.Protocol):
         def __call__(self) -> typing.Tuple[typing.Union[typing.Any, None], typing.Callable]:
             ...
 
         async def aio(self, *args, **kwargs) -> typing.Tuple[typing.Union[typing.Any, None], typing.Callable]:
             ...
 
@@ -327,27 +265,27 @@
     def serialize_exception(self, exc: BaseException) -> typing.Union[bytes, None]:
         ...
 
     def serialize_traceback(self, exc: BaseException) -> typing.Tuple[typing.Union[bytes, None], typing.Union[bytes, None]]:
         ...
 
     class __handle_user_exception_spec(typing_extensions.Protocol):
-        def __call__(self) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
+        def __call__(self) -> typing.Generator[None, None, None]:
             ...
 
-        def aio(self) -> typing.AsyncContextManager[None]:
+        def aio(self) -> typing.AsyncGenerator[None, None]:
             ...
 
     handle_user_exception: __handle_user_exception_spec
 
     class __handle_input_exception_spec(typing_extensions.Protocol):
-        def __call__(self, input_id, started_at: float) -> synchronicity.combined_types.AsyncAndBlockingContextManager[None]:
+        def __call__(self, input_id, started_at: float) -> typing.Generator[None, None, None]:
             ...
 
-        def aio(self, input_id, started_at: float) -> typing.AsyncContextManager[None]:
+        def aio(self, input_id, started_at: float) -> typing.AsyncGenerator[None, None]:
             ...
 
     handle_input_exception: __handle_input_exception_spec
 
     class __complete_call_spec(typing_extensions.Protocol):
         def __call__(self, started_at):
             ...
@@ -389,42 +327,52 @@
             ...
 
         async def aio(self, *args, **kwargs) -> None:
             ...
 
     volume_commit: __volume_commit_spec
 
-    class __interact_spec(typing_extensions.Protocol):
-        def __call__(self):
-            ...
 
-        async def aio(self, *args, **kwargs):
-            ...
+def call_function_sync(function_io_manager, imp_fun: ImportedFunction):
+    ...
+
 
-    interact: __interact_spec
+async def call_function_async(function_io_manager, imp_fun: ImportedFunction):
+    ...
+
+
+class ImportedFunction:
+    obj: typing.Any
+    fun: typing.Callable
+    stub: typing.Union[modal.stub._Stub, None]
+    is_async: bool
+    is_generator: bool
+    data_format: int
+    input_concurrency: int
+    is_auto_snapshot: bool
+    function: modal.functions._Function
 
-    @classmethod
-    def stop_fetching_inputs(cls):
+    def __init__(self, obj: typing.Any, fun: typing.Callable, stub: typing.Union[modal.stub._Stub, None], is_async: bool, is_generator: bool, data_format: int, input_concurrency: int, is_auto_snapshot: bool, function: modal.functions._Function) -> None:
         ...
 
+    def __repr__(self):
+        ...
 
-def is_local() -> bool:
-    ...
+    def __eq__(self, other):
+        ...
 
 
-async def _interact() -> None:
+def import_function(function_def: modal_proto.api_pb2.Function, ser_cls, ser_fun, ser_params: typing.Union[bytes, None], function_io_manager, client: modal.client.Client) -> ImportedFunction:
     ...
 
 
-class __interact_spec(typing_extensions.Protocol):
-    def __call__(self) -> None:
-        ...
+def call_lifecycle_functions(event_loop: UserCodeEventLoop, function_io_manager, funcs: collections.abc.Iterable[typing.Callable]) -> None:
+    ...
 
-    async def aio(self, *args, **kwargs) -> None:
-        ...
 
-interact: __interact_spec
+def main(container_args: modal_proto.api_pb2.ContainerArguments, client: modal.client.Client):
+    ...
 
 
-MAX_OUTPUT_BATCH_SIZE: int
+MAX_OUTPUT_BATCH_SIZE: 'int'
 
-RTT_S: float
+RTT_S: 'float'
```

## Comparing `modal-0.62.87.dist-info/LICENSE` & `modal-0.62.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `modal-0.62.87.dist-info/METADATA` & `modal-0.62.9.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: modal
-Version: 0.62.87
+Version: 0.62.9
 Summary: Python client library for Modal
 Author: Modal Labs
 Author-email: support@modal.com
 Project-URL: Homepage, https://modal.com
 Keywords: modal,client,cloud,serverless,infrastructure
 Classifier: Topic :: System :: Distributed Computing
 Classifier: Operating System :: OS Independent
@@ -15,17 +15,17 @@
 License-File: LICENSE
 Requires-Dist: aiohttp
 Requires-Dist: aiostream (~=0.5.2)
 Requires-Dist: certifi
 Requires-Dist: click (>=8.1.0)
 Requires-Dist: fastapi
 Requires-Dist: grpclib (==0.4.7)
-Requires-Dist: protobuf (!=4.24.0,<6.0,>=3.19)
+Requires-Dist: protobuf (!=4.24.0,<5.0,>=3.19)
 Requires-Dist: rich (>=12.0.0)
-Requires-Dist: synchronicity (~=0.6.6)
+Requires-Dist: synchronicity (~=0.6.3)
 Requires-Dist: toml
 Requires-Dist: typer (~=0.9.0)
 Requires-Dist: types-certifi
 Requires-Dist: types-toml
 Requires-Dist: watchfiles
 Requires-Dist: typing-extensions (~=4.6)
```

## Comparing `modal-0.62.87.dist-info/RECORD` & `modal-0.62.9.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,203 +1,197 @@
-modal/__init__.py,sha256=6W6XMQXIVGGIZVUXHk5xrk1vrllk6OndXMs_SC-zjX8,2100
+modal/__init__.py,sha256=IM8hIP9NlXodRITGUlnQDpnyGuOmmJEo7M6Ci7FVQY8,2102
 modal/__main__.py,sha256=EKalcwy_6N0L5iqIvlYpXihi3zxy9HNuUMvnbraoTrk,1141
 modal/_asgi.py,sha256=CEiJv4CBgJ30UtZ6KTlisNSwRJswV_ccDTMUDUn-g_8,15476
-modal/_container_entrypoint.py,sha256=axJu_riLjkcjwgYauBLq9PpcirbwIIVQlCcBuSp_sTQ,28448
+modal/_container_entrypoint.py,sha256=YilbMiHSVMlZd7HjuqRGgKFWLO9GjqqqCl0rt5L-CvI,52214
+modal/_container_entrypoint.pyi,sha256=pYKqWTac9q1VPQT6D62YOok6mwMIvgbGWv5wx5MYGek,11148
 modal/_container_exec.py,sha256=FiaSBUD0UkQRF8hW0AFf0rbqC8lb87f3EGeg7Mr5dx4,4349
-modal/_container_io_manager.py,sha256=eISAzNK-3mU3z_fnW_k9aJo0GOLdwS9LSoHuwW-8G30,28709
-modal/_container_io_manager.pyi,sha256=BALNBQ4JKjOC8SSkyGV1SM6sjPIqGXWGJAHLLhH9cU4,12668
 modal/_ipython.py,sha256=HF_DYy0e0qM9WnGDmTY30s1RxzGya9GeORCauCEpRaE,450
 modal/_location.py,sha256=_SdCPzVOl7HRwLWIxzTJjhdpOo1sIl4flfEJ-bAbiDE,929
 modal/_output.py,sha256=1GXKp-Qr-C5oe0e4A5epTgcIFhcp5Cko3HG_ircLFWg,20565
 modal/_proxy_tunnel.py,sha256=gnKyCfmVB7x2d1A6c-JDysNIP3kEFxmXzhcXhPrzPn0,1906
 modal/_pty.py,sha256=GhzrHKZpoI-YHMDN7LoySlSYLpoJ4yGPSF-fqiNsFrM,1336
-modal/_resolver.py,sha256=FAkA_3_VK74MEiNOmYH9s2mThD38Z7QNWhNNY3r7qcI,7077
-modal/_resources.py,sha256=4pOUFIgF2nwiKm744Q49fU4ZBZABywOAScttxUhGAgs,1142
+modal/_resolver.py,sha256=_Wi3MbgOCnYfFv9ytHdanpZQvm51oqWdoNEKRsTNgZc,7208
 modal/_sandbox_shell.py,sha256=OtkeMqOymFVqDBWt3kmT6_4mslPu0ZCfjqYjwUb6cGE,1658
-modal/_serialization.py,sha256=evTjYWEqh3A7dGnvrFf8sPRyYgn53rBUPHL7QJLyw2o,12133
+modal/_serialization.py,sha256=nC2-Wz4tyg1va_4l2SutdiLaW0glmONxxnf4er0E4eY,12329
 modal/_traceback.py,sha256=l6y-flU7rQK3YelSthHv4Paurw3rA5q9KofBRRVHCVM,10029
 modal/_tunnel.py,sha256=acWThUm-ct-Ry_nvytxEhCH-djdttGC_qbDdz2y4ddM,5073
 modal/_tunnel.pyi,sha256=MhcFhkpt-Fo9dA59qhXtoNA9ktyJzwtmGj0vWaUdFiM,1337
 modal/_watcher.py,sha256=RWkOgkg3FXpa1RgNr86ZrdrMpD9RB6I1_uuUMy7_yjY,3609
-modal/app.py,sha256=lHMTFiEK76QGGfKtWcTB0BbYiwvcvsuLEaZTv2pzYRs,35268
-modal/app.pyi,sha256=vgD1uIUoU73eLuNt37ZIWJ-x9Xawwc1xrFqfcVC5QSY,18642
-modal/app_utils.py,sha256=EuRot0Xhrq5m7bQgM8yVZkRxiKfnBMJSFguu-lsJ5Vs,748
-modal/app_utils.pyi,sha256=9u0YQMoIt158Uh-HxUGrnuBeMFdBLdi_DLHHZNSGU-M,613
+modal/app.py,sha256=FYFIFoKPBt8vCRfNNEUmTbtJIOD637jkzjXpvBS7k9k,15555
+modal/app.pyi,sha256=PkwSmPsDnXXD0ooZtVdynC5xXI95rUhEgbtBIJy0lxk,8360
 modal/call_graph.py,sha256=KhqmbJPlcpL-PO0N_lrK8VHoXK_xAHq5IhZ9A9rW9Zk,2524
-modal/client.py,sha256=ORQojL4QndWBnCB8MjNsv8OAPbKVp4X1od9ESHRW1-g,10828
-modal/client.pyi,sha256=paxFVJMCBCTHgEdONI8ZWU5NMg3tl1IiRlfwoLc1mfw,4006
-modal/cloud_bucket_mount.py,sha256=H7tbC4tH22mOm3t41Zz5KCtKBmKir10xYOftR0u7o0Y,5550
-modal/cloud_bucket_mount.pyi,sha256=HQy0m3uXIziIrTbftNY_tja5m0tlYro1Q5vQ2fQq2lM,1217
-modal/cls.py,sha256=qjpzsWBGDQzSwpKCKW1o1mO9mdaaUcz1AFQomj_8Po8,12738
-modal/cls.pyi,sha256=Wv4AqJ96Hc8YOUx75z28HCXpsN8rm6WLcMjJvNtK_Rs,6525
-modal/config.py,sha256=_5PGiL2HsxDAL1THv1-FpZXvaiFIaNnT0xd1vGjBhTI,9980
-modal/dict.py,sha256=fPJaHaAiJQauYBvHIrtgHV_qmTCzY7qkqHebqbN5CWs,10777
+modal/client.py,sha256=BO2i7-tCPtnSBLYmuRm8xSVpYWYc4Z51dNoDrSJLiN0,9751
+modal/cloud_bucket_mount.py,sha256=gti25rJPIfeapokZixcsQ-f19l0gSDQrRAquSRQPd_I,2942
+modal/cloud_bucket_mount.pyi,sha256=8J673RncR2-JqjXnXA65O7PZe8VzfU46INEu6k8flkc,1418
+modal/cls.py,sha256=x-oGCqBZi45-zX1bNIvMsHKmrxOhkZZciwrB6B9g2IA,12799
+modal/cls.pyi,sha256=3cpx8HcGgRImPn6fsaq9GSWKicEc_p__2UK2q5xP_Z8,6482
+modal/config.py,sha256=8FfI-0LTPNa0kSz5lPLjQQBnlc27uOF30M-GRH1tRrM,9934
+modal/dict.py,sha256=hZsP-5rXnJ3CNKudKHYfhZwSjD8GqWlA8fxDf51Prck,10194
 modal/dict.pyi,sha256=6BZ8LU4NXrrA_amPbTf1tgPn0FgH64cr2oJnQ4Zr4i4,5809
 modal/environments.py,sha256=xOsBpVpjyBwA-0PVYAQAV_qUtEMxnYzu3Qpmkcqsbeo,2452
 modal/environments.pyi,sha256=HMSB6AeWCXeQcrsEyNiWwSzou3lD0G6h6R_4-AFU3gg,1439
-modal/exception.py,sha256=TtBYDwV3_JfAVGBIKcmaeyV28CFbsVsRwtVhVdd8BuE,5868
-modal/experimental.py,sha256=-nXs5bQoFlQHsJWf6YfIO41BrMoYJuVKr1Pil-EUOOU,316
-modal/functions.py,sha256=TWAxJU8Ov8Rb3_Q6lfiPIo0jobSABfXWJnQ_KfFu63M,67936
-modal/functions.pyi,sha256=aKGi_l3y9YX9kRuVjxW9QBjdzEl6dq4n45THNhUVSls,23960
+modal/exception.py,sha256=h-LDmGhdwViC_yxBulNQHJpwhuKnkh40fAbwX-Y4D0c,5865
+modal/experimental.py,sha256=R-3pEE_X_NB891MIaUxWMkqlW7FMjpmqp2IrI_Tds10,293
+modal/functions.py,sha256=vG8qmvdl0k1D-BdNmaEVZYsyIs6cUCBqOnQ2Ns1yxKc,63450
+modal/functions.pyi,sha256=biuQhH788mNh_0tGL3JhM_hkywFme6IuYhrW7tvYhk8,19548
 modal/gpu.py,sha256=QDfghe3SgOecMXAc8DRbUuGKhhaLeuG8pVEpO-rQvA0,8038
-modal/image.py,sha256=a1_Ps__TG6vCQ5y32wy7Kfjzske_Ju8Xmmq1-rlkWy4,68298
-modal/image.pyi,sha256=hA0O-YEJKOyyUo3ie9AaNpCgnkjUBXhTIfx0xcfSlMo,18339
-modal/mount.py,sha256=Kgu6NkWsSVfnEGEgpSjopxanrkx_qfAV59JnWGGpm3I,23177
+modal/image.py,sha256=JWYROEpW1wuX7SO752yXkmTbBw12P4MV2YZHkPPF1jI,62658
+modal/image.pyi,sha256=0oF1q7sTr5ce2KZxNz5eVNXlP48P8fe8HmUUNsYKBmQ,17457
+modal/mount.py,sha256=FTsWO_IgcJXlq1tCIWNMH2CuFH-GJT5Tzrz3-zN7cj8,23182
 modal/mount.pyi,sha256=xhLXRhKkMmlBiSiNNtA03_4S8m1NzUW6TUhM2FpME2k,9588
-modal/network_file_system.py,sha256=yt_GNgUln7PCXUS4fNo9WPS6EnAp5446gIF3SeV3BUQ,14380
-modal/network_file_system.pyi,sha256=TI04zO5AUG58HwPIGCD0aB3G53DjUx-XdZy3EkbxvDo,6280
-modal/object.py,sha256=Ja5F9kJlATJvJM7WGBG26cQwW--DICSq8N0aSJL2oTA,8329
+modal/network_file_system.py,sha256=iOj897r3SXbv4fnWiGjusYAyS8VQoIpjRGboFQ9Psds,14356
+modal/network_file_system.pyi,sha256=2Bp38k_zZsN8UhhsJ8dF3GpKEpXqhcsVNSLG1hIaxJw,6404
+modal/object.py,sha256=T5ORM_18kMSrVMy2wWsfefXzZbgaGdGdTFOl2c4LSgc,8258
 modal/object.pyi,sha256=NVYTuTldnB5j8724Qx-dd0R8tWeuXBfh9IGSBMaw2aw,7573
 modal/partial_function.py,sha256=n4qGXD_DdZlWJNNzjwXoh2ovX3Oxl4XhrfnmzAPBdw4,19988
-modal/partial_function.pyi,sha256=1nfByULFi2rNIiCaFQR3YTv-FwD76aHpN_7iiO1o30E,6172
+modal/partial_function.pyi,sha256=U-hSV_-KXRoaygOMGd6G3kBaMMbwrHqXctrbgH9t_WY,6270
 modal/proxy.py,sha256=ey5IlDkPfb0zHGSsIQjgtFsLZwdjCoCp-ecIJlbDo5w,1307
 modal/proxy.pyi,sha256=Db8boRetc0K7sAUuKw-mg4eoRAX0GB7vyHOHlwuCEs8,428
 modal/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-modal/queue.py,sha256=BZVTc4IYUIgQu1v4VzIluyvn7oXLriNWgKVVFyUN-6s,17136
-modal/queue.pyi,sha256=FytGwS0VK2jUffqZb90cAIuQR7-bcykazijgbm4ohe4,7397
+modal/queue.py,sha256=c8JJZc8wjjYofXVkTyRNCIdMGa7ttzN43R_iThWPazw,12296
+modal/queue.pyi,sha256=dLrgajEzVAx74ekC56MuZ3n0qEhCezClOH4-MDRDRSU,5673
+modal/requirements.312.txt,sha256=zWWUVgVQ92GXBKNYYr2-5vn9rlnXcmkqlwlX5u1eTYw,400
+modal/requirements.txt,sha256=OjsbXFkCSdkzzryZP82Q73osr5wxQ6EUzmGcK7twfkA,502
 modal/retries.py,sha256=bRDYRTUNFzmjRAggCcoMViqwu-PNnqbokc_QYCf_2nc,3730
-modal/runner.py,sha256=N2XxMWzZNVFYxBq4GbOLxY-l5bRnUzejrHZeaBOXVBk,19091
-modal/runner.pyi,sha256=JlwrUukT0NGQWVz8KDn-0X3edalbTPKCpqM8uFYErwc,5753
-modal/running_app.py,sha256=MRLSMzdAryNbDTI9ox0KbY4X8WBsCxZnhdYje9PP6Ts,461
-modal/sandbox.py,sha256=TL7t0dFRBJdm0eCn8B1Yp_l2Uh48Ait8B0T1GujdaPM,15040
-modal/sandbox.pyi,sha256=LMiJPsxEfw-w9omPCpWkohZBUONN4AJ1fN_chuDPmKM,6514
+modal/runner.py,sha256=IAO4oyybCTDSQY-eZFY_VKjifSHz5CUCmKZ5a4bub2E,12295
+modal/runner.pyi,sha256=3BxRQ-P-5DRvzz_ufaQiu6hpuGbhaomha7JNdaYFEI8,3114
+modal/sandbox.py,sha256=QzqU08-_iF3W6yKuvzrP11mwvFs8j1zDRhu8EZt22u4,15301
+modal/sandbox.pyi,sha256=MDvAfhxeQckL6CDYo0ccDl_Dxv2MsdY3pLO06MXav8Y,6466
 modal/schedule.py,sha256=GjvGQxXhAf0oPvHlwyWsppanel7LrSBuarWEINB_xTY,2621
 modal/scheduler_placement.py,sha256=15NhfoI1W6qklc8BabM5t19oVNhx9ZbPByaQO7YWSYw,662
-modal/secret.py,sha256=gzCDPPozRH9Yey17f1Zu2yy_vVdVSWy6cZjVZXJ_jcY,8916
+modal/secret.py,sha256=GKyTm8Bo7DAvCyzG8nSr4bIaWyOf-egEp8CDHlgewe0,8898
 modal/secret.pyi,sha256=kh0LL3VkJpZHQc71c-6xE2HcnrgSsw9xOHMDWXMatwk,2225
-modal/serving.py,sha256=YMHrWH1lcoKSgsJqRSBnWMtpgGWvyRDpEYv67NGDKRo,4800
-modal/serving.pyi,sha256=Sr_7eADe4cCg3cZqnKj0HsDgeytsWqh9KJk2XXMtt9c,2977
+modal/serving.py,sha256=h0gPQYbLixn8qP64OLJjAGdcuCbbS2sCZqTn9-0etrA,4681
+modal/serving.pyi,sha256=S5yq9KgrC2ece1gBzPFlwas2XEFTqnfA6YSDf3bjl5U,1957
 modal/shared_volume.py,sha256=vB-QLl7EMeusFWT91bVxsmYhzlBNYYYJukLmbF_5Rgk,888
 modal/shared_volume.pyi,sha256=JSrQyY3M0nn6cbhEV_Xd8AqN7VdzWr2_jsHEGH7HYOQ,405
-modal/token_flow.py,sha256=M28JpcN2KCeIaXmlGqtj6NBPcb8qvjEd59_ojR3tMDo,6742
+modal/stub.py,sha256=heGve7FVwsnnv4Nn8QLgU_4rL2uEbEb4wKPYg9GlGuQ,33852
+modal/stub.pyi,sha256=egq2GR_n9FCtKwmb4FwHcQjb41VvarDmRzzy5E-EKhM,17567
+modal/token_flow.py,sha256=sCsSCtBDuKxJXyNbUPbSW7uc2Q7elUkkfsjIzpFilWw,6771
 modal/token_flow.pyi,sha256=F5_ty7M18ulmLG9I-DJoqcg7CrbEgLDndEWvL_p9708,1890
-modal/volume.py,sha256=KAoNUZUt9AcDdflCuo781V0tqR43_wotAoUEZ9v-j9E,28278
-modal/volume.pyi,sha256=h5NY-9l_FDCn_2-w9xepr60pdPXzt4Dhbr1xDUj_SGU,9748
+modal/volume.py,sha256=YrZ3bAN_YwXaEknXl-f9XSGGWktjQewewFKBJd6ui0c,24410
+modal/volume.pyi,sha256=OpJxb-xQX0gCnnfcX-etFwE1zNkF5O1LqpAbfaI59o4,9111
 modal/_utils/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
 modal/_utils/app_utils.py,sha256=Uf9oNKcC6wJg3wcHNygWxMlkXuIMVRYIE5nutDSafkQ,465
-modal/_utils/async_utils.py,sha256=9w5DHi61IwHA18z1_gHgHyo4U7rgQXg7S9OITNYNJr0,16049
+modal/_utils/async_utils.py,sha256=XqGtqrgtOFsW-NBAIxBgKpGSRXWoBbHPlB-f4LSDN58,12930
 modal/_utils/blob_utils.py,sha256=oHWcx13W9NNXx0Das_x50jvVbA29jpGnLJM7c74WfxQ,15054
-modal/_utils/function_utils.py,sha256=hLFTGZ20k8ZV8Od-VqzTcZapO2NyqU87kxnh4XfTJiw,15339
-modal/_utils/grpc_testing.py,sha256=n_HsO6pkTx2CStvGvLCl2-Rrj25X35wQ8Jln83QUySM,7927
-modal/_utils/grpc_utils.py,sha256=HinIKh1t4GznJwQ_OS1SDWLWa3IiV_uBdePXJSJx_dk,9518
-modal/_utils/hash_utils.py,sha256=HefF7zPQPxFxyx3fpz-AdSm4QsHZNNvgL9-iQHY-_F4,1790
+modal/_utils/function_utils.py,sha256=Orkons1_SFHxfI7NIE3-O-FhQ9wA1CxjC_or4PR_bm8,13520
+modal/_utils/grpc_testing.py,sha256=LLwhZ4XJgnWSxnTEFd1OxgUjJ-z91xrMiWm9i44H54Q,7780
+modal/_utils/grpc_utils.py,sha256=OssGLNWGUpc8PHxd8UvTGvZAalW9BKgJ8phKeZlm_SM,9322
+modal/_utils/hash_utils.py,sha256=YOE1LZwyqaTQSZajrF-H_5_iJkjxMclJewC0SgH_8xA,1597
 modal/_utils/http_utils.py,sha256=DGKvrSQxAHjP_LNdM6EaL-TIQPGDX5vt3gcMGg2xkz4,1426
 modal/_utils/logger.py,sha256=0QvxZpyhhZwKZ5xOcMC9btS_XBE1wAKhtQmISU3gdd0,1311
-modal/_utils/mount_utils.py,sha256=f2q_tlDLVHcttJBkuoGfy99G-krZ-s5vZARi-bYzv-E,2341
+modal/_utils/mount_utils.py,sha256=oAbmYe3RT29EoysJou_NmmmQ5FwxkdWd2qbOK1cZgXA,2327
 modal/_utils/package_utils.py,sha256=mh5zRXmY9wSmvqaCqFBAdPntLP1EmSkDWJt3GB4ey4s,1640
-modal/_utils/rand_pb_testing.py,sha256=_dRz09XhVoY9ZO7SN7xqi0MDVT22sY_ONnS0ts_3sUg,3857
+modal/_utils/rand_pb_testing.py,sha256=KESWmCi8lwmRNllknQKM2Y67bm7XwRIFZUW_X2mOesE,3871
 modal/_utils/shell_utils.py,sha256=_nQIZb4jbwcfjg-qCLgtlXEx2I9TCsmY1G0IQY8fFg4,3633
 modal/_vendor/__init__.py,sha256=MIEP8jhXUeGq_eCjYFcqN5b1bxBM4fdk0VESpjWR0fc,28
 modal/_vendor/a2wsgi_wsgi.py,sha256=2AnQcS5VhLZxod0trVNxnIYGH1SRbzT3dmJ4owVXzxA,22144
 modal/_vendor/cloudpickle.py,sha256=CcpkVlNqP3rtFiPK1Ffpub_i0bc7EThN8kU7nC0WXlc,55225
 modal/_vendor/tblib.py,sha256=g1O7QUDd3sDoLd8YPFltkXkih7r_fyZOjgmGuligv3s,9722
 modal/cli/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
-modal/cli/_download.py,sha256=TMlfUFfo-YS18SqUciBKtMIBaVBj4N6AO24I35fgjSU,2234
-modal/cli/app.py,sha256=KDadUG5ZP9IALjeQ2a4p0L5TOewhVhuL-W13niONko0,3041
+modal/cli/_download.py,sha256=wPhkhENC24x2cZcdkeaKdAacncNXIhIZcrEUPpdhsEU,2601
+modal/cli/app.py,sha256=C-4a7FJY6-KbS-blwqHKfk0rKo2zYbcnm6KMwsS_9J4,3068
 modal/cli/config.py,sha256=cSTH2oy0HTT-8GgH0tMvjqMKk_zYi_UcRGqRXkqCH3w,1294
 modal/cli/container.py,sha256=LC9VnnidDmMF0lr-MclewJAqqByVdC-CQqi6kdlIwsE,1748
 modal/cli/entry_point.py,sha256=aUB_UYJeqbapCBN0bX4SyCagk1PIm94-oPan2PIsAwk,3538
 modal/cli/environment.py,sha256=Jgen64Wu6MUdvX8iDoqE3aJtiCD8PiHVjd3dfDOjT_U,3414
-modal/cli/import_refs.py,sha256=LJxySja_NrCUEA59sBmSszv6_Lx43oNqjn0nuqjy_us,9529
-modal/cli/launch.py,sha256=Y1qiy_Q-25puFt4bmlfnD4XorBNxiKsevq6Yqq2Z7CY,2136
-modal/cli/network_file_system.py,sha256=5aNRGbHFZEVWmuLvEUkVyttoHch2XgBl4wv7OwX-kSQ,8256
+modal/cli/import_refs.py,sha256=OEVxtlarpItVu9ifi8V_Lt3Fmr3MKrufnBz0pKH684c,9083
+modal/cli/launch.py,sha256=qunqsZ26u60tY5fLAz1hneWGXfkJHBIML_qnUf5u2UY,1702
+modal/cli/network_file_system.py,sha256=wQDlACewu7ucYRAKvgW2r1SlhTJnXYhsT1tvEVbDf6I,8449
 modal/cli/profile.py,sha256=s4jCYHwriOorEFCKxeGZoSWX8rXTR_hDTNFZhOA565s,3109
-modal/cli/run.py,sha256=c83V78I79HBU86tWeY_RqhsGZpRw6OoqyFcSj-uRTac,13769
+modal/cli/run.py,sha256=3Hbztt4nw5PrJdNpextY5jg-m9rON-G4BTZOWc2p0iE,13798
 modal/cli/secret.py,sha256=oylN52070uWS4NLT8bbHmt93KWV1DBmmuAWds677amE,4181
 modal/cli/token.py,sha256=Vsxy1ViEfuauWgPnpV8R3mRtKFQUtNmmTc2FYOY82Tg,1875
 modal/cli/utils.py,sha256=GUG0Vb_hYv6yEGm2Q9fM84PnRniFo2W2FygN7lyWm8c,1330
-modal/cli/volume.py,sha256=6X_RdLsttT_LqP3nkSrKiwBRwPrnHeRGVYIKSFRBJQI,10775
+modal/cli/volume.py,sha256=W8dMgYMJ3x7VDRpWJf1bI5lEnn4kNgWyQrVnxiRSMOQ,10871
 modal/cli/programs/__init__.py,sha256=svYKtV8HDwDCN86zbdWqyq5T8sMdGDj0PVlzc2tIxDM,28
-modal/cli/programs/run_jupyter.py,sha256=UqUIE6PY6RfcdsRgwTRfAq3sLGDZhnwPwK5sxTUdol8,2143
-modal/cli/programs/vscode.py,sha256=EAmPQ3Q9eKojrvdIvawudNZevJt3IjcdfPTrfIVvSdE,1898
+modal/cli/programs/run_jupyter.py,sha256=FJ6tmscw2CNaEZeD-14VUDZPMvUuF3T13emGPHpm5bk,2010
+modal/cli/programs/vscode.py,sha256=sh7iGUqOHOplsow9-2iopHT0jsiPW3ie1CwZAq3tqOM,1765
 modal/extensions/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
 modal/extensions/ipython.py,sha256=Q66tGs_PWzuL1M6zZ8yNplv25qHog_aXyy8NED0csh0,988
-modal/requirements/2023.12.312.txt,sha256=zWWUVgVQ92GXBKNYYr2-5vn9rlnXcmkqlwlX5u1eTYw,400
-modal/requirements/2023.12.txt,sha256=OjsbXFkCSdkzzryZP82Q73osr5wxQ6EUzmGcK7twfkA,502
-modal/requirements/2024.04.txt,sha256=ahcvUgDTsw7slMDPneAX6rZdeIc6fsW00ZzODccAC4A,520
 modal_docs/__init__.py,sha256=svYKtV8HDwDCN86zbdWqyq5T8sMdGDj0PVlzc2tIxDM,28
 modal_docs/gen_cli_docs.py,sha256=c1yfBS_x--gL5bs0N4ihMwqwX8l3IBWSkBAKNNIi6bQ,3801
 modal_docs/gen_reference_docs.py,sha256=g37KpE5P5ZxZl-vj0ie3Ewx1waTRMKCGIXQ_iYSy0e0,6555
 modal_docs/mdmd/__init__.py,sha256=svYKtV8HDwDCN86zbdWqyq5T8sMdGDj0PVlzc2tIxDM,28
 modal_docs/mdmd/mdmd.py,sha256=F9J0KdYVz8WmdLDnInTIlm8SmNZJLAiu3ZAZcxVIZ0k,6268
-modal_docs/mdmd/signatures.py,sha256=Jqy5AosHsQLAQJJe5cgYbciyFvb8xVwPIYwyBn-6RzU,3243
+modal_docs/mdmd/signatures.py,sha256=cTum8S5ydixC_HaniltEoTwadtgQU9IJrE4FwF9XzQg,3095
 modal_global_objects/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
 modal_global_objects/images/__init__.py,sha256=MIEP8jhXUeGq_eCjYFcqN5b1bxBM4fdk0VESpjWR0fc,28
 modal_global_objects/images/conda.py,sha256=5Tqd_62p7zGwVezJj1Qp2Vcxtt2WHWVBMzMNbjuNW-M,324
 modal_global_objects/images/debian_slim.py,sha256=9iB8L0tuEDWas3Mge1jyGvfBXiKsJpcjqJT5M1fWRos,330
 modal_global_objects/images/micromamba.py,sha256=k5m5P4xSBzliiTrdMITHx6iLhdQGIBuhPnP65JlBdG0,329
 modal_global_objects/mounts/__init__.py,sha256=MIEP8jhXUeGq_eCjYFcqN5b1bxBM4fdk0VESpjWR0fc,28
 modal_global_objects/mounts/modal_client_package.py,sha256=W0E_yShsRojPzWm6LtIQqNVolapdnrZkm2hVEQuZK_4,767
 modal_global_objects/mounts/python_standalone.py,sha256=_vTEX3PECUsatzhDs8lyJmDK0LbFetT1sJB6MIDfFAo,1870
 modal_proto/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
-modal_proto/api.proto,sha256=r5yfVhNpSCveGAVOkzvJaaP1LrjS4mh3taQnbXFnbQw,56704
-modal_proto/api_grpc.py,sha256=-YaCdYIMUPYMArNyLGOs2ie3FS0MblcePF2EwhZOIS0,85985
-modal_proto/api_pb2.py,sha256=6EZ2ky0RTzWXTdwshReo6IpWbA0YVqnISdabAJY2G6I,219214
-modal_proto/api_pb2_grpc.py,sha256=DIYSL60RpXh7DW0fx4kBAGyaMEYCTFgrSwuSXqdh3Nw,185802
+modal_proto/api.proto,sha256=C12wjpZgioFcFVnRDbirdkyVsvs9Wo-b20M9utOV8NY,56021
+modal_proto/api_grpc.py,sha256=3eorhXe6eSERza1QPwaGDFui5ClzVoGkO-JPL2c_tEU,85328
+modal_proto/api_pb2.py,sha256=RaCwh-9sW254itYxe0JdclclWsxkk1VuVafHpTt-Z1I,217994
+modal_proto/api_pb2_grpc.py,sha256=__5-RlGE2VhrKxgvhbIcmfX5EwNee9OKYuy1MgOSlts,184288
 modal_proto/options.proto,sha256=a-siq4swVbZPfaFRXAipRZzGP2bq8OsdUvjlyzAeodQ,488
 modal_proto/options_grpc.py,sha256=M18X3d-8F_cNYSVM3I25dUTO5rZ0rd-vCCfynfh13Nc,125
 modal_proto/options_pb2.py,sha256=OC2Oob8Yz_3Gs58hwpS_jSFWpGsWMcxlgXbJCyw3gMk,1827
 modal_proto/options_pb2_grpc.py,sha256=1oboBPFxaTEXt9Aw7EAj8gXHDCNMhZD2VXqocC9l_gk,159
 modal_version/__init__.py,sha256=HTM4O90gT-ndgKJIFiD8JRMsdauRAWxc9TDGhBuhkYI,470
 modal_version/__main__.py,sha256=2FO0yYQQwDTh6udt1h-cBnGd1c4ZyHnHSI4BksxzVac,105
-modal_version/_version_generated.py,sha256=J637MP_oxW1YKieMZ2bK93TyPyRPjdeZaFbJ15xoDLA,149
+modal_version/_version_generated.py,sha256=sEzCzfiHo79C_7v_AspiYbHiJ5mN549B2PJO7byP7Go,148
 test/__init__.py,sha256=waLjl5c6IPDhSsdWAm9Bji4e2PVxamYABKAze6CHVXY,28
-test/aio_test.py,sha256=u8nTraWcjAxPU7mV5w3m5hI2Xi9U1rXEEYyBnXTIEVs,199
-test/async_utils_test.py,sha256=T4HJlfMNe1cL5cEzFxv2e4rJoaJkqygvZYSgS75FAo4,7247
+test/aio_test.py,sha256=YOmAEt1WaBh0VxlmCa7gzjM-VEvIQNQ6By959y2WwSk,203
+test/async_utils_test.py,sha256=1NGazu9XBc1YJYhKPB2vt49dJyDheisA1YDl3iYVlGc,6792
 test/blob_test.py,sha256=72LpqPJreb0mtLlkS14IN6rsvsQUWr1yrM97uTANUQo,2539
-test/cli_imports_test.py,sha256=qngK4HaXnOfq8IBgYKjkDa7jpZ1qrxRlUkt6Pkp5QjM,4665
-test/cli_test.py,sha256=mF6Ij8SGzBjMMIDaNsvSYeBC6hHdwKLVB7aUQXWcRSU,27017
+test/cli_imports_test.py,sha256=hayUSViVXhY0Vxl-WJ1b3dapx8TGi76I8ql-Snm5bJI,4680
+test/cli_test.py,sha256=lqbjyOBMH-u_ixx646LUnpqrrDBPBKbGVW2lQn3x5to,26607
 test/client_test.py,sha256=E9lfLgW9EFnA_stbw0vX8vUA7djIiO5fW-x-JR7tKeM,6426
-test/cloud_bucket_mount_test.py,sha256=5BwaEZbiWaDQ-9rzifhB7Uq6vzaqgTI6sL17SnAZqHA,530
-test/cls_test.py,sha256=pYwU78XRpXCgCt4ag3FINjvClmLObHy1ezm5NbiBWAA,16131
-test/config_test.py,sha256=oQgLnKLMZYTLxz_LNvE_Re99gYrAjR0DcfivI-U1btc,5121
-test/conftest.py,sha256=46o0GkzaF4YF0Csp3FkhbfIr_Q1uIJy6fVnOf705tls,61518
-test/container_app_test.py,sha256=MDFTc_J9R1i4nXWD-k2Rk9SRiyGhxaBLFB6SwbG2GGs,1506
-test/container_test.py,sha256=cOfVv7vsCs4EBVFdGv2X-dFIWv627A2MVmgebg2xbf0,50596
-test/cpu_test.py,sha256=36StHgZlfblNGYxzwRvqL137x1Dk6C7oEb6y6cDWQi0,398
-test/decorator_test.py,sha256=ec0oAO-oBE0fMVehr83Gj7gnRtqL4pz8jN6vwMDYXaw,1850
+test/cls_test.py,sha256=MSRzjUJAngQVHPXglYcwxl-nNwGgRlgXfAEWWJfWkEk,15906
+test/config_test.py,sha256=mhacVtiOB5jAwLjoiDy_IUqP7z3n5DKzW6ugNtwmwnQ,4768
+test/conftest.py,sha256=dZL3RbDkgQyY1TlTLI_jRWIe0pZCCp8g7kay5DoC9w0,59766
+test/container_app_test.py,sha256=hWP6DZTZx6zJYudOfNq5386-rEvRGXSdU7XImaUtKYE,895
+test/container_test.py,sha256=-zS-Pf5ao9mwnTJEFBwT1wvyU4IBRxUiGkxXxBEwCEI,50061
+test/cpu_test.py,sha256=CoJhxhHH9ZjZh3wnpKFfPku2Vq6xIZJoB6j7O-v2278,405
+test/decorator_test.py,sha256=MS90m-9IiVzNgTMfr_2pVlemR3nwMQKAhuJktckq9Xo,1871
 test/deprecation_test.py,sha256=IS_pracoNgt0vuEdLThukZpBkPBX4LUCQP5sAUMhjiY,1055
-test/dict_test.py,sha256=-i4aZjw-W8N5TRlB2lMbLFIVd9hEt4VDpl14UxAg58g,1286
+test/dict_test.py,sha256=FQyWl6ZZ0ktUExxh8xBEuu1LCKsworW6Io1bAD2z1Ug,851
 test/e2e_test.py,sha256=OBBTB8p422PBD0iCoOfNTQQsIs_sX6_SkT1dNv3DiY4,2152
 test/error_test.py,sha256=bDxKUwE4WJ1qwAfm0Jlzqkb3WetF0YLxplrgjevAv4U,165
-test/function_serialization_test.py,sha256=4SJM8EdH9YXJ1woBla5L2VdnxWqSkOiL_EzDUU85KKY,966
-test/function_test.py,sha256=CgPO7oGt3hXU-iXBD5n-frDkebdCf7-guRpbT0T8lPE,23757
+test/function_serialization_test.py,sha256=1rili8CT3bdOfIMabIAEBn699GTBWqc11xCe-wu5_bY,971
+test/function_test.py,sha256=pWtmXnVHSVWoSd9EUc37oA2q9HIPX2O-g-J-7NY3mYk,19697
 test/function_utils_test.py,sha256=AZr8ZPh2h152epv-kn0rWZXwhM8MbPcaVSsOa5thzhY,1892
-test/gpu_test.py,sha256=lwL0nvQfcMHzS1tld9Pj3ElsujI6GaZTRA09Ik85lWA,4571
+test/gpu_test.py,sha256=i8Zp0NRI6LhXAaPc1hATJJjQLlFtmxZ-RxUEDD6pTls,4611
 test/grpc_utils_test.py,sha256=me9sGIvRO3bmj4M4UmwU3dbHj5IpKCF7aKy6W5JaP3U,5048
-test/helpers.py,sha256=MCCSkKM0Qw7_TjhIUB-5zgcJ7z8M3K0c-n6KLjmqn2M,1709
-test/image_test.py,sha256=NUTBREVJlohusHCpwZBqtiNGiceYwbCRC1cwOvF9P1M,32798
-test/live_reload_test.py,sha256=iP3TdMu4JjkS6rI7oaIPfgNqDBRpkg2v9OQD9iae5mA,2722
-test/lookup_test.py,sha256=uGauFaxUBNJPl3unMgqM8lHhQMWDAGM8W_r_aBotZgQ,2160
+test/helpers.py,sha256=-kxMLEjyQeBj6K0AHLrGrzWGw_ZGj2ST4eJaLMX9Rbo,1443
+test/image_test.py,sha256=DsKT8hiZDnaxkrnzG2YeeLaYfeyz5EgqCpTNnmW79mk,25515
+test/live_reload_test.py,sha256=8bk6MAbrzUe9spuQ_-0zQE1p4fvLsbAOhL5g41qEJt4,2742
+test/lookup_test.py,sha256=jcjGt7lIf6iC_HOg0sQE06yg_KcgmHc2pvOYg956chw,2172
 test/mdmd_test.py,sha256=G3B0986QneKtIrui34hu95h09No9mM06O7mPp2sTm50,5118
-test/mount_test.py,sha256=n7Qar7sDVMl1kTKfN2Je0kXbQ8DWPsAQ5q_v-VbrE3o,5475
-test/mounted_files_test.py,sha256=-6k2ATfdTxZsuepNefYW2_h5vCsGAmMnQDl0utxQj0M,12461
-test/network_file_system_test.py,sha256=EBg_Tqtg0ryZeGANjSvwSeRFy1FGDdP29b6XTxNJGYM,6265
+test/mount_test.py,sha256=-76FWFPCRDJpmG4UXdTWpOwG6YgoF6W94F6MAlFB578,5489
+test/mounted_files_test.py,sha256=Noasj7JaCVjVAyTEZIjq-MJkelnVES9FpbE2Ldhe8lQ,12471
+test/network_file_system_test.py,sha256=GxP0fklqOgcXFiqaPnpvN5ZJ8tlqT6e8vNwV9jB9KQI,6073
 test/notebook_test.py,sha256=IkOFP9Tat8pEazSOV73OU7QMGgOlw3sU_5vTzOigrZU,2157
-test/object_test.py,sha256=jyLxikU4quvVnDhdhIC9-rKRie1WOnB3hjaD-u383BM,1291
+test/object_test.py,sha256=Ncv3B3-Sd78MeHnhmvuyBN1qXRICONW_qckzVndFWc4,1298
 test/package_utils_test.py,sha256=ic3cJqwVbe7p2jOwnnTuGTwP9aDduz9QueqVwcA8r9I,795
-test/queue_test.py,sha256=YZMJePJSsq5pGoRK1Vam8MZhJIggP_6kxxSfHXBSddg,3667
-test/resolver_test.py,sha256=YlwYPYZO6ZNBH9lQ8ynABwMUfvIs0Mg1h3lKz3PsmAs,1782
-test/retries_test.py,sha256=4x43pJc0Xt6VoPbk9ScnO2Xo_qFVX8QdtuoemO-prLk,1806
-test/runner_test.py,sha256=DG0WVFKidX0KGIIOxn3AEiYRbNDge7wiu7lOISJytt0,2909
-test/sandbox_test.py,sha256=SRsZbZdZjaQwv7VfsUu2MjT7P23m4YmaVMNlvDm77mA,5254
-test/schedule_test.py,sha256=ZDFaaG7WAA6mx6qwlYPNlcf0LHE96uvGFcxfmrnqi78,354
-test/scheduler_placement_test.py,sha256=nd0OizTcsfMiTA6F8KholcZfUlD8Np-QjCReaT2P8us,737
-test/secret_test.py,sha256=QrS6hudMghVU61Bg5N2C9aqvGl85jzIzzRkin2JAJ8E,2506
+test/queue_test.py,sha256=5a8RQMH6DBvm2BvUmbSRGy8JykDtmqOguYTlmvmyJcE,3270
+test/resolver_test.py,sha256=pmQeuSodsGd8tZg3Ja6bKB8lgg-HNxx6Q1aGn2BjQ0Q,1751
+test/retries_test.py,sha256=05W440EOUJ-aSNGyu46SgcwCCS-Gt3NkFnI_8m5aRCo,1817
+test/runner_test.py,sha256=0tke6A5B7AqDpRkCbcPL2sU4O2wuOY9Y1tsQlodVEcU,2582
+test/sandbox_test.py,sha256=U2qFwmHmm4rnVDZOU_MP2irqbSx5dtf_pEIXjBdVODQ,5282
+test/schedule_test.py,sha256=lyj6pHchOg_CqyJNQY8NbRUTfh-RxwAugcX5-P4b-0k,359
+test/scheduler_placement_test.py,sha256=efdPOIy0MxrrioSzzEBTufM-ea5DXUbYiJ-UTdbj-fE,742
+test/secret_test.py,sha256=NGa3HrHvu97XlyOquf-ZanqV6s2gclC9lZarkYSu7O8,2527
 test/serialization_test.py,sha256=d_r5xDn85WkAK3ovMiNGtSxHC90_WM2kS2GSzhRjAnU,1867
-test/stub_composition_test.py,sha256=yMW7P0JqqwvKP_cP6kj7GTRiJl3dUjOd-nSED9mQItk,555
-test/stub_test.py,sha256=LDYpGaCNmeCY73bZcHH_RQDD-ElRCG3Om0IS2cpczUI,10547
+test/stub_composition_test.py,sha256=8JncwauejwbPM4ttm_IBhp6bQwabVdY7_hf6P_XCV44,558
+test/stub_test.py,sha256=CzKjxrKCRawOeWroKTq2wEvbhRcuukvgy154LeSMngQ,11284
 test/test_asgi_wrapper.py,sha256=NzhKYIVVQhL9mTGJh8AWBEv_cTZ1uT9W9ngoLufL09M,7320
 test/token_flow_test.py,sha256=tbxqsDFzRiMCkxfexDzxbbEomrmNlqJjOpPm8ndPE_c,614
 test/traceback_test.py,sha256=3FNfnb00ek1gMVj8xcT8veAcyF4j4Fdrz3G9_giljsc,4747
 test/tunnel_test.py,sha256=5v3FGmYS3VAcQB1xMjoXnmU1jLGvKu27TgFkH42N09w,768
 test/utils_test.py,sha256=VRBxxpIAl3TvvY8X_rumvqCR-5ZozV2tfm_1Gy4ok6M,2679
 test/version_test.py,sha256=lii24csueJzXzv0HM0fvHD83N_9FOOFRl-VRmDQZnDA,403
-test/volume_test.py,sha256=auMLjBdHB7Zti1Mo8JXnngBTMwcIbVGya9a-5cHNZbc,14309
+test/volume_test.py,sha256=j7B6tKwvURCHAahaeaI-nFFItKXDihui5xWHeXmzSNw,12461
 test/watcher_test.py,sha256=RdtFZVsdQEdKo7WUy98SK6UJ795u8v--x3nuOTNEghc,1193
-test/webhook_test.py,sha256=FDD-gR0LUP3tTSZKHDq9313CG7lxZ8LL1fakU_P7zj0,4172
-modal-0.62.87.dist-info/LICENSE,sha256=psuoW8kuDP96RQsdhzwOqi6fyWv0ct8CR6Jr7He_P_k,10173
-modal-0.62.87.dist-info/METADATA,sha256=VZOCuuzLsfvAbR8YjwlxW3nqGspvgrdrXnu88OeqD4Q,2302
-modal-0.62.87.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-modal-0.62.87.dist-info/entry_points.txt,sha256=An-wYgeEUnm6xzrAP9_NTSTSciYvvEWsMZILtYrvpAI,46
-modal-0.62.87.dist-info/top_level.txt,sha256=AfUN7pr_AWNHp5av5c2h5ntw_83jrhQ2-ytzdC-S4Z0,69
-modal-0.62.87.dist-info/RECORD,,
+test/webhook_test.py,sha256=wyG3nG93En4-tXyo3GMMX__0GKfKnH0ZpChssyKlTXI,4205
+modal-0.62.9.dist-info/LICENSE,sha256=psuoW8kuDP96RQsdhzwOqi6fyWv0ct8CR6Jr7He_P_k,10173
+modal-0.62.9.dist-info/METADATA,sha256=oRZjAhkdxCJpmzkD5tmsIIMdGT0L93zTJwQ-FE5yjUc,2301
+modal-0.62.9.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+modal-0.62.9.dist-info/entry_points.txt,sha256=An-wYgeEUnm6xzrAP9_NTSTSciYvvEWsMZILtYrvpAI,46
+modal-0.62.9.dist-info/top_level.txt,sha256=AfUN7pr_AWNHp5av5c2h5ntw_83jrhQ2-ytzdC-S4Z0,69
+modal-0.62.9.dist-info/RECORD,,
```

